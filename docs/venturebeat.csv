url,title,text
https://venturebeat.com/2021/04/27/how-merck-works-with-seeqc-to-cut-through-quantum-computing-hype/,How Merck works with Seeqc to cut through quantum computing hype,"When it comes to grappling with the future of quantum computing, enterprises are scrambling to figure just how seriously they should take this new computing architecture. Many executives are trapped between the anxiety of missing the next wave of innovation and the fear of being played for suckers by people overhyping quantum’s revolutionary potential. That’s why the approach to quantum by pharmaceutical giant Merck offers a clear-eyed roadmap for other enterprises to follow. The company is taking a cautious but informed approach that includes setting up an internal working group and partnering with quantum startup Seeqc to monitor developments while keeping an open mind. According to Philipp Harbach, a theoretical chemist who is head of Merck’s In Silico Research group, a big part of the challenge remains trying to keep expectations of executives reasonable even as startup funding to quantum soars and the hype continues to mount. “We are not evangelists of quantum computers,” Harbach said. “But we are also not skeptics. We are just realistic. If you talk to academics, they tell you there is no commercial value. And if you talk to our management, they tell you in 3 years they want a product out of it. So, there are two worlds colliding that are not very compatible. I think that’s typical for every hype cycle.” Merck’s desire for the dream of quantum computing to become reality is understandable. The fundamental nature of its business — biology and chemistry — means the company has been building molecular or “quantum” level models for more than a century. Part of the role of the In Silico Research group is to develop those models that can solve quantum problems using evolving technologies such as data analytics and AI and applying them to natural sciences to make experimental work less time-consuming. But those models are always limited and imperfect because they are being calculated on non-quantum platforms that can’t fully mimic the complexity of interactions. If someone can build a fully fault-tolerant quantum computer that operates at sufficient scale and cost, Merck could unlock a new generation of efficiencies and scientific breakthroughs. “The quantum computer will be another augmentation to a classical computer,” Harbach said. “It won’t be a replacement, but an augmentation which will tackle some of these problems in a way that we cannot imagine. Hopefully, it will speed them up in a way that the efficacy of the methods we are employing will be boosted.” About 3 years ago, Merck decided it was time to start educating itself about the emerging quantum sector. The company’s venture capital arm, M Ventures, began looking within the company for experts who could help it with due diligence as it began to assess quantum startups. That included mapping out the players and the whole value chain of quantum computing, according to Harbach. That led to the formal creation of the Quantum Computing Task Force, which has roughly 50 members who try to communicate with quantum players large and small as well as peers among Merck’s own competition. “We are basically an interest group trying to understand this topic,” Harbach said. “That’s why we have a quite good overview and understanding on timelines, player possibilities, and applications.” As part of that exploration, M Ventures eventually began investing in quantum-related startups. In April 2020, the venture fund announced a $5 million investment in Seeqc, a New York-based startup that bills itself as the “Digital Quantum Computing” company. “We thought that it might be good to have partners in the hardware part and in the software part,” Harbach said. “Seeqc will partner with us within Merck to really work on problems basically as a hardware partner.” Seeqc is developing a hybrid approach that it believes will make quantum computing useful sooner. The idea is to combine classical computing architectures with quantum computing. It does this through its system-on-a-chip design. This technology was originally developed at Hypres, a semiconductor electronics developer which spun out Seeqc last year. The M Ventures funding for Seeqc followed a previous $6.8 million seed round. Seeqc raised a subsequent round of $22 million last September in a round led by EQT Ventures. According to Seeqc CEO John Levy, the company’s technology allows it to address some of the fundamental challenges facing quantum systems. Despite rapid advancements in recent years, quantum computers remain too unstable to deliver the high-performance computing needed to justify their costs. Part of the reason for that is that qubits, the unit of quantum computing power, need to be kept at near-freezing temperatures to process. Scaling then becomes costly and difficult because a system operating with thousands of qubits would be immensely complex to manage, in part because of the massive heating issue. Levy said Seeqc can address that problem by placing classic microchips over a qubit array to stabilize the environment at cryogenic temperatures while maintaining speed and reducing latency. The company uses a single-flux quantum technology that it has developed and that replaces the microwave pulses being used in other quantum systems. As a result, the company says its platform enables quantum computing at about 1/400 of the cost of current systems in development. “We have taken much of the complexity that you’ve seen in a quantum computer and we’ve removed almost all of that by building a set of chips that we’ve designed,” Levy said. Just as important is a philosophical approach Seeqc is taking. It’s not building a general-purpose quantum computer. Instead, it plans to build application-specific ones that are tailored specifically to the problems a client is trying to solve. Because Seeqc has its own chip foundry, it can customize its chips to the needs of application developers as they create different algorithms, Levy said. In that spirit, Merck’s Quantum Computing Task Force is working closely with Seeqc to create viable quantum computers that can be used by its various businesses. “Their technology is a key technology to scale a quantum computer, which is actually much more important because it will make quantum computers bigger and cheaper,” Harbach said. “And this is, of course, essential for the whole market.” For all this activity, Harbach’s view of quantum’s potential remains sober. He sees nothing on the market that will have any commercial impact, certainly not for Merck. At this point, many of the company’s questions remain academic. “What we are basically interested in is how — or will — the quantum computer hardware ever be scalable to a level that it can tackle problems of realistic size to us,” Harbach said. “And the same question also goes to the software side. Will there ever be algorithms that can basically mimic these problems on a quantum computer efficiently so that they don’t run into noise problems? We are not interested in simulating a molecule right now on a quantum computer. Everything we try to understand is about the timelines: What will be possible and when will it possible.” Harbach has watched the rise in quantum startup funding and various milestone announcements but remains dubious of many of these claims. “They are creating a new market where there’s not even the technology ready for it,” Harbach said. “You have to stay realistic. There’s a lot of money at the moment from governments and VCs. There’s a lot of boost from consultancies because they try to sell the consultancy. And if you talk to experts, it’s the other way around. They tell you not before 15 years.” The questions Merck asks internally are split into 2 fundamental categories: When will there be a quantum computer that can be more efficient at processing its current quantum models? And when will there be a quantum computer that is so powerful that it opens up new problems and new solutions that the company cannot even imagine today? “Quantum will be a thing, definitely,” Harbach said. “The only question is when, and I’m really, really sure it won’t be in the next two years. I wouldn’t even say three years. There will be a quantum winter. Winter is coming.”"
https://venturebeat.com/2021/04/27/amazon-makes-deepracer-software-available-in-open-source/,Amazon releases DeepRacer software in open source,"In November 2018, Amazon launched AWS DeepRacer, a car about the size of a shoebox that runs on AI models trained in a virtual environment with reinforcement learning techniques. DeepRacer has expanded since then, with a women’s league and new miniature race cars. Starting today, Amazon is making the DeepRacer device software available in open source. The pandemic has boosted automation and robotics in the enterprise. The global market for robots is expected to grow at a compound annual growth rate of around 26% to reach just under $210 billion by 2025, according to Statista. Deloitte anticipates that of the almost 1 million robots sold for business use in 2020, just over half were professional service robots, generating more than $16 billion in revenue — 30% more than in 2019. With the release of DeepRacer, developers can change the behavior of their cars, enabling the prototyping of new robotics apps. As Amazon notes, DeepRacer is essentially an Ubuntu-based computer powered by Robot Operating System, open source robotics middleware originated by Willow Garage and Stanford’s Artificial Intelligence Laboratory that provides low-level device control. Six sample applications — Follow the Leader, Mapping, Off Road, RoboCat, DeepBlaster, and DeepDriver — are available to help with brainstorming, including one that creates visualizations of a home or office. As the DeepRacer community creates projects, Amazon says it will add them to the DeepRacer GitHub organization, as well as featuring them in future blogs. “[W]e want to make it easier for developers of all skill levels to prototype new and interesting uses for their car. By making the AWS DeepRacer device software openly available, now anyone with the car and an idea has the ability to make it a reality,” Amazon wrote in a blog post. “Want to block other cars from overtaking it by deploying countermeasures? Want to deploy your own custom algorithm to make the car go faster from point A to B? You just need to dream it and code it.  We can’t wait to see the ideas you come up with, from new racing formats to new uses for the AWS DeepRacer.” Amazon previously partnered with Udacity to offer machine learning courses and a scholarship based around DeepRacer. The ostensible goal was to educate students on the creation, training, and optimization of reinforcement learning models, or models that employ rewards to achieve goals. In a recent analysis, McKinsey pointed out that reinforcement learning can be applied to solve real-life problems beyond autonomous driving, including classification, continuous estimation, and clustering."
https://venturebeat.com/2021/04/27/campfire-raises-8-million-to-advance-ar-vr-for-product-design/,Campfire raises $8 million to advance AR/VR for product design,"Campfire has raised $8 million in funding for its holographic technology that enables both augmented reality and virtual reality for the purpose of enterprise product design. In stealth until now, Campfire has created a holographic collaboration system for professional 3D designers. The hardware and software headset system is based on patents and technology formerly created by Meta, which ran out of money in 2018. It will come out as a subscription in the fall of this year, after years of research and development. The company has fewer than 15 people and said the funding came from OTV, Kli  Capital, Tuesday Capital, and others. “This was designed for a very specific purpose, for designers and engineers who need to share with other stakeholders inside their company and others,” Campfire CEO Jay Wright told VentureBeat in an interview. “So I’m designing something that I need to show to my colleagues. Paramount for them is the visual experience.” Campfire combines proprietary devices and applications built on a foundation of more than 60 patents. It’s based on the lessons of the past few years of headsets, including the failure of Meta. “The problem with today’s devices is that the experience is not good,” Wright said. “The experience suffers from big problems. They have a small field of view and poor image quality. The devices press on our face, cause pain, get warm, and make us uncomfortable. The users also have to learn new interfaces and tools. And integrating with your regular workflow has been really poor.” With Campfire, Wright said the team had a chance to redo hardware and software and rethink it all. The team focused on creating a full system with good visual performance, ease of use, and an integrated workflow. The Campfire headset delivers a real AR experience with a 92-degree diagonal field of view, Wright said. He added that it doesn’t touch your face and it’s comfortable. The Campfire system is being used by industrial design firm Frog Design. Frog is a Campfire development partner, along with a select group of companies. The Campfire system, which consists of three devices and two applications, is available for preview through Campfire’s Pioneer Program. The Campfire headset can provide VR and AR with one device. Designers can visualize physical products with a natural view of the real world or an environment of their choice using a single headset. You wear the headset on your head, but it doesn’t completely obstruct your view. “You retain your peripheral vision. And by retaining your peripheral vision, you feel a lot more comfortable walking around your desk or table,” Wright said. “It goes a long way for people who get VR sickness because they can still maintain the horizon and see what’s around them.” The Campfire Console is a new device that acts like a holographic projector. And the Campfire Pack transforms a mobile phone into an intuitive controller with tools for working with 3D models. It attaches to the back of the phone and eliminates the learning curve of proprietary controllers and gesture interfaces. “The vision for holographic collaboration has been talked about for decades but not realized in products with any measure of success,” Wright said. “By focusing on specific needs for design and engineering, we’ve reimagined the entire stack to deliver an experience that takes a giant step toward the vision — and more importantly enables a giant step in productivity.” Meanwhile, Campfire Scenes enables users to create scenes from existing 3D models for quick reviews or elaborate presentations. “Campfire Scenes is the tool that solves the workflow gap,” Wright said. “It gets your existing 3D files in a way that can be shared easily.” And the Campfire Viewer enables users to work alone or together during video calls, using a Campfire Headset, tablet, or phone. “The Campfire Viewer is what you use to open the Campfire documents,” Wright said. “You plug the headset into the laptop and you’re in.” Frog Design’s teams have used the device to work together virtually while spread out, Frog venture design lead Graeme Waitzkin said in a statement. “I wanted to make sure we had people on board that were part of the process and were the top of their game for design,” Wright said. “Frog Design was at the top of that list.” Waitzkin said his teams jumped at the chance to test the system as a development partner. “It doesn’t stop with devices,” Wright said. “The Campfire Scenes are like a mashup of Google Docs and Powerpoint. It allows you to take more than 40 different CAD and 3D formats and organize them into a series of three-dimensional scenes that can then be shared with Campfire users. And they can open up, and you can view them with the headset.” You could compose something using a normal computer and then be able to see what it looks like through a live preview. Besides Wright, the team includes chief operating officer Roy Ashok and founding adviser Avi Bar-Zeev. Campfire was started as Meta View in December 2018 by venture capital firm OTV to purchase and commercialize IP developed by the Meta Company, including an AR visor design and a patent portfolio. “Two years ago, I took on a new mission with Campfire, and I couldn’t be more excited to tell you about today,” Wright said. “We really got some rock stars that have been behind the scenes for the last couple of years building this. And the space we’re in is holographic collaboration. It’s been called spatial collaboration. You hear it talked about. It’s the promise we’ve seen in science fiction, the promise we’ve also seen for a lot of companies in the AR/VR space. They have presented compelling visuals but haven’t delivered on them.” OTV recruited Wright, a former Qualcomm executive and president of Vuforia at PTC, as CEO in May 2019 to execute his vision for an integrated hardware and software solution designed from the ground up for collaboration. Campfire’s goal is to bring existing 3D workflows into 3D space for knowledge workers, not to replace the phone or computer, but to extend them. “This is about knowledge workers, specifically people that are working with 3D to build things today, like CAD and similar tools,” Wright said. “So it’s not about full immersion that we need for gaming. And it’s not about full immersion for making somebody feel like they’ve got an emergency situation in a cockpit for training. It’s about visualizing products, and doing it in the easiest and most flexible way.” Campfire utilizes a unique headset to provide holographic views of 3D models and data in AR and VR. The holographic views result from a stereo image generated by two separate displays (left and right) that reflect on the inner surface of the headset visor. The displays are driven by a discrete graphics processing unit (GPU) on a PC connected with a USB-C cable. This is a different operating principle than employed by holographic projectors/displays that seek to generate holograms that are visible with the naked eye. Campfire works with more than CAD/3D file formats directly from a desktop PC. The hardware connects to a PC with a discrete GPU, Windows 10, and a Thunderbolt-3 port with a USB-C connector. The Campfire Pack requires a recent iOS or Android phone."
https://venturebeat.com/2021/04/28/cloudcheckr-survey-says-cloud-computing-adoption-is-accelerating/,CloudCheckr survey says cloud computing adoption is accelerating,"Cloud transformation is moving quickly, according to a report released today by cloud visibility management platform CloudCheckr. In a survey of over 300 IT professionals and business stakeholders at companies with more than 500 employees, 57% reported that more than half of their infrastructure is in the cloud and 64% expected they will be fully in the public cloud within five years. The global public cloud computing market is set to exceed $362 billion in 2022, according to Statista. (In 2018 alone, Amazon Web Services earned $26 billion in revenue for parent company Amazon.) IDG reports that the average cloud budget is up from $1.62 million in 2016 to a whopping $2.2 million today. But cloud adoption continues to present challenges for enterprises of any size. A separate Statista survey identified security, managing cloud spend, governance, and lack of resources and expertise as significant barriers to adoption. Indeed, respondents told CloudCheckr that security concerns (44%), compliance and regulations (42%), and lack of application support (41%) remain significant barriers to cloud migration. Ninety-three percent said that their organizations face blockers with budgeting infrastructure cloud costs, and 94% said that they’d experienced unexpected cloud costs. Only 31% reported that they were able to monitor and optimize public cloud costs “effectively,” meanwhile. But on the whole, the professionals surveyed said that their investments in internal cloud strategies and teams were paying off. Those at organizations with a cloud center of excellence, a team tasked with developing a framework for cloud operations, told CloudCheckr that they saw higher benefits than those where cloud expertise wasn’t organized. “While it’s no surprise to anyone how strong cloud adoption is today, this report shows the tremendous growth ahead and how quickly it will happen over the next half decade. Now is the time for IT organizations to define the right strategies to utilize the full potential of the cloud and for cloud service providers to enhance their capabilities to lead their customers through cloud transformations,” CloudCheckr CEO Tim McKinnon said in a press release. “Migrating to the cloud is only the first step. It’s up to organizations to adopt the right technology and form teams — be it internally or externally — to develop and manage cloud strategy, governance, and best practices.” It seems likely that as the pandemic continues, organizations will increasingly rely on remote work — and by extension, cloud services — to help stem the spread of the virus. Gartner projects that this will cause the cloud market to grow 18.4% in 2021, with cloud predicted to make up 14.2% of total global IT spending. “As enterprises increase investments in mobility, collaboration, and other remote working technologies and infrastructure, Gartner expects growth in public cloud to be sustained through 2024,” the firm wrote in a November 2020 study. Cloud providers are reaping the windfall benefits. In its most recent earnings report, Google said that its cloud division brought in $4.047 billion in sales for the first quarter of 2021, up 46% from the year prior. Amazon’s AWS (Amazon Web Services) posted a record $13.5 billion in profits for 2020. And Azure, Microsoft’s cloud business, notched third quarter 2021 revenue growth of 50% year-over-year, beating analyst expectations."
https://venturebeat.com/2021/04/28/atlassians-jira-work-management-encourages-team-collaboration/,Atlassian’s Jira Work Management encourages team collaboration,"At its Team21 conference today, Atlassian unveiled Jira Work Management, a new product built specifically for enterprise teams. The next generation of Jira Core, Jira Work Management is described as a platform that enables marketing, HR, finance, and design employees to connect with their technical counterparts and work together more efficiently. As the pace of change increases, new ways of working are forcing teams to become more agile. Gartner reports that 74% of companies plan to shift some of their employees to remote working permanently. But more than half of remote employees say they feel disconnected from in-office employees, according to an Owl Labs survey, highlighting remote work challenges that must be overcome. With Jira Work Management, which was first announced in March, Atlassian aims to enable business customers to take advantage of the capabilities native to its Jira product family. The architecture that Jira Work Management shares with Jira Software and Jira Service Management lets data flow between projects within organizations. This means a request for a website update can pass through designers using Jira Work Management, as well as developers using Jira Software, for example. “Jira Work Management combines cutting-edge work management capabilities with the power and customizability of Jira to create a new standard for business teams managing projects,” Atlassian product marketing head Chase Wilson said in a blog post. “This is the first Jira built for business teams, by business teams. We crafted this new Jira experience directly with customer design partners of all sizes and industries, as well as internal Atlassian teams of every type — including our own Jira Work Management teams.” Tasks from teams and projects in Jira Work Management can be linked together to reveal dependencies and connections. These links support custom names, allowing employees to create dashboards that report on organization-wide progress. Users can choose from over 35 options, including workload, heat map, filters, and charts, or bring in data with direct integrations. Jira Work Management offers a number of features, including lists, a calendar, and a board that displays tasks from one or more projects. The platform’s timeline feature shows tasks with assignees and statuses, while its forms creator, which is available to licensed Jira Cloud users, lets employees create forms with a drag-and-drop interface. Beyond this, Jira Work Management offers templates of industry-researched workflows and configurations, each with custom issue types, workflows, and fields. Teams see tasks and issues such as “asset” (in design or marketing use cases) and “candidate” (in recruiting use cases) instead of “stories” and “bugs” (software development). And in place of software-specific functionalities like code, backlog, components, and releases, the left navigation menu in the platform highlights views, forms, and a summary tab for insights. Jira Work Management also offers free automation rules and actions within projects. Teams can choose from premade rules from a business automation library or create rules for use cases and departments. All of the features are available for Jira Work Management project on every instance, across each pricing edition, including the free plan, according to Atlassian. The company says that over the next six months, Jira Work Management will gain improvements to all of its views, optimized reporting functionality, approvals for sign-off, and new collaboration features."
https://venturebeat.com/2021/04/28/gartner-says-low-code-rpa-and-ai-driving-growth-in-hyperautomation/,"Gartner says low-code, RPA, and AI driving growth in ‘hyperautomation’","Research firm Gartner estimates the market for hyperautomation-enabling technologies will reach $596 billion in 2022, up nearly 24% from the $481.6 billion in 2020. Gartner is expecting significant growth for technology that enables organizations to rapidly identify, vet, and automate as many processes as possible and says it will become a “condition of survival” for enterprises. Hyperautomation-enabling technologies include robotic process automation (RPA), low-code application platforms (LCAP), AI, and virtual assistants. As organizations look for ways to automate the digitization and structuring of data and content, technologies that automate content ingestion, such as signature verification tools, optical character recognition, document ingestion, conversational AI, and natural language technology (NLT), will be in high demand. For example, these tools could be used to automate the process of digitizing and sorting paper records. Gartner currently anticipates the hyperautomation market reaching $532.4 billion this year. Gartner said process-agnostic tools such as RPA, LCAP, and AI will drive the hyperautomation trend because organizations can use them across multiple use cases. Even though they constitute a small part of the overall market, their impact will be significant, with Gartner projecting 54% growth in these process-agnostic tools. Through 2024, the drive toward hyperautomation will lead organizations to adopt at least three out of the 20 process-agonistic types of software that enable hyperautomation, Gartner said. The demand for low-code tools is already high as skills-strapped IT organizations look for ways to move simple development projects over to business users. Last year, Gartner forecast that three-quarters of large enterprises would use at least four low-code development tools by 2024 and that low-code would make up more than 65% of application development activity. Software automating specific tasks, such as enterprise resource planning (ERP), supply chain management, and customer relationship management (CRM), will also contribute to the market’s growth, Gartner said. Hyperautomation extends the idea of intelligent automation, as it promises end-to-end process automation with minimal human intervention required. The convergence of intelligent process automation technologies and cloud computing, along with the need to process unstructured content, helps make the case for hyperautomation across several industries, including shared services, hospitality, logistics, and real estate. Some day-to-day examples of automation include self-driving cars, self-checkouts at grocery stores, smart home assistants, and appliances. Business use cases include applying data and machine learning to build predictive analytics that react to consumer behavior changes and implementing RPA to streamline operations on a manufacturing floor. Gartner earlier included hyperautomation in its Top 10 Strategic Technology Trends for 2021. Gartner said tools that provide visibility to map business activities, automate and manage content ingestion, orchestrate work across multiple systems, and provide complex rule engines make up the fastest-growing category of hyperautomation-enabling software. Organizations will be able to lower operational costs 30% by 2024 through combining hyperautomation technologies with redesigned operational processes, Garner projected. “Hyperautomation has shifted from an option to a condition of survival,” Gartner VP Fabrizio Biscotti said in a statement. “Organizations will require more IT and business process automation as they are forced to accelerate digital transformation plans in a post-COVID-19, digital-first world.”"
https://venturebeat.com/2021/04/28/saas-provider-boostup-ai-nabs-6m-to-support-revenue-operations/,Saas provider BoostUp.ai nabs $6M to support revenue operations,"BoostUp.ai, provider of a software-as-a-service (SaaS) platform for managing revenue operations (revenue ops) infused with AI capabilities, today announced it has garnered $6 million in additional series A funding, bringing its total raised to $14 million, after an initial seed round last year. The company is part of a growing cadre of startups attempting to unify sales, marketing, and customer service processes in a way that enables organizations to boost sales and increase overall profitability. BoostUp claims its revenue increased by more than 1,000% in fiscal 2020, thanks in part to customers such as Udemy, Degreed, Plume, and Windstream. The company’s Connected Revenue Intelligence & Operations platform is usurping customer relationship management (CRM) applications as the single source of truth for organizations that need to tightly integrate sales, marketing, and customer service process, CEO Sharad Verma told VentureBeat. “The CRM is no longer the system of record,” he said. The Connected Revenue Intelligence & Operations platform differs from rival offerings in that it includes predictive analytics capabilities enabled by machine learning algorithms, Verma added. The platform ingests unstructured data from sources like emails, phone calls, calendars, meetings, and messaging applications that are then matched to accounts and opportunities found in CRM applications. Natural language parsing, sentiment analysis, and proprietary indexing of spoken and written keywords are then applied to better understand patterns of sales trends to forecast more accurately whether deals will close. BoostUp claims customers have achieved 95% forecast accuracy, reviewed 5 times the number of opportunities per manager, and increased sales manager and sales representative capacity by over 15%. The latest round of funding was led by Canaan Partners, with participation from Emergent Ventures and BGV Ventures. It will be employed to scale product development and increase customer growth. It’s not clear to what degree providers of CRM applications are responding to any shift toward a more integrated approach to revenue Ops. Salesforce, for example, offers separate CRM, marketing, and customer service applications that are integrated on the same cloud. But Boostup.ai is making a case for a single platform that aggregates data in a way that makes it simpler to apply AI to identify, for example, when the level of customer engagement is misaligned with the sales forecast. Organizations that are shifting toward a Revenue Ops approach to engaging customers have typically appointed a chief revenue officer (CRO) to assume responsibility for all sales channels. At a time when most sales engagements are occurring via some form of a digital channel, Verma said organizations must measure and monitor the actual level of engagement with customers alongside other historical data that might indicate whether, for example, a customer tends to always wait until the last week of a quarter to sign a purchase order as part of an effort to obtain better pricing. Conversely, sales representatives may simply not be all that good at forecasting, which Sherma noted would identify an opportunity to improve training. Regardless of companies’ motivations for embracing Revenue Ops, it’s clear sales, marketing, and customer service processes are becoming more integrated. Many organizations now realize customer service representatives that regularly engage customers are in many cases more adept at identifying additional revenue opportunities they can close on their own. Sales teams in many cases are now focusing the bulk of their time and effort on trying to land new customers versus making sure every product or service has been delivered to the precise specifications on the contract. Naturally, it may take a while before every organization is able to fully transition to a Revenue Ops model. However, the way organizations engage customers is poised to change fundamentally."
https://venturebeat.com/2021/04/28/devops-orchestration-platform-opsera-raises-15m/,DevOps orchestration platform Opsera raises $15M,"Opsera, a continuous orchestration platform for DevOps, today announced it has closed a $15 million series A round led by Felicis Ventures. Opsera says it will put the capital toward growing its engineering team and accelerating its sales, marketing, and customer success initiatives. An estimated 19% to 23% of software development projects fail, with that statistic holding steady for the past couple of decades, according to data compiled by Ask Wonder. Standish Group found that “challenged” projects — i.e., those that fail to meet scope, time, or budget expectations — account for about 52% of software projects. Often, a lack of user involvement, executive support, and clear requirements are to blame for missed benchmarks. Opsera, which was founded in 2020, aims to combat DevOps challenges with a self-service, no-code orchestration platform that lets engineers provision or integrate their CI/CD tools from a common framework. With Opsera, users can build declarative pipelines for a range of use cases, including software delivery lifecycle, infrastructure as code, and software-as-a-service app releases. Opsera correlates and unifies data throughout the development process to provide contextualized diagnostics, metrics, and insights. The DevOps market is projected to reach $14.97 billion by 2026, at a compound annual growth rate of 19.1%, according to a Fortune Business Insights report. Opsera competes directly or indirectly with companies including Harness, a continuous integration and delivery platform for engineering and DevOps teams, and Tasktop, which recently nabbed $100 million. There’s also OpsRamp, which applies AI to DevOps processes. And Productboard offers a product planning interface designed for DevOps orchestration. But Opsera claims to have a growing client roster that includes “several” Fortune 500 customers. “Our mission is to democratize software delivery by abstracting any CI/CD tools into a common framework that can empower engineers to build pipelines in minutes, not days or weeks,” cofounders Chandra Ranganathan and Kumar Chivukula said in a press release. “We offer the only DevOps platform that connects and orchestrates the entire tool stack with complete choice and visibility. Our customers can focus on their core product and will never waste time and resources building and managing toolchains and pipelines in-house or be stuck with single-vendor solutions. Having the support of Felicis and all of our investment partners will accelerate how we help customers along their DevOps journey.” Beyond Felicis Ventures, existing backers Clear Ventures, Trinity Partners, and Firebolt Ventures and new investor HMG Ventures also participated in San Francisco, California-based Opsera’s latest financing round. It brings the startup’s total raised to $19.3 million."
https://venturebeat.com/2021/04/28/ai-powered-construction-project-platform-openspace-nabs-55m/,AI-powered construction project platform OpenSpace nabs $55M,"OpenSpace, a platform that helps construction companies track building projects through AI-powered analytics and 360-degree photo documentation, has raised $55 million in a series C round of funding led by Alkeon Capital Management. The raise comes amid a cross-industry digital transformation boom, spurred in large part by the pandemic. Construction has often lagged behind other sectors in terms of efficiency, but technology such as robotics, artificial intelligence (AI), and remote collaboration tools have played a sizable role in getting the $11 trillion industry back on track. Founded out of San Francisco in 2017, OpenSpace leans on AI to create 360-degree photos of construction sites, which are captured by builders or site managers who traverse an area with cameras strapped to their hats. All the imagery is sent to the cloud, where computer vision and machine intelligence tools arrange, stitch, and map the capture visuals to the associated project plans. It’s all about documenting activities on each site so stakeholders can check in on progress from afar or resolve conflicts that arise by checking back on a visual history of the project’s evolution. OpenSpace also offers AI-powered analytics, including “progress tracking,” which uses computer vision to analyze site images and automatically figure out how much of the scheduled work has been completed. Elsewhere, “object search” enables site managers to select an object from a scene and find similar objects elsewhere on the site. Other notable players in the space include SiteAware and Buildots, both of which have raised sizable VC investments over the past nine months, highlighting the growing demand for digital technologies in the construction space. Prior to now, OpenSpace had raised around $34 million, including a $15.9 million series B last July. With its latest cash injection, the company is well-financed to capitalize on its rapid growth over the past year, which it said has seen its revenue triple and customer count grow by 150%. With another $55 million in the bank, OpenSpace said it will double down on its suite of analytics products and expand them into areas such as safety management and quality control."
https://venturebeat.com/2021/04/28/viso-trust-assesses-third-party-cybersecurity-risk-with-ai-raises-3m/,"Viso Trust assesses third-party cybersecurity risk with AI, raises $3M","Viso Trust, a  platform that uses AI to perform cyber risk assessments, today announced it has raised $3 million. The company plans to use the funds to support expansion and hiring efforts, as well as sales, marketing, and R&D. It’s estimated that over 65% of security breaches are attributable to third-party failures. The pandemic has heightened the concern among legal and compliance leaders, 52% of whom worry about the risks posed by remote work. While the need for faster vendor security reviews has prompted some companies to use abbreviated questionnaires or outside-in assessments to conduct shorter reviews, security analysts can spend hours every day sending and processing documents. Viso Trust aims to lighten the workload by offering a holistic view of risk, leveraging a “social due diligence” network and AI to deliver continuous reports about third parties. The platform automatically extracts data from source documents and audits to surface key information about third-party relationships. “The goal of third-party risk management ranges from reducing the likelihood of data breaches and costly operational failures to meeting regulatory requirements,” cofounder Paul Valente told VentureBeat via email. “Unfortunately, the tools available for us to manage third-party risk, such as GRC platforms, security ratings, and audit exchanges, were too clunky, overly time-consuming, inaccurate, and most of all, expensive. Adding to the mix was the scale of our operations as a global fintech. We knew there needed to be a better way to run the vendor due diligence process.” One early customer, Ilumio, claims Viso Trust has enabled it to bring the security staff time per third-party relationship down from more than eight hours to 30 minutes. “Leveraging our prior experience and vast networks, we built a solution that solved the problem and validated the core concepts and value proposition with over 300 chief information security officers and security professionals,” Valente said. “Going forward, we believe we can reduce time spent in covering additional major areas of risk, such as business continuity and privacy, to nearly instantaneous.” Kelley Mak, principal at Work-Bench, a Viso Trust investor, says he saw a need in the market due to the proliferation of software-as-a-service tools in the enterprise. While cumbersome processes hamstring security teams attempting to evaluate tools at the speed of business, they face rising security threats and the hidden risk of third parties. Just 35% of organizations rate their third-party risk management program as highly effective, and only 34% have an inventory of their vendors, a 2018 study from Opus and Ponemon Institute found. “Viso Trust [is] building a cyber due diligence platform that leverages intelligence and automation to eliminate all questionnaire-based interactions and deliver continuous automated due diligence accurately across any number of vendors,” Mak told VentureBeat via email. “The founders felt this pain firsthand when they led security at LendingClub and ASAPP and had to onboard and evaluate the risk of hundreds of third parties.” Work-Bench led San Francisco-based Viso Trust’s seed round, with participation from Sierra Ventures and Lytical Ventures."
https://venturebeat.com/2021/04/28/messagebird-acquires-email-data-platform-sparkpost-closes-1b-round/,"MessageBird acquires email data platform SparkPost, closes $1B round","MessageBird, a cloud communications platform that creates APIs for developers and AI-powered contact center tools, is shelling out $600 million to acquire SparkPost, an email delivery, optimization, and analytics platform. Alongside the acquisition, Netherlands-based MessageBird announced that it has significantly extended its series C round of funding from the $200 million it announced in October to $1 billion. Founded in 2011, MessageBird serves up a Twilio-like platform that enables app makers to add WhatsApp messaging, voice, SMS, and email functionality to their software through APIs — saving them from having to develop the infrastructure internally. Last year, MessageBird launched Inbox, an omnichannel contact center platform that centralizes in-bound communications. MessageBird, which has accrued an impressive roster of customers, including Uber, Facebook, and SAP, has been on something of an acquisition spree of late. In December, it snapped up Pusher to help developers integrate real-time communication features into their own software. And last month, MessageBird bought two companies — video meeting platform 24Sessions and customer data platform Hull. With SparkPost under its wing, MessageBird gains access to “predictive email intelligence,” as well as a host of high-profile clients, including JP Morgan, PayPal, Disney, Zillow, Adobe, LinkedIn, and Pinterest. Founded in 2008, Columbia, Maryland-based SparkPost serves businesses with insights and analytics spanning all of their email campaigns, including delivery rate and engagement, bounce rates, spam traps, and ISP responses. With transactional emails, it’s particularly important to know if any are going astray, as this can lead to customer churn and revenue loss. It’s worth noting that the two companies first partnered back in 2019, an integration that enabled MessageBird customers to send emails through the API they were already using. “We made this acquisition to double down on our partnership,” MessageBird CEO Robert Vis told VentureBeat. “In terms of functionality, the SparkPost platform will give our enterprise customers more control over delivery and inbox placement by providing them with the world’s best email analytics platform and optimization tooling.” Despite the hype around communication tools such as Microsoft Teams and Slack, estimates suggest roughly 80% of businesses still use email as their primary communication tool. And email-related technology remains big business, with Exclaimer recently raising $133 million to help companies manage their email signatures, while SparkPost itself locked down $180 million in funding just a few months ago. SparkPost’s acquisition comes in the wake of a handful of similar deals in the email management and analytics space from companies including Twilio, which bought SendGrid for $2 billion, and private equity firm Thoma Bravo, which bought a majority stake in Mailgun. “Ninety-nine percent of interactions are on email — and globally, it remains the largest customer communication channel by volume,” Vis said. “So, whilst it may feel like an older technology to folks like me who live and breathe technology and communications, it’s still as crucial to customers in 2021 as it always has been.” With its huge series C extension, MessageBird is now valued at $3.8 billion post-money equity valuation, including debt. As a result of the acquisition, Sparkpost will continue to be offered as a standalone product."
https://venturebeat.com/2021/04/28/secrets-management-and-authentication-platform-akeyless-raises-14m/,Secrets management and authentication platform Akeyless raises $14M,"Akeyless, a software-as-a-service platform for authentication and digital access, today announced it has raised $14 million in a series A round led by Team8. Akeyless has offices in New York and Tel Aviv and says the round will be put toward hiring and global expansion as it looks to grow its customer base. As the transformation to hybrid and multicloud evolves — with 92% of organizations characterizing their operations as at least “somewhat” in the cloud — secrets like passwords, credentials, certificates, and keys are seeing heavy use in workloads, as well as by privileged users, teams, and IT security software. What was previously solved by key management systems has become much more challenging, given the scale of the cloud-distributed infrastructure. According to a CyberArk survey, 99% of security and DevOps pros fail to identify all the places where privileged accounts or secrets exist. Akeyless aims to solve this with a platform that combines secrets management with zero-trust solutions. In a cybersecurity context, “zero trust” refers to the belief that organizations shouldn’t automatically trust anything and must instead verify everything trying to connect to their systems before granting access. Founded in 2018, Akeyless is the brainchild of Shai Onn, Oded Hareven, and former Intuit senior engineer Refael Angel. Onn is the founder and chair of cybersecurity startup FireGlass, which Symantec acquired in 2017 for $250 million. Hareven, a veteran of the Israeli Defence Forces cybersecurity unit, held a number of product and project management positions, including director of product management at Moovit, which Intel purchased last year for $900 million. Akeyless automatically manages secrets for DevOps tools and cloud platforms using a secure vault for credentials, tokens, API keys, and passwords. The platform supports ephemeral, “just-in-time” access permissions and is built on an architecture that can be deployed on any internal environment, with containerized packages and virtual machines. Twenty-two-employee Akeyless isn’t the only secrets management product in an identity and access market that’s anticipated to be worth $24.12 billion by 2025, according to Grand View Research. Among others, there’s Doppler, as well as 1Password and tech giants like Amazon. But Akeyless claims to uniquely leverage a technique called distributed fragments cryptography to provide a root-of-trust in untrusted multicloud environments. Distributed fragments cryptography performs cryptographic operations using fragments of encryption keys without ever combining them, ensuring third parties can’t access secrets or keys, Akeyless says. “We are witnessing an unprecedented migration of technology development and data to the cloud,” CEO Hareven told VentureBeat via email. “As such, protecting access has never been more challenging or more important. Akeyless’ cloud-native … solution is the first unified vault that allows enterprises to manage credentials, certificates and keys seamlessly, without the need for the lengthy, costly installation of an on-premise solution. The innovative zero-knowledge key management system, where encryption keys never exist as a whole, means neither Akeyless nor any third party can access a customer’s secrets and keys. Our solution offers a seamless onboarding experience, fast time-to-production and ease-of-use. Our solution offers secrets management, zero-trust access and data protection, based on Akeyless… technology.” Jerusalem Venture Partners also participated in Akeyless’ latest funding round."
https://venturebeat.com/2021/04/28/accenture-says-it-investments-are-bearing-fruit/,Accenture says IT investments are bearing fruit,"The divide between organizations that derive maximum financial benefit from IT investments and comparative laggards is widening in the wake of the economic downturn brought on by the COVID-19 pandemic. Accenture today published a report based on a survey of 4,300 business and IT leaders that finds industry leaders are now growing revenue at 5 times the rate of rivals that have not invested as much in IT. That’s twice the growth rate a similar Accenture study found in 2017. Leaders represent the top 10% of that sample, while the bottom 25% are considered laggards. Cloud security and internet of things (IoT) technologies tied for top areas of IT investment among leaders, followed closed by hybrid cloud (68%) and AI and machine learning (59%), according to the survey. Nearly two-thirds (65%) of leaders have also prioritized flexible work arrangements using digital technologies, the survey found. But while Accenture identified many of the same leaders in the two reports, in today’s report the consulting firm also identified 18% of the organizations it surveyed as “leapfroggers,” defined as companies whose aggressive investments in digital transformation initiatives are having a significant impact on revenues. The existence of a leapfrogger category proves it’s not too late for laggards to make strategic IT investments that will enable them to compete more effectively, Accenture Integrated Global Services lead Ramnath Venkataraman told VentureBeat. “It’s become a matter of survival,” he said. “It’s not an option.” Overall, leapfroggers increased their investments in advanced and emerging technologies by 17%, the report finds. For example, 80% of leapfroggers had already adopted some form of cloud technology by 2017, but that figure rose to 98% by 2020. More than two-thirds of respondents (67%) said their organizations are also seeking to aggressively increase revenue from non-core lines of business. It’s not yet clear to what degree industry laggards will be able to remain viable in the face of more aggressive competition, even as the global economy continues to improve, Venkataraman said, adding that leaders and leapfroggers both cited leadership teams’ strategic commitment to investing in IT. In the absence of such commitment, Venkataraman said it will become even more difficult for laggards to close the gap that will grow as AI technologies are employed more widely. Another attribute leaders and leapfroggers share is that they have narrowed the divide between IT and the rest of their business, Venkataraman said. Laggards that want to become more competitive will need to not only become comfortable taking on additional risks, but also migrate as many legacy platforms to the cloud as possible to become more agile, Venkataraman said. As part of that effort, these organizations need to honestly assess their current digital capabilities, he added. Venkataraman said the pandemic has shown how much more resilient organizations that make the right strategic IT decisions can be in the face of great economic uncertainty. Many of those organizations will soon be in an even better position to capitalize on their investments as governments around the world continue to stimulate the economy, he added. While the post-pandemic future is still uncertain, organizations that consistently invest in IT have shown an ability to pivot in the face of adversity that rival organizations can now only envy."
https://venturebeat.com/2021/04/28/salesforce-launches-employee-upskilling-toolkit-for-businesses/,Salesforce launches employee upskilling toolkit for businesses,"Salesforce today announced the launch of Salesforce Learning Paths, which surfaces personalized learning content directly in Salesforce, enabling employees to learn in the flow of work. Salesforce Learning Paths will become generally available and free to all Salesforce customers this summer, when MyTrailhead customers gain the ability to integrate custom content into Salesforce. Workplace learning can be a driver of success for both employees and employers. But with remote work, it’s often harder to find opportunities to develop skills while on the job. According to a Salesforce survey, 59% of employees say they’ve had less access to workplace learning since the start of the pandemic. Salesforce Learning Paths aims to address the training gap with personalized learning content delivered via Salesforce. This allows employees to work and learn in a single environment, tapping into Trailhead, MyTrailhead, and guides that include articles, videos, and quizzes. Salesforce Learning Paths introduces Learning Home in Salesforce, a dashboard where employees can view assignments, track progress, and discover new learning paths. Business leaders and managers can personalize and monitor learning according to an individual, role, team, or for the entire organization. Eighty percent of employees find it easier to retain information they learn on the job, compared to siloed training, according to Salesforce. They’re also more productive (68%), more engaged in their work (70%), and more likely to stay at their job (60%) when their companies invest in continuous learning. Among others, Elekta and United Utilities are already using Salesforce Learning Paths in early access. Salesforce says more than 3 million people have used its Trailhead resources to date, up from 200,000 in October 2016. The launch of Salesforce Learning Paths comes as the global economy inches toward recovery following pandemic headwinds. In the U.S., job growth in March boomed at the fastest pace since summer 2020, as an aggressive vaccination effort contributed to a surge in hospitality and construction jobs. But the labor market simultaneously faces upskilling and wage disparity challenges. It’s estimated that as many as 30 million U.S. workers without college degrees have the skills necessary to earn 70% more, but employer education requirements frequently hold these workers back. The Bureau of Labor Statistics reports that U.S. workers with a bachelor’s degree make $1,248 per week, on average, while workers with only a high school degree earn closer to $746. In 2017, McKinsey said as many as 375 million workers would have to switch occupations or acquire new skills by 2030 because of automation and AI. A recent survey by the firm found that 87% of executives reported experiencing skill gaps in the workforce but less than half of respondents had a clear sense of how to address the problem."
https://venturebeat.com/2021/04/28/sysdig-raises-189m-to-monitor-containers-and-apps-in-the-cloud/,Sysdig raises $189M to monitor containers and apps in the cloud,"Sysdig, a container, Kubernetes, and cloud security company, today announced it has closed a $189 million series F round at a $1.19 billion valuation. The funding brings the company’s total raised to $395 million and will be used to grow its R&D teams, as well as sales and marketing. A key emphasis will be on continuing to build ecosystem and channel partnerships around the world, the company says. Gartner predicts that the public cloud market will reach $304.9 billion in value this year, up from $257.5 billion in 2020.  Modern apps are increasingly built as distributed microservices, leveraging both containers and services. While this shift accelerates innovation, it presents challenges legacy tools are sometimes unable to address. A 2018 Cybersecurity Insiders survey found that 62% of respondents believe misconfiguration is the single biggest threat to cloud security, followed by unauthorized access through the misuse of employee credentials. Sysdig was launched in 2013 as an open source effort to address the security problems facing enterprises adopting cloud apps. The company created projects to leverage visibility as a foundation for security, including Sysdig and Falco, which have become standards for threat detection and incident response. Falco, which was contributed to the Cloud Native Computing Foundation (CNCF) in 2018, is now an incubating hosted-level project with nearly 24 million downloads. “The pandemic accelerated the march to the cloud by a few years, and it exposed to many what we already knew — we are in fact in the midst of a transformative shift in the way applications are developed and secured,” Sysdig founder Loris Degioanni, a cocreator of Wireshark, the open source packet analyzer used for network troubleshooting, threat investigation, and incident response, told VentureBeat via email. “The winner is going to be an innovative open source stack built for secure DevOps, containers, and cloud. The winner will not be a legacy security stack retrofitted for the cloud.”  Sysdig also offers a managed platform that performs image scanning, runtime security, incident response, and continuous monitoring. It delivers alerts on threats, operational issues, and compliance risks with a detailed activity record. Out-of-the-box integrations enable customers to plug into existing workflows. As Degioanni explained, Sysdig uses machine learning to originate tailored profiles for different container images. The technology learns process and file activity, network connections, and system calls to build profiles for images from which policies can be created. Separately, Sysdig is able to auto-tune open source Falco rules. By observing security events, the platform can determine if behavior is expected or a potential security threat — knowledge it uses to add exceptions to the default Falco ruleset. Sysdig’s cloud security posture management incorporates the popular Cloud Custodian, an open source project for configuration checks. Last year, Sysdig announced compatibility and support for Prometheus, the CNCF project second in popularity to Kubernetes. Sysdig competes with rivals like Dome9, Datadog, and Orca Security, which recently raised $210 million to simplify enterprise security with cloud-native tools. Like Sysdig, Orca’s platform helps with compliance across cloud providers and automatically reads a customer’s configuration to detect vulnerabilities, malware, misconfigurations, and authentication risks.  But 2020 was a major growth year for Sysdig, which saw 2.3 times the new annual contract value compared with the previous fiscal year. Sysdig now has “tens of thousands” of users across over 450 enterprise customers, including “dozens” of large global enterprises with an average annual recurring revenue of more than $500,000 across the top 50 purchasing companies. Third Point Ventures and Premji Invest led Sysdig’s latest funding round, with participation from Accel, Bain Capital Ventures, DFJ Growth, Goldman Sachs, Insight Partners, and Next47. The company employs over 300 people and expects to have more than 450 by 2022."
https://venturebeat.com/2021/04/28/amd-bets-on-strong-demand-for-chips-as-revenue-soars-93/,AMD bets on strong demand for chips as revenue soars 93%,"(Reuters) — Advanced Micro Devices (AMD) raised its annual revenue forecast on Tuesday, betting on strong demand for its chips used in data centers and personal computers as its chief executive said she was confident the company could source the chips despite a global supply shortage. The chip designer’s shares rose about 4% in extended trading after it also reported better-than-expected results for the first quarter. CEO Lisa Su said AMD has “good visibility” into being able to secure additional chips from its manufacturing partners. “The entire semiconductor supply chain is very, very tight,” she told analysts on a conference call. “That being said, we’ve been working very closely with our supply chain partners. We have seen improvements that have led to the improved full year guide.” AMD has been prying away central processor chip market share from Intel, whose manufacturing operations have fallen behind contract factories used by AMD, such as Taiwan Semiconductor Manufacturing Co. It has also benefited from a surge in demand for its graphics chips from gamers who have spent more time playing and upgrading their equipment during the COVID-19 pandemic. The company said it now expected 2021 revenue to rise 50% from a year earlier, implying a figure of $14.64 billion, compared with its previous forecast of a 39% jump. Revenue in the first quarter soared 93% to $3.45 billion, beating a Refinitiv IBES estimate of $3.21 billion, thanks in part to higher average selling prices for its chips. “We feel very good about our progress, particularly in notebooks,” Su said. “We’re seeing traction in the premium ultrathin, gaming and commercial” notebook markets. Sales in AMD’s computing and graphics business, which includes graphics and central processor chips for personal computers, rose 46% to $2.10 billion. Its enterprise, embedded and semi-custom segment, the unit that houses data center chips, posted an almost four-fold jump in sales to $1.35 billion. Excluding items, the company earned 52 cents per share, exceeding expectations of 44 cents per share."
https://venturebeat.com/2021/04/27/microsoft-beats-q3-revenue-expectations-spurred-by-strong-cloud-sales/,"Microsoft beats Q3 revenue expectations, spurred by strong cloud sales","(Reuters) — Microsoft on Tuesday met analysts’ quarterly sales expectations and beat profit estimates, but its shares fell slightly as investors hoped for an even stronger performance after a year-long rally to a massive market valuation. The Redmond, Washington company has become one of the world’s most valuable companies, worth close to $2 trillion after its stock jumped 50% over the past year, by entering the booming market for cloud computing. Microsoft has remained a household name during the pandemic through its Teams collaboration software. Sales have even boomed for its Windows operating system for PCs, which had waned for decades as smartphone have proliferated. Microsoft’s Azure cloud service is closing ground on market-share leader Amazon Web Services, and it is doubling down on productivity software used by businesses worldwide. Revenue and adjusted earnings per share for the third quarter ended March 31 were $41.7 billion and $1.95 per share, above analysts’ estimates of $41.03 billion and $1.78 per share, according to data from Refinitiv. Shares initially fell as much as 3.2% after the results were released, but they pared losses to 1.7%, at $257.50, after Microsoft executives gave a better-than-expected forecast during a conference call with investors. “One-off tax and currency advantages have boosted Microsoft’s third-quarter numbers, and as a result the market isn’t being quite as welcoming of expectation-beating numbers as you might expect,” said Nicholas Hyett, equity analyst at Hargreaves Lansdown. “That is the danger of trading on the kind of valuation Microsoft enjoys, 32.8 times next year’s earnings. Disappoint even a little and the market will be unforgiving.” Sales for what Microsoft calls its “commercial cloud” – which contains server infrastructure such as Azure along with cloud-based versions of its Office software – was up 33% at $17.7 billion. Sales for Dynamics 365, which competes directly with Salesforce.com, rose 45% and the business version of Office 365 added 15% more users. “That’s the fourth consecutive quarter of 15% seat growth on a very large base,” Microsoft Chief Financial Officer Amy Hood said of the Office 365 results for commercial customers. Microsoft has continued to double down on cloud-base software and said earlier this month it would buy artificial intelligence software firm Nuance Communications Inc for $16 billion, excluding net debt, to bolster its healthcare business. Microsoft said Azure, its closely watched cloud computing business that competes with Amazon.com Inc’s Amazon Web Services and Alphabet’s Google Cloud, grew 50% in the quarter, or 46% when adjusted for currency variations. This is down from a currency-adjusted 48% the quarter before but in line with analysts’ expectations of 46.3% growth, according to data from Visible Alpha. Overall sales at Microsoft’s “intelligent cloud” unit that contains Azure were $15.1 billion, above analysts’ estimates of $14.92 billion, according to Refinitiv data. Microsoft Teams has 145 million daily users, up from 115 million in October, Microsoft said. Sales for Microsoft’s productivity software unit, which includes Office and Teams, were $13.6 billion, compared with estimates of $13.49 billion, according to Refinitiv. Sales for its LinkedIn social network were up 23% on a currency adjusted basis, slightly above Visible Alpha estimates of 21.9%, as revenue continued to recover from a sharp decline in job listings and hiring at the onset of the pandemic. Microsoft’s personal computing unit, which contains its Windows operating system and Xbox gaming console, had $13.0 billion in sales, compared with analysts’ expectations of $12.57 billion, according to Refinitiv data. Sales of Windows to PC makers were up 10%, compared to a 1% rise the quarter earlier. On a call with investors, Microsoft forecast fiscal fourth-quarter productivity segment revenue with a midpoint of $13.93 billion, above Refinitiv estimates of $13.57 billion. Its sales forecasts for its intelligent cloud and personal computing businesses had midpoints of $16.32 billion and $13.80 billion, respectively, above estimates of $16.0 billion and $13.26 billion, according to Refinitiv data."
https://venturebeat.com/2021/04/27/formation-launches-ai-driven-sales-offer-optimization-platform/,Formation launches AI-driven sales offer optimization platform,"Marketers send out billions of offers each year to engage customers. But succes depends on creating the optimal offer for each customer, a challenge that’s been compounded by changes in customer behaviors during the pandemic. A recent Forrester study found that only 33% of customers believe offers they received from brands were relevant. Marketing tech startup Formation claims it has a solution in Dynamic Offer Optimization, a platform that enables travel, retail, and quick service restaurants to create, deploy, and optimize sales offers. Formation claims Dynamic Offer Optimization can increase customer engagement by helping businesses respond more quickly to market forces and changing customer needs. Dynamic Offer Optimization consists of two components. There’s Offer Builder, which creates and optimizes individual offers, and Offer Engine, which drives real-time delivery, tracking, fulfillment, and measurement of each offer. Offer Builder ingests segments and customer journey data from marketing tech stacks and builds millions of unique offer variants. Offer Engine makes the offers available via APIs to channels and tracks, fulfills, and measures each offer’s performance. Formation applies machine learning to optimize each customer’s offer for subsequent deployments. The company says this process can take less than an hour and that customers — including Starbucks and United Airlines — have used it to deliver over 2 billion unique offers during the pandemic. “We’re really excited to take all the knowledge and experience from working with some of the world’s biggest brands for the past five years and make that breakthrough technology available to all companies,” Formation cofounder and CEO Christian Selchau-Hansen said in a press release. “Automating the creation, deployment, and optimization of billions of offers not only drives material impact to the business but enables brands and marketers to also focus on all the other critical mindset and organizational shifts needed for comprehensive digital transformation.” The release of Dynamic Offer Optimization coincides with the launch of Loyalty Innovators. As Selchau-Hansen explains, Loyalty Innovators’ mission is to connect and support digital, marketing, and loyalty leaders in their journeys to adapt their company’s products to the changing consumer landscape. “We are excited to offer marketers unique technology to help them engage customers with relevant and valuable offers with tremendous speed and agility by better leveraging first-party data through automation and machine learning,” Selchau-Hansen continued. “Our mission is to arm digital and marketing leaders with best-in-class optimization tech, as well as to support them through their digital transformation journey with the Loyalty Innovators community.”"
https://venturebeat.com/2021/04/27/pricefx-launches-ai-powered-market-simulation-tool/,Pricefx launches AI-powered market simulation tool,"AI has the potential to add value to marketing and sales operations. According to a McKinsey survey, 40% of marketing departments using AI achieve 6% or higher revenue growth on average. In sales, respondents most often report revenue increases from AI use in pricing, prediction of likelihood to buy, and customer-service analytics. But AI is also credited with improving the scale and speed of price optimization, or the use of analysis to determine how customers will respond to prices for products across different channels. Against this backdrop, Pricefx today launched what it’s calling AI-powered market simulation. Market simulation, which enables price optimization in the context of a product portfolio, as well as the competition, uses algorithms to predict the impact of pricing on customer purchasing behavior. According to chief product officer Toby Davidson, the goal is to help businesses make better business decisions informed by predicted impact. “Market simulation provides our customers with the ability to evaluate data driven market impact to pricing changes they have, or are looking to implement,” he told VentureBeat via email. “Whether you are looking to grab market share through price or make sure you do not create a price war with your competitors and resulting margin and profit leakage, market simulation lets you evaluate the impact your decisions have on your customer base and product portfolio, and the potential reaction from your competitors — before they go live in market.” Market simulation employs “multiagent” AI modeling of price elasticity among a company’s products and projects the demand impacts of various price changes. Through “what if” scenarios, market simulation guesses at possible competitive counter-moves and reflects customer behavior, in addition to the market response to price adjustments. AI is inevitably subjected to bias and unforeseen confounders, which is why AI-driven hedge funds, for example, don’t reliably outperform the market. Indeed, for enterprises using predictive models to forecast consumer behavior, data drift was a major challenge in 2020 due to never-before-seen circumstances related to the pandemic. Organizations were forced to constantly retrain and update their machine learning models, and 12 months later, many are still wrestling with the challenge. But there’s precedent for economy-simulating AI that works at scale. In August, Salesforce open-sourced the AI Economist, a research environment for exploring global tax policies. During experiments, Salesforce says the AI Economist arrived at a more equitable tax policy than a free-market baseline, the U.S. federal single-filer 2018 tax schedule, and a prominent tax framework called the Saez tax formula. McKinsey forecasts that AI-based price and promotion have the potential to deliver between $259.1 billion and $500 billion in market value. It’s estimated that 55% of marketing decision-makers plan to increase their spending on marketing technology, including AI and machine learning, with one-fifth of the respondents expecting to increase by 10% or more. Pricefx’s newly redesigned Plan, Price, and Profit solution sets include market simulation for subscribers of its Price and Profit packages. The market simulation feature is available beginning this week."
https://venturebeat.com/2021/04/27/red-hat-open-sources-trustyai-an-auditing-tool-for-ai-decision-systems/,"Red Hat open-sources TrustyAI, an auditing tool for AI decision systems","The ability to automate decisions is becoming essential for enterprises that deal in industries where mission-critical processes involve many variables. For example, in the financial sector, assessing the risk of even a single transaction can become infinitely complex. But while the utility of AI-powered, automated decision-making systems is undeniable, utility often plays second fiddle to transparency. Automated decision-making systems can be hard to interpret in practice, particularly when they integrate with other AI systems. In search of a solution, researchers at Red Hat developed the TrustyAI Explainability Toolkit, a library leveraging techniques for explaining automated decision-making systems. Part of Kogito, Red Hat’s cloud-native business automation framework, TrustyAI enriches AI model execution information through algorithms while extracting, collecting, and publishing metadata for auditing and compliance. TrustyAI arrived in Kogito last summer but was released as a standalone open source package this week. As the development team behind TrustyAI explains in a whitepaper, the toolkit can introspect black-box AI decision-making models to describe predictions and outcomes by looking at a “feature importance” chart. The chart orders a model’s inputs by the most important ones for the decision-making process, which can help determine whether a model is biased, the team says. TrustyAI offers a dashboard, called Audit UI, that targets business users or auditors, where each automated decision-making workload is recorded and can be analyzed at a later date. For individual workloads, the toolkit makes it possible to access the inputs, the outcomes the model produced, and a detailed explanation of every one of them. Monitoring dashboards are generated based on model information so users can keep track of business aspects and have an aggregated view of decision behaviors. TrustyAI’s runtime monitoring also allows for business and operational metrics to be displayed in a Grafana dashboard. Moreover, the toolkit can monitor operational aspects to keep track of the health of the automated decision-making system. “Within TrustyAI, [we combine] machine learning models and decision logic to enrich automated decisions by including predictive analytics. By monitoring the outcome of decision making, we can audit systems to ensure they … meet regulations,” Rebecca Whitworth, part of the TrustyAI initiative at Red Hat, wrote in a blog post. “We can also trace these results through the system to help with a global overview of the decisions and predictions made. TrustyAI [relies] on the combination of these two standards to ensure trusted automated decision making.” Transparency is an aspect of so-called responsible AI, which also benefits enterprises. A study by Capgemini found that customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and punish those that don’t. The study suggests companies that don’t approach the issue thoughtfully can incur both reputational risk and a direct hit to their bottom line."
https://venturebeat.com/2021/04/27/open-source-ai-stack-is-ready-for-its-moment/,Open source AI stack is ready for its moment,"Open source stacks enabled software to eat the world. Now several innovative companies are working to build a similar open source software stack for AI development. Dan Jeffries was there when the LAMP stack kicked this off. LAMP is an acronym representing the key technologies first used in open source software development — Linux, Apache, MySQL, and PHP. These technologies were once hotly debated, but today they are so successful that the LAMP stack has become ubiquitous, invisible, and boring. AI, on the other hand, is hotter than ever. Much as the LAMP stack turned software development into a commodity and made it a bit boring, especially if you’re not a professional developer, a successful AI software stack should turn AI into a commodity — and make it a little boring too. That is precisely what Jeffries is setting out to do with the AI Infrastructure Alliance (AIIA). Jeffries wears many hats. His main role, in theory at least, is chief technical evangelist at data science platform Pachyderm. Jeffries also describes himself as an author, futurist, engineer, systems architect, public speaker, and pro blogger. It’s the confluence of all those things that led him to start the AIIA. The AI Infrastructure Alliance’s mission is to bring together the tools data scientists and data engineers need to build a robust, scalable, end-to-end, enterprise artificial intelligence and machine learning (AI/ML) platform. This sounds like such an obvious goal — one that would be so beneficial to so many — you’d think somebody would have done it already. But asking why we’re not there yet is the first step toward actually getting there. Vendor lock-in is a reason, but not the only one. Vendor lock-in, after all, is becoming increasingly less relevant in a cloud-first, open source-first world, although technology business moats live on in different ways. Jeffries was surprised that he did not see an organization actually trying to capture the energy around AI activity, bring different companies together, and get their integrations teams talking to each other. “Every founder and every engineer I ended up talking to was very excited. I really didn’t have to work very hard to get people interested in the concept. They understood it intuitively, and they realized that the innovation is coming from these small to mid-sized companies,” Jeffries said. “They are getting funded now, and they’re up against giant, vertically integrated players like SageMaker from Amazon. But I don’t think any of the innovation is coming from that space.” Having spent more than 11 years out of his 20-year career at Red Hat, Jeffries recalls how the proprietary software companies used to come up with all the ideas, and then open source would copy them “in a kind of OK way.” But over time, most of the innovation started flowing to open source and to the smaller companies’ projects, he said. The Amazons of the world have their place, as the cloud is where most AI workloads run. Big vertically integrated proprietary systems serve their own purpose, and they’re always going to make money. But the difference is Kubernetes and Docker don’t become Kubernetes and Docker if they only run on Google, Jeffries said. Innovation is going to come from a bunch of these companies working like little Lego pieces that we stack together, he added. That’s precisely what the AIIA is working on. So, when can we expect to have a LAMP stack for AI? In all likelihood, not very soon, which brings us to the other key reason this has not happened yet. Jeffries expects a LAMP stack, or a MEAN stack, for AI and ML to emerge in the next five to 10 years and to change over time. The LAMP stack itself is kind of passé now. In fact, the cool dev kids these days are all about the MEAN stack, which includes MongoDB, ExpressJS, AngularJS, and NodeJS. Jeffries has described these as canonical stacks, which arise with greater and greater frequency “as organizations look to solve the same super challenging problems.” The kind of momentum that happened with LAMP will occur in the ML space, Jeffries suggested. But he warned against believing that anyone has an end-to-end ML system at this point. This can’t be true because the sector is moving too fast. The space itself and the problems to solve are shifting as the software is being created. That makes sense, but then the question is — what exactly is the AIIA doing at this point? And what does the fact that its ranks include some of the most innovative startups in this space, alongside the likes of Canonical and NewRelic, actually mean? Now some innovators are working to build an open source stack specifically for AI. Enthusiasm is good, but there’s a gap between saying “Hey, that sounds like a good idea, sign me up” and actually coming up with a plan to make it happen. So how are the AIIA and Jeffries going to pull it off? As a writer, Jeffries used George R.R. Martin’s metaphor of gardeners and architects to explain how he sees the evolution of AIIA over time. Architects plan and execute; gardeners plant seeds and nurture them. Jeffries identifies as a gardener and sees a lot of the people in the organization as gardeners. He thinks it’s necessary at this phase and envisions the AIIA evolving over time. Right now, the idea is to get people talking at a lot of different levels, rather than working in their own little silos. Micro-alliances are fair game though: “If you look at 30 logos on the website, you’re not going to build an ML stack with all 30 of those things,” Jeffries said. A concern is the fact that building bridges, and communities, takes time and energy. But Jeffries is enthusiastic about the prospect of helping shape what he sees as the AI revolution, is inspired by the open source ethos, and has the leeway from Pachyderm to run with his ideas. That seems to be what he’s doing with AIIA. Currently, he’s working on turning the AIIA into a foundation, and he’s also in talks with the Linux Foundation. The goal is to get to the point of bringing in some revenue. Jeffries is working on finances and a governance structure for the AIIA. “You get people who are just firmly focused on this, and it becomes a balance of volunteer efforts and people paid to work on different aspects. The next step really is a lot of logistical work — the boring stuff,” Jeffries said. Another metaphor Jeffries uses is that of a strategic board game, where you have to think about everything that can go wrong in advance — a bit like a reverse architect. Inevitably, there is going to be at least some amount of boring work, and somebody needs to do it. But for Jeffries, it’s all worth it. “When I look at AI at this point, I think very few people understand just how important it’s going to be. And I think they have an inkling of it, but it’s usually a fear-based kind of thing,” he said. “They don’t understand fully that in the future, there are two kinds of jobs: one done by AI, and one assisted by AI.” Isn’t it actually three types of jobs, as someone has to build the AI? The people building AI are going to be assisted by AI, so that falls into the second category, Jeffries said. There’s a creative aspect, as someone has to come up with an algorithm. But things like hyper-parameter tuning are already being automated, he added. Jeffries waxed poetic about how “the boring stuff” will be done by AI so people can move up the stack and do more interesting things. Even the creative parts will be a co-creative process between people and AI, in Jeffries’ view. As for the “AI destroys all the jobs” narrative, we’ve heard this one before, but the previous industrial revolutions worked out fine, Jeffries argued. Same goes for the argument that the pace of innovation is so rapid that we don’t have time to create jobs to replace those that are going to be displaced. What even an AI optimist like Jeffries can’t easily dismiss is the fact that innovation may not necessarily be coming from the Big Tech companies, but this is where the data is. This creates a reinforcement loop, where more data begets more AI leading to more data, and so on. Jeffries acknowledges data as a legitimate moat. But he believes ML is progressing in ways that make the dependency on data less vital, such as few-shot learning and transfer learning. This, and the fact that the amount of data the world is creating is not getting any smaller, may spur change. What seems inevitable, however, is the need to do lots of work, often boring work, to be able to chase dreams of creativity."
https://venturebeat.com/2021/04/27/amazon-makes-deepracer-software-available-in-open-source/,Amazon releases DeepRacer software in open source,"In November 2018, Amazon launched AWS DeepRacer, a car about the size of a shoebox that runs on AI models trained in a virtual environment with reinforcement learning techniques. DeepRacer has expanded since then, with a women’s league and new miniature race cars. Starting today, Amazon is making the DeepRacer device software available in open source. The pandemic has boosted automation and robotics in the enterprise. The global market for robots is expected to grow at a compound annual growth rate of around 26% to reach just under $210 billion by 2025, according to Statista. Deloitte anticipates that of the almost 1 million robots sold for business use in 2020, just over half were professional service robots, generating more than $16 billion in revenue — 30% more than in 2019. With the release of DeepRacer, developers can change the behavior of their cars, enabling the prototyping of new robotics apps. As Amazon notes, DeepRacer is essentially an Ubuntu-based computer powered by Robot Operating System, open source robotics middleware originated by Willow Garage and Stanford’s Artificial Intelligence Laboratory that provides low-level device control. Six sample applications — Follow the Leader, Mapping, Off Road, RoboCat, DeepBlaster, and DeepDriver — are available to help with brainstorming, including one that creates visualizations of a home or office. As the DeepRacer community creates projects, Amazon says it will add them to the DeepRacer GitHub organization, as well as featuring them in future blogs. “[W]e want to make it easier for developers of all skill levels to prototype new and interesting uses for their car. By making the AWS DeepRacer device software openly available, now anyone with the car and an idea has the ability to make it a reality,” Amazon wrote in a blog post. “Want to block other cars from overtaking it by deploying countermeasures? Want to deploy your own custom algorithm to make the car go faster from point A to B? You just need to dream it and code it.  We can’t wait to see the ideas you come up with, from new racing formats to new uses for the AWS DeepRacer.” Amazon previously partnered with Udacity to offer machine learning courses and a scholarship based around DeepRacer. The ostensible goal was to educate students on the creation, training, and optimization of reinforcement learning models, or models that employ rewards to achieve goals. In a recent analysis, McKinsey pointed out that reinforcement learning can be applied to solve real-life problems beyond autonomous driving, including classification, continuous estimation, and clustering."
https://venturebeat.com/2021/04/27/red-hat-touts-safety-in-future-linux-os-for-cars/,Red Hat touts safety in future Linux OS for cars,"Red Hat has announced plans to create a new Linux-based operating system for the automotive industry. With this push, the enterprise-focused open source software company is touting safety and continuous certification as core selling points. Red Hat, which IBM acquired for $34 billion in 2018, is already well known for its enterprise-grade Linux distribution, which will serve as the basis of its new platform for road vehicles. For the initiative, Red Hat has partnered with Exida, a company that specializes in functional safety and product certification, to provide ongoing certifications spanning a range of applications, from “infotainment to driver operations,” according to a press release. Red Hat said it’s working with Exida to achieve ISO 26262 certification, the global standard governing functional safety in road vehicles. It’s worth noting that Linux is already being used in the automotive industry. Tesla uses its own flavor of Linux in its vehicles, while the Linux Foundation-led Automotive Grade Linux (AGL) project counts founding members such as Ford, Jaguar Land Rover, Nissan, Denso, and Toyota among its members. The AGL’s ultimate goal is to further develop its Unified Code Base (UCB) Linux distribution to give automakers a “70% to 80%” leg up on their infotainment development projects. The 2018 Toyota Camry became the first production vehicle to use AGL in its infotainment system in the U.S. Open source software is used in most applications these days, as it saves companies having to develop every component internally and allows them to scale more quickly through the power of community-led software. But for mission-critical software — like that found in automobiles — safety is of paramount importance. That’s why Red Hat is setting out to bring a specialized Linux product to market with road-faring vehicles specifically in mind. This is consistent with what Red Hat has done elsewhere, building a multi-billion dollar business through serving enterprises with a hardened product with security and add-on services at its core. For potential customers — likely major automotive companies around the world — the prospect of using open source software (compared with proprietary software from a company such as Google) will be hugely appealing, as it ensures they retain control of everything, including their data. Red Hat hasn’t shared timing for its new automotive Linux product, which will depend on how long it takes to certify Red Hat Enterprise Linux components for vehicles. But the company did note it could eventually expand the product’s scope beyond highways and into robotics and manufacturing equipment."
https://venturebeat.com/2021/04/27/cigent-technology-melds-security-and-storage-to-protect-sensitive-data/,Cigent Technology melds security and storage to protect sensitive data,"To protect data on an endpoint, an enterprise has to get everything right. Otherwise, hackers will find a way to slip in and access sensitive data. But Cigent Technology says it has found a new way to protect data by combining methods from security, storage, and data recovery to make an impenetrable system. The company has its roots in a military project it is now turning into a commercial offering, boosted by a new round of funding led by the CIA’s venture capital firm, In-Q-Tel. “This has been a large development, most of it in stealth, with a lot of moving parts,” said Cigent CEO Brad Rowe. “And it culminates with this release.” The company today announced the commercial availability of its service, as well as a $7.6 million round that included money from CyberJunction, WestWave Capital, and a roster of prominent business angels. The money will be used to commercialize its core product, Cigent Data Defense, a combination of hardware and software designed to prevent data theft even when a hacker has penetrated a network. Following Edward Snowden’s leak of classified information, the U.S. government began seeking ways to toughen its internal and external cyberdefenses. Officials turned to a company called CPR Tools, a security and data recovery company based in Fort Meyers, Florida. The company has been around for about 15 years and has 40 employees, most of them from intelligence backgrounds, including its founder, who previously worked at the National Security Administration. Rowe said a multi-year research project generated dozens of hardware prototypes based on CPR’s storage devices. In early 2018, In-Q-Tel approached some team members about commercializing the technology, and Cigent was officially founded with about $2.8 million in seed funding. Cigent engineering VP Tony Fessel explained that the goal of the security system is to get the security as close to the data as possible — in this case, placing the security inside the storage device. In the most basic version, that could be just using Cigent’s software, which allows users and administrators to designate certain files as extra sensitive. The software applies a multi-factor authorization to specific data or files that are required to open them and access the information. While other solutions offer file-based encryption, Cigent prevents them from even copying the file to crack the encryption. Even if a hacker gets access, they won’t be able to open the files without those credentials, Fessel said. “It’s to stop ransomware and stop insiders from getting and doing things with data that they shouldn’t be doing,” he said. By using a secure storage device, Cigent can extend the protection even further. This storage device creates a hidden drive on a computer that intruders wouldn’t be able to spot, and a user can make it visible when they need to access it. The storage device has additional security built into the firmware and uses machine learning and AI to monitor for attacks. When it detects an attack, it automatically responds and self-defends to protect the drive and the data on it from an attack. While the company is now selling the system, it’s also working with partners like Dell and Microsoft to build the protection capabilities directly into their PCs and software. Rowe said the long-term goal is to focus on licensing the technology to other vendors."
https://venturebeat.com/2021/04/27/2nd-watch-most-enterprises-lack-in-house-skills-for-data-strategy/,2nd Watch: Most enterprises lack in-house skills for data strategy,"Most organizations want to make better use of their data, but most — 70% of enterprises — lack a mature data strategy, according to a recent survey on data management and analytics in large enterprises conducted by 2nd Watch, a cloud migration and managed cloud services provider. These enterprises lack the resources, skills and the vision to be successful A key finding of the 2nd Watch 2021 Data Management & Analytics study is that most organizations do not have a mature data strategy. Only 26% of survey respondents said they have any data strategy at all, and 70% don’t have what they consider to be a mature data strategy. It turns out that legacy systems and IT architecture are impeding the ability of many organizations to optimize their data, and IT can be slow to respond to the analytics needs of end users due to complex data integration and management challenges. Nevertheless, progress is being made. 57% of survey respondents said they are using a cloud data warehouse, and 64% said their data is in the cloud. Why are they doing this? 41% said moving to the cloud has allowed them to be more agile. “Agile data and analytics capabilities are essential to build sense-and-respond capabilities and are leading organizations to unprecedented cycles of rapid innovation to meet the new requirements,” market research firm Gartner said in its IT Roadmap for Data and Analytics. There are multiple steps in the process to becoming a data-driven organization, beginning with having a vision and strategy to establishing an operating framework and governance to “continuous intelligence” and refinement and progress, according to Gartner. The 2nd Watch 2021 Data Management & Analytics study includes responses from over 150 cloud-focused IT professionals in companies with at least 4,000 employees. See the full infographic from 2nd Watch."
https://venturebeat.com/2021/04/27/how-merck-works-with-seeqc-to-cut-through-quantum-computing-hype/,How Merck works with Seeqc to cut through quantum computing hype,"When it comes to grappling with the future of quantum computing, enterprises are scrambling to figure just how seriously they should take this new computing architecture. Many executives are trapped between the anxiety of missing the next wave of innovation and the fear of being played for suckers by people overhyping quantum’s revolutionary potential. That’s why the approach to quantum by pharmaceutical giant Merck offers a clear-eyed roadmap for other enterprises to follow. The company is taking a cautious but informed approach that includes setting up an internal working group and partnering with quantum startup Seeqc to monitor developments while keeping an open mind. According to Philipp Harbach, a theoretical chemist who is head of Merck’s In Silico Research group, a big part of the challenge remains trying to keep expectations of executives reasonable even as startup funding to quantum soars and the hype continues to mount. “We are not evangelists of quantum computers,” Harbach said. “But we are also not skeptics. We are just realistic. If you talk to academics, they tell you there is no commercial value. And if you talk to our management, they tell you in 3 years they want a product out of it. So, there are two worlds colliding that are not very compatible. I think that’s typical for every hype cycle.” Merck’s desire for the dream of quantum computing to become reality is understandable. The fundamental nature of its business — biology and chemistry — means the company has been building molecular or “quantum” level models for more than a century. Part of the role of the In Silico Research group is to develop those models that can solve quantum problems using evolving technologies such as data analytics and AI and applying them to natural sciences to make experimental work less time-consuming. But those models are always limited and imperfect because they are being calculated on non-quantum platforms that can’t fully mimic the complexity of interactions. If someone can build a fully fault-tolerant quantum computer that operates at sufficient scale and cost, Merck could unlock a new generation of efficiencies and scientific breakthroughs. “The quantum computer will be another augmentation to a classical computer,” Harbach said. “It won’t be a replacement, but an augmentation which will tackle some of these problems in a way that we cannot imagine. Hopefully, it will speed them up in a way that the efficacy of the methods we are employing will be boosted.” About 3 years ago, Merck decided it was time to start educating itself about the emerging quantum sector. The company’s venture capital arm, M Ventures, began looking within the company for experts who could help it with due diligence as it began to assess quantum startups. That included mapping out the players and the whole value chain of quantum computing, according to Harbach. That led to the formal creation of the Quantum Computing Task Force, which has roughly 50 members who try to communicate with quantum players large and small as well as peers among Merck’s own competition. “We are basically an interest group trying to understand this topic,” Harbach said. “That’s why we have a quite good overview and understanding on timelines, player possibilities, and applications.” As part of that exploration, M Ventures eventually began investing in quantum-related startups. In April 2020, the venture fund announced a $5 million investment in Seeqc, a New York-based startup that bills itself as the “Digital Quantum Computing” company. “We thought that it might be good to have partners in the hardware part and in the software part,” Harbach said. “Seeqc will partner with us within Merck to really work on problems basically as a hardware partner.” Seeqc is developing a hybrid approach that it believes will make quantum computing useful sooner. The idea is to combine classical computing architectures with quantum computing. It does this through its system-on-a-chip design. This technology was originally developed at Hypres, a semiconductor electronics developer which spun out Seeqc last year. The M Ventures funding for Seeqc followed a previous $6.8 million seed round. Seeqc raised a subsequent round of $22 million last September in a round led by EQT Ventures. According to Seeqc CEO John Levy, the company’s technology allows it to address some of the fundamental challenges facing quantum systems. Despite rapid advancements in recent years, quantum computers remain too unstable to deliver the high-performance computing needed to justify their costs. Part of the reason for that is that qubits, the unit of quantum computing power, need to be kept at near-freezing temperatures to process. Scaling then becomes costly and difficult because a system operating with thousands of qubits would be immensely complex to manage, in part because of the massive heating issue. Levy said Seeqc can address that problem by placing classic microchips over a qubit array to stabilize the environment at cryogenic temperatures while maintaining speed and reducing latency. The company uses a single-flux quantum technology that it has developed and that replaces the microwave pulses being used in other quantum systems. As a result, the company says its platform enables quantum computing at about 1/400 of the cost of current systems in development. “We have taken much of the complexity that you’ve seen in a quantum computer and we’ve removed almost all of that by building a set of chips that we’ve designed,” Levy said. Just as important is a philosophical approach Seeqc is taking. It’s not building a general-purpose quantum computer. Instead, it plans to build application-specific ones that are tailored specifically to the problems a client is trying to solve. Because Seeqc has its own chip foundry, it can customize its chips to the needs of application developers as they create different algorithms, Levy said. In that spirit, Merck’s Quantum Computing Task Force is working closely with Seeqc to create viable quantum computers that can be used by its various businesses. “Their technology is a key technology to scale a quantum computer, which is actually much more important because it will make quantum computers bigger and cheaper,” Harbach said. “And this is, of course, essential for the whole market.” For all this activity, Harbach’s view of quantum’s potential remains sober. He sees nothing on the market that will have any commercial impact, certainly not for Merck. At this point, many of the company’s questions remain academic. “What we are basically interested in is how — or will — the quantum computer hardware ever be scalable to a level that it can tackle problems of realistic size to us,” Harbach said. “And the same question also goes to the software side. Will there ever be algorithms that can basically mimic these problems on a quantum computer efficiently so that they don’t run into noise problems? We are not interested in simulating a molecule right now on a quantum computer. Everything we try to understand is about the timelines: What will be possible and when will it possible.” Harbach has watched the rise in quantum startup funding and various milestone announcements but remains dubious of many of these claims. “They are creating a new market where there’s not even the technology ready for it,” Harbach said. “You have to stay realistic. There’s a lot of money at the moment from governments and VCs. There’s a lot of boost from consultancies because they try to sell the consultancy. And if you talk to experts, it’s the other way around. They tell you not before 15 years.” The questions Merck asks internally are split into 2 fundamental categories: When will there be a quantum computer that can be more efficient at processing its current quantum models? And when will there be a quantum computer that is so powerful that it opens up new problems and new solutions that the company cannot even imagine today? “Quantum will be a thing, definitely,” Harbach said. “The only question is when, and I’m really, really sure it won’t be in the next two years. I wouldn’t even say three years. There will be a quantum winter. Winter is coming.”"
https://venturebeat.com/2021/04/27/legal-management-startup-clio-nabs-110m-to-expand-its-platform/,Legal management startup Clio nabs $110M to expand its platform,"Legal practice management startup Clio today announced it has closed a $110 million series E round, valuing the startup at $1.6 billion post-money. Clio says the funding will be used for strategic acquisitions and partnerships and to grow its workforce, with a focus on product and engineering teams. Contract management is often a time- and money-sucking endeavor for law firms, regardless of their size. According to a 2016 survey conducted by Apptus, 39% of legal departments are forced to rely on people without law degrees to manage their contracts, while 50% take a week or longer to turn out documents like non-disclosure agreements. Moreover, only 40% of firms say they have an automated contract management tool. Jack Newton and Rian Gauvreau cofounded Vancouver, Canada-based Clio in 2007 with the goal of building a legal services orchestration platform. When the company launched its software in 2008, it was among the first cloud-based practice management software products developed for law firms, Clio claims. Clio helps manage cases, organize contacts, and automate documents, as well as keeping track of financials and client legal accounts. The platform supports a range of billing options, including online payments, automated billing, and customized plans. Clio also offers tools that allow clients to create intake forms; automate client communication like emails, reminders, and requests; and organize referrals. According to a recent Gartner survey, the percentage of corporate legal operations leaders responsible for coordinating law firm billing increased 53% in the past two years, while the percentage of those tracking outside counsel spend increased 32%. One report predicts that the global legal operations software market will reach $3.57 billion by 2027, growing from just $1.08 billion in 2018. Clio made its first acquisition in October 2018, snapping up Los Angeles, California-based software provider Lexicata. Lexicata’s product then served as the foundation for Clio Grow, a client intake and invoicing toolkit. By the time Clio purchased Lexicata, the former’s app directory — a portfolio of integrations — had reached 120 connectors. It has now grown to 200, enabling Clio’s platform to sync with other commonly used legal software apps. T. Rowe Price and Omers Growth Equity led Clio’s latest funding round. It brings the company’s total raised to date to over $386 million, following a $250 million series D in September 2019. In recent years, “legaltech” has become one of the most active sectors for investors. Investment activity accelerated as the pandemic spurred legal firms to shift to digital-first business models. As of November 2020, there were 80 investments in legaltech, totaling $490 million for the year."
https://venturebeat.com/2021/04/27/automated-contract-negotiation-platform-pactum-raises-11m/,Automated contract negotiation platform Pactum raises $11M,"Pactum, a platform that leverages AI to automatically negotiate supplier contracts for enterprises, today announced it has raised $11 million in a series A round of funding led by Atomico. The company’s “negotiation-as-a-service” platform hits general availability from today and has already been used by several major enterprises. Clients include Walmart, the world’s largest retailer, which first signed up for a pilot program last year to automate negotiations with its supplier network. Pactum’s software essentially scans a contract, extracts what it believes to be the key priorities, and then sets about negotiating terms with the supplier through a chatbot. At its core, Pactum is all about making contract negotiations less laborious and enabling businesses to go beyond the few key terms that normally revolve around price and payments. “When we work with our customers, we go through a discovery process lasting several weeks,” Pactum cofounder and CTO Kristjan Korjus told VentureBeat. “During that time, we have identified up to 30 items to negotiate about in just one use case alone. Each of these items can be exchanged for value for both sides. Examples include freight, warehousing, contract length, contract cancelation terms, growth rebates, and so on.” The full deployment process can take a few months, including interviews with key stakeholders involved in the contract process. After Pactum’s system is built for the company in question, it’s able to work completely autonomously. This includes making decisions based on data, including current commercial terms, gleaned from enterprise resource planning (ERP) systems and other sources. “The system learns from each negotiation, so every negotiation will make every other negotiation better,” Korjus explained. “This creates a virtuous cycle.” It’s worth noting that Pactum may be best suited to smaller contracts or contracts that involve renegotiating an existing deal. This may be particularly true for larger enterprises that have dozens or hundreds of smaller contracts with different suppliers — and is why Pactum is targeting businesses with more than $1 billion in revenue that are more likely to benefit from this type of technology. Pactum isn’t purely about helping its own customers — it has to benefit both sides, as any typical negotiation would. “This is the quickest way to reach a Pareto optimal outcome — which is a situation where one party (supplier or the enterprise) can be better off without making the other party worse off,” Korjus said. Founded out of Estonia in 2019, Pactum is now headquartered in Mountain View, California. The company had previously raised around $4 million in funding and with another $11 million in the bank is now well-financed to scale its technology beyond the handful of enterprises it was already working with to refine its AI smarts. In other words, Pactum is now officially open for business, and it plans to build a platform that can “simultaneously work with many new enterprise customers on multiple deployments,” according to Korjus. Aside from Atomico, investors in this series A round included Project A, Metaplanet, Checkout.com CTO Ott Kaukver, TransferWise cofounder Taavet Hinrikus, and Teleport cofounder Sten Tamkivi."
https://venturebeat.com/2021/04/27/arms-neoverse-server-chips-generate-at-least-40-better-performance/,Arm’s Neoverse server chips generate at least 40% better performance,"Arm unveiled the performance numbers for its Arm Neoverse V1 and N2 server chip platforms, with processing boosts ranging from 40% to 50% over the previous generation. The demands of datacenter workloads and internet traffic are growing exponentially, and new solutions are needed to keep up with these demands while reducing the current and anticipated growth of power consumption. But  Arm said the variety of workloads and applications being run today means the one-size-fits all approach to computing is no longer the answer. That’s a jab at dominant server vendors Intel and Advanced Micro Devices, which use the x86 architecture. The Arm Neoverse V1 is a server chip microarchitecture that Arm’s customers — the big chipmakers of the world — can design chips around for servers in the big datacenters that power the internet. The V1 supports Scalable Vector Extension (SVE) and delivers more than 50% performance increases for high-performance computing machine learning workloads. “The time for Neoverse across all infrastructure is now,” Chris Bergey, senior VP for the infrastructure line of business at Arm, said in a press briefing. And another chip microarchitecture, the Arm Neoverse N2 platform, uses the new Armv9 architecture that Cambridge, United Kingdom-based Arm recently announced. It can deliver 40% more performance for a variety of workloads. “I think the N2 will pleasantly surprise people how performant designs will be in single-threaded designs,” said Patrick Moorhead, an analyst at Moor Insights & Strategy. “V1 looks to be a strong start in a nichey market, HPC. Overall, Arm is raising its game in the compute market.” Bergey said the journey to producing competitive server chips began a decade ago. Chips based on the designs should be hitting the market either late this year or early next year. Arm said the Arm Neoverse CMN-700 is the industry’s most advanced mesh interconnect to unleash the performance and performance/watt benefits of Neoverse V1 and N2 platforms. This device is a key element for constructing high-performance Neoverse V1 and Neoverse N2-based systems-on-chip (SoCs). It enables higher core counts and cache memory sizes. As Moore’s law comes to an end, solution providers are seeking specialized processing. Enabling specialized processing has been a focal point since the inception of the Neoverse line of platforms, and Arm expects these latest additions to accelerate this trend. Back in September, Arm unveiled the new Neoverse N2 and Neoverse V1 platforms without talking about performance. Now the company is talking about the performance per watt, the total cost of ownership benefits, and partners adopting the designs. “We believe Arm processors are coming to servers in a big way. We believe Arm is actually going to be everywhere, from the edge to the cloud,” Oracle senior VP Bev Crair said in a press briefing. Among the customers: These partners are taking full advantage of what is under the hood of Neoverse platforms. This is just the tip of the iceberg for infrastructure workload benefits and how partners plan to implement and take Neoverse IP to market, Bergey said. Arm argues that innovators shouldn’t have to choose between performance and power efficiency. The chips can target a range of cloud-to-edge uses. “The Neoverse V1 and N2 are huge improvements for Arm,” Tirias Research analyst Kevin Krewell said in an email to VentureBeat. “The V1 with the Scalable Vector Extensions (SVE) [is] powerful enough to be the CPU core for supercomputers. Even though Arm didn’t provide performance numbers against AMD and Intel, it seems to be very competitive, based on Arm’s data. The N2 is not an insignificant improvement over the N1. It’s the core to use for designs with very high core count, trading off some performance and a narrower SVE implementation for a smaller core size and lower power. These improvements are in line with Nvidia’s goals for the Arm architecture in the datacenter, and one of these cores could well be the core used in Nvidia’s Project Grace CPU.” Linley Gwennap, principal analyst at the Linley Group, said in an email that third-party test results are telling. “AMD’s latest Epyc processor outperforms the fastest Neoverse N1 chip on almost every test, often by a wide margin, despite the Arm chip having more cores,” Gwennap said. “Even after adjusting for TDP, the two chips have about the same performance per watt. Arm’s superiority claims rely on synthetic benchmarks that scale ideally across 64 or more cores, which isn’t representative of the real workloads that Phoronix measures. I also estimate that AMD Zen 3 leads the N1 by 60% on single-thread applications.” He added, “If you take this N1 comparison and project based on Arm’s data, the N2 will still be about 20% behind Zen 3 for single-thread (scale-up) workloads. According to Arm, N2 power rises by more than its performance, so power efficiency is actually worse for N2 (and much worse for V1). So if AMD matches N1 on performance per watt, N2 won’t give Arm the lead on that metric. In summary, until Arm achieves parity in single-thread performance, it will be limited to scale-out workloads. And unless it can demonstrate a sizable advantage in performance per watt on real applications, its main selling point is lower prices.” This chip design delivers a 50% uplift, as well as a 1.8 times improvement for a range of vector workloads and a 4 times improvement for machine learning workloads over N1. Neoverse V1 is the first in a new performance-first computing tier for Arm. Neoverse V1 gives silicon partners the flexibility to build compute for applications more reliant on CPU performance and bandwidth while providing system-on-chip (SoC) design flexibility. The performance-first design philosophy behind Neoverse V1 was to build the widest microarchitecture Arm has ever produced to accommodate more instructions in flight in support of markets like high performance and exascale computing. The wide and deep architecture — with the addition of scalable vector extensions (SVE) — gives Neoverse V1 the lead in per-core performance, as well as code longevity with SVE, and provides SoC designers implementation flexibility, Arm said. You can see the benefits of some of these design elements in SiPearl and ETRI’s HPC SoCs, and Arm thinks this is the direction HPC compute is heading, Bergey said. The Neoverse N2 is aimed at cloud-to-edge performance. A few weeks ago, Arm introduced the Armv9 architecture to address global demand for ubiquitous specialized processing. The Neoverse N2 platform is the first based on the Armv9 architecture with improvements to security, power efficiency, and performance. Delivering 40% higher single-threaded performance compared to N1, Neoverse N2 still retains the same level of power and area efficiency as Neoverse N1. The scalability of Neoverse N2 extends from high-throughput computing, such as in hyperscale cloud, where Arm sees 1.3 times improvement on NGINX over N1. The Neoverse N2 platform delivers superior performance per thread and industry-leading performance per watt, driving a reduced total cost of ownership for users. Neoverse N2 is the first platform to feature SVE2, an Armv9 feature that drives a significant uplift in cloud-to-edge performance efficiency. For a broader set of use cases, like machine learning, digital signal processing, multimedia, and 5G systems, SVE2 brings performance and ease of programming, as well as the portability benefits of SVE."
https://venturebeat.com/2021/04/27/campfire-raises-8-million-to-advance-ar-vr-for-product-design/,Campfire raises $8 million to advance AR/VR for product design,"Campfire has raised $8 million in funding for its holographic technology that enables both augmented reality and virtual reality for the purpose of enterprise product design. In stealth until now, Campfire has created a holographic collaboration system for professional 3D designers. The hardware and software headset system is based on patents and technology formerly created by Meta, which ran out of money in 2018. It will come out as a subscription in the fall of this year, after years of research and development. The company has fewer than 15 people and said the funding came from OTV, Kli  Capital, Tuesday Capital, and others. “This was designed for a very specific purpose, for designers and engineers who need to share with other stakeholders inside their company and others,” Campfire CEO Jay Wright told VentureBeat in an interview. “So I’m designing something that I need to show to my colleagues. Paramount for them is the visual experience.” Campfire combines proprietary devices and applications built on a foundation of more than 60 patents. It’s based on the lessons of the past few years of headsets, including the failure of Meta. “The problem with today’s devices is that the experience is not good,” Wright said. “The experience suffers from big problems. They have a small field of view and poor image quality. The devices press on our face, cause pain, get warm, and make us uncomfortable. The users also have to learn new interfaces and tools. And integrating with your regular workflow has been really poor.” With Campfire, Wright said the team had a chance to redo hardware and software and rethink it all. The team focused on creating a full system with good visual performance, ease of use, and an integrated workflow. The Campfire headset delivers a real AR experience with a 92-degree diagonal field of view, Wright said. He added that it doesn’t touch your face and it’s comfortable. The Campfire system is being used by industrial design firm Frog Design. Frog is a Campfire development partner, along with a select group of companies. The Campfire system, which consists of three devices and two applications, is available for preview through Campfire’s Pioneer Program. The Campfire headset can provide VR and AR with one device. Designers can visualize physical products with a natural view of the real world or an environment of their choice using a single headset. You wear the headset on your head, but it doesn’t completely obstruct your view. “You retain your peripheral vision. And by retaining your peripheral vision, you feel a lot more comfortable walking around your desk or table,” Wright said. “It goes a long way for people who get VR sickness because they can still maintain the horizon and see what’s around them.” The Campfire Console is a new device that acts like a holographic projector. And the Campfire Pack transforms a mobile phone into an intuitive controller with tools for working with 3D models. It attaches to the back of the phone and eliminates the learning curve of proprietary controllers and gesture interfaces. “The vision for holographic collaboration has been talked about for decades but not realized in products with any measure of success,” Wright said. “By focusing on specific needs for design and engineering, we’ve reimagined the entire stack to deliver an experience that takes a giant step toward the vision — and more importantly enables a giant step in productivity.” Meanwhile, Campfire Scenes enables users to create scenes from existing 3D models for quick reviews or elaborate presentations. “Campfire Scenes is the tool that solves the workflow gap,” Wright said. “It gets your existing 3D files in a way that can be shared easily.” And the Campfire Viewer enables users to work alone or together during video calls, using a Campfire Headset, tablet, or phone. “The Campfire Viewer is what you use to open the Campfire documents,” Wright said. “You plug the headset into the laptop and you’re in.” Frog Design’s teams have used the device to work together virtually while spread out, Frog venture design lead Graeme Waitzkin said in a statement. “I wanted to make sure we had people on board that were part of the process and were the top of their game for design,” Wright said. “Frog Design was at the top of that list.” Waitzkin said his teams jumped at the chance to test the system as a development partner. “It doesn’t stop with devices,” Wright said. “The Campfire Scenes are like a mashup of Google Docs and Powerpoint. It allows you to take more than 40 different CAD and 3D formats and organize them into a series of three-dimensional scenes that can then be shared with Campfire users. And they can open up, and you can view them with the headset.” You could compose something using a normal computer and then be able to see what it looks like through a live preview. Besides Wright, the team includes chief operating officer Roy Ashok and founding adviser Avi Bar-Zeev. Campfire was started as Meta View in December 2018 by venture capital firm OTV to purchase and commercialize IP developed by the Meta Company, including an AR visor design and a patent portfolio. “Two years ago, I took on a new mission with Campfire, and I couldn’t be more excited to tell you about today,” Wright said. “We really got some rock stars that have been behind the scenes for the last couple of years building this. And the space we’re in is holographic collaboration. It’s been called spatial collaboration. You hear it talked about. It’s the promise we’ve seen in science fiction, the promise we’ve also seen for a lot of companies in the AR/VR space. They have presented compelling visuals but haven’t delivered on them.” OTV recruited Wright, a former Qualcomm executive and president of Vuforia at PTC, as CEO in May 2019 to execute his vision for an integrated hardware and software solution designed from the ground up for collaboration. Campfire’s goal is to bring existing 3D workflows into 3D space for knowledge workers, not to replace the phone or computer, but to extend them. “This is about knowledge workers, specifically people that are working with 3D to build things today, like CAD and similar tools,” Wright said. “So it’s not about full immersion that we need for gaming. And it’s not about full immersion for making somebody feel like they’ve got an emergency situation in a cockpit for training. It’s about visualizing products, and doing it in the easiest and most flexible way.” Campfire utilizes a unique headset to provide holographic views of 3D models and data in AR and VR. The holographic views result from a stereo image generated by two separate displays (left and right) that reflect on the inner surface of the headset visor. The displays are driven by a discrete graphics processing unit (GPU) on a PC connected with a USB-C cable. This is a different operating principle than employed by holographic projectors/displays that seek to generate holograms that are visible with the naked eye. Campfire works with more than CAD/3D file formats directly from a desktop PC. The hardware connects to a PC with a discrete GPU, Windows 10, and a Thunderbolt-3 port with a USB-C connector. The Campfire Pack requires a recent iOS or Android phone."
https://venturebeat.com/2021/04/27/rewind-extends-saas-data-backup-and-recovery-to-trello/,Rewind extends SaaS data backup and recovery to Trello,"Team communication and collaboration software revealed its true worth over the past 12 months, as businesses across the spectrum rapidly transitioned to remote work. From Zoom to Slack and beyond, companies that weren’t already all-in on the digital workforce were given little choice — it was either sink or swim. However, with cloud spending going through the roof in 2020, a trend that’s set to continue in 2021 and beyond, this opens a Pandora’s box of questions for businesses embracing the giant hard-drive in the sky — how safe is all their data? Privacy issues aside, companies that entrust all their mission-critical information to third-party SaaS companies and clouds need a backup plan if disaster strikes. A recent cloud threat report published by Oracle and KPMG found that 75% of organizations in the study had experienced data loss from a cloud service on more than a single occasion. And this is something that Canadian company Rewind is setting out to solve with automated data backup and recovery services for many of the popular SaaS tools of today. Up until now, Rewind offered data backup and recovery for Shopify, BigCommerce, Intuit QuickBooks, and — as of two months ago — GitHub. Today, the company is extending support to Trello, the popular team collaboration and project management platform operated by Atlassian. It’s worth noting that most SaaS platforms offer their own disaster recovery tools for when a systemwide catastrophe occurs, so if a fire rips through one of their datacenters they can restore all the accounts to their former state from an alternative (backup) datacenter. But this doesn’t work at an individual account level, and the SaaS company typically doesn’t enable customers to recover individual data specific to them on-demand. This is what is widely known as a “shared responsibility” model, where the platform owner (e.g. Trello or GitHub) is responsible for infrastructure-level security and disaster recovery, and the customer is responsible for managing password security, permissions, and backing up all the data in their account. There are various existing methods open to Trello users looking to create backups for their data, such as setting reminders to capture screenshots of boards, exporting JSON or CSV files, or manually creating copies of project boards. Ignoring the significant time and resource drain this creates for companies, the process of restoring the data in these scenarios doesn’t bear thinking about. “The main issue with these types of manual backups is the inability to easily restore data,” Rewind CEO and cofounder Mike Potter told VentureBeat. “Manually backing up data means manually restoring it, which tends to be a slow and tedious process. Manual backups are also frequently forgotten and left out-of-date.” And that, essentially, is the role that Rewind fulfills. It not only creates and stores automated backups of each customers’ Trello instance, it restores it all to its former glory with the click of a button. The integration is available via Trello’s Power-Up marketplace, and it requires no real technical prowess — the full backup and recovery service is accessible via a browser. Moreover, Rewind backs up individual items of data and all their dependencies and relationships. This includes each Trello board, as well as all the cards (tasks or ideas), lists (collection of cards), labels, custom fields, checklists, and attachments on that board. At launch, however, users can only back up their boards and all the associated entities as a whole package. In the near future, users will also be able to choose on a more granular level, so they can just back up specific cards, lists, or attachments, for example. This all leads us to one lingering question. Why don’t SaaS companies offer such account-level backup services natively? This would surely be a huge selling point, particularly for enterprise clients. “While backups might seem like basic functionality, the fact is that building and continuously supporting a full-featured, scalable backup and restore solution presents non-trivial technical and usability challenges that tend to lie outside the core capabilities of commonly used SaaS platforms,” Potter hypothesized. Moreover, it’s good practice to house backups away from the host platform. This isn’t purely for reasons related to natural disasters — how do you access your Trello backup if, for example, you’re locked out of your Trello account? “A true backup gives you full access to your data at all times,” Potter said. “Best practices for data security and business continuity call for the 3-2-1 backup method — three total copies of your data, two of which are local but on different mediums or devices, and at least one copy off-site.” This latest launch comes just a few months after Rewind raised its first notable outside funding, securing $15 million in a series A round led by Inovia Capital. In the future, Rewind said that it plans to extend support to other popular SaaS tools such as Jira, GitLab, Xero, Bitbucket, and Zendesk."
https://venturebeat.com/2021/04/27/adobe-extends-ai-infused-customer-analytics-platform-to-offline-data/,Adobe extends AI-infused customer analytics platform to offline data,"Adobe today unfurled an enhanced Adobe Customer Journey Analytics cloud service that enables organizations to apply AI to data from both online and offline sources to gain deeper insights into customer behavior. Available as an extension to the Adobe Experience Platform, the Adobe Customer Journey Analytics service extends the analytics capabilities based on its Sensei AI platform that is already widely employed to track online engagements into the realm of data collected offline. That omnichannel approach to tracking a customer journey is becoming more critical as customers begin to return to retail outlets as more people receive COVID-19 vaccinations, said John Bates, director of product management for Adobe. He spoke during an Adobe Summit event. Adobe Customer Journey Analytics provides a unified view of a complete customer journey using real-time analytics dashboards that can be accessed from anywhere on any type of device, including via a mobile application for accessing Adobe dashboards that was also unveiled today. That approach is intended to democratize the advanced analytics enabled by the Adobe Sensei platform as an alternative to relying solely on data scientists to sift through data. Alerts that are generated by Adobe Customer Journey Analytics will automatically surface insights that would have otherwise gone unnoticed, Bates said. In comparison, legacy analytics applications assume the end user knows what queries should be launched to answer a set of already known questions. Adobe Customer Journey Analytics “will help you identify the unknown unknowns,” said Bates. Over time those alerts, currently available in preview mode, will become more personalized as the Adobe Sensei platform identifies what data is being accessed most often, added Bates. Adobe is also making it easier to collect and process data. A data views feature allows organizations to ingest more of their data in its original format, then apply logic at the time of reporting. That capability will make it easier for organizations to slice and dice data without having to first normalize it into a specific format. The goal is to enable organizations to track customer behaviors more easily whenever they see fit, noted Bates. As part of that effort, data collection tasks that once occurred on a mobile device or in a browser are being shifted to the Adobe server that is part of Adobe Experience Platform Collection Enterprise. The Adobe Experience Platform Edge Network then makes it possible to receive and send an event or individual piece of data in milliseconds in a way that enables privacy and governance controls to be applied. In effect, Adobe is making a case for extending the level of visibility that many organizations have into online customer behavior to include offline activities such as the number of times they visit a mall. Customers may never return to retail outlets in the same numbers they did prior to the pandemic, but it’s also clear many have become more adept at moving back and forth between an online application and a brick-and-mortar experience. It’s not uncommon these days for shoppers to employ a mobile application within a retail outlet to check, for example, inventory availability before deciding whether to make a purchase now at the store or later online. Adobe isn’t trying to replace data scientists. In many instances, Bates noted, data collected by the Adobe cloud offerings will be shared with data scientists that will be using tools from Adobe and others to analyze massive amounts of data. However, it’s also apparent business executives want to be able to identify and track different types of customer journeys in near real time. The challenge and the opportunity now is to enable that capability in the most frictionless way possible."
https://venturebeat.com/2021/04/27/automox-raises-110m-to-help-enterprises-manage-endpoints/,Automox raises $110M to help enterprises manage endpoints,"Endpoint management platform Automox today announced it has closed a $110 million equity round led by Insight Partners. The funds, which come as Automox’s customer base grew 200% from June 2019, bring the startup’s total raised to over $152 million. The company says it will use the funding to support product development. The average cost of a data breach is nearly $4 million, and yet 74% of companies say they can’t patch vulnerabilities quickly enough because they lack the necessary staff. That’s why former HP national account director Jay Prassl founded Automox, which is developing a platform that automates endpoint configuration, patching, management, and inventory. Automox’s platform, which works across operating systems, servers, and PCs, lets security teams automate and conduct cybersecurity actions through policies. From a dashboard, users can orchestrate checks to ensure patch compliance of assets, regardless of location, and perform technical and top-level reporting. Automox also enables critical patches and software updates throughout enterprise computing environments, as well as security configurations and custom scripting. “When we started out, we built the company around the most compelling pain points in the IT Ops space, which was the inability to see and remediate system vulnerabilities across Windows, Mac, and Linux from a single platform,” Prassl told VentureBeat via email. “A lot has changed since those early days, and we have continued to broaden our functionality to become the only fully cloud-native endpoint management platform.” This April, Automox announced the limited release Data Extract, a new report to gather historical patch management insights within a customer-specified time frame. The company also rolled out the Community Worklet Catalog, which allows customers to review, customize, and deploy workflow automations from the wider Automox community. Automox occupies a global cybersecurity market that’s anticipated to be worth $120 billion by 2024 — Israeli startups alone raised $6.32 billion between 2013 and 2019. That’s not surprising, considering Juniper Research pegs the number of digital records that will be stolen in 2023 at 33 billion, compared with the 12 billion stolen in 2018. As recently as 2017, the U.S. outranked all other countries in the volume of ransomware attacks, according to Symantec. And analysts elsewhere estimate that the cybercrime economy has grown to $1.5 trillion in annual profits and that damages will reach $6 trillion by 2021. Indeed, Tel Aviv- and Boston-based CybeReason raised $200 million in August 2019 for its enterprise endpoint protection platform, shortly after SentinelOne nabbed $120 million. CrowdStrike, an AI-powered cybersecurity platform specializing in endpoint protection and threat intelligence, also recently raised $200 million. And AI-powered cybersecurity startup Cylance snagged $120 million in June 2018 to expand its platform globally. For its part, Automox, which has over 160 employees, claims that its nearly 2,000 customers — including Greyhound, NASA, Xerox, and Unicef — have seen a 50% reduction in labor hours to manage patches and an 80% reduction in corporate attack surfaces. That’s in addition to 2-3 times less effort and 15 times faster hardening of infrastructure than with competing solutions. “While there are other players in the IT Ops and endpoint management space, such as Ivanti, ManageEngine, and Tanium, Automox is the only truly cloud-native platform in the market that has the flexibility to solve the problem that modern IT teams face. IT operators don’t need another siloed on-prem tool to manage,” Prassl said. “Many use up to seven different tools, which can leave operators with outdated views of their infrastructure and no single source of truth. As the strategic value of IT Ops continues to rise, so does the demand for a modern cloud-native platform that can interface with security tools they use today, such as CrowdStrike and SentinalOne.” Blackstone and existing investors Koch Disruptive Technologies and TechOperators also participated in Automox’s new series C round."
https://venturebeat.com/2021/04/27/databook-raises-16m-to-automate-key-sales-processes/,Databook raises $16M to automate key sales processes,"Databook, a customer intelligence platform tailored for enterprise applications, today announced that it raised $16 million in series A funding. The company plans to put the funds toward developing new products and expanding its workforce, in addition to acquiring new clients around the world. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. Forrester predicts that 57% of business-to-business sales leaders will invest more heavily in tools with automation.  And that’s perhaps why Salesforce anticipates the addressable market for customer intelligence will grow to $13.4 billion by 2025, up from several billion today. Palo Alto, California-based Databook, which was founded in 2017 by Anand Shah and Alex Barrett, offers a dashboard that lets sales teams determine the strengths and weaknesses of their prospects. Each salesperson is able to see how likely prospects are to make a purchase and estimate the impact of sales solutions, as well as view recommendations and suggestions that might help to drive the next steps. Teams using Databook get notifications about account events and news including buyer names and responsibilities. They’re also afforded access to Databook’s automated content creation tools, which can generate documents like executive one-pagers, emails, and benchmarking books. Leveraging algorithms, templates, and big data analytics, Databook can deliver an overview of a customer’s financial and operational performance compared with its industry peer group or an account plan deck for the customer in question. Beyond this, Databook features a module that analyzes publicly available company data across different accounts. It can cross-reference prospect data against a team’s performance and pipeline activities, and highlight deals requiring further inspection and possible adjustments to forecasts. Moreover, the module can show which prospects are at the right stage of their budgeting cycle and outline priorities that rank highest for leadership at the companies. “In the sales and marketing tech space, we are defining a new category of customer intelligence focused on enabling a highly consultative, enterprise selling motion that’s designed to originate and close large opportunities,” a spokesperson told VentureBeat via email. “The vast majority of other tools in sales and marketing tech are designed to accelerate or automate the high volume, transactional selling motion that’s predominant in B2B organizations today. Since this is a necessary motion for companies to maintain in some capacity, we view Databook as additive and a complement to these solutions.” Databook squares off against a number of rivals in a sales enablement market that’s anticipated to be worth $2.6 billion by 2024, according to Markets and Markets. Seismic has raised hundreds of millions of dollars to build out its automated sales and marketing enablement suite. Highspot nabbed $60 million in June 2019 for a sales enablement toolset that taps AI to power features like semantic search. In June 2019, Showpad secured $70 million for its cloud-based sales tools, and that April, Outreach raked in a cool $114 million to grow its semiautomated sales engagement software. But in spite of the competition, Databook claims to have experienced 300% year-over-year growth since its founding. Microsoft’s M12 and Salesforce Ventures led Databook’s latest tranche, which saw participation from Threshold Ventures, Haystack, and Firebolt. The new capital follows a seed round raised in January 2020 and brings Databook’s total investment to $22 million."
https://venturebeat.com/2021/04/27/zoom-brings-alexa-for-business-to-conference-room-calls/,Zoom brings Alexa for Business to conference room calls,"Zoom has announced that Amazon’s business-focused Alexa integration is now available for everyone in Zoom Rooms appliances. The launch comes a day after Zoom launched a new Immersive View feature that allows meeting hosts to arrange participants in a single virtual environment. Amazon launched Alexa for Business back in 2017 to bring the tech titan’s voice assistant to enterprise communication tools such as Cisco, Polycom, and Zoom itself. This meant companies using these tools could say something like, “Alexa, join my meeting” to start their call or “Alexa, find me a free room.” This was made possible due to an array of features Amazon had previously launched to make Alexa more compatible with office environments, including the ability to integrate with video- and audio-call software and integrations with Office 365 and Google Calendar. At its annual Zoomtopia event back in October, Zoom announced that Alexa for Business would eventually arrive for Zoom Rooms Appliances, a program Zoom launched back in 2019 to bring dedicated Zoom hardware to meeting rooms around the world. This launched in beta in January, and now it’s ready for prime time. With this integration, companies can start and stop all their Zoom Rooms meetings using voice commands, as well as finding available rooms in the building to host a meeting by asking something like: “Alexa, find me an available room for 45 minutes.” As the world tries to regain some semblance of normality, businesses will be exploring new contactless ways to run their meetings. With Zoom Rooms and Alexa, a small team in one office can communicate with another team in another office without having to touch any buttons. It’s worth noting that businesses could already set up Zoom Rooms, which is basically Zoom’s software product for physical conference rooms, to work with Alexa on Amazon’s own Echo hardware and other third-party devices such as Logitech’s. But today’s announcement opens Alexa to businesses that don’t have (or don’t want to buy) other Alexa-enabled devices, as admins can now enable Alexa for Business directly through the Zoom management portal."
https://venturebeat.com/2021/04/27/earthquake-monitoring-platform-safehub-raises-9m/,Earthquake-monitoring platform Safehub raises $9M,"Safehub, a startup developing an internet of things platform to monitor the structural integrity of buildings, today announced the closure of a $9 million series A round led by A/O PropTech. The company says it will use the capital to expand its platform and acquire new customers, as well as growing its engineering team. A recent FEMA study pegged U.S. losses from earthquakes at $4.4 billion per year. Each year, there are on average about 15 earthquakes with a magnitude of 7 or greater, strong enough to cause damage in the billions and significant loss of life. In spite of this, more than 60% of U.S. small businesses don’t have a formal emergency-response plan and fail to back up their sensitive data offsite. Safehub, which was founded in 2015, aims to address the risk with a real-time earthquake-monitoring product that leverages motion sensors, analytics, and third-party data to provide building integrity information. Safehub’s cell-connected sensors measure earthquake ground motion and building response, in addition to changes in buildings’ natural and resonant frequencies. The company uses this information to estimate damage to buildings and related business interruption losses. If an earthquake happens, Safehub sends damage alerts and financial loss estimates via text, email, and a web dashboard. In 2019, Safehub teamed up with the Global Earthquake Model (GEM) foundation to model structural robustness directly from sensors installed within buildings. The company used the data to refine the algorithmic predictions of damage from earthquakes that inform its vulnerability estimates and risk and insurance calculations, plus the other planning information it provides to customers. More recently, Safehub released the latest generation of its sensor technology, which includes a rechargeable battery, an accelerometer, and connectivity options like a long-range radio for mesh networking and LTE. The company says 99% of calculations can be performed on the sensor, reducing the need for communication with the cloud. Safehub isn’t the only company intent on tackling the earthquake detection and risk assessment problem. There’s Grillo, an in-home alarm that claims to provide warnings up to two minutes before an earthquake hits. SkyAlert not only provides an early warning for earthquakes, it can also turn off gas and assembly lines. That’s not to mention One Concern, which taps AI and machine learning to advise fire departments how to plan for earthquakes and respond to them. Some experts are skeptical about these systems’ accuracy. In February 2019 and August 2019, SkyAlert’s app issued alerts that overestimated the magnitude of earthquakes and caused tens of thousands of people to unnecessarily evacuate. In response, the Mexico City government adopted a measure preventing private companies from sending alerts to businesses and residents. But studies suggest algorithms can be trained to anticipate earthquakes with reasonable accuracy. Researchers from Google’s AI division and Harvard University created an AI model capable of pinpointing the location of aftershocks up to one year after a major earthquake. And scientists at Stanford developed an AI system — CRED — that can identify seismic signals from both historical and continuous data. Hannover Digital Investments and JLL Spark, the strategic investment arm of commercial real estate services firm JLL, also participated in Safehub’s latest funding round. Existing backers Fusion Fund, Ubiquity Ventures, Promus Ventures, Bolt, Blackhorn Ventures, Maschmeyer Group Ventures, and Team Builder Ventures also contributed. The San Francisco-based company has 12 employees, and this round brings its total raised to $14 million."
https://venturebeat.com/2021/04/26/hashicorp-revoked-private-key-exposed-in-codecov-security-breach/,HashiCorp revoked private key exposed in Codecov security breach,"A private code-signing key was exposed by a compromised Codecov script, open source company HashiCorp said in its discussion forum. Codecov, which makes software auditing tools for developers to see how thoroughly their code is being tested, revealed earlier this month that the script used to upload data to its servers had been modified by unknown actors. The script took advantage of the fact that Codecov’s tools have access to internal accounts and exported those credentials to an unauthorized server. HashiCorp was one of Codecov’s customers affected by the tampered script, HashiCorp product security director Jamie Finnigan wrote on the company’s discussion forum last week. HashiCorp’s Terraform product is an open source infrastructure-as-a-code software tool widely used for automated cloud deployments. “[HashiCorp] found that a subset of HashiCorp CI pipelines used the affected Codecov component,” Finnigan wrote, noting that the GPG [Gnu Privacy Guard] private key used for signing hashes used to validate HashiCorp product downloads had been exposed. The dangerous thing about having a private key exposed is that an attacker can use it to sign anything and the signed file will look like a legitimate file from the owner of the key. In this case, the concern was that someone could have modified one of HashiCorp’s downloads to include malicious code and then re-signed it with the private key. As far as anyone would be able to tell, that file would appear to be an update from HashiCorp that was safe to download and install. Finnigan said the company’s investigation did not show that any of its existing releases had been modified. HashiCorp revoked the exposed key and re-signed its downloadables with a brand-new key. “[The] GPG key used for release signing and verification has been rotated,” Finnigan wrote. “Customers who verify HashiCorp release signatures may need to update their process to use the new key.” While all official downloads on HashiCorp’s website have been signed with the new key, there are still some problems for HashiCorp customers. In environments where HashiCorp product downloads are manually or automatically validated, customers will need to manually update to reflect the key change. Also, Terraform downloads provider binaries and performs signature verification as part of one process during automatic code verification, and that process is still using the revoked key. “HashiCorp will publish patch releases of Terraform and related tooling, which will update the automatic verification code to use the new GPG key,” Finnigan said. Until then, customers can manually verify Terraform has the new key and signatures. This is just one of many disclosures as companies assess whether they were impacted by Codecov’s security breach. More than 29,000 enterprise customers worldwide use Codecov’s tools, and the malicious script was present from January 31 until its discovery on April 1. Codecov discussed the breach and how credentials, tokens, and keys could potentially have been exposed in a blog post on April 15. CircleCI, a continuous integration and delivery platform, confirmed to Cybersecurity Dive that the Codecov breach had impacted its integration with the code testing firm CircleCI Orb. Codecov’s breach is a form of supply chain attack, where attackers target a company’s suppliers or vendors. By compromising Codecov, the attackers got their hands on all kinds of API keys, login credentials, and other security information. In the case of HashiCorp, if the attackers had tampered with the company’s tools, that would be yet another supply chain attack because those tools are widely used within enterprises. It’s possible the attackers used the harvested credentials in other attacks that have not yet been discovered. The fact that HashiCorp’s private key was exposed is bad enough — but the company hasn’t said whether anything else has been stolen or compromised. “HashiCorp has performed additional remediations related to information potentially exposed during this incident,” Finnigan said, but he did not provide details about what else may have been harvested."
https://venturebeat.com/2021/04/26/apple-will-focus-on-machine-learning-ai-jobs-in-new-nc-campus/,"Apple will focus on machine learning, AI jobs in new NC campus","(Reuters) — Apple on Monday said it will establish a new campus in North Carolina that will house up to 3,000 employees, expand its operations in several other U.S. states and increase its spending targets with U.S. suppliers. Apple said it plans to spend $1 billion as it builds a new campus and engineering hub in the Research Triangle area of North Carolina, with most of the jobs expected to focus on machine learning, artificial intelligence, software engineering and other technology fields. It joins a $1 billion Austin, Texas campus announced in 2019. North Carolina’s Economic Investment Committee on Monday approved a job-development grant that could provide Apple as much as $845.8 million in tax reimbursements over 39 years if Apple hits job and growth targets. State officials said the 3,000 jobs are expected to create $1.97 billion in new tax revenues to the state over the grant period. The iPhone maker said it would also establish a $100 million fund to support schools in the Raleigh-Durham area of North Carolina and throughout the state, as well as contribute $110 million to help build infrastructure such as broadband internet, roads, bridges and public schools in 80 North Carolina counties. “As a North-Carolina native, I’m thrilled Apple is expanding and creating new long-term job opportunities in the community I grew up in,” Jeff Williams, Apple’s chief operating officer, said in a statement. “We’re proud that this new investment will also be supporting education and critical infrastructure projects across the state.” Apple also said it expanded hiring targets at other U.S. locations to hit a goal 20,000 additional jobs by 2026, setting new goals for facilities in Colorado, Massachusetts and Washington state. In Apple’s home state of California, the company said it will aim to hire 5,000 people in San Diego and 3,000 people in Culver City in the Los Angeles area. Apple also increased a U.S. spending target to $430 billion by 2026, up from a five-year goal of $350 billion Apple set in 2018, and said it was on track to exceed. The target includes Apple’s U.S. data centers, capital expenditures and spending to create original television content in 20 states. It also includes spending with Apple’s U.S.-headquartered suppliers, though Apple has not said whether it applies only to goods made in those suppliers’ U.S. facilities."
https://venturebeat.com/2021/04/26/apples-new-iphone-privacy-changes-explained/,"Apple’s new iPhone privacy changes, explained","(Reuters) — Apple on Monday will begin rolling out an update of its iOS operating system with new privacy controls designed to limit digital advertisers from tracking iPhone users. For Apple’s more than 1 billion iPhone users, the change will mean a new pop-up notification in some apps seeking their permission to collect data that Apple believes could be used to track their browsing habits across third-party apps and websites. For businesses, the rules could bring seismic changes to the nearly $100 billion mobile advertising market if most iPhone users decline to allow data collection, though the exact impact remains a question, according to industry experts. Apple is requiring app developers who want to collect a digital advertising identifier from iPhone users to show a pop-up saying that the app “would like permission to track you across apps and websites owned by other companies,” along with an explanation from the app developer about why permission is being sought. Some mobile advertising analysts believe that fewer than one in three users are likely to grant permission. IPhone owners also have a “tracking” menu in their phone’s privacy settings where they can opt-out of tracking from all apps on their phone with a single switch, or pick and choose among apps to grant permission to. Both advertisers and app developers who sell ad inventory say if many iPhone users opt-out of tracking, it will make advertising less effective. The ad industry has long gathered data about people’s web browsing behavior in order to serve up ads, such as for clothes or cars, that users might be interested in. A shrinking pool of user data could lead to lower sales for brands and lower ad revenue for mobile apps and publishers. Apple’s move has deepened a rift with Facebook, which has said the change will hurt small businesses because it will impede their ability to cost-effectively find local customers to target with advertisements. Apple has said it wanted to give its customers more control over whether data collected on them by apps is shared with third parties. Yes, data collection is still allowed if it is spelled out in an app’s privacy policy. The changes only affect whether app developers share data they collect with third parties, or mix their data with outside data from third parties, to help target ads. Apple has introduced privacy “nutrition labels” to its App Store to show users what data apps collect. Yes, iPhone users can still see ads even if they decline the new pop-up, as long as those ads are targeted using data the app developer has collected on its own. For example, a social network like Facebook can still target ads based on first-party data such as which groups users join or which posts they like. But if Facebook wants to target ads based on data from which third-party websites users have used their Facebook credentials to log into, it will need to seek permission."
https://venturebeat.com/2021/04/26/toyota-acquires-lyft-self-driving-division-for-550-million/,Toyota subsidiary acquires Lyft self-driving division for $550M,"Woven Planet, a newly established Toyota subsidiary, today announced plans to acquire Lyft’s Level 5 self-driving unit in a deal worth $550 million. The companies say the purchase will bring together roughly 1,200 scientists and software engineers from Level 5; Woven Planet; and Toyota Research Institute, which had researchers already working with Woven Planet. The pandemic and its effects, including testing delays, have resulted in consolidation, tabled or canceled launches, and shakeups across the autonomous transportation industry. Ford pushed the unveiling of its self-driving service from 2021 to 2022. Former Waymo CEO John Krafcik told the New York Times the pandemic delayed work by at least two months. And Amazon acquired driverless car startup Zoox for $1.3 billion. According to Boston Consulting Group managing director Brian Collie, broad commercialization of AVs won’t happen before 2025 or 2026 — at least three years later than originally anticipated. Once the acquisition is complete, Woven Planet says it will have an expanded footprint beyond its Tokyo headquarters, with offices and engineering teams in Palo Alto, California and London. In addition to the purchase of Level 5, Woven Planet has signed commercial agreements with Lyft to use the latter’s system and fleet data to support the commercialization of the driverless technology Woven Planet intends to develop. Lyft will receive approximately $550 million in cash, with $200 million paid up front and $350 million in payments over a five-year period. The transaction is expected to close in the third quarter of 2021. “Today’s announcement launches Lyft into the next phase of an incredible journey to bring our mission to life,” Lyft CEO Logan Green said in a press release. “Lyft has spent nine years building a transportation network that is uniquely capable of scaling autonomous vehicles. This deal brings together the vision, talent, resources, and commitment to advance clean, autonomous mobility on a global scale.” Lyft’s Level 5 R&D division was founded in July 2017 and has developed novel 3D segmentation frameworks, methods of evaluating energy efficiency in vehicles, and techniques for tracking vehicle movement using crowdsourced maps, among other things. In 2019, Lyft announced the opening of a new road test site in Palo Alto, California, near its Level 5 division’s headquarters. That development came after a year in which Lyft expanded access to its employee self-driving service in Palo Alto with human safety drivers on board in a limited area. In November 2019, Lyft said its autonomous cars were driving 4 times more miles on a quarterly basis than they were six months before and that it has about 400 employees dedicated to development globally (up from 300). In May 2020, the company partnered with Google parent company Alphabet’s Waymo to enable customers to hail driverless Waymo cars from the Lyft app in Phoenix. And Lyft has an ongoing collaboration with the self-driving Hyundai-Aptiv joint venture known as Motional, which makes a small fleet of autonomous vehicles available to Lyft customers in Las Vegas. Lyft recently revealed it has begun leveraging data from its ride-hailing network to improve the performance of its autonomous vehicle systems. A subset of drivers’ cars are equipped with inexpensive camera sensors, enabling them to capture challenging scenarios while helping solve problems like generating 3D maps and improving simulation tests. Post-acquisition, Lyft says its Open Platform team, which focuses on the deployment of third-party self-driving technology on the Lyft network, will become the new Lyft Autonomous team. Lyft expects the restructuring to remove $100 million in operating expenses from its books, primarily from reduced R&D spend."
https://venturebeat.com/2021/04/26/iot-development-platform-prescient-devices-nabs-2m/,IoT development platform Prescient Devices nabs $2M,"Prescient Devices, a platform for internet of things (IoT) software and service development, today announced that it raised $2 million in seed funding. The company says it’ll put the proceeds toward product ideation and ramping up its sales and marketing programs. Global IoT revenue hit an estimated $1.7 trillion in 2019, when the number of edge devices connected to the internet exceeded 23 billion, according to CB Insights. But despite the industry’s growth, not all organizations think they’re ready for it. In a recent Kaspersky Lab survey, 54% said the risks associated with connectivity and integration of IoT ecosystems remained a significant blocker. Prescient offers a low-code programmable platform that allows system integrators, IT engineers, and data scientists to build IoT and edge computing solutions. The platform, which can deploy firmware to fleets of IoT devices, delivers templates that connects sensors to the cloud, enabling remote monitoring and industrial automation. Prescient customers gain access to drag-and-drop graphical programming interfaces, modules, and recipes that they can use to program edge devices, edge and cloud dashboards, and cloud functions. They’re also provided a library of reference solutions for popular sensors and devices. There’s an abundance of tools promising to simplify IoT development and management at the edge including Google’s Cloud IoT Edge, Amazon’s Amazon Web Services (AWS) IoT, Microsoft’s Azure Sphere, and Baidu’s OpenEdge, as well as Zededa, Particle, and Balena. But CEO Andy Wang asserts that Prescient has an advantage in the scalability of its approach.  “We uniquely focus on removing the technology barrier for engineers, integrators, and data scientists to build, and accelerate IoT applications, helping deliver new business applications to the commercial market. The growing interest and active engagement from our users have been amazing,” Wang said in a press release. In what’s been a boon for Prescient, the pandemic has contributed to the growth of the larger IoT market. Microsoft’s 2020 IoT Signals report indicates that 33% of decision makers plan to up their IoT investments, while 41% say their existing investments will remain the same. Meanwhile, a recent Deloitte survey found that respondents believe IoT will have the largest impact on their organizations compared with AI and cloud infrastructure. “Our growing community has already developed active IoT applications for predictive maintenance, machine vision, and test automation within weeks of concept and transforming the entire approach to IoT business automation and edge intelligence applications,” Wang continued. “This round of funding will help accelerate our ability to better support our customers while expanding [the Prescient platform’s] functionality.” Z5 Capital led Boston, Massachusetts-based Prescient’s latest funding round, which had participation from angel investors at MIT and the Harvard Business School."
https://venturebeat.com/2021/04/26/network-security-company-proofpoint-goes-private-in-12-3b-deal/,Network security company Proofpoint goes private in $12.3B deal,"Private equity firm Thoma Bravo has announced plans to acquire cybersecurity company Proofpoint in a deal worth $12.3 billion. The deal serves as further evidence, if any was needed, that demand for cloud-based security is at an all-time high, driven in large part by the continued embrace of cloud computing and the rise of remote work, which necessitates robust network security. Founded in 2002 by former Netscape CTO Eric Hahn, Proofpoint was originally known for an email security product that helped businesses identify spam, viruses, and other electric correspondence that might contravene company policies. In the subsequent years, the Sunnyvale, California-based company has expanded its scope to include an array of cloud-based security products designed to protect enterprises from targeted threats. Proofpoint went public back in 2012, with its shares initially trading at around $13 — these have grown steadily over the past decade, hitting an all-time high of $140 earlier this year and giving it a market capitalization of more than $7 billion. Thoma Bravo has a track record of taking publicly traded cybersecurity companies private, having done just that with network security company Barracuda in a 2017 deal worth $1.6 billion and with Sophos last year for $3.9 billion. The Proofpoint deal, which is expected to close in Q3 2021, sees Thoma Bravo paying a 34% premium on Proofpoint’s closing price at the last full trading day (April 23), with shareholders set to receive $176 for each share they own. It’s worth noting that the $12.3 billion price tag positions this as the biggest cybersecurity acquisition of all time, putting it ahead of the $7.68 billion Intel shelled out for McAfee 11 years ago. And by VentureBeat’s calculations, the Proofpoint acquisition represents one of the biggest overall technology acquisitions ever, putting it in the top 20, alongside megadeals that include Dell’s $67 billion EMC purchase, IBM’s $34 billion Red Hat deal, and Salesforce’s impending $27.7 billion Slack acquisition."
https://venturebeat.com/2021/04/26/zoom-launches-immersive-view-to-unify-participants-in-the-same-virtual-room/,Zoom launches Immersive View to unify participants in the same virtual room,"Zoom has officially launched a new immersive video feature to help businesses create more engaging and collaborative virtual meetings. While a growing number of fledgling startups have adopted remote-first mindsets from the get-go, the transition for larger enterprises is fraught with challenges, given that they may have hundreds of thousands of workers spread across multiple regions and time zones. Despite these hurdles, major businesses — including Salesforce, Microsoft, Shopify, VMware, Dropbox, and Fujitsu — have already confirmed a permanent shift to a remote-first or hybrid working policy. But better and more adaptable virtual collaboration tools will prove vital to the long-term success of these programs — Zoom fatigue is real, after all. Zoom first announced its new Immersive View (then called Immersive Scenes) feature at its annual Zoomtopia event back in October, positioning the technology against Microsoft Teams’ Together Mode, which had launched a few months before. In a nutshell, video call hosts can use Immersive View to arrange participants — anyone from employees to panelists — in a single virtual environment. This deviates from the established norm of displaying participants in a grid-like format with each individual’s personal background showing. Immersive View supports up to 25 participants, and they can be placed in any number of environments, including a boardroom, auditorium, or classroom, depending on the event. Hosts can manually move people around on the screen or let Zoom do it automatically. Immersive View is available now in Zoom’s desktop client (version 5.6.3 or higher) for Windows and MacOS and is activated by default for all free and individual Pro accounts."
https://venturebeat.com/2021/04/26/thetaray-launches-anti-money-laundering-ai-and-analytics-for-the-cloud/,ThetaRay launches anti-money laundering AI and analytics for the cloud,"ThetaRay today announced that its AI-based anti-money laundering (AML) analytics will be available on public and private clouds, including Azure, Google, and AWS. ThetaRay’s AML platform uses unsupervised machine learning to monitor financial transactions, integrating data and triaging alerts in real time. And its new cloud-agnostic version aims to increase the speed at which the cybersecurity company’s clients — banks and fintech firms — can detect potential threats. Under a decades-old treaty called The United Nations Convention against Transnational Organized Crime, nearly 200 countries have pledged to help each other prosecute individuals involved in money laundering schemes. Many domestic financial institutions also take precautions, including closely monitoring clients’ activity for red flags, requiring deposits to stay in a client’s account for a minimum number of days before being transferred, and recording each transaction in detail. Now that nefarious actors can pass money through the internet via cryptocurrencies and digital currency exchanger services to conceal its illegal origins, AML efforts have become even more complicated. Whether complicated means difficult is controversial, however. Internet service providers make anonymity a gamble, and some industry leaders say cryptocurrencies identify and prevent illegal activities better than traditional payment systems do. Either way, big data and AI mechanisms can make it easier for financial institutions to protect themselves. AML software became popular in the early 2000s for customer transactions, identity management, and regulatory compliance. In recent years, providers like ThetaRay, Unit21, C3, and Splunk have developed increasingly intelligent solutions to analyze laundered money’s first two stages: placement (where it’s deposited into a bank) and layering (which involves transferring money before integrating and extracting it). Cloud-based systems became particularly important to banks during the COVID-19 pandemic as they scrambled to understand more data in less time. ThetaRay says its new service is the only cloud-based AML offering that analyzes SWIFT traffic, risk indicators, and client data to detect anomalies indicating “money laundering, terrorism financing, and other criminal activities across complex, cross-border transaction paths.” According to ThetaRay, its platform also helps users reduce operational costs and increase revenue by relying on its “artificial intuition” proprietary machine learning technology that interprets supervised and unsupervised data with fewer false positives. Traditional on-premise AML solutions present several challenges that can make them a poor fit for payments between people in different countries. They require months to put into production and are inefficient for organizations with limited internal resources. ThetaRay’s cloud alternative starts up quickly and does not require additional hardware. Company CEO Mark Gazit claims customers are choosing cloud-based solutions for their stability and security. “Our fully scalable cloud service … empowers the payments ecosystem to enjoy the long-term operational benefits of secure cross-border transactions without having to worry about the maintenance of additional infrastructure,” Gazit said. ThetaRay was founded in 2013 as the brainchild of Tel-Aviv University computer science professor Amir Averbuch and Yale math professor Ronald Coifman. Just one year later, it raised $10 million — its first of multiple rounds — from investors like General Electric to expand in the U.S. When VentureBeat interviewed Gazit in 2018, he said “Human beings are not enough to look at all this data, and we’re lucky to have tens of years of academic research to analyze all those … transactions.” At that time, ThetaRay claimed it had a 100% money laundering detection rate across more than 200 million transactions and that it had flagged novel fraud patterns in millions of ATM sessions across the globe. These numbers will certainly rise with ThetaRay’s shift to cloud AML as it seeks to help more organizations tackle tricky transactions."
https://venturebeat.com/2021/04/26/airehealth-appoints-kien-nguyen-as-chief-executive-officer/,AireHealth Appoints Kien Nguyen as Chief Executive Officer,"ORLANDO, Fla.–(BUSINESS WIRE)–April 26, 2021– AireHealth, a respiratory digital health company based in the U.S., announced today that its Board of Directors has appointed Kien Nguyen as Chief Executive Officer and member of the AireHealth Board of Directors effective immediately. Mr. Nguyen is a proven medical device and technology leader with a distinguished track record of product innovation, market development and commercialization. His career spans nearly three decades leading global teams within medical device and life sciences industries for both start-up and Fortune 500 companies, most recently serving as Chief Commercial Officer at Progenerative Medical. Mr. Nguyen has received multiple advanced degrees including a Master of Business Administration from Columbia University and a Doctorate in Neuroscience from the University of Colorado. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210426005069/en/ “Kien is absolutely the right person to lead AireHealth as we enter this next phase of innovation and growth,” said Stacie Ruth, AireHealth co-founder and Board Director. “Kien’s ability to translate technologies into solutions with high market value and his deep commercialization expertise will accelerate the execution of AireHealth’s mission to improve the lives of millions of people living with chronic lung disease.” AireHealth recently received 510(k) clearance for a connected nebulizer from the FDA to address the growing challenges in chronic respiratory illness that cost more than $130 billion per year and are the cause of nearly 50 deaths per 100,000 people. The portable, connected AireHealth nebulizer is part of a comprehensive digital health platform designed to track medication adherence, symptoms, and behaviors to enable earlier clinician intervention and reduce unnecessary hospitalization. “I am honored to have been chosen by AireHealth’s board of directors to lead the company,” said Mr. Nguyen. “AireHealth is truly revolutionizing respiratory care for people living with chronic respiratory illness and their care teams. My primary focus will be to work with the board and a talented , passionate leadership team to execute on the strategy and vision to bring meaningful respiratory care innovation to market faster.” Learn more about how AireHealth’s digital solution can support chronic care management and remote monitoring programs for healthcare providers. About AireHealth AireHealth is an innovative digital health company empowering and improving healthy living through affordable treatments, symptom tracking and early detection of respiratory conditions. With strong IP and clinically validated products in the pipeline, AireHealth enables more proactive care and early interventions to improve outcomes and reduce costs. The company provides monitored drug delivery through its FDA cleared Class II portable nebulizer, designed to deliver medicine directly to a patient’s lungs where it is most effective. AireHealth’s digital platform allows for earlier detection of respiratory decline through connected devices, the MyAirHealth diary companion app, and provider portal. The result is that patients not only take a more proactive approach to managing their respiratory care but enables faster clinical intervention and fewer hospitalizations, which helps achieve the goal of improving treatment outcomes and lowering costs. For more information, please visit Aire.Health.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210426005069/en/ Media Contact AireHealthCara Collinscara.collins@aire.health 574-376-5528"
https://venturebeat.com/2021/04/25/cisa-warns-of-credential-theft-via-solarwinds-and-pulsesecure-vpn/,CISA warns of credential theft via SolarWinds and PulseSecure VPN,"Attackers targeted both the Pulse Secure VPN appliance and the SolarWinds Orion platform in an organization, the U.S. government said in an incident report last Thursday. Enterprises have been rocked by reports of cyberattacks involving mission-critical platforms over the past year. In the past few months, security teams have been busy investigating a growing list of cyberattacks and vulnerabilities to figure out whether they were affected and to apply fixes or workarounds as needed. The supply chain attack and compromise of the SolarWinds Orion platform reported at the beginning of the year was just the beginning. Since then, there have been reports of attacks against Microsoft Exchange, the Sonicwall firewall, and the Accellion firewall, to name just a few. Defenders also have a long list of critical vulnerabilities to patch, which have been found in multiple widely used enterprise products, including Vmware and F5’s BIGIP appliance. The alert from the U.S. Cybersecurity and Infrastructure Security Agency (CISA) is an unsettling reminder that attackers often chain vulnerabilities in multiple products to make it easier to move around within the victim network, cause damage, and steal information. Compromising the Pulse Secure virtual private network appliance gave attackers initial access to the environment. SolarWinds Orion platform has been used to perform supply chain attacks. In the incident report, CISA said the attackers initially obtained credentials from the victim organization by dumping cached credentials from the SolarWinds appliance server. The attackers also disguised themselves as the victim organization’s logging infrastructure on the SolarWinds Orion server to harvest all the credentials into a file and exfiltrate that file out of the network. The attackers likely exploited an authentication bypass vulnerability in SolarWinds Orion Application Programming Interface (API) that allows a remote attacker to execute API commands, CISA said. The attackers then used the credentials to connect to the victim organization’s network via the Pulse Secure VPN appliance. There were multiple attempts between March 2020 and February 2021, CISA said in its alert. The attackers used the Supernova malware in this cyberattack, which allowed them to perform different types of activities, including reconnaissance to learn what’s in the network and where information is stored, and to move laterally through the network. This is a different method than was used in the earlier SolarWinds cyberattack, which compromised nine government agencies and about 100 private sector companies. “Organizations that find Supernova on their SolarWinds installations should treat this incident as a separate attack [from Sunburst],” CISA wrote in a four-page analysis report released Thursday. It appears the attackers took advantage of the fact that many organizations were scrambling in March 2020 to set up remote access for employees who were suddenly working from home because of the pandemic. It’s understandable that in the confusion of getting employees connected from completely different locations, the security team missed the fact that these particular remote connections were not from legitimate employees. None of the user credentials used in the initial compromise had multi-factor authentication enabled, CISA said. The agency urged all organizations to deploy multi-factor authentication for privileged accounts, use separate administrator accounts on separate administrator workstations, and check for common executables executing with the hash of another process. While CISA did not attribute the combined cyberattack to anyone in its alert, it did note that this cyberattack was not carried out by the Russian foreign intelligence service. The U.S. government had attributed the massive compromise of government and private organizations between March 2020 and June 2020 to the Russian Foreign Intelligence Service (SVR). Security company FireEye last week said Chinese state actors had exploited multiple vulnerabilities in Pulse Secure VPN to break into government agencies, defense companies, and financial institutions in the U.S. and Europe. Reuters said Supernova was used in an earlier cyberattack against the National Finance Center — a federal payroll agency inside the U.S. Department of Agriculture — reportedly carried out by Chinese state actors."
https://venturebeat.com/2021/04/25/how-low-code-platforms-can-aid-intelligent-business-process-management/,How low-code platforms can aid intelligent business process management,"The potential for low-code/no-code platforms is enormous. Low-code increases the productivity of IT developers — sometimes by several orders of magnitude. And no-code empowers experts and subject matter experts primarily on the business or operations side (as opposed to IT) to become “citizen developers.” But as I explained in a previous article, low-code and no-code platforms are not a panacea; they face challenges. Given the broad spectrum of low-code and no-code platforms, how should enterprises find the best options for their specific needs? And what are the use cases for using multiple low-code/no-code platforms? I will address these questions in a series of articles to help you navigate this transformational landscape while avoiding the pitfalls. Specifically, I will be looking at low-code/no-code related to intelligent business process management (BPM), intelligent databases, automated integration, and a number of other areas. In this first installment, I will be focusing on low-code/no-code in the context of intelligent BPM, or iBPM. iBPM’s core value proposition is the collaboration and orchestration of people, applications, connected devices, and trading partners to achieve and continuously improve business objectives. Intelligence and automation are two essential conjuncts for BPM. Intelligence for BPM comes in many forms: digitizing business rules, intelligent virtual assistants, and increasingly process mining. A BPM solution will involve fully automated robotic process automation sub-processes for repetitive tasks that do not need human intervention and automated tasks assigned to human participants. Thus, increasingly RPA is becoming part of the complete intelligent BPM platform. Here is a simple order-to-cash process example:  Some tasks will be performed by humans — for instance, approving the orders. Others could involve automation with RPA — for example, receiving the goods. There will also be tasks accessing systems of record — for instance, preparing and paying the invoice. An iBPM platform will model, execute, monitor, and improve the end-to-end process. Other terms are also often used to denote end-to-end processes. These include “workflow” and “case management.” Intelligent BPM is much more than technology. At its core, it is a transformational management discipline that helps organizations achieve their strategic goals. Automation is a crucial component of iBPM solutions. As a discipline, BPM drives the operations of enterprises. It includes several iterative phases from design to execution to monitoring and continuous improvement. There is a remarkably close affinity between low-code/no-code and BPM. As far as back in 2005 or earlier, BPM suites were touted as platforms for model-driven development, which is akin to what we now call low-code/no-code. What were the “models?” Well, check the next section on how low-code/no-code manifests itself in iBPM platforms. Low-code/no-code iBPM platforms handle: There are other components of a complete iBPM low-code/no-code platform — such as the decisioning (aka business rules), integration, and data model — but I won’t be getting to those in this post. The following is a simple purchase request process model from Bizagi, using shapes from the BPMN graphical notation for business processes (the de-facto standard):  The swim lanes represent the participants in the process. The rectangular shapes are tasks or activities. The diamond shape is for a decision, and the circles represent the start and end of the process. There are many other shapes in the BPMN standard, but these three are the most common. If human participants, such as Boss, Requester, etc, are involved in a particular workflow, the low-code/no-code BPM modeling also supports the creation of UI forms to enable that interaction, and these are pretty easy to model. This “drag and drop” paradigm of building user experience is common and similar across all low-code/no-code platforms that support Web or mobile applications. The following figure illustrates a simple user experience designer from Kissflow. There will be elements such as buttons, input fields, drop-downs, images, etc., that a non-technical developer can use to create the user experience. The elements are then connected to the properties or fields in the business process being modeled and automated.  The interface builder of the iBPM platform is robust enough to allow the designer to build a user experience — preferably without any code. Once the application is deployed, the various participants can then monitor the performance of the activities through interactive analytics. These are actionable analytics dashboards, which means that if there is a bottleneck or issue, the stakeholder can take action, such as escalating or re-assigning a task. The analytics dashboards will typically have pre-built analytics that also support low-code/no-code customization. Here is an example of an actionable business process analytics dashboard from Nintex:  Why is it important for organizations to be able to model, automate, monitor, and improve their business processes without coding? An organization is a collection of business processes for production, marketing, sales, service, and support functions. So any optimizations and improvements of the most critical processes will enhance the bottom line: cost savings, revenue generation, and compliance. These are called operational excellence (OE) improvements. iBPM low-code/no-code platforms are an enabling technology for OE. Here are my recommendations for iBPM low-code/no-code. Prioritize your improvements: There will typically be many mission-critical and support processes that need improvement. By balancing the complexity of implementation with business value, you will identify the low-hanging fruit. (For more details, check out this explanation of four intelligent automation methodologies). The result will be a list of automation and OE business processes that you can optimize through an iBPM low-code/no-code platform. Make sure you start with process mining: To find the top priority low-hanging fruit, you need to know the most common process paths, the bottlenecks, the variations, and improvement opportunities. In other words, you need to understand what processes your transactional data is subject to and then improve them. That is precisely the domain of process mining. Do not automate bad processes. The figure below illustrates the OE reference architecture with iBPM low-code/no-code. At the bottom, you have the systems of record that generate the transactions for specific processes. After aggregating and cleaning the transactional data, a process mining tool — such as Celonis — can then identify the most common process path and the variations and the root causes for the issues. Like data mining, process mining algorithmically “mines” and discovers the processes from the transactional data, including the variations and bottlenecks. Based on these, a iBPM low-code/no-code platform is used to improve, implement, and automate the processes, leveraging workflows with human participants and robotic process automation.  Create and fund an operational excellence competency center: iBPM low-code/no-code — and all other low-code/no-code, for that matter — is technology. As noted above, it is also a management discipline for operational excellence. For organizations that use this approach, it is a good idea to have a competency center that does three things at a minimum: balances innovation through iBPM low-code/no-code with best practices for security and reliability, enables non-technical subject matter experts to leverage iBPM low-code/no-code and become participants in development, and governs the continuous improvement from process mining to automation. Understand the landscape and leverage experts: There is quite a bit of confusion when it comes to classifying what solutions are BPM solutions. Some analysts classify these platforms as “workflow,” “business process,” or “case management” solutions. For example, see these classification schemes: There are also low-code/no-code development platforms that are closely affiliated with the BPM space but that might be classed into other low-code/no-code categories: The low-code/no-code ecosystem is constantly evolving. There are hundreds of platforms — and new ones are entering the market all the time. Sometimes inexpensive and straightforward low-code/no-code tools will be sufficient for your needs. Do not pay for what you will rarely use. Also, avoid vendor lock-in. There are emerging new and innovative low-code/no-code platforms that support plug-ins and add-ons, including those that address process mapping. Dr. Setrag Khoshafian is a cofounder at Startup Assistant and Principal and Chief Scientist at Khosh Consulting. He was previously VP of BPM Technology at Pega, Senior VP of Technology at Savvion, and CTO at Portfolio Technologies and is a member of the Cognitive World Think Tank on enterprise AI."
https://venturebeat.com/2021/04/25/you-need-to-be-constantly-exploring-the-data-in-your-ai-pipeline/,You need to be constantly exploring the data in your AI pipeline,"Poor data quality is hurting artificial intelligence (AI) and machine learning (ML) initiatives. This problem affects companies of every size from small businesses and startups to giants like Google. Unpacking data quality issues often reveals a very human cause. More than ever, companies are data-rich, but turning all of that data into value has proven to be challenging. The automation that AI and ML provide has been widely seen as a solution to dealing with the complex nature of real-world data, and companies have rushed to take advantage of it to supercharge their businesses. That rush, however, has led to an epidemic of sloppy upstream data analysis. Once an automation pipeline is built, its algorithms do most of the work with little to no update to the data collection process. However, creating those pipelines isn’t a one-and-done task. The underlying data must be explored and analyzed over time to spot shifting patterns that erode the performance of even the most sophisticated pipelines. The good news is that data teams can curtail the risk of erosion, but it takes some serious effort. To maintain effective automation pipelines, exploratory data analysis (EDA) must be regularly conducted to ensure that nothing goes wrong. EDA is one of the first steps to successful AL and ML. Before you even start thinking about algorithms, you need to understand the data. What happens in this phase will determine the course of the automation that takes place downstream. When done correctly, EDA will help you identify unwanted patterns and noise in the data and enable you to choose the right algorithms to leverage. In the EDA phase, you need to be actively inquiring about the data to ensure it’s going to behave as expected. As a start, below are 10 important questions to ask for a thorough analysis: These questions may lead to additional questions and even more after that. Don’t think of this as a checklist but as a jumping off point. And at the end of this process, you will be armed with a better understanding of the data patterns. You can then process the data correctly and choose the most appropriate algorithms to solve your problem. The underlying data is constantly changing, which means that a significant amount of time must be spent on EDA to make sure that the input features to your algorithms are consistent. For example, Airbnb found that nearly 70% of the time a data scientist spends on developing models is allocated toward data collection and feature engineering, which requires extensive data analysis to ascertain the structures and patterns. In short, if a company does not invest the time to understand its data, its AI and ML initiatives can easily spin out of control. Let’s look at an example from companies that have used data exploration effectively to develop and build successful data products. One of the most important aspects of digital services is cybersecurity and fraud detection, now a market valued at more than $30 billion and projected to reach more than $100 billion by the end of the decade. While there are tools such as Amazon Fraud Detector and PayPal’s Fraud Management Filters for general detection of online fraud, the only constant in fraud detection is that fraud patterns are always changing. Companies are constantly trying to stay prepared for new kinds of fraud while fraudsters are trying to innovate to get ahead. Every new kind of fraud may have a novel data pattern. For instance, new user sign-ups and transactions may be coming from an unexpected ZIP code at a rapid rate. While new users may come from anywhere, it would be suspicious if a ZIP code that was previously very quiet suddenly started screaming. The more difficult part of this calculus would be knowing how to flag a fraud transaction versus a normal transaction that occurred in that ZIP code. AI technologies can definitely be applied to find a model for fraud detection here, though you as the data scientist must first tell the underlying algorithm which sign-ups and subsequent transactions are normal and which ones are fraud. This can only be done by searching through the data using statistical techniques. You dissect the customer base to ascertain what distinguishes the normal customers from the fraudsters. Next, you would identify information that can help categorize these groups. Details may include sign-up information, transactions made, customer age, income, name, etc. You may also want to exclude information that would introduce significant noise into the downstream modeling steps; flagging a valid transaction as fraud could do more damage to your customer experience and product than the fraud itself. The frustrating (or fun, depending who you ask) part is that this EDA process must be repeated for all products throughout their life cycles. New fraudulent activities mean new data patterns. Ultimately, companies must invest the time and energy into doing EDA so that they can come up with the best fraud detection features to maintain their AI and ML pipelines. Understanding the data is the key to AI and ML success, not a vast repertoire of algorithms. In fact, businesses can easily fail when they force their data to fit their AI and ML pipelines rather than the other way around. Henry Li is Senior Data Scientist at Bigeye."
https://venturebeat.com/2021/04/24/ban-facial-recognition-in-europe-urges-eu-privacy-watchdog/,"Ban facial recognition in Europe, says EU privacy watchdog","(Reuters) — Facial recognition should be banned in Europe because of its “deep and non-democratic intrusion” into people’s private lives, EU privacy watchdog the European Data Protection Supervisor (EDPS) said on Friday. The comments come two days after the European Commission proposed draft rules that would allow facial recognition to be used to search for missing children or criminals and in cases of terrorist attacks. The draft rules, which need to be thrashed out with EU countries and the European Parliament, are an attempt by the Commission to set global rules for artificial intelligence, a technology dominated by China and the United States. The privacy watchdog said it regretted that the Commission had not heeded its earlier call to ban facial recognition in public spaces. “A stricter approach is necessary given that remote biometric identification, where AI may contribute to unprecedented developments, presents extremely high risks of deep and non-democratic intrusion into individuals’ private lives,” it said in a statement. “The EDPS will focus in particular on setting precise boundaries for those tools and systems which may present risks for the fundamental rights to data protection and privacy.” The Commission’s proposals have drawn criticism from civil rights groups, concerned about loopholes that may allow authoritarian governments to abuse AI to clamp down on people’s rights."
https://venturebeat.com/2021/04/24/thistle-tackles-iot-security-by-helping-vendors-update-devices/,Thistle tackles IoT security by helping vendors update devices,"Thistle Technologies emerged this week to tackle the problem of delivering security updates to the internet of things (IoT). The IoT market — which includes printers, edge devices, remote systems, consumer electronics, and automobiles — is booming, and security experts worry about the expanding attack surface. There are ways to update traditional networked devices, such as routers, cameras, and printers, but that isn’t the case for IoT. Each of these devices is now a mini-computer on the network, and a software vulnerability on any one of them means a network compromise. Once in, the attacker can move around looking for other systems to compromise and information to steal. Thistle, led by security veteran Window Snyder, launched on Thursday with $2.5 million in seed funding from True Ventures. The startup plans to address the vulnerability by helping IoT manufacturers securely and reliably deploy updates to their products. Thistle will build a framework for securing printers, ATMs, consumer electronics, and automobiles. The goal is to give embedded device manufacturers the ability to integrate updated mechanisms into their products. “Security-sensitive mechanisms, like updates, should be built and tested by an experienced security team,” the company said in a statement. Snyder has spent over 20 years making some of the biggest brands more secure. She worked in senior cybersecurity positions at Apple, Intel, and Microsoft and was chief security officer at Mozilla, Square, and Fastly. While at Microsoft, she contributed to the Security Design Lifecycle (SDL) and codeveloped the methodology for threat modeling software. She was also part of the effort to reduce Microsoft Windows’ attack surface and make the operating system more resilient to attack. That kind of resiliency is currently missing in the IoT space. If there is a vulnerability in sensors deployed over a large geographic area or in medical devices in a health care setting, the flaws remain unfixed until the system can be replaced. Many of these devices cannot be updated at all, or have a very difficult update mechanism, which means the owners are less likely to bother with the update. These vulnerable devices can cause a lot of problems beyond giving attackers a way to break into a target network. Botnets are networks of hijacked devices used to launch distributed denial-of-service (DDoS) attacks that flood websites and other online services with junk traffic to knock them offline. Last year, BitDefender researchers uncovered the “dark_nexus” botnet, which specifically preys on vulnerable IoT. The botnet compromised more than a thousand connected devices, including home and small office routers, thermal cameras, and video recorders from multiple vendors. Another IoT botnet, Mirai, launched a DDoS attack on internet infrastructure giant Dyn back in 2016 that was devastating enough to knock several major brands — including Shopify — offline and cripple parts of the internet for hours. There are many reasons it is difficult to securely update connected devices. The manufacturer may not know how to build resilience and security updates into its devices. When the goal is speed to market, the developers and engineers often prioritize features over security. Or the device may have limited processing power and memory — just enough to do the task it is designed to do, but not much else. In critical environments, restarting the devices to install updates may not be an option. And in situations where IoT is designed to be deployed over a large geographic area for long periods of time, delivering security updates can be a logistical challenge. Some devices are off-network most of the time and connect only briefly to send data, which may not be enough time to receive and install an update. And it’s a problem that’s just going to get bigger. IoT is well-entrenched in businesses, homes, and industrial plants. Current estimates peg the number of connected devices worldwide at around 25 billion, and that number is expected to explode with the rollout of 5G networks. Data from International Data Corporation (IDC) predicts there will be 55.7 billion connected devices worldwide by the end of 2025, of which 75% will be connected to some kind of IoT platform. “We’re making it easier for device makers to deliver on their security requirements,” Snyder said in a statement. “When the update mechanism is resilient and reliable, the business can leverage that beyond security fixes to provide updates for new features with confidence.”"
https://venturebeat.com/2021/04/24/cisos-must-help-their-boards-manage-cyber-risk-heres-how/,CISOs must help their boards manage cyber risk — here’s how,"In one of the more memorable scenes from the film “Jerry Maguire,” Tom Cruise’s character, a football agent, can be seen pleading with his one client, begging him to just “help me, help you.” Maguire kept repeating the line, hoping to break through to the player, trying to convince him to change his attitude in the hopes it would help him land a big contract from his team. This scene came to mind recently when I was thinking about the relationship between CISOs and their boards of directors. Cyber attacks on a corporation can exact a high price — in money, reputation, and lost business. CISOs battle day and night to prevent their company from suffering a crippling cyber attack, yet too often they don’t receive the help or support they need to properly execute their roles. As a result, CISOs often can’t get enough money to hire staff and purchase the systems that can prevent cyberattacks, can’t raise consciousness among executives to pay attention to cybersecurity issues, and can’t persuade boards of directors to focus more of their attention on cybersecurity needs. For CISOs today to be successful, therefore, their responsibilities must not only include building a robust cyber defense strategy on a limited budget but also convincing their corporate boards of directors — the group eventually responsible for their budget — that cybersecurity needs to be a budgeting priority. Yet, according to a report issued by consulting firm EY, the board is not engaged in the cybersecurity debate. In the report, nearly half of CISOs said their board “does not yet have a full understanding of cybersecurity risk,” and that just 54% of organizations regularly schedule cybersecurity as a board agenda item. How then, can CISOs convince their boards that cybersecurity spending needs to be a priority, and how should they express that need in a way boards can relate to? The first priority for CISOs to advance their objectives is to ensure that board members understand the business issues — and not just the IT issues — involved in cybersecurity, stressing the damage that a cyber attack can have on an organization. Using real-life case studies at quarterly board meetings will help drive the point home — such as the object lesson furnished by Yahoo’s 2013 data breach, perhaps the most expensive in history. That breach cost Yahoo $50 million in damages, paid to customers whose details were revealed; millions of dollars more in fees for free credit monitoring it agreed to supply victims as part of its settlement; and a $350 million discount in its sale price to Verizon. However, it is not enough for CISOs to highlight the potential damage a cyber attack can cause. Working with colleagues from across the company, they must also convincingly demonstrate the benefits that a robust cyber program can have for a business, stressing the opportunity to pursue additional revenue streams, target new customers, and upsell to existing clients. Along with the business aspects of cybersecurity, board members need to both better understand the threats and come to appreciate the steps required to mitigate those threats so they can make informed, strategic decisions for the business. CISO presentations to the board need to include a discussion of the constantly evolving threat landscape, with discussions focused on how hackers choose their victims, how they penetrate networks, which security systems are likely to prevent attacks, and how effective they are. Just as the CEO presents budget and corporate strategy reports to directors, CISOs should present security plans, with details on how security teams plan to defend the company and what they can do to minimize damage if an attack does take place. Once boards understand the technical issues, they will be able to understand the strategies presented to them — and weigh in on whether even more needs to be done. To further make their case to board members, CISOs should propose a formal governance structure — similar to what the board would use for other business objectives — that will allow for effective reporting and analysis of data. That structure should include periodic audits and reviews, assigning ownership, ensuring that funding is adequate to meet challenges and needs, and developing monitoring mechanisms and accountability systems with measurable KPIs. Members of a board of directors usually get to that position because of their business acumen. But in today’s cyber-environment, that business experience must be filtered through the lens of the potential impact a cyber event can have on a company. By helping their board of directors have a “cyber-first” mentality, CISOs will help themselves, allowing their company to develop a healthier and more robust cyber posture. Ronen Lago is CTO at CYE."
https://venturebeat.com/2021/04/24/waymos-leadership-shift-spotlights-self-driving-car-challenges/,Waymo’s leadership shift spotlights self-driving car challenges,"Waymo, Alphabet’s self-driving car subsidiary, has reshuffled its top executive lineup. John Krafcik, Waymo’s CEO since 2015, announced on April 2 that he would be stepping down from his role. Krafcik is being replaced by former COO Tekedra Mawakana and former CTO Dmitri Dolgov and will remain as an advisor to the company. “[With] the fully autonomous Waymo One ride-hailing service open to all in our launch area of Metro Phoenix, and with the fifth generation of the Waymo Driver being prepared for deployment in ride-hailing and goods delivery, it’s a wonderful opportunity for me to pass the baton to Tekedra and Dmitri as Waymo’s co-CEOs,” Krafcik wrote on LinkedIn. The change in leadership could have significant implications for Waymo, which has seen many ups and downs as it develops its driverless car business. This move also hints at the broader state of the self-driving car industry, which has failed to live up to its hype in the past few years. In 2015, Krafcik joined Google’s self-driving car effort, then called Project Chauffeur. At the time, there was a lot of excitement around deep learning, the branch of artificial intelligence that has made great inroads in computer vision, one of the key components of driverless cars. The belief was that continued advances in deep learning meant it was only a matter of time before self-driving cars became the norm on public streets. Deep learning models rely on vast amounts of training data to develop stable behavior. If the AI algorithms were ready, as it seemed at the time, reaching deployment-level self-driving car technology was only a question of having a scalable data-collection strategy to train deep learning models. While some of this data can be generated in simulated environments, the main training of deep learning models used in self-driving cars comes from driving in the real world. Therefore, what Project Chauffeur needed was a leader who had longtime experience in the automotive industry and could bridge the gap between carmakers and the fast-developing AI sector and deploy Google’s technology on roads. And Krafcik was the perfect candidate. Before joining Google, he was the CEO of Hyundai Motor America, had held several positions at Ford, and had worked in the International Motor Vehicle Program at MIT as a lean production researcher and consultant. With Krafcik at the helm, Project Chauffeur spun off as Waymo under Google parent Alphabet and quickly transformed into a leader in testing self-driving cars on roads. During this time, Waymo struck partnerships with several automakers, integrated Waymo’s AI and lidar technology into Jaguar and Chrysler vehicles, and expanded its test-driving project to more than 25 states. Today, Waymo’s cars have driven more than 20 million miles on roads and 20 billion miles in simulation, more than any other self-driving car company. Like the executives of other companies working on driverless car technology, Krafcik promised time and again that fully autonomous vehicles were on the horizon. In Waymo’s 2020 Web Summit, Krafcik presented a video of a Waymo self-driving car driving in streets without a backup driver. “We’ve been working on this technology a long time, for about eight years,” Krafcik said. “And every company, including Waymo, has always started with a test driver behind the wheel, ready to take over. We recently surveyed 3,000 adults across the U.S. and asked them when they expected to see self-driving vehicles, ones without a person in the driver’s seat, on their own roads. And the common answer we heard was around 2020 … It’s not happening in 2020. It’s happening today.” But despite Krafcik’s leverage in the automotive industry, Google’s crack AI research team, and Alphabet’s deep pockets, Waymo — like other self-driving car companies — has failed to produce robust driverless technology. The cars still require backup drivers to monitor and take control as soon as the AI starts to act erratically. The AI technology is not ready, and despite the lidar, radar, and other sensor technologies used to complement deep learning models, self-driving cars still can’t handle unknown conditions in the way humans do. They can run thousands of miles without making errors, but they might suddenly make very dumb and dangerous mistakes when they face corner cases, such as an overturned truck on the highway or a fire truck parked at the wrong angle. So far, Waymo has avoided major self-driving incidents like Tesla and Uber’s fatal accidents. But it has yet to deliver a technology that can be deployed at scale. Waymo One, the company’s fully driverless robo-taxi service, is only available in limited parts of Phoenix, AZ. After two years, Waymo still hasn’t managed to expand the service to more crowded and volatile urban areas. The company is also far from becoming profitable. Alphabet’s Other Bets segment, which includes Waymo, had an operating cost of $4.48 billion in 2020, against $657 million in revenue. And Waymo’s valuation has seen a huge drop amid cooling sentiments surrounding self-driving cars, going from nearly $200 billion in 2018 to $30 billion in 2020. While Krafcik didn’t explicitly state the reason for his departure, Waymo’s new leadership lineup suggests the company has acknowledged that the “fully self-driving cars are here” narrative is a bit fallacious. Driverless technology has come a long way, but a lot more needs to be done. It’s clear that just putting more miles on your deep learning algorithms will not make them more robust against unpredictable situations. We need to address some of the fundamental problems of deep learning, such as lack of causality, poor transfer learning, and intuitive understanding of physics. These are active areas of research, and no one has yet provided a definitive answer to them. The self-driving car industry also faces several legal complications. For instance, if a driverless car becomes involved in an accident, how will culpability be defined? How will self-driving cars share roads with human-driven cars? How do you define whether a road or environment is stable enough for driverless technology? These are some of the questions the self-driving car community will have to solve as the technology continues to develop and prepare for mass adoption. In this regard, the new co-CEOs of Waymo are well-positioned to face these challenges. Dolgov, who was Waymo’s CTO before taking on his new role, has a Ph.D. in computer science with a focus on artificial intelligence and a long history of working on self-driving car technology. As a postdoc researcher, he was part of the Stanford University self-driving car team that won second place in DARPA’s 2007 Urban Challenge. He was also a researcher at Toyota’s Research Institute in Ann Arbor, Michigan. And since 2009, he has been among the senior engineers in Google’s self-driving car outfit that later became Waymo. In a nutshell, he’s as good a leader you can have to deal with the AI software, algorithm, and hardware challenges a driverless car company will face in the coming years. Mawakana is a Doctor of Law and had led policy teams at Yahoo, eBay, and AOL before joining Waymo and becoming COO. She’s now well-positioned to tackle the legal and policy challenges Waymo will face as self-driving cars gradually try to find their way into more jurisdictions. The dream of self-driving cars is far from dead. In fact, in his final year as CEO Krafcik managed to secure more than $3 billion in funding for Waymo. And there’s a lot of interest in self-driving cars and their potential value. But Waymo’s new lineup suggests self-driving cars have a bumpy road ahead. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/23/4-reasons-to-learn-machine-learning-with-javascript/,4 reasons to learn machine learning with JavaScript,"In the past few years, Python has become the preferred programming language for machine learning and deep learning. Most books and online courses on machine learning and deep learning either feature Python exclusively or along with R. Python has become very popular because of its rich roster of machine learning and deep learning libraries, optimized implementation, scalability, and versatile features. But Python is not the only option for programming machine learning applications. There’s a growing community of developers who are using JavaScript to run machine learning models. While JavaScript is not a replacement for the rich Python machine learning landscape (yet), there are several good reasons to have JavaScript machine learning skills. Here are four. Most machine learning applications rely on client-server architectures. Users must send their data where the machine learning models are running. There are clear benefits to the client-server architecture. Developers can run their models on servers and make them available to user applications through web APIs. This makes it possible for developers to use very large neural networks that can’t run on user devices. In many cases, however, it is preferable to perform the machine learning inference on the user’s device. For instance, due to privacy issues, users may not want to send their photos, private chat messages, and emails to the server where the machine learning model is running. Fortunately, not all machine learning applications require expensive servers. Many models can be compressed to run on user devices. And mobile device manufacturers are equipping their devices with chips to support local deep learning inference. But the problem is that Python machine learning is not supported by default on many user devices. MacOS and most versions of Linux come with Python preinstalled, but you still have to install machine learning libraries separately. Windows users must install Python manually. And mobile operating systems have very poor support for Python interpreters. JavaScript, on the other hand, is natively supported by all modern mobile and desktop browsers. This means JavaScript machine learning applications are guaranteed to run on most desktop and mobile devices. Therefore, if your machine learning model runs on JavaScript code in the browser, you can rest assured that it will be accessible to nearly all users. There are already several JavaScript machine learning libraries. An example is TensorFlow.js, the JavaScript version of Google’s famous TensorFlow machine learning and deep learning library. If you head to the TensorFlow.js demo page with your smartphone, tablet, or desktop computer, you’ll find plenty of ready examples using JavaScript machine learning. They will run the machine learning models on your device without sending any data to the cloud. And you don’t need to install any additional software. Other powerful JavaScript machine learning libraries include ML5.js, Synaptic, and Brain.js. Privacy is not the only benefit of on-device machine learning. In some applications, the roundtrip of sending data from the device to server can cause a delay that will hamper the user experience. In other settings, users might want to be able to run their machine learning models even when they don’t have an internet connection. In these cases, having JavaScript machine learning models that run on the user’s device can come in very handy. Another important use for JavaScript machine learning is model customization. For example, suppose you want to develop a text generation machine learning model that adapts to the language preferences of each user. One solution would be to store one model per user on the server and train it on the user’s data. This would put extra load on your servers as your users grow and it would also require you store potentially sensitive data in the cloud. An alternative would be to create a base model on your server, create a copy on the user’s device, and finetune the model with the user’s data using JavaScript machine learning libraries. On the one hand, this would keep data on users’ devices and obviate the need to send them to the server. On the other hand, it would free up the resources of the server by avoiding to send extra inference and training loads to the cloud. And users would still be able to use their machine learning capabilities even when they’re disconnected from your servers. Another benefit of JavaScript machine learning is easy integration with mobile applications. Python support in mobile operating systems is still in the preliminary stages. But there is already a rich set of cross-platform JavaScript mobile app development tools such as Cordova and Ionic. These tools have become very popular because they enable you to write your code once and deploy it for iOS and Android devices. To make the code compatible across different operating systems, cross-platform development tools launch a “webview,” a browser object that can run JavaScript code and can be embedded in a native application of the target operating system. These browser objects support JavaScript machine learning libraries. One exception is React Native, a popular cross-platform mobile app development framework that does not rely on webview to run applications. However, given the popularity of mobile machine learning applications, Google has released a special version of TensorFlow.js for React Native. If you have written your mobile app in native code and want to integrate your JavaScript machine learning code, you can add your own embedded browser object (e.g., WKWebView in iOS) to your app. There are other machine learning libraries for mobile applications, such as TensorFlow Lite and Core ML. However, they require native coding in the mobile platform you are developing your app for. JavaScript machine learning, on the other hand, is very versatile. If you have already implemented a version of your machine learning application for the browser, you can easily port it to your mobile application with little or no changes. One of the main challenges of machine learning is training the models. This is especially true for deep learning, where learning requires expensive backpropagation computations over several epochs. While you can train deep learning models on user devices, it could take weeks or months if the neural network is large. Python is better suited for server-side training of machine learning models. It can scale and distribute its load on server clusters to accelerate the training process. Once the model is trained, you can compress it and deliver it on user devices for inference. Fortunately, machine learning libraries written in different languages are highly compatible. For instance, if you train your deep learning model with TensorFlow or Keras for Python, you can save it in one of several language-independent formats such as JSON or HDF5. You can then send the saved model to the user’s device and load it with TensorFlow.js or another JavaScript deep learning library. But it is worth noting that server-side JavaScript machine learning is also maturing. You can run JavaScript machine learning libraries on Node.js, the JavaScript application server engine. TensorFlow.js has a special version that is suited for servers running Node.js. The JavaScript code you use to interact with TensorFlow.js is the same you would use for applications running in the browser. But in the background, the library makes use of the special hardware of your server to speed up training and inference. PyTorch, another popular Python machine learning library, doesn’t yet have an official JavaScript implementation, but the open source community has developed JavaScript bindings for the library. Machine learning with Node.js is fairly new, but it is fast evolving because there is growing interest in adding machine learning capabilities to web and mobile applications. As the JavaScript machine learning community continues to grow and the tools continue to mature, it might become a go-to option for many web developers who want to add machine learning to their skillset. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/23/imperva-bad-bots-polluting-web-traffic-pose-security-risks-to-websites/,Imperva: Bad bots polluting web traffic pose security risks to websites,"Last year, 40.8% of all web traffic requests were not human, underscoring the growing scale of bot traffic across the Internet, Imperva Research Labs, a research group within security software and services company Imperva, said after an in-depth analysis of Internet traffic across every industry. If you couldn’t find a new gaming console to purchase online in late 2020, this report explains why. Through an analysis of billions of bot requests collected from Imperva’s global network, Imperva Research Labs’ 8th annual Bad Bot Report provides an in-depth look at the bot problem that now spans every industry and every region. Whether it’s a bot hoarding gaming hardware or a bot scraping for data as a means to collect and share helpful information with the public, when a site is polluted with automated traffic, it slows web performance and makes it harder for legitimate users to access the information or services they need. While some bot activity may appear benign, Imperva’s report shows that bot activity can be a business risk. Last year, 34% of all login attempts originated from malicious bots – those that closely mimic human behavior and are harder to detect and stop. This particular breed of bots should be a concern for businesses as they are most often responsible for content scraping, account creation, account takeover, denial of service and denial of inventory. The issue of bad bots is unlikely to slow down in the future. In 2020, bad bot traffic reached a record high — 25.6% of all web requests — while human traffic decreased by 5.7%. This is not a traditional security risk; it’s a 24/7 abuse of business’ websites, mobile apps and APIs. Read more in Imperva’s full report Bad Bot Report."
https://venturebeat.com/2021/04/23/gooddata-unveils-analytics-as-a-set-of-microservices-in-data-as-service-platform/,GoodData unveils analytics as a set of microservices in data-as-a-service platform,"GoodData this week unfurled a data-as-a-service platform that employs Docker containers and microservices running on Kubernetes clusters to dynamically scale analytics up and down on demand. The GoodData.CloudNative (GoodData.CN) platform heralds a new cloud-native era that enables easier embedding of analytics within applications. Key to that is a well-defined set of application programming interfaces (APIs), said Roman Stanek, CEO of GoodData. “It makes analytics much more flexible,” he said. Initially available for free via a community edition of the platform that comes in the form of a single Docker container image, GoodData also plans to make GoodData.CN available in Freemium, Growth, and Enterprise editions that come with additional capabilities, along with support from GoodData. Most existing analytics applications are based on monolithic architectures originally created for desktop PCs. These are not designed to dynamically scale up and down on demand. GoodData.CN takes advantage of the orchestration capabilities of Kubernetes to provide application developers with as much compute and storage resources as they can afford to consume, either via a public cloud or in an on-premises IT environment. The ability to deploy GoodData.CN anywhere is crucial because multiple centers of data gravity will always exist in the enterprise, noted Stanek. It’s unlikely any major enterprise is ever going to be able to standardize on a single data warehouse or data lake, he said. The GoodData.CN platform provides all the metadata capabilities required to maintain a single source of truth across what are rapidly becoming highly federated environments, noted Stanek. A programmable API also makes it feasible to deploy a headless data-as-a-service platform for processing analytics that can be readily accessed and consumed as a service by multiple applications. Previously, individual developers had to take the time and effort to embed analytics capabilities directly within their application, noted Stanek. The GoodData.CN platform makes applications more efficient and, as a consequence, smaller. That is because more analytics processing is offloaded to the headless platform, added Stanek. Pressure to embed analytics in every application is mounting as end users seek to make faster and better fact-based decisions. Rather than having to move data into a separate application to analyze it, Stanek said the GoodData.CN platform makes it simpler to infuse real-time analytics within an application. The need to embed analytics within applications is becoming more pronounced with the acceleration of various digital business transformation initiatives. The expectation is that next-generation applications will all provide some type of embedded analytics capability that enables end users to make better decisions in the moment versus long waits for a report prepared by a business analyst, Stanek said. In many cases, the query that was launched by a business analyst is no longer especially relevant by the time that a report can be delivered. GoodData is not likely the last provider of software that will be going cloud-native. A microservices-based application makes it easier to add new features and capabilities to software by ripping and replacing containers. It also makes applications more resilient. That is because, should any microservice become unavailable for any reason, calls are dynamically rerouted to other microservices to ensure redundancy. Most software developers are rapidly moving down the path to microservices as an alternative to monolithic applications that may be easier to build but that are increasingly viewed as being inflexible. In the case of GoodData, it’s not clear to what degree they may be ahead of rivals making similar transitions. However, enterprise IT organizations should expect in the months ahead a wave of headless services based on microservices architectures that will change the way data is consumed and managed."
https://venturebeat.com/2021/04/23/ai-weekly-mit-aims-to-reconcile-data-sharing-with-eu-ai-regulations/,AI Weekly: MIT aims to reconcile data sharing with EU AI regulations,
https://venturebeat.com/2021/04/23/now-is-the-time-for-a-transatlantic-dialog-on-the-risk-of-ai/,Now is the time for a transatlantic dialog on the risk of AI ,"Artificial intelligence is no longer the world’s darling; no longer the “15 trillion dollar baby.” Mounting evidence that AI applications can cause harm and pose risk to communities and citizens has lawmakers under pressure to come up with new regulatory guardrails. While the US government is deliberating on how to regulate big tech, all eyes are on the unbeaten valedictorian of technology regulation: the European Commission. This past Wednesday, April 21, the Commission released wide-ranging proposed regulation that would govern the design, development, and deployment of AI systems. The proposal is the result of a tortuous path that involved the work of a high-level expert group (full disclosure: one of us was a member), a white paper, and a comprehensive impact assessment. The proposal has already elicited both enthusiastic and critical comments and will certainly be amended by the European Parliament and the Council in the coming months, before becoming a final piece of legislation. It is, however, the first of its kind, and marks an important milestone. In particular, it sends a signal to regulators in the US that they will have to address AI as well, especially since the proposal underscores the need for AI risk assessment and accountability for both material and immaterial damage caused by AI — a major concern for both industry and society. The proposed regulation identifies prohibited uses of AI (for example, using AI to manipulate human behavior to circumvent users’ free will, or allowing “social scoring” by governments), and specifies criteria for identifying “high-risk” AI systems, which can fall under eight areas: biometric identification, infrastructure management, education, employment, access to essential services (private and public, including public benefits), law enforcement, and migration and border control. Whether or not an AI system is classified as “high-risk” depends on its intended purpose and its modalities, not just the function it performs. When an AI system is “high-risk,” it will need to undergo a pre-deployment conformity assessment and be registered in a to-be-established EU database. The focus on transparency in the proposed regulation is laudable and will change industry practice. Specifically, the new regulations would emphasize thorough technical documentation and recording a technology’s intentions and assumptions. But the strategy of pre-classifying risk has a blindspot. It leads the Commission to miss a crucial feature of AI-related risk: that it is pervasive, and it is emergent, often evolving in unpredictable ways after it has been developed and deployed. Imposing strict procedures on a subset of AI systems and checking them mostly while they are still “in the lab,” may not capture the evolution of risks emerging from the interaction between AI systems in the real world and the evolution of their behaviour over time. The Commission’s proposal contains provisions for post-market surveillance and monitoring, but these provisions appear weaker than the pre-deployment ones. As it stands, the Commission’s proposal relies heavily on the development of algorithmic auditing practices by so-called “notified bodies” and in the private sector as a whole. Auditing practices, ideally, should be consistent across the markets and geographies where an AI system is deployed; it should also be oriented towards the main requirements of so-called  “trustworthy AI,” and be grounded in principles of equity and justice. The spotlight is now on US regulators, as well as industry leaders. If they aren’t able to promise consistent auditing in US markets as well, that will impact the whole AI ecosystem. Instead of playing regulatory ping-pong across the pond, leaders on both sides of the Atlantic would benefit from initiating a research- and stakeholder-led dialog to create a transnational ecosystem focused on maximizing the impact of AI risk identification and mitigation approaches. At the moment, such a transnational approach is hindered by different cultural approaches to regulation, strong tech lobbying, lack of consensus on what constitutes AI risk assessments and AI auditing, and very different litigation systems. All these barriers can be overcome, and we can reap the real benefits of AI, if the European Commission’s proposal is taken as a cue to harmonize approaches across borders for the maximum protection of citizens. This dialog should focus on equity and impact, outlining optimal procedures for effective risk and audit documentation, and identifying what is needed from governments, civil society, and higher education to build up and maintain a transnational ecosystem of AI risk assessment and auditing. The benefits are obvious. Strong regulation would meet a strong technology research landscape. Rather than reconciling approaches after the fact, co-developing the regulatory approach from the outset and creating the preconditions for mutual learning would be far more effective. The renewed prospects for an  enlightened transatlantic dialog on digital issues are a one-time opportunity to make this happen. Mona Sloane is an Adjunct Professor at NYU’s Tandon School of Engineering and Senior Research Scientist at the NYU Center for Responsible AI. Andrea Renda is Senior Research Fellow and Head of Global Governance, Regulation, Innovation & Digital Economy at the Centre for European Policy Studies."
https://venturebeat.com/2021/04/23/the-5-jobs-you-need-to-consider-applying-for-this-week/,The 5 jobs you need to consider applying for this week,"Are you looking for a new role at the moment? If so, then you’re in luck today folks. We have compiled a list of some of the most exciting open roles all over the U.S. right now, in the hopes that you can land your dream job. You’re just a click away. After participating in our technology boot camp, you’ll get to dive immediately into building a product that is appreciated by hundreds of customers such as: Salesforce.com, Netflix, lululemon athletica, and many others. With every dev project you work on, you’ll create a direct impact on Workday’s product suite that pushes the latest innovations in cloud and mobile technology. Your contributions will be part of what makes Workday unique: the culture, the core values, the company meetings, the commitment to sustainability, the recognition programs, but most importantly, the people. You will collaborate with the Product Manager and QA counterparts on functional design and analysis of requirements, and design and develop products in a metadata-driven development environment. In this role, you will model excellent UX principles and mentor junior staff. The Staff UX Designer is an experienced individual contributor responsible for designing usable, accessible, and engaging user experiences for Visa’s digital product line. Although they would prefer candidates to be located in one of the Design Research hubs a– Austin, San Francisco or New York City — they are open to the possibility of a remote work arrangement should it suit all parties. The EBC Demo, Content and Technical Briefing Manager will manage the planning and execution of briefings, present solutions to customers, ensure the environment reflects Palo Alto Networks cybersecurity leadership through content and relevant demos, and serve as technical team lead. This role will liaise with Palo Alto Networks organizations including Sales, Product Marketing, and Product Management. This individual will maintain expertise on corporate strategy, products, and solutions in order to provide effective counsel to sales teams and effectively message to customers. As the customer advocate, this role ensures that the right message is being delivered to the customers at the right time to solve their most important security challenges. A Sr/Principal Thin Films Equipment Engineer is an integral member of the Technology Development Engineering team at Micron Technology and enables our development of tomorrow’s memory and storage technology! They develop groundbreaking semiconductor processing hardware — both internally with process engineering and externally with vendors — to enable novel process technology. They redefine what is possible within amazing R&D facilities. The Account Sales Manager (ASM) primary function is to perform outside sales of product inventory while driving an assigned route of accounts. The ASM is responsible for increasing product sales and placement of product displays at all large and small format “off premise” stores. ASM’s are responsible for upselling inventory, cooler space, and product displays. The primary role of the ASM is to effectively service all customers in a safe, productive, and professional manner to Red Bull executional standards. The successful candidate will manage the relationships with the customer contact for sale of Red Bull products at assigned RBDC accounts.Manage Red Bull products in assigned RBDC accounts. Good luck!"
https://venturebeat.com/2021/04/23/stolen-identities-sold-in-criminal-marketplace-soared-250-since-2019/,Netacea: Stolen identity sales in criminal marketplace up 250% since 2019,"The number of stolen digital identities available on the Genesis Market has risen from 100,000 in April 2019 to over 350,000 in March 2021, with over 18,000 added each month, Netacea, the bot detection and mitigation specialist, said in new research into the world’s largest invite-only deep web marketplace for stolen information. The Genesis Market is an illegal online marketplace for stolen credentials. While many underground markets for stolen credentials operate from the anonymity of the dark web, Genesis Market is accessible from the open web. Access to the illegal marketplace is closely guarded by a strict invitation system, but once inside, users are presented with a well-organized one-stop-shop of stolen digital identities. This data takes the form of device fingerprints, which allow users to essentially wear the “mask” of their victim online, gaining access to all their online accounts whilst bypassing traditional anti-fraud and cybersecurity defenses. Cybercriminals target victims with malware and account takeover (ATO) bots to infiltrate their devices and harvest login credentials, as well as cookies, form autofill data and device fingerprints. These are then put up for sale on Genesis Market as packaged “bots” which are used to impersonate victims online. The asking price per bot can range from as little as $0.70 up to around $350 depending on the amount and nature of the data. The most expensive will contain financial details to allow access to online banking accounts. Upon purchase, consumers are provided with a custom browser to load the data into and are free to browse the internet masquerading as the hapless victim, use saved logins to access their accounts and – where login cookies exist – continue a victim’s session. All without any access to the original device. Read more in Netacea’s full report Buying Bad Bots Wholesale: The Genesis Market"
https://venturebeat.com/2021/04/23/tackling-the-endpoint-security-hype-can-endpoints-actually-self-heal/,Tackling the endpoint security hype: Can endpoints actually self-heal?,"Imagine that every endpoint on an IT network is self-aware — it knows if it’s under attack and immediately takes steps to thwart the attack. It then shuts itself down and autonomously rebuilds itself with new software patches and firmware updates. This is the promise of self-healing endpoints: endpoints that continually learn about new attack techniques while keeping their configurations optimized for network and security performance. Unfortunately, the reality does not match the hype. A self-healing endpoint is defined by its self-diagnostics, combined with the adaptive intelligence needed to identify a suspected or actual breach attempt and take immediate action to stop the breach. Self-healing endpoints can shut themselves off, complete a recheck of all OS and application versioning, and then reset themselves to an optimized, secure configuration. All these activities happen autonomously, with no human intervention. What differentiates self-healing endpoint offerings on the market today is their relative levels of effectiveness in deploying resilience techniques to achieve endpoint remediation and software persistence to the OS level. Self-healing endpoints with multiple product generations of experience have learned how to create persistence to the firmware, OS, and application layer of endpoint system architectures. This is distinguished from automated patch updates using scripts governed by decision rules or an algorithm. That doesn’t qualify as a true self-healing endpoint and is better described as endpoint process automation. The self-healing endpoint is one of the most overhyped areas of cybersecurity today, with over 100 vendors currently vying for a piece of the market. The anticipated growth of business endpoint security is feeding this frenzy. Gartner predicts the endpoint protection platform (EPP) market will grow 18.5% in 2021 and climb from an estimated $8.2 billion in 2019 to about $18.8 billion by 2024. By the end of 2025, more than 60% of enterprises will have replaced older antivirus products with combined EPP and endpoint detection and response (EDR) solutions that supplement prevention with detection and response capabilities. Taken in total, Gartner’s Top Security and Risk Management Trends for 2021 underscores the need for more effective EDR, including self-healing endpoints. Growth is also being driven by rapidly changing cybersecurity threats. The recent SolarWinds hack forever changed the nature of cyberattacks by exposing how vulnerable software supply chains are as a primary threat vector and showing how easily endpoints could be rendered useless by compromised monitoring systems. The hackers embedded malicious code during DevOps cycles that propagated across customers’ servers. These techniques have the potential to render self-healing endpoints inoperable by infecting them at the firmware level. The SolarWinds attack shows how server, system, and endpoint device firmware and operating systems now form a launchpad for incursions initiated independently of the OS to reduce detection. Endpoints that were sold as self-healing are still being breached, and current gaps in the effectiveness and reliability of endpoints must be addressed. Runtime protection, containment, and fault tolerance-based endpoint security systems were oversold under the banner of self-healing endpoints. In fact, many don’t have the adaptive intelligence to recognize a breach attempt in progress. Fortunately, newer technologies that rely on behavioral analytics techniques found in EDR systems, threat hunting, AI-based bot detection, and firmware-based self-healing technologies have proven more reliable. Further complicating the self-healing endpoint landscape is the speed with which EDR and EPP begin merging to form unified endpoint security stacks. The value of EDR/EPP within an endpoint security stack depends on how well cybersecurity vendors strengthen platforms with new AI and machine learning. EPP offers a prime example of the need for AI and machine learning. The primary role of EPP in an endpoint security stack is to identify and block malicious code that seeks to overtake control of endpoints. It takes a solid combination of advanced threat detection, antivirus, and anti-malware technologies to identify, stop, and then eradicate the endpoint threat. A knowledge base comprising fully documented adversary tactics and techniques provides tooling to truth-test self-healing endpoint claims. Known as MITRE ATT&CK, this knowledge base has captured and cataloged data from actual breach attempts, supplying the verifications teams need to test out self-healing endpoint security claims. The knowledge base for endpoint validation also benefits vendors, as it discloses whether an endpoint is truly self-healing. Using the MITRE dataset, cybersecurity vendors can discover gaps in their applications and platforms. MITRE ATT&CK’s 14 categories of adversarial tactics and techniques form a framework that provides organizations and self-healing endpoint vendors with the data they need to simulate activity cycles. MITRE sponsors annual evaluations of cybersecurity products, including endpoint detection and response (EDR), where vendors can test their solutions against the MITRE ATT&CK datasets. The methodology process is based on a design, execute, and release evaluation process. Simulations of APT29 attacks comprise the 2019 dataset and the Carbanak+FIN7 2020 dataset. Evaluations for 2021 are now open for Wizard Spider and Sandworm. The ATT&CK Matrix for Enterprise serves as the framework for evaluations of each vendor’s EDR capabilities. EDR and self-healing endpoint vendors create test environments that include detection sensors designed to identify, block, and prevent intrusions and breaches from the datasets MITRE provided. Next, MITRE creates a red team comprising emulated adversarial attacks. APT29-based data was the basis of the evaluation in 2019 evaluations and Carbanak+FIN in 2020 and Wizard Spider and Sandworm data. The test involves a simulation of 58 attacker techniques in 10 kill chain categories. MITRE completes attack simulations and relies on detection types to evaluate how effective each EDR solution is in identifying a potential attack. The detection times are classified into alerts, telemetry, or none generated. Microsoft Threat Defender 365 was able to identify all 64 active alerts and successfully identified eight MITRE attack categories from the Enterprise Matrix. The following is an example of the type of data generated based on the simulated MITRE attack scenario. MITRE ATT&CK data has come to influence self-healing endpoint product design. When cybersecurity EDR vendors test their existing self-healing endpoints against MITRE ATT&CK data, they often find areas for improvement and innovation. For Microsoft, 365 Defender’s advances in identifying credential access, initial access, and privilege escalation attack scenarios based on modeled data help improve Threat Defender analytics. Based on the cumulative lessons learned from three years of MITRE ATT&CK data evaluations, the most effective self-healing endpoints are designing in self-generative persistence, resilience, and adaptive intelligence. The three techniques delivering the best results are AI-enabled bots that threat-hunt and remediate self-healing endpoints, behavior-based detections and machine learning to identify and act on threats, and firmware-embedded persistence. Companies across all industries can successfully use automation bots to anticipate security threats, reduce help desk workloads, troubleshoot network connectivity issues, reduce unplanned outages, and self-heal endpoints by continually scanning network activity for any signs of a potential or actual breach. Throughout the pandemic, software vendors have fast-tracked much of their AI and machine learning-based development to help customers improve their service management, asset management, and self-healing endpoint security. In the case of Ivanti, a decision to base its latest IT service management (ITSM) and IT asset management (ITAM) solutions on its AI-based Ivanti Neurons platform reflects the way AI-based bots can contribute to protecting and self-healing endpoints in real time in the “Everywhere Workplace.” The goal with these latest innovations is to improve ITSM and ITAM so IT teams have a comprehensive picture of IT assets from cloud to edge. Ivanti’s product strategy reflects its customers’ main message that virtual workforces are here to stay. They need to proactively and autonomously self-heal and self-secure all endpoints and provide personalized self-service experiences to support employees working from anywhere, anytime. VentureBeat spoke with SouthStar Bank IT specialist Jesse Miller about how effective AI-based bots are at self-healing endpoints. Miller said a major goal of the bank is to have endpoints self-remediate before any client ever experiences an impact. He also said the bank needs to have real-time visibility into endpoint health and have a single pane of glass for all ITSM activity. “Having an AI-based system like Ivanti Neurons allows what I call contactless intervention because you can create custom actions,” Miller said. “We’re relying on Ivanti Neurons for automation, self-healing, device interaction, and patch intelligence to improve our security posture and to pull in asset data and track and resolve tickets.” SouthStar’s business case for investing in a hyper-automation platform is based on hours saved compared to more manual service desk functions and preemptive self-healing endpoint security and management. Below is an example of how self-healing configurations can be customized at scale across all endpoints. Continually scanning every artifact in Outlook 365, Microsoft Defender 365 is one of the most advanced self-healing endpoints for correlating threat data from emails, endpoints, identities, and applications. When there’s a suspicious incident, automated investigation results classify a potential threat as malicious, suspicious, or no threat found. Defender 365 then takes autonomous action to remediate malicious or suspicious artifacts. Remediation actions include sending a file to quarantine, stopping a process, isolating a device, or blocking a URL. The Microsoft 365 Defender suite, which provides autonomous investigation and response, includes a Virtual Analyst. Earlier this month, Microsoft made Microsoft 365 Threat Defender analytics available for public preview. Most recent threats, high-impact threats, and threat summaries are all available in a single portal view. Absolute Software offers an example of firmware-embedded persistence providing self-healing endpoints. The company’s approach to self-healing endpoints is based on a firmware-embedded connection that’s undeletable from every PC-based endpoint. Absolute’s customers say the Persistence technology is effective in remediating endpoints, providing resilience and autonomous responses to breach attempts. Dean Phillips is senior technology director at customer PA Cyber, one of the largest and most experienced online K-12 public schools in the nation, serving over 12,000 students based in Midland, PA. Phillips said it’s been helpful to know each laptop has active autonomous endpoint security running and that endpoint management is a must-have for PA Cyber. “We’re using Absolute’s Persistence to ensure an always-on, two-way connection with our IT management solution, Kaseya, which we use to remotely push out security patches, new applications, and scripts. That’s been great for students’ laptops, as we can keep updates current and know where the system is,” Phillips said. Such an agent enables capable endpoint management on student laptops, which he called “a big plus.” Absolute’s 2021 Q2 earnings presentation reflects how quickly the self-healing endpoint market is expanding today. Cybersecurity vendors all claim to have self-healing endpoints. Absolute Software, Akamai, Blackberry, Cisco, Ivanti, Malwarebytes, McAfee, Microsoft 365, Qualys, SentinelOne, Tanium, Trend Micro, Webroot, and many others attest that their endpoints can autonomously heal themselves. Separating hype from results starts by evaluating just how effective the technologies they’re based on are at preemptively searching out threats and removing them. Evaluating self-healing endpoints using MITRE ATT&CK data and sharing the results with prospects needs to happen more. With every cybersecurity vendor claiming to have a self-healing endpoint, the industry needs better benchmarking to determine how effective threat hunting and preemptive threat assessments are. What’s holding more vendors back from announcing self-healing endpoints is how difficult it is to provide accurate anomaly detection and incident response (IR) results that can autonomously track, quarantine, or remove an inbound threat. For now, the three most proven approaches to providing autonomous self-healing endpoints are AI-enabled bots, behavioral-based detections, and firmware-embedded self-healing technologies."
https://venturebeat.com/2021/04/23/ronovo-surgical-completes-series-a-financing-aims-to-transform-surgery-with-innovations-focused-on-simplicity-precision-and-intelligence/,"Ronovo Surgical Completes Series A Financing, Aims to Transform Surgery with Innovations Focused on Simplicity, Precision and Intelligence","SHANGHAI–(BUSINESS WIRE)–April 23, 2021– Ronovo Surgical, an emerging medtech company focused on innovating minimally invasive (MIS) and digital surgery to address the tremendous needs of the Chinese surgical market, recently announced successful closing of Series A financing. Developed by renowned surgeons, medtech veterans and robotics experts, Ronovo was founded in Shanghai in 2019 to establish a transformative technology platform that democratizes MIS and digital surgery in the vastly underserved Chinese surgical market. By focusing on the three pillars of simplicity, precision, and intelligence, Ronovo is positioning itself as a gateway to China for cutting edge MIS and digital surgery technology companies from around the world. The Series A financing was co-led by Matrix Partners China and Vivo Capital, with strong, continued support from seed investor Lilly Asia Ventures (LAV) and participation from GGV Capital. “With the successful closing of our Series A, we are extremely proud of the strong support for Ronovo’s vision and strategy from leading technology and life science investors, Matrix Partners China, Vivo Capital and GGV Capital, as well as the continued support from our seed investor, LAV,” said Dr. John Ma, Founder, Chairman and CEO of Ronovo. “With our core team already deep in R&D, we are now well-positioned to accelerate our technology development efforts and pursue multiple global strategic partnerships in support of accelerated path to commercialization.” “We firmly believe that our team is the key to strong competitive advantage of Ronovo Surgical,” said Dr. Ying Mao, Co-Founder and CTO of Ronovo. “With world-class talent from top medtech MNCs and industry leaders in robotics, our team is uniquely qualified to take on the mission of enabling China’s hospitals with digital surgery solutions to optimize patient clinical outcome and reduce health economic burden.” “Ronovo Surgical represents our key investment in the surgical robotics space,” said Roger Sun, Director at Matrix Partners China. “We are greatly optimistic about the enormous market potential for endoscopic procedures and surgical robotics in China. Under the experienced leadership of Dr. John Ma, Ronovo has already recruited a world-class R&D team and began working with renowned clinical KOLs. We look forward to seeing Ronovo democratize MIS with innovations that are tailored to clinical needs.” “I have had the pleasure and privilege of working with the Ronovo Surgical founding team since conception and company inception,” said Dr. Hongbo Lu, Managing Partner at Vivo Capital. “Vivo has made significant investments in the space outside of China. We are proud to support Ronovo and its exceptional team to achieve its vision of providing robotic solutions to the surgical market in China through innovative internal development and strategic partnerships.” “Robotics-driven digital surgery is clearly a rising trend for MIS, with great market potential and tremendous room for innovation both globally and in China,” said Dr. Yi Shi, Founding Managing Partner of LAV. “As a highly committed investor in the digital surgery space, we have backed Ronovo Surgical from incubation stage. We are proud of what the company has achieved so far and have great conviction of Ronovo’s team and mission. We are also very excited to work with the prestigious investors from Series A who share the same vision to support Ronovo’s exciting journey to leapfrog existing surgical technologies.” “We believe in the team’s strong focus on customers and global partnership approach, which not only augments their internal capabilities but also accelerates the realization of their technology platform vision to address many unmet clinical needs in surgery,” said Jenny Lee, Managing Partner of GGV Capital. “We would love to continue to support Ronovo Surgical to success.” About Ronovo Surgical Founded in 2019 and headquartered in Shanghai, Ronovo Surgical is built by industry veterans from global leaders in surgical and industrial robotics, such as Intuitive Surgical, Johnson & Johnson, Medtronic and KUKA. Aiming to transform how surgery is performed in China, Ronovo is leveraging robust R&D capabilities and strategic partnerships globally to accelerate the development of a broad portfolio of MIS and digital surgery solutions that exemplify the core themes of simplicity, precision, and intelligence. About Matrix Partners China Matrix Partners China is an early-stage venture capital firm in China that was founded in 2008. With biopharmaceutical and medical technologies as the fund’s most dedicated areas, Matrix Partners China is committed to developing long-term relationships with outstanding entrepreneurs and helping them build significant, industry-leading companies. About Vivo Capital Founded in 1996, Vivo Capital is a leading global healthcare investment firm with a diverse, multi-fund investment platform in venture capital, growth equity, buyout, and public equities. The firm has approximately $5.8 billion in assets under management and has invested in over 280 public and private companies worldwide. Headquartered in Palo Alto, California, the Vivo team consists of more than 50 multi-disciplinary professionals. Vivo invests broadly in healthcare across all fund strategies, including biotechnology, pharmaceuticals, medical devices, and healthcare services, with a focus on the largest healthcare markets globally. About Lilly Asia Ventures Lilly Asia Ventures (LAV) is a leading biomedical venture capital firm founded in 2008, with offices in Shanghai, Hong Kong, and Menlo Park. LAV’s vision is to become the trusted partner for exceptional entrepreneurs seeking smart capital and to build great companies developing breakthrough products that can treat diseases and improve human health. About GGV Capital GGV Capital is a global venture firm that invests in local founders, with investments in the United States, Canada, China, Southeast Asia, India, Latin America, and Israel from offices in Silicon Valley, San Francisco, Singapore, Shanghai, and Beijing. As a multi-stage, sector-focused firm, GGV Capital invests in seed-to-growth stage companies across three sectors: Social/Internet, Enterprise Tech, and Smart Tech. Over the past two decades, the firm has backed more than 400 companies around the world.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210423005102/en/ ZiHan LinVP of Business Developmentzihan.lin@ronovosurgical.com"
https://venturebeat.com/2021/04/23/the-role-open-cloud-plays-in-accelerating-innovation-vb-live/,The role open cloud plays in accelerating innovation (VB Live),"Presented by Supermicro There are three main components for cloud success: the right hardware, the best software stack, and the appropriate network choice. To learn why flexible, innovation-fostering open clouds are the answer, what’s needed to produce the best outcome, and more, join this VB Live event. Register here for free. During the pandemic, cloud has played a major role in helping companies navigate the technological challenges that came in rapid succession. And in response, those companies are realizing the need to accelerate their move to the cloud, pivoting from a long-term IT strategy to an urgent requirement, says Rick Villars, group vice president, worldwide research, at IDC. “Cloud isn’t a substitute for hardware, or SaaS software — in reality, cloud is a foundation that enables people to consume any new technology, as rapidly as possible,” Villars says. “Rather than having to spend five years to move to the next architecture, whether that’s a new chip design or a new software design, cloud is used as a way to take advantage of that new processor, that new algorithm, that new data set immediately, and do that at scale.” Having made that call, IT leaders are faced with some major choices. “The key job of the IT organization in a cloud world is governance, because cloud brings with it so much automation of the delivery of resources,” Villars says. “Administration and classic management can waste time and add complexity, but setting the right rules for governance and making sure they’re being carried out, letting the cloud systems themselves do the work, you can be much faster.” It means being that much more efficient, and gaining the benefits of scale — one of cloud computing’s most valuable benefits. The second major consideration, whether you’re modernizing your applications or developing new applications, is that it all comes down to data, he says. That boils down to three essential considerations: ensuring you have complete control over the data that you have, ensuring you have the ability to make that data available wherever it’s needed, and paying constant attention to new data sources that can improve your business strategy and IT stack. “We like to say that cloud is very data-driven — basically your justification for cloud investment comes down to how it helped you more effectively use your data,” he explains. “If you’re always keeping that in the back of your mind — thinking about any development or strategy in adopting this, and asking is this helping me achieve this goal — then you’re going to be in a real leadership position in making cloud part of your business.” When it comes to accelerating that shift to cloud, there are practical decisions that need to be made immediately. “For those applications that have been a core part of the business, they’re facing three choices,” Villars says. “Do I modernize that application? Do I shift to a re-architected solution, developed in a way that’s more cloud-enabled and more cloud-linked? Or do I just do a simple lift and shift?” In many cases, this kind of lift and shift can appear to be faster, though it doesn’t usually save a lot of money in the long term, primarily because of administrative hurdles, he says. But it does allow you to begin that first part of the transition, getting your data moved into the cloud, and allowing you to start strategizing about the longer term: how to modernize that application to leverage it as part of your future business needs. Once you’ve addressed that urgent need to shift to the cloud, the other big question is how do you begin to use it to innovate? “That’s absolutely where I want to start looking at cloud as a foundation for new application development, new services,” Villars says. “I want a platform that can grow over the next few years to let me accelerate my business transformation.” And for that, open cloud offers the most comprehensive, flexible solution, he says. While public cloud environments offer the types of data, resources, and capacity necessary for innovation, organizations also want to have the option to take advantage of capabilities in another cloud, or keep the ability to develop solutions within their own environments that are strategic or have specific data control sovereignty issues to it. And an open environment layers across all these different cloud areas. By leveraging more open technologies in terms of standard hardware foundation or as-a-service consumption models for these solutions, you accelerate that ability to use new technology as quickly as possible everywhere you need it. “It’s not an either/or choice. It’s more ‘How do we ensure that we get the full advantage of this new and critical resource without sacrificing our ability to innovate and do innovation where we want to in the long term?’” he says. “Is this open option allowing me to get faster access, whether it’s a new processor, new memory, new algorithm, new data set? That’s where the value has to be as IT leaders think about using any of these technologies.” Don’t miss out! Register here for free! Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/23/nokia-and-newcore-wireless-bring-5g-broadband-to-tribal-nations-in-u-s/,Nokia and NewCore Wireless bring 5G broadband to tribal nations in U.S.,"Tribal nations haven’t had the best access to the internet. But Nokia and NewCore Wireless have partnered to bring 5G wireless networking and 4.9G/LTE service to underserved communities in the U.S. The network uses the 117MHz part of the Tribal Educational Broadband Service (EBS) radio band, made available last year by the Federal Communications Commission (FCC), to serve tribal lands with a license for broadband and wireless networking buildouts. More than 400 Native American tribes will have access to this spectrum across tens of thousands of square miles, often in remote rural areas, putting broadband within reach of millions more people. The initial buildouts by Nokia and NewCore Wireless will reach 15,000 tribal members, many living in high-poverty areas. The companies will focus first on North and South Dakota, Oklahoma, and California. The Native American communities involved include the Standing Rock Sioux Tribe, the Cheyenne Tribe, and the Arapaho Tribe. The initiatives will support e-learning, telehealth, and remote work during the pandemic. This band of spectrum is very valuable and can be used to deploy carrier-grade voice or broadband connectivity. The 400 included tribes received this spectrum, and Nokia is using its wireless technologies for rural broadband connectivity, such as 4.9G/LTE and 5G. The company aims to provide rapid and cost-effective wireless connectivity across large areas and the scope for multiple home and business connections from a single base station. The private wireless network, which is based on 4.9G/LTE technology, opens the door to services such as high-speed internet for the home or business (up to 1Gbps); mobile phone options where cellular coverage is not available; and educational enhancements, such as distance learning. John Pretty Bear, councilperson for the Standing Rock Sioux Tribe’s Cannonball District, said in a statement that this will level the technology playing field for the Standing Rock Sioux Tribe. He noted that high-speed internet is coming at a time when such networks are the driving force behind health care, education, and commerce. The 2.5Ghz band of spectrum offered by the Tribal EBS program is mature and can be found in the majority of mobile phones, telephone switching equipment, and add-on devices in the market today. Because current networks are already designed for this spectrum, it allows new carriers to deploy services immediately — using already available hardware. Access to more than 100MHz of that spectrum is good for 4G and offers a viable transition into 5G when communities are ready. “In 2019, the FCC announced a new spectrum opportunity to pull back spectrum that wasn’t being used in the educational band,” NewCore Wireless GM Albert Kangas said in an interview with VentureBeat. “A large part of the country was what we call ‘white space,’ where it was not being leased to either a school or an institution.” The Native American groups applied to take over that EBS band, which the FCC approved. As the spectrum was capable of supporting both 4G and 5G wireless service, this was like giving 400 small companies the chance to become wireless operators. The groups began looking at ways to implement the service and found Nokia. The Nokia Digital Automation Cloud platform offers high-bandwidth, low-latency wireless connectivity, local edge computing capabilities, and applications such as voice and video services. Nokia senior VP Ed Cholerton said the tech was really created to serve enterprise markets such as industrial automation. It turns out it was useful in this particular consumer market, where it could provide a combination of fixed wireless and mobile phone service. It is a compact, easy-to-deploy platform, comprising private cellular network equipment and a cloud-based operation monitoring system. “We felt this was the best way to move forward,” Kangas said. “We proposed this to a few of our tribal partners. And we’ve built three different networks so far. We feel that this would be a great opportunity for other tribal nations across this country to use this solution to build out their wireless network.” One of the first networks approved was Standing Rock Telecommunications, which covers parts of North Dakota and South Dakota, Standing Rock Telecommunications general manager Fred McLaughlin said in an interview with VentureBeat. While other tribal nations are underdeveloped, Standing Rock already had a lot of infrastructure. Many of the areas have standard telecommunications and networks with towers. These were built by the tribes because most commercial companies chose not to develop networks in the sparsely populated areas. “We were in a fortunate situation, where we could utilize what we’ve already invested into ourselves,” McLaughlin said. “For us, it was plug and play. We just had to purchase equipment [like base stations and antennae], put it up on the existing tower, and then use the frequency.” McLaughlin said not many tribes have been aggressively setting up such networks but that the project turned out to be very cost-effective. There are towns with around 200 to 800 people spread across a region the size of the state of Connecticut. Previously, the tribes were using 3G networks for cellphones, getting 5 megabits or 10 megabits a second. Now it’s more like 60 megabits a second, capable of 100 megabits a second, Kangas said. “For tribal people in general, you’re starting to see a lot of people kind of come back home,” McLaughlin said. “For at-home education, it gives them an alternative. The schools are using our equipment. There is a fiber alternative, but some people can’t afford it. Some of these people are living — not paycheck to paycheck, but under the poverty level. They are finally getting connected to the internet. That’s who we are providing this service for.” Institutions such as schools and police departments can set up a private network and not have their traffic go out on the internet, which would slow down the network. “Thanks to the CARES Act, there is money available to build out the networks and there is a lot of greenfield opportunity to provide coverage out there, where none existed before,” Cholerton said. “It serves a good social need as well. Tribal members do not have to rely on moving away from their community to have a good-paying job.” He added, “We started off with this solution for enterprise customers, but it turns out to be really great. And it serves a good social need as well, in this case. Tribal nations are underserved.” “This is another way to achieve coverage in an underserved community,” Kangas said. “We hope to be doing a lot more of this.”"
https://venturebeat.com/2021/04/23/taiwan-predicts-its-chip-industry-will-weather-global-shortage/,Taiwan predicts its chip industry will weather global shortage,"(Reuters) — Taiwan’s key semiconductor industry has years of growth ahead of it with no worries about oversupply despite a massive capital investment program and only a few competitors in the next decade or so, a senior government minister said on Friday. Kung Ming-hsin, the head of Taiwan’s economic planning agency, the National Development Council, told Reuters the business opportunities presented by the global transformation to a digital economy were “very, very enormous”. Kung also sits on the board of Taiwan Semiconductor Manufacturing Co as a representative of the largest shareholder, the government’s National Development Fund, which holds around 6% of the stock of the world’s most valuable semiconductor company. He said between now and 2025, Taiwan companies have planned more than T$3 trillion ($107 billion) in investment in the semiconductor sector, citing expansion plans from chip giants including TSMC and Powerchip Semiconductor Manufacturing. “Once they are built, Taiwan’s competitors in semiconductors in the next decade will be very few,” Kung said in an interview in his office building, which overlooks the presidential office. Taiwan’s semiconductor firms are ramping up production to tackle a global chip shortage, which has affected everything from carmakers to consumer products, and meet booming demand following the work-from-home trend during the COVID-19 pandemic. Soaring demand is set to continue, driven by 5G, artificial intelligence and electric vehicles, Kung said. “In the next decade or even longer there won’t be oversupply for semiconductors,” he added, when asked if the massive investment plans could have a downside. Taiwan is currently in the grip of its worst drought in more than half a century, but Kung said the impact on chip firms was limited at present, citing the amount of water they are able to recycle and the location of their main factories in Hsinchu in northern Taiwan, and in the island’s south. “These two places are okay at the moment. So the impact on semiconductors is not bad.” Still, Taiwan does face other challenges, not least from China where President Xi Jinping has made semiconductors a strategic priority. Kung named Samsung Electronics as Taiwan’s most serious competitor and also able to match TSMC’s advanced chipmaking, but said U.S. tech restrictions had for now blunted the Chinese threat. Intel — both a TSMC client and competitor — last month announced a $20 billion plan to expand its advanced chip making capacity. Kung said there was perhaps room for TSMC to cooperate with Intel, but “what’s important is really how you upgrade yourself”. To that end, the government is helping the industry develop the next generation of semiconductor manufacturing technology like 1 nanometre and beyond with funding support and talent recruitment programmes in the works, he added. ($1 = 28.1070 Taiwan dollars)"
https://venturebeat.com/2021/04/23/analytics-startup-unsupervised-raises-35m-to-spot-patterns-in-enterprise-data/,Analytics startup Unsupervised raises $35M to spot patterns in enterprise data,"Boulder, Colorado-based Unsupervised, a big data analytics company leveraging AI to find patterns in business data, today announced that it raised $35 million in a series B round led by Cathay Innovation and Signalfire. Unsupervised says that it intends to use the funding to hire additional employees as it continues to develop its platform. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why the corporate sector has taken an interest in solutions that ingest, understand, organize, and act on digital content from multiple digital sources. Unsupervised claims to accomplish this by analyzing unstructured and structured datasets to arrive at insights “without ignoring the long tail.” The company automates data science processes including preparation and prioritization, making predictions on data in industries spanning transportation, supply chain, ecommerce, and sales and marketing. “We’re seeing a shift in the market where customers are seeking out analytics and AI platforms that don’t just do simple reporting — they reveal opportunities to change the business. BI and traditional AI is great for probing handfuls of known problems, but when you’re really trying to understand what’s happening you need to investigate beyond known issues,” CEO Noah Horton told VentureBeat via email. “This is where unsupervised learning is uniquely valuable. COVID really revealed the need for what we’ve built and this round will help us expand our footprint faster.” Unsupervised says that its AI can identify statistically significant patterns that highlight the differences across subgroups within the data. Using a technique called unsupervised learning or self-supervised learning, Unsupervised’s systems can generate labels from data by exposing the relationships between the data’s parts. That’s as opposed to traditional, supervised AI systems, which require annotated datasets in order to learn patterns and make predictions. For example, in the supply chain domain, Unsupervised’s AI can ostensibly look at the nuances of the local economy, logistics site, employee details, and shipments and inventory to spotlight areas with excess or insufficient supply. On the finance side, Unsupervised can drawn on databases to find fraud schemes and spot financial trends like where people are willing to spend versus save. The technology even has applications in health care, Unsupervised says, where it can reveal opportunities to minimize the time spent on administrative tasks. Unsupervised’s platform presents AI-discovered patterns to customers for review in a web dashboard. Teams can track the performance of these patterns over time, and the AI system learns from what’s prioritized and acted on to continuously improve the insights. Unsupervised isn’t disclosing many customers at this point. That said, the company volunteered that it has “a number” of Fortune 500 customers using the product, including teams at ADP, Disney, and Coatue. “Unsupervised’s customers use the platform for multiple use cases. The average customer is using the platform across three or more use cases. Some customers are supporting as many seven use cases with Unsupervised at one time,” a spokesperson told VentureBeat. In its recent Augmented Analytics Is the Future of Analytics report, Gartner predicts that by 2021, “augmented analytics” like Unsupervised’s will drive new purchases of analytics and business intelligence, as well as data science and machine learning platforms. Assuming this comes to pass, 75-employee Unsupervised’s prospects in the $168.8 billion business analytics market look bright — even in the face of competition from companies like Outlier. “Most companies recognize that data is the new ‘gold’ but still struggle to derive meaningful insights given the deluge of siloed data, both structured and unstructured, across organizations — exasperating teams that are already understaffed and overwhelmed,” Cathay Innovation cofounder and CEO Denis Barrier told VentureBeat. “However, Unsupervised’s unique approach to ‘AI-augmented analytics’ has the potential to be a game-changing tool. It is disrupting the entire process by ingesting data from everywhere and automating the time consuming, tedious portions so users can quickly draw the most interesting insights that are revenue-generating and actionable. We’re honored to support the company on their journey, which very well may usher in a transformation of big data and decision-making in the enterprise.” Eniac Ventures and Coatue also participated in the company’s latest funding round. It brings Unsupervised’s total raised to over $55 million following a $12.8 million series A round in August 2019."
https://venturebeat.com/2021/04/23/alpine-investors-acquires-outdoor-recreation-software-leader-aspira/,Alpine Investors Acquires Outdoor Recreation Software Leader Aspira," Alpine’s investment and partnership will accelerate organic and M&A-driven growth  SAN FRANCISCO & DALLAS–(BUSINESS WIRE)–April 23, 2021– Alpine Investors (“Alpine”), a people-driven private equity firm committed to building enduring software and services companies, announced today it has acquired Aspira (the “Company”), a market-leading software provider for the outdoor recreation industry, from Vista Equity Partners. Alpine’s partnership will accelerate organic growth across Aspira’s business lines and provide capital for strategic acquisitions. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210423005075/en/ Headquartered in Dallas with eight offices worldwide, Aspira’s mission is to cultivate a connected world through shared outdoor experiences. It employs over one thousand team members and provides a comprehensive suite of software used to manage all aspects of outdoor recreation access, including reservation, registration, licensing, and day use entry for public agencies and private entities. Aspira’s enterprise solutions enable its clients to manage multiple mission-critical activities using one centralized platform. Through ReserveAmerica.com-a leading booking site for public and privately-owned campground reservations-Aspira directly connects consumers with outdoor recreation experiences. Fraser Cameron will join Aspira as CEO, leveraging his deep experience leading passion community companies through rapid growth. Most recently serving as CEO of SmartPak Equine, Cameron brings a substantial history of operational, financial, and consumer subscription experience, including eight years leading the team at Velcro Companies in the roles of CEO and CFO. Further expanding the Aspira management team, Marlena Slowik and Graham Ballbach will assist in leading the Company during its next phase. Former CEO Mark Trivette and former President Seth Rosenberg will maintain leadership roles within the Company. Cameron, Ballbach, and Slowik all join Aspira through Alpine’s CEO program which brings supplemental leadership to its portfolio companies. “We are thrilled to welcome such a culturally-aligned and experienced leader in Fraser Cameron to the Aspira team,” said Trivette. “The Alpine team brings a wealth of knowledge and resources that will serve Aspira’s customers and team members exceptionally well throughout this next chapter of growth,” added Rosenberg. “I am excited to work alongside the talented Aspira team to build upon the great work Mark and Seth have done in leading the business. Our first objective is to listen to our team and customers as we implement growth initiatives throughout all lines of business-including significant investments in technology, marketing, and customer support,” said Cameron. “We’ve seen a surge of Americans seeking to explore the outdoors, especially over the past 12–18 months, and believe such growth will continue. Aspira has certainly benefitted from these market tailwinds, and we are delighted to partner with this dynamic Company-a true leader in an industry that brings so much joy to people by leveraging technology to connect them with nature,” said Billy Maguy, partner at Alpine. “We are committed to investing not just in the growth of the business, but also in the hardworking people who make it thrive.” LionTree Advisors acted as exclusive financial advisor to Vista in the transaction, and Kirkland & Ellis LLP served as its legal counsel. Wilson Sonsini Goodrich & Rosati served as legal counsel to Alpine. About Aspira Aspira provides connected experiences for the outdoor recreation industry. Its comprehensive suite of reservation and licensing technology and service solutions support federal, state, provincial, privately-owned, and local government parks, campgrounds, and conservation agencies, conveniently connecting them with outdoor adventure seekers from around the world. Aspira is headquartered in Dallas, Texas, with eight offices worldwide. For more information, visit www.AspiraConnect.com. About Alpine Investors Alpine Investors is a people-driven private equity firm that is committed to building enduring companies by working with, learning from, and developing exceptional people. Alpine specializes in investments in middle-market companies in the software and services industries. Its PeopleFirst strategy includes a CEO-in-Residence program which allows Alpine to bring proven leadership to situations where additional or new management is needed post-transaction. Alpine is currently investing out of its $1 billion seventh fund. For more information, visit http://www.alpineinvestors.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210423005075/en/ MiddleM Creative, on behalf of Alpine InvestorsJoanne Verkuilen, Managing Partnerjoanne@middlemcreative.com | 980.785.4356"
https://venturebeat.com/2021/04/22/challenges-of-applied-machine-learning/,The challenges of applied machine learning,"Every year, machine learning researchers fascinate us with new discoveries and innovations. There are a dozen artificial intelligence conferences where researchers push the boundaries of science and show how neural networks and deep learning architectures can take on new challenges in areas such as computer vision and natural language processing. But using machine learning in real-world applications and business problems—often referred to as “applied machine learning” or “applied AI”—presents challenges that are absent in academic and scientific research settings. Applied machine learning requires resources, skills, and knowledge that go beyond data science, that can integrate AI algorithms into applications used by thousands and millions of people every day. Alyssa Simpson Rochwerger and Wilson Pang, two experienced practitioners of applied machine learning, discuss these challenges in their new book Real World AI: A Practical Guide for Responsible Machine learning. Rochwerger, a former director of product at IBM Watson, and Pang, the CTO of Appen, draw on their personal experience and knowledge to provide many examples of how organizations succeeded or failed in integrating machine learning into their products and business models. Real World AI explains the common challenges and pitfalls of machine learning strategies and how product leaders can avoid repeating the failures of other organizations. Here are four of the key challenges that Rochwerger and Pang highlight in their book. Knowing the problem you want to solve is a challenge that applies to all software engineering tasks. Any experienced developer will acknowledge that “doing the right thing” is different from “doing the thing right.” In applied machine learning, defining the problem plays a crucial role in the choices you make for the technologies, data sources, and people who will be working on your product. “Only 20 percent of AI in pilot stages at major companies make it to production, and many fail to serve their customers as well as they could,” Rochwerger and Pang write in Real World AI. “In some cases, it’s because they’re trying to solve the wrong problem. In others, it’s because they fail to account for all the variables — or latent biases –that are crucial to a model’s success or failure.” Consider image classification problems. Deep neural networks can perform such tasks with stunning accuracy. But if you want to apply them to a real application, a detailed definition of the problem will determine the kind of model, data, talent, and investment you’ll need. For instance, if you want a neural network that can label the files in your image archive, there are plenty of pre-trained convolutional neural networks (e.g., ResNet, Inception) and public datasets (e.g., ImageNet and Microsoft COCO) that you can use out of the box. You can set up the deep learning model on your own server and run your images through it. Alternatively, you can sign up for an API-based service such as Amazon Rekognition or Microsoft Azure Computer Vision. In this case, inference will be done in the service provider’s servers. But suppose you’re working for a large agriculture company and want to develop an image classifier that runs on drones and can detect weeds in crops. Hopefully, the technology will help your company switch to precision application of herbicide to cut down costs, waste, and the negative effects of chemicals. In this case, you’ll need a more specialized approach. You’ll have to consider constraints on the machine learning model and the data. You need a neural network that is light enough to run on the compute resources of edge devices. And you’ll need a special dataset of labeled images of weed vs non-weed plants. In machine learning, defining the problem also includes determining how well you want to solve the problem. For example, in the case of image archive labeling, if your machine learning model mislabels five of every hundred images, you shouldn’t have much of a problem. But if you’re creating a cancer-detection neural network, then you’ll need a much higher standard. Every missed case can have life-impacting consequences. One of the key challenges of applied machine learning is gathering and organizing the data needed to train models. This is in contrast to scientific research where training data is usually available and the goal is to create the right machine learning model. “When creating AI in the real world, the data used to train the model is far more important than the model itself,” Rochwerger and Pang write in Real World AI. “This is a reversal of the typical paradigm represented by academia, where data science PhDs spend most of their focus and effort on creating new models. But the data used to train models in academia are only meant to prove the functionality of the model, not solve real problems. Out in the real world, high-quality and accurate data that can be used to train a working model is incredibly tricky to collect.” In many applied machine learning applications, public datasets are not useful for training models. You need to either gather your own data or buy them from a third party. Both options have their own set of challenges. For instance, in the herbicide surveillance scenario mentioned earlier, the organization will need to capture a lot of images of crops and weeds. For the machine learning model to work reliably, the engineers will need to take the photos under different lighting, environmental, and soil conditions. After gathering the data, they’ll need to label the images as “plant” or “weed.” Data labeling requires manual effort and is a tiring job and has given rise to an entire industry of its own. There are dozens of platforms and companies that provide data labeling services for AI applications. In other settings, such as health care and banking, the training data will contain sensitive information. In such cases, outsourcing labeling tasks can be tricky, and the product team will have to be careful not to run afoul of privacy and security regulations. Yet in other applications, the data might be fragmented and scattered across different databases, servers, and networks. When organizations are drawing data from various sources, they’ll face other challenges too, such as inconsistency between database schemas, mismatching conventions, missing data, outdated data, and more. In such cases, one of the main challenges of the machine learning strategy will be to clean the data and consolidate different sources into a data lake that can support the training and maintenance of the ML models. In cases where the data comes from different databases, verifying data quality and provenance is also crucial to the quality of machine learning models. “It’s incredibly common in an enterprise to find data scattered throughout databases in different departments without any documentation about where it’s from or how it got there,” Rochwerger and Pang warn. “As data makes its way from the point where it’s collected into the database where you find it, it’s very likely that it has been changed or manipulated in a meaningful way. If you make assumptions about how the data you’re using got there, you could end up producing a useless model.” Machine learning models are prediction machines that find patterns in data obtained from the world and forecast future outcomes from current observations. As the world around us changes, so do the data patterns, and models trained on past data gradually decay. “AI isn’t a ‘set it and forget it’ type of system that will keep churning out results without human intervention. It requires constant maintenance, management, and course-correction to continue to provide meaningful, desired output,” Rochwerger and Pang write in Real World AI. A stark example was the COVID-19 pandemic, which caused a worldwide lockdown and changed many living habits, which disrupted many machine learning models. For instance, as shopping transitioned from brick-and-mortar to online stores, machine learning models used in supply chain management and sales forecasting became obsolete and needed to be retrained. Therefore, a key part of any successful machine learning strategy is making sure you have the infrastructure and processes to collect a continuous stream of new data and update your models. If you’re using supervised machine learning models, you’ll also have to figure out how to label the new data. In some cases, you can do this by providing tools that allow users to provide feedback on the predictions made by the machine learning models. In others, you’ll need to label new data manually. “Don’t forget to allocate resources for the ongoing training of your model. Models have to be trained continually, or they’ll become less accurate over time as the real world changes around them,” Rochwerger and Pang write. In applied machine learning, your models will affect people’s work and lives (and your company’s bottom line). That’s why an isolated team of data scientists will seldom implement a successful machine learning strategy. “A business problem that can be solved by a model alone is very unusual. Most problems are multifaceted and require an assortment of skills — data pipelines, infrastructure, UX, business risk analysis,” Rochwerger and Pang write in Real World AI. “Put another way, machine learning is only useful when it’s incorporated into a business process, customer experience or product, and actually gets released.” Applied machine learning needs a cross-functional team that includes people from different disciplines and backgrounds. And not all of them are technical. Subject matter experts will need to verify the veracity of training data and the reliability of the model’s inferences. Product managers will need to establish the business objectives and desired outcomes for the machine learning strategy. User researchers will help to validate the model’s performance through interviews with and feedback from end-users of the system. And an ethics team will need to identify sensitive areas where the machine learning models might cause unwanted harm. “The nontechnical components of a successful AI solution are just as important, if not more important, than the purely technical skills necessary to build a model,” Rochwerger and Pang write. Applied machine learning also needs technical support beyond data science skills. Software engineers will have to help integrate the models into other software being used by the organization. Data engineers will need to set up the data infrastructure and plumbing that feed the models during training and maintenance. And the IT team will need to provide the compute, network, and storage resources needed to train and serve the machine learning models. “Even with a wonderful business strategy, a well-articulated, specific problem, and a great team, it’ll be impossible to achieve success without access to the data, tools, and infrastructure necessary to ingest each dataset, save it, move it to the right place, and manipulate it,” Rochwerger and Pang write. These are just some of the key challenges you’ll face in applied machine learning. You still need more elements to make your machine learning strategy work. In their book, Rochwerger and Pang discuss pilot programs, the “build vs. buy” dilemma, dealing with production challenges, security and privacy issues, and the ethical challenges of applied machine learning. The authors provide plenty of real-world examples that show how you can do things right and avoid botching your machine learning initiative. “There’s no reason to be afraid of AI. It’s not magic, and it’s not even rocket science. With hard work and the right team working together collaboratively, you can do this, and you can do it well,” Rochwerger and Pang write. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/22/intel-signals-aggressive-market-share-push-in-wake-of-improved-q1/,Intel signals aggressive market share push in wake of improved Q1,"Buoyed by strong continuing demand for PC semiconductors, Intel today announced flat non-GAAP revenue of $18.6 billion on a year-over-year basis that exceeded its previous guidance by $1 billion. Overall, Intel reported a net income of $5.7 billion, down 6% year over year. In addition to increased demand for PC semiconductors, which remain in short supply, Intel reported that the decline in demand for semiconductors used in enterprise servers has reached the bottom. Intel is forecasting increased sales of semiconductors in servers in the second half of the year as the COVID-19 pandemic subsides to the point that IT organizations begin investing in datacenters again. Intel launched 3rd Gen Intel Xeon Scalable processors, code-named Ice Lake, this quarter. And the company is banking on driving server refreshes in the second half of 2021. In addition, Intel is expecting to see increased demand from cloud service providers that are currently working through the massive amount of inventory they accumulated in 2020. The bulk of the revenue Intel is forecasting will be generated by 10-nanometer class processors in 2021. Intel is increasing its cadence for transitioning to 7-nanometer processors as part of an effort to regain processing power supremacy over rivals, Intel CEO Pat Gelsinger said. This was Gelsinger’s first call with industry analysts since returning to Intel after several years of leading VMware as its CTO and then CEO. “It’s amazing to be back at Intel, and Intel is back,” Gelsinger said. Intel also launched an Integrated Device Manufacture (IDM) 2.0 initiative this quarter to address the current processor shortage. The company is opening foundries to partners that build substrates and other components it depends on to build processors. Additionally, Intel is codesigning processors with cloud service providers. It expects cloud service providers to begin increasing orders from next quarter. In the meantime, strong demand for notebook PCs, in particular, has enabled Intel to weather the economic downturn brought on by the COVID-19 pandemic, as well as Apple’s decision to abandon Intel in favor of an M1 system-on-chip (SoC) architecture. The new M1 SoC architecture combines ARM CPUs with GPUs and other accelerators to deliver twice as much processing power as an x86 platform. As demand for other classes of processors starts to increase, along with PC components, Gelsinger has promised Intel will be very aggressive at the expense of rivals that can’t match its manufacturing muscle. In addition, Gelsinger notes that Intel processors are now optimized for new classes of workloads based on AI models that need to first be trained by processing massive amounts of data and then deployed using inference engines that require maximum processor performance. It’s not clear to what degree enterprise IT organizations are going to invest in 10-nanometer processor platforms when they know that systems based on next-generation 7-nanometer processors will become increasingly available in the second half of this year. Cloud service providers are also now making greater use of a wide array of processors to run workloads that might previously have been deployed on x86-based servers. Regardless of past missteps, Gelsinger said Intel is now better prepared to fight for control of every processor core being employed. Overall, Intel is now forecasting $17.8 billion in revenue in the second quarter. This is despite the efforts of rivals such as AMD and Nvidia, which are unable to meet demand for processing horsepower now being driven by everything from gaming sites to digital business transformation initiatives that continue to multiply as the global economy improves."
https://venturebeat.com/2021/04/22/bank-of-england-warns-of-potential-risks-from-cloud-data-providers/,Bank of England warns of potential risks from cloud data providers,"(Reuters) — The Bank of England might strengthen its controls on cloud data providers and other technology firms to counter possible risks to the stability of the financial system from the rise of fintech, Deputy Governor Dave Ramsden said. The Bank of England (BoE) has expressed concerns before about the reliance by financial firms, especially fintech startups, on third-party technology companies for key parts of their operations, and Ramsden said this scrutiny would intensify. “We plan to analyse further whether we need even stronger tools to manage the risk that critical third parties, including potentially cloud and other major tech providers, may pose to the Bank’s … objectives,” Ramsden told the Innovate Finance conference on Wednesday. Regulators globally have been tightening scrutiny of outsourced functions as they worry that core services financial firms provide to customers are vulnerable to outages at third parties. Britain’s government is keen to promote fintech as an area of growth and hopes that nimbler regulation will enable it to steal a march over the European Union, where British financial firms now have reduced access due to Brexit. The BoE has said it will not water down regulatory standards, but does see scope for more streamlined regulation of smaller banks and in some areas of insurance. On Monday, finance minister Rishi Sunak asked the BoE to work with the finance ministry on whether the central bank should set up a digital version of sterling to compete with cryptocurrencies, which he dubbed ‘Britcoin’. The government is also consulting over proposals to relax stock market listing rules due to a concern that Britain is less attractive than the United States as a listing venue, especially for tech companies whose founders want to keep an sizeable role. Ramsden said the BoE had taken a step to make life easier for smaller financial companies on Monday by giving firms more direct ways to access its high-value payments system, which is dominated by major banks and processing companies. Other steps included work standardising the identification of businesses involved in financial transactions, and looking at whether artificial intelligence could ease the burden of regulatory compliance."
https://venturebeat.com/2021/04/22/how-walmart-adapted-its-iot-strategy-to-the-pandemic/,How Walmart adapted its IoT strategy to the pandemic,"Walmart made $559 billion in total revenue during the COVID-19 pandemic’s first fiscal year, up from $514.4 billion in fiscal 2019, thanks in part to newly integrated internet of things (IoT) capabilities to improve food quality and lower energy consumption. Walmart claims its systems for IoT deployments are built at a scale unmatched across the retail industry: The company reports that, every day, it takes in approximately 1.5 billion messages and analyzes over one terabyte of data. This proprietary software includes a cloud-based dashboard application to manage volume and detect anomalous events, such as refrigeration failures, so they can ostensibly be fixed more quickly, saving ice cream from melting while driving corporate profit. VP of technology Sanjay Radhakrishnan oversees Walmart’s IoT platforms and applications. Radhakrishnan sat down with VentureBeat to describe the giant retail chain’s long-term data strategy and how it’s changed since last March to accommodate changing store ecosystems across the U.S. This interview has been edited for clarity and brevity. VentureBeat: How would you describe Walmart’s approach to IoT at a high level? Sanjay Radhakrishnan: When we started on this journey, we had three key objectives. One was to address this at the scale of Walmart’s, that Walmart can actually leverage the impact of IoT at Walmart scale. The second objective was to ensure that we are the control plane for our data. So, we control where our data lands, and we have the ability to convert into business insights. And then the last objective was really maintaining that connection to our end customer experience. And then ensuring that we are being good corporate citizens, with respect to our sustainability initiatives. So just want to set the stage that when we started on this IoT journey, those were the three main drivers that we were looking to solve. VentureBeat: I’m really interested in that IoT journey. Could you tell me more about how Walmart has evolved its tech platforms over the past couple of years? And what has that progression has looked like, maybe in the past 5, 10, even 15 years? Radhakrishnan: You know, with those three objectives in the background, we have always had all kinds of devices in our stores. And these devices typically come from vendors or original equipment manufacturers (OEMs) that actually manufactured these devices. Typically this equipment comes with some kind of an HDMI human machine interface that’s on the machine, so you can actually go connect to it and collect data out of these devices in a one-off fashion. And we’ve always done that. But with this IoT journey, what we really wanted to do was we wanted to move into the driver’s seat, where we can actually normalize these datasets coming from all these different machines, different devices, and different OEMs. We normalize that data, and we control our data using IoT from these devices and provide those data sets to our business in a way where we can actually convert them into useful information and useful insights and really improve that end customer experience. So our journey really has been, instead of individual point-to-point access from these individual machines, on how we can grow this at scale by being the control plane and getting all this data from equipment, normalize it, and simplify it into our language so that we can do intelligent things things with it, right? And so that focus is really shifted inside Walmart by building our own software that we are using to form that control. VentureBeat: That’s fascinating. And, in building this proprietary software at such a great scale, did Walmart run into any particular kinds of challenges or problems that it then worked to overcome? Radhakrishnan: The biggest challenge we have is just the variety of devices that we have in our ecosystem. They come from different OEMs, they are across different generations of these devices, and they all speak different languages. And what this means to us is, in our world, we are dealing with a wide mix of sensors, a range of protocols, and really a myriad of information models. So our approach has been to look at how we build our software and where we are talking to all these devices. You know, talking to the different protocols. But we have an ability to kind of normalize all of that data into one consistent IoT specification. That’s a Walmart IoT specification. And then we apply the right kind of data quality checks, so that we can certify the data and drop it into our control plane. And then we take it from there. So once we are able to connect the devices to our control plane, then we can land the data, either at the edge or the cloud. And afterward our software engineers can build all kinds of applications for our business customers. And we really kind of looked at this in a cloud-agnostic fashion. So we ensure that we have a dual-pronged strategy with our infrastructure. We leverage infrastructure in our own datacenters, and we also leverage infrastructure as a top cloud provider. The focus really has been to ensure that our IoT pipeline software can access the right infrastructure at scale, considering things like latency and connectivity concerns. VentureBeat: Within this group of devices you mentioned, are you including in-store ones like refrigeration systems from the ice cream case study? Radhakrishnan: Yeah, that’s right. So you walk into the store and you see a lot of refrigeration cases. We are talking about sensors that are actually inside these refrigeration cases. And they are connected to what we call controllers in the store. We are actually connecting into those controllers and pulling device telemetry signals. It’s a lot of operating functions that you’re getting out of the equipment, and we are getting it in a consistent manner, in a continuous stream, to do intelligent things. VentureBeat: For the refrigeration IoT tech, could I hear more about how that’s architected in the cloud? Are there any specific foods, like ice cream or frozen pizzas for example, that are easier or more difficult to maintain with the technology? Radhakrishnan: We stream from edge to the cloud, and we have different pathways in the cloud based on data usage patterns. Our IoT applications can access data across the edge and cloud to solve business problems. We are cloud agnostic and leverage a dual-prong strategy that includes access to infrastructure in our own datacenters and top cloud providers. And our focus has been to ensure that our IoT pipeline software can access the right infrastructure at scale, considering connectivity and latency constraints. The type of food in the refrigeration cases does not cause differing complexity of our system. VentureBeat: Do you have any statistics on whether Walmart food quality has been more consistent since IoT tech was implemented? I’m curious if there are any specific stores or products that have seen a particularly measurable difference. Radhakrishnan: What I’ll say is, our focus has been on how you drive operational efficiencies in the store. For example, when things go wrong in the store, technicians actually fix problems with this equipment that is in the store. So the focus has been on how you get the right technician to the right place at the right time so that we can proactively address issues. Because if you don’t, it could impact product quality. Since we have started this journey, just by looking at reference duration, we have been able to improve our refrigeration equipment health by an average of 30%. VentureBeat: On a related note, I remember reading about Walmart’s intention to limit energy consumption. Could you tell me more about how that energy approach is architected in the cloud? Are there any specific frameworks or data strategies that Walmart is using to accomplish that? Radhakrishnan: If you look at our architecture and our frameworks, I would say it’s everything from connecting to the devices to using sophisticated infrastructure and software that runs on the edge and actually knows how to connect to these devices while holding telemetry data. Now, it depends on our use cases. If they’re kind of low latency use cases, then we store data at the edge, and we have logic at the edge to fulfill those use cases. Otherwise, we are streaming data to the cloud. And in the cloud, we have multiple kind of patterns depending on data usage. We might extend the data into kind of a cold pack, or a one pack, and our IoT applications have the ability to access the data, either at the edge or in the cloud. They can basically build business applications and solve business problems. So, if you’re talking about frameworks and the technology stack, it’s a mix. We use Walmart homegrown and open standard frameworks like Spring and .NET Core, our device protocols. We can connect to devices all the way from BACnet to Modbus to serial communications to some of the more recent protocols like HTTP and Simple Mail Transfer Protocol (SMTP). If you look at the tech stack itself, typically our device drivers are written in Java, and the applications themselves are all ReactJS, not GS applications that use Linux-type operating systems. VentureBeat: I’m interested in hearing more about how individual elements of Walmart’s tech stack — choices like Spring tools, for example — specifically help with IoT deployments. How and why do specific tools work well for Walmart’s use cases, like scaling large volumes of data? Radhakrishnan: Messages are generated by the equipment (such as HVAC and refrigeration controllers) in the stores and processed by software on edge infrastructure. From the IoT edge infrastructure, messages are then sent to our cloud storage to be processed and consumed by software applications. We use a hybrid approach of edge and cloud computing depending on the type of data. The data is sent over our secured network to our proprietary solution that has multiple architectural components and micro services. We use a mix of Walmart internally developed and open standard frameworks like Spring Boot and .NET Core. Our strategy is to build our software to be cloud agnostic, so we use common frameworks and languages such as Java, Embedded C, React, Node JS, and Linux technologies. Our focus is really trying to make sure that we map the right technology to solve the right business problem. We always start with the customer in mind. What is the use case? What’s the business? How do our internal customers solve thinking of the end customer in mind, and then work our way back to what does that mean for tech and then what’s the right tech stack to actually fulfill that. So, I mean we are pretty open, and the focus really is on understanding the customer problem, and then marrying it to the right tech stack to solve that problem. VentureBeat: Could you tell me a little bit more about Walmart’s IoT developments in the last year, and how they’ve helped the chain adjust to the COVID-19 pandemic’s challenges? Radhakrishnan: The pandemic has definitely opened new proper business problems and use cases for us where IoT is extremely useful to leverage. For example, when the pandemic hit, we reduced hours in our stores, so our associates could restock inventory and sanitize stores for our customers. We have this system called Demand Response, which is one of the IoT applications that we have built in-house. And we were able to leverage that to a working model, where we can control the temperature settings in the stores to adjust to these new hours, and that  brought a lot of productivity to our associates. Instead of using more constrained and manual approaches, now they have an actual system, where they can do remote deployment of capabilities and really control our high-performance computing (HPC) systems in a remote manner at scale. From a productivity angle, it helped the business, and also from a sustainability angle, we were able to reduce the energy consumption on the grid. So to give you an example, our system was able to execute shredding events. We did it for about 200 sites, and we were able to save enough electricity to roughly power 20-plus U.S. households for a year. That gives you a scale for how we are giving back, both in terms of productivity for our associates and also in terms of sustainability. VentureBeat: How has Walmart’s existing IoT and tech infrastructure allowed for its engineers to create new capabilities, like the COVID-19 responses, so quickly? Radhakrishnan: Over the last few years, we have moved to the driver’s seat, where we built software that will normalize and control our data using IoT from these devices, converting the data to insights that the business can use in decision-making. We apply the right data quality checks to certify the data and bring the data into our control plane. We were able to incorporate real-time data streaming and improve the speed at which issues are identified and resolved in a highly accurate manner. Having this foundation in place has allowed us to quickly respond to external factors, like adjusting store hours overnight during the early response to COVID-19. Another recent example of the IoT technology allowing us to respond quickly would be in February, when the extreme cold weather impacted energy grids in numerous communities. We had the necessary controls in place for demand shedding already, so we were able to apply the tool in a new way that controlled HVAC heating set points and reduced our energy consumption. In less than two days, we used the technology to successfully reduce the HVAC energy consumption in almost 500 stores. VentureBeat: Are there any other digital platform technologies related to ML, blockchain, IoT, or ERP Walmart is deploying? And are there any in particular that Walmart wants to research next? Radhakrishnan: For our IoT use cases, we are looking at ways we can further improve the customer experience and our impact on the communities we serve. Through algorithms, we will continue to update our algorithms as we identify trends between what the data is telling us and how we should respond. Through equipment, we will identify other equipment that we can connect to that would provide a benefit to our customer for remote diagnostics and proactive maintenance."
https://venturebeat.com/2021/04/22/ally-partners-with-microsoft-to-explore-quantum-computing-use-cases-in-fintech/,Ally explores fintech products using quantum computing with Microsoft,"Microsoft today announced that fintech firm Ally will join the former’s Azure Quantum program to explore how quantum computing can create opportunities in the financial sector. The two companies say that they’ll apply research on quantum-inspired algorithms to understand ways they can be tapped to solve complex optimization challenges. Experts believe that quantum computing, which at a high level entails the use of quantum-mechanical phenomena like superposition and entanglement to perform computation, could one day accelerate AI workloads compared with classical computers. Scientific discoveries arising from the field could transform energy storage, chemical engineering, drug discovery, financial portfolio optimization, machine learning, and more, leading to new business applications. Emergen Research anticipates that the global quantum computing market for the enterprise will reach $3.9 billion by 2027. Using optimizers designed to run on classical hardware in the cloud, Ally says it’ll be able to explore quantum use cases without having to invest in specialized hardware. Collaborating with Microsoft as part of its Enterprise Acceleration Program, Ally plans to begin developing quantum subject-matter expertise and fostering relationships with an ecosystem of quantum computing partners. “We have been able to benefit from the deep experience of the Microsoft Quantum research team, learn about the types of problems quantum can help solve within the financial services industry, and begin developing quantum computing skills using Microsoft’s quantum development kit,” Ally wrote in a blog post. “Through this relationship, we are gaining the insight and experience from Microsoft’s leading researchers. Leveraging Microsoft’s Azure capabilities, Ally has access to the software languages, APIs, and infrastructure to build quantum skills. This will open many exciting opportunities and empower us to explore the use of quantum computing technology across the landscape of financial services [applications].” As for the problems Ally hopes to solve with quantum computing, they could include anything from knowing why a customer is likely to contact a call center to portfolio management and streamlining business processes, according to Microsoft. It’s Microsoft Quantum director Julie Love’s belief that future algorithms and quantum hardware might draw on Ally’s financial datasets to help professionals make decisions. For example, Monte Carlo, a popular method for analyzing risk in finance, requires many simulations to achieve high confidence. That’s because Monte Carlo simulations account for uncertainty and randomness by constructing probability distributions over many possible outcomes as opposed to one. In finance, Monte Carlo methods are applied to portfolio evaluation, planning, risk evaluation, and derivatives pricing. Quantum algorithms might improve these computations since quantum computers can run multiple scenarios simultaneously and, through quantum interference, reduce the error in simulation. “Technology has always played an important role in the financial sector, and it has been shown, time and time again, that those who lead with technology make the market … Even though large-scale quantum computers won’t be available in the near term, their future availability is something for which businesses across the board need to prepare,” Love said in a press release. “With access to Azure Quantum, Ally is preparing a quantum-ready workforce that will be able to leverage scalable hardware as it becomes available.” While work progresses toward viable quantum computing hardware, the private sector is increasingly investing in the technology. Last month, IBM announced it would install a quantum computer at the Cleveland Clinic, marking one of the first times the company has physically placed a quantum computer on-premises. And Microsoft previously partnered with global advisory Willis Towers Watson to experiment with ways quantum computing might assist the firm with its work in insurance, financial services, and investing. “Current modelling techniques to quantify risk require a huge amount of computing power, using thousands of computers over many hours,” Willis Towers Watson CEO John Haley said in a whitepaper. “Quantum computing offers us the chance to look at our clients’ problems in a different way. By focusing on how we would model the problems on quantum computers when they become available at scale, we are able to work with Microsoft to redefine the problems and speed up our solutions on existing hardware.” Microsoft launched Azure Quantum in private preview two year ago alongside a developer kit, compilers, and simulators. Partnerships with quantum hardware providers Honeywell, IonQ, or QCI enable developers in the program to use existing Microsoft products — like Visual Studio or the Quantum Development Kit — along with quantum computers. Quantum venture funding dipped 12% in 2020, but quantum investments rose 46%, according to CB Insights. The total amount raised in the sector reached $365 million that year."
https://venturebeat.com/2021/04/22/supervised-vs-unsupervised-learning-whats-the-difference/,Supervised vs. unsupervised learning: What’s the difference?,"At the advent of the modern AI era, when it was discovered that powerful hardware and datasets could yield strong predictive results, the dominant form of machine learning fell into a category known as supervised learning. Supervised learning is defined by its use of labeled datasets to train algorithms to classify data, predict outcomes, and more. But while supervised learning can, for example, anticipate the volume of sales for a given future date, it has limitations in cases where data falls outside the context of a specific question. That’s where semi-supervised and unsupervised learning come in. With unsupervised learning, an algorithm is subjected to “unknown” data for which no previously defined categories or labels exist. The machine learning system must teach itself to classify the data, processing the unlabeled data to learn from its inherent structure. In the case of semi-supervised learning — a bridge between supervised and unsupervised learning — an algorithm determines the correlations between data points and then uses a small amount of labeled data to mark those points. The system is then trained based on the newly-applied data labels. Unsupervised learning excels in domains for which a lack of labeled data exists, but it’s not without its own weaknesses — nor is semi-supervised learning. That’s why, particularly in the enterprise, it helps to define the business problem in need of solving before deciding which machine learning approach to take. While supervised learning might be suited for tasks involving classifying, like sorting business documents and spreadsheets, it would adapt poorly in a field like health care if used to identify anomalies from unannotated data, like test results. Supervised learning is the most common form of machine learning used in the enterprise. In a recent O’Reilly report, 82% of respondents said that their organization opted to adopt supervised learning versus supervised or semi-supervised learning. And according to Gartner, supervised learning will remain the type of machine learning that organizations leverage most through 2022. Why the preference for supervised learning? It’s perhaps because it’s effective in a number of business scenarios, including fraud detection, sales forecasting, and inventory optimization. For example, a model could be fed data from thousands of bank transactions, with each transaction labeled as fraudulent or not, and learn to identify patterns that led to a “fraudulent” or “not fraudulent” output. Supervised learning algorithms are trained on input data annotated for a particular output until they can detect the underlying relationships between the inputs and output results. During the training phase, the system is fed with labeled datasets, which tell it which output is related to each specific input value. The supervised learning process progresses by constantly measuring the resulting outputs and fine-tuning the system to get closer to the target accuracy. Supervised learning requires high-quality, balanced, normalized, and thoroughly cleaned training data. Biased or duplicate data will skew the system’s understanding, with data diversity data usually determining how well it performs when presented with new cases. But high accuracy isn’t necessarily a good indication of performance — it might also mean the model is suffering from overfitting, where it’s overtuned to a particular dataset. In this case, the system will perform well in test scenarios but fail when presented with a real-world challenge. One downside of supervised learning is that a failure to carefully vet the training datasets can lead to catastrophic results. An earlier version of ImageNet, a dataset used to train AI systems around the world, was found to contain photos of naked children, porn actresses, college parties, and more — all scraped from the web without those individuals’ consent. Another computer vision corpus, 80 Million Tiny Images, was found to have a range of racist, sexist, and otherwise offensive annotations, such as nearly 2,000 images labeled with the N-word, and labels like “rape suspect” and “child molester.” In machine learning problems where supervised learning might be a good fit but there’s a lack of quality data available, semi-supervised learning offers a potential solution. Residing between supervised and unsupervised learning, semi-supervised learning accepts data that’s partially labeled or where the majority of the data lacks labels. The ability to work with limited data is a key benefit of semi-supervised learning, because data scientists spend the bulk of their time cleaning and organizing data. In a recent report from Alation, a clear majority of respondents (87%) pegged data quality issues as the reason their organizations failed to successfully implement AI. Semi-supervised learning is also applicable to real-world problems where a small amount of labeled data would prevent supervised learning algorithms from functioning. For example, it can alleviate the data prep burden in speech analysis, where labeling audio files is typically very labor-intensive. Web classification is another potential application; organizing the knowledge available in billions of webpages would take an inordinate amount of time and resources if approached from a supervised learning perspective. Where labeled datasets don’t exist, unsupervised learning — also known as self-supervised learning — can help to fill the gaps in domain knowledge. Clustering is the most common process used to identify similar items in unsupervised learning. The task is performed with the goal of finding similarities in data points and grouping similar data together. Clustering similar data points helps to create more accurate profiles and attributes for different groups. Clustering can also be used to reduce the dimensionality of the data where there are significant amounts of data. Reducing dimensions, a process that isn’t unique to unsupervised learning, decreases the number attributes in datasets so that the data generated is more relevant to the problem being solved. Reducing dimensions also helps cut down on the storage space required to store datasets and potentially improve performance. Unsupervised learning can be used to flag high-risk gamblers, for example, by determining which spend more than a certain amount on casino websites. It can also help with characterizing interactions on social media by learning the relationships between things like likes, dislikes, shares, and comments. Microsoft is using unsupervised learning to extract knowledge about disruptions to its cloud services. In a paper, researchers at the company detail SoftNER, a framework that Microsoft deployed internally to collate information regarding storage, compute, and outages. They claim that it eliminated the need to annotate a large amount of training data while scaling to a high volume of timeouts, slow connections, and other product interruptions. More recently, Facebook announced SEER, an unsupervised model trained on a billion images that ostensibly achieves state-of-the-art results on a range of computer vision benchmarks. SEER learned to make predictions from random pictures found on Instagram profile pages. Unfortunately, unsupervised learning doesn’t eliminate the potential for bias in the system’s predictions. For example, unsupervised computer vision systems can pick up racial and gender stereotypes present in training datasets. Some experts, including Facebook chief scientist Yann LeCun, theorize that removing these biases might require a specialized training of unsupervised models with additional, smaller datasets curated to “unteach” specific biases. But more research must be done to figure out the best way to accomplish this. Between supervised, semi-supervised, and unsupervised learning, there’s no flawless approach. So which is the right method to choose? Ultimately, it depends on the use case. Supervised learning is best for tasks like forecasting, classification, performance comparison, predictive analytics, pricing, and risk assessment. Semi-supervised learning often makes sense for general data creation and natural language processing. As for unsupervised learning, it has a place in performance monitoring, sales functions, search intent, and potentially far more. As new research emerges addressing the shortcomings of existing training approaches, the optimal mix of supervised, semi-supervised, and unsupervised learning is likely to change. But identifying where these techniques bring the most value — and do the least harm — to customers will always be the wisest starting point."
https://venturebeat.com/2021/04/22/battery-ventures-leads-growth-investment-of-more-than-150-million-in-growing-subscription-management-sector/,Battery Ventures Leads Growth Investment of More Than $150 Million in Growing Subscription Management Sector," Investments in financial-software companies SaaSOptics and Chargify will help SaaS companies better automate, manage financial operations  SAN FRANCISCO–(BUSINESS WIRE)–April 22, 2021– Battery Ventures, a global, technology-focused investment firm, today announced it led a combined growth-equity investment of more than $150 million in two complementary, cloud-software platforms that manage billing and automate related financial functions including payments, revenue recognition and analytics for software-as-a-service (SaaS) companies. The two companies, SaaSOptics and Chargify, together are trusted by more than 2,000 customers and manage more than $10 billion in customer annual recurring revenue. Some of the world’s most high-profile SaaS brands use SaaSOptics and Chargify to power their subscription billing and financial operations. The Battery investment is intended to power growth at both companies and allow them to further invest in their products. Battery has extensive experience making growth-stage, financial-technology investments, including in companies selling cloud software to corporate CFOs, including Avalara, Coupa, Intacct (now part of Sage), AuditBoard, Soldo and Carta. Today, SaaS subscription management and revenue management services are growing rapidly given the broader transition to subscription-based software. The growth also reflects the push by many SaaS CFOs to streamline financial functions like billing, invoicing, and revenue recognition-and tap their internal financial data for key metrics to help them better serve customers and boost revenue. “We are thrilled to partner with both SaaSOptics and Chargify, each of which has built a powerful business in their respective financial-software markets,” said Chelsea Stoner, a Battery Ventures general partner. “Both companies’ growing businesses highlight the fact that today’s software companies require more robust billing and revenue solutions, driven in large part by new, usage-based, cloud models. We are excited to invest in more innovative products that help all software companies better manage their businesses.” SaaSOptics, based in Atlanta, Ga., is a B2B subscription management platform that specializes in automating the financial operations of SaaS companies by streamlining the order-to-revenue process and automating revenue recognition; invoicing and payments; and subscription analytics and metrics. Chargify, based in San Antonio, Tex. and also a leader in billing and subscription management for B2B SaaS, specializes in complex usage and events-based billing, subscription management, payment collections, and data management tools. “We are very excited to partner with Battery Ventures. SaaSOptics is committed to delivering end-to-end financial operations solutions that enable subscription commerce,” said Tim McCormick, CEO of SaaSOptics. “With Battery’s deep expertise and success in partnering with cloud-based financial management software companies, we see a huge opportunity to transform the subscription management market. This vision provides a complete financial operations platform across business models: B2B, B2C; regardless of billing structure: fixed price, tiered, seat-based or events-based and any combination of the above.” Paul Lynch, CEO of Chargify, added: “With Battery’s history of partnering with prominent SaaS businesses, the firm recognizes the massive potential in the subscription-management space. Industries are shifting to subscription, as well as usage and event-based business models, and our solutions allow those industries to monetize and optimize customer value.” *For more information about Battery Ventures and a complete list of Battery Ventures’ investments, click here. About SaaSOptics SaaSOptics is a subscription management platform that automates financial operations for growing B2B SaaS businesses. A cloud-based solution, the SaaSOptics platform allows businesses to pull accurate SaaS metrics and analytics quickly, scale billing and payments smoothly and automate GAAP/IFRS-compliant revenue recognition. Businesses built on SaaSOptics eliminate their risky dependency on spreadsheets and streamline financial operations. SaaSOptics is easy to use, trusted by investors, within reach for early-stage startups and provides a streamlined implementation process. About Chargify Founded in 2009, Chargify has helped thousands of businesses manage millions of offers that drive billions in annual revenue. Chargify removes billing bottlenecks and gives front, corner, and back-office teams the speed and flexibility to grow faster. Over the past decade, Chargify has continued to expand its offerings to address the complexities of the entire subscription lifecycle: recurring billing, subscription management, revenue retention, prepaid subscriptions, revenue operations, and events-based billing. The company has headquarters in San Antonio, Texas and Dublin, Ireland. Learn more about Chargify at www.chargify.com. About Battery Ventures Battery partners with exceptional founders and management teams developing category-defining businesses in markets including software and services, enterprise infrastructure, online marketplaces, healthcare IT and industrial technology. Founded in 1983, the firm backs companies at all stages, ranging from seed and early to growth and buyout, and invests globally from six strategic locations: Boston; San Francisco and Menlo Park, Calif.; Herzliya, Israel; London; and New York. Follow the firm on Twitter @BatteryVentures, visit our website at www.battery.com and find a full list of Battery’s portfolio companies here.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210422005899/en/ Battery VenturesRebecca Buckman, 650-703-0364becky@battery.com"
https://venturebeat.com/2021/04/22/is-poor-data-quality-undermining-your-marketing-ai/,Is poor data quality undermining your marketing AI?,"Marketing’s potential to deliver results relies on data quality, but data accuracy, consistency, and validity continue to be a challenge for many organizations. Inconsistent data quality is holding marketing teams back from converting leads into sales, accurately tracking campaign performance, and taking on the larger challenges of optimizing product mix and product/service revenue forecasts. The latest analytics, Account-Based Marketing (ABM), CRM, marketing automation, and lead scoring tools all provide real-time data capture and analysis. How the tools ensure consistent data quality directly impacts the quality of the AI and machine learning models the tools use. Marketing teams can’t deliver on their goals with bad data quality. For example, inaccurate prospect data clogs sales pipelines by slowing down efforts to turn marketing qualified leads (MQLs) into sales qualified leads (SQLs). Problems with data quality increase the odds of failure for AI initiatives such as predictive audience offers and promotions, personalization, AI-enabled chatbots for advanced service, and automated service recovery. A quarter of organizations attempting to adopt AI report an up to a 50% failure rate, IDC said recently. The leading causes of inconsistent data quality in marketing include problems with taxonomy and meta-tagging, lack of data governance, and loss of productivity. The most common reason AI and ML fail in the marketing sector is that there’s little consistency to the data across all campaigns and strategies. Every campaign, initiative, and program has its unique meta-tags, taxonomies, and data structures. It’s common to find marketing departments with 26 or more systems supporting 18 or more taxonomies, each created at one point in a marketing department’s history to support specific campaigns. O’Reilly’s The State of Data Quality In 2020 survey found that over 60% of enterprises see their AI and machine learning projects fail due to too many data sources and inconsistent data. While the survey was on the organization level, it would not be a stretch to assume the failure rate would be higher within marketing departments, as it’s common to create unique taxonomies, databases, and metatags for each campaign in each region. The larger, more globally based, and more fragmented a marketing department is, the harder it is to achieve data governance. The O’Reilly State of Data Quality Survey found that just 20% of enterprises publish information about data provenance or data lineage, which are essential tools for diagnosing and resolving data quality issues. Creating greater consistency across taxonomies, data structures, data field definitions, and meta-tags would give marketing data scientists a higher probability of succeeding with their ML models at scale. Up to a third of a typical marketing team’s time is spent dealing with data quality issues, which has a direct impact on productivity, according to Forrester’s Why Marketers Can’t Ignore Data Quality study. Inaccurate data makes tactical decisions harder to get right, which could impact revenues. Forrester found that 21 cents of every media dollar have been wasted over the last 12 months (as of 2019) due to poor data quality. Taking the time to improve data quality and consistency in marketing would convert the lost productivity to revenue. Too often, marketers and the IT teams supporting them rely on data scientists to improve inconsistent data. It’s time-consuming, tedious work and can consume up to 80% or more of the data scientist’s time. It is no surprise that data scientists rate cleaning up data as their least-liked activity. Instead of asking data scientists to solve the marketing department’s data quality challenges, it would be far better to have the marketing department focus on creating a single, unified content data model. The department should consolidate diverse data requirement needs into a single, unified model with a taxonomy rigid enough to ensure consistency, yet adaptive enough to meet unique campaign needs. Change management makes the marketer’s job easier and more productive because there is a single, common enterprise taxonomy. Data governance is key to solving this problem, and marketing leaders have to be able to explain how improving metadata consistency and content data models fits within the context of each team member’s role. After that, the marketing organization should focus on standardizing across all taxonomies and the systems supporting them. The bottom line is that inconsistent data quality in marketing impacts the team by jeopardizing new sales cycles and creating confusion in customer relationships. The ability to get AI and ML pilots into production and provide insights valuable enough to change a company’s strategic direction depends on reliable data. Companies will find their marketing campaigns’ future contributions to growth are defined by how the team improves data quality today."
https://venturebeat.com/2021/04/22/microsoft-details-the-latest-developments-in-machine-learning-at-gtc-21/,Microsoft details the latest developments in machine learning at GTC 21,"This article is part of the VB Lab Microsoft / NVIDIA GTC insight series. With the rapid pace of change taking place in AI and machine learning technology, it’s no surprise Microsoft had its usual strong presence at this year’s NVIDIA GTC event. Representatives of the company shared their latest machine learning innovations in multiple sessions, covering inferencing at scale, a new capability to train machine learning models across hybrid environments, and the debut of the new PyTorch Profiler that will help data scientists be more efficient when they’re analyzing and troubleshooting ML performance issues. In all three cases, Microsoft has paired its own technologies, like Azure, with open source tools and NVIDIA’s GPU hardware and technologies to create these powerful new innovations. Much is made of the costs associated with collecting data and training machine learning models. Indeed, the bill for computation can be high, especially with large projects — into the millions of dollars. Inferencing, which is essentially the application of a trained model, is discussed less often in the conversation about the compute costs associated with AI. But as deep learning models become increasingly complex, they involve huge mathematical expressions and many floating point operations, even at inference time. Inferencing is an exciting wing of AI to be in, because it’s the step at which teams like Microsoft Azure are delivering an actual experience to a user. For instance, the Azure team worked with NVIDIA to improve the AI-powered grammar checker in Microsoft Word. The task is not about training a model to offer better grammar checking; it’s about powering the inferencing engine that actually performs the grammar checking. Given Word’s massive user base, that’s a computationally intensive task — one that has comprised billions of inferences. There are two interrelated concerns: one is technical, and the other is financial. To reduce costs, you need more powerful and efficient technology. NVIDIA developed the Triton Inference Server to harness the horsepower of those GPUs and marry it with Azure Machine Learning for inferencing. Together, they help you get your workload tuned and running well. And they support all of the popular frameworks, like PyTorch, TensorFlow, MXNet, and ONNX. ONNX Runtime is a high-performance inference engine that leverages various hardware accelerators to achieve optimal performance on different hardware configurations. Microsoft closely collaborated with NVIDIA on the TensorRT accelerator integration in ONNX Runtime for model acceleration on NVIDIA GPUs. ONNX Runtime is enabled as one backend in Triton Server. Azure Machine Learning is a managed platform-as-a-service platform that does most of the management work for users. This speaks to scale, which is the point at which too many AI projects flounder or even perish. It’s where technological concerns sometimes crash into the financial ones, and Triton and Azure Machine Learning are built to solve that pain point. Creating a hybrid environment can be challenging, and the need to scale resource-intensive ML model training can complicate matters further. Flexibility, agility, and governance are key needs. The Azure Arc infrastructure lets customers with Kubernetes assets apply policies, perform security monitoring, and more, all in a “single pane of glass.” Now, the Azure Machine Learning integration with Kubernetes builds on this infrastructure by extending the Kubernetes API. On top of that, there’s native Kubernetes code concepts like operators and CI/CDs, and an “agent” runs on the cluster and enables customers to do ML training using Azure Machine Learning. Regardless of a user’s mix of clusters, Azure Machine Learning lets users easily switch targets. Frameworks that the Azure Machine Learning Kubernetes native agent supports include SciKit, TensorFlow, PyTorch, and MPI. The native agent smooths organizational gears, too. It removes the need for data scientists to learn Kubernetes, and the IT operators who do know Kubernetes don’t have to learn machine learning. The new PyTorch Profiler, an open source contribution from Microsoft and Facebook, offers GPU performance tuning for popular machine learning framework PyTorch. The debugging tool promises to help data scientists and developers more efficiently analyze and troubleshoot large-scale deep learning model performance to maximize the hardware usage of expensive computational resources. In machine learning, profiling is the task of examining the performance of your models. This is distinct from looking at model accuracy; performance, in this case, is about how efficiently and thoroughly a model is using hardware compute resources. It builds on the existing PyTorch autograd profiler, enhancing it with a high-fidelity GPU profiling engine that allows users to capture and correlate information about PyTorch operations and detailed GPU hardware-level information. PyTorch Profiler requires minimal effort to set up and use. It’s fully integrated, part of the new Profiler profile module, new libkineto library, and PyTorch Tensorboard Profiler plugin. You can also visualize it all Visual Studio Code. It’s meant for beginners and experts alike, across use cases from research to production, and it’s complementary to NVIDIA’s more advanced NSight. One of PyTorch Profiler’s key features is its timeline tracing. Essentially, it shows CPU and GPU activities and lets users zoom in on what’s happening with each. You can see all the operators that are typical PyTorch operators, as well as more high-level Python models and the GPU timeline. One common scenario that users may see in the PyTorch Profiler is instances of low GPU utilization. A tiny gap in the GPU visualization represents, say, 40 milliseconds when the GPU was not busy. Users want to optimize that empty space and give the GPU something to do. PyTorch Profiler enables them to drill down and see what the dependencies were and what events preceded that idle gap. They could trace the issue back to the CPU and see that it was the bottleneck; the GPU was sitting there waiting for data to be read by another part of the system. Examining inefficiencies at such a microscopic level may seem utterly trivial, but if a step is only 150 milliseconds, a 40-millisecond gap in GPU activity is a rather large percentage of the whole step. Now consider that a project may run for hours, or even weeks at a time, and it’s clear why losing such a large chunk of every step is woefully inefficient in terms of getting your money’s worth from the compute cycles you’re paying for. PyTorch Profiler also comes with built-in recommendations to guide model builders for common problems and possible. In the above example, you may simply need to tweak DataLoader’s number of workers to ensure the GPU stays busy at all times. Don’t miss these GTC 2021 sessions. Watch on demand at the links below: VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/22/nhost-is-an-open-source-firebase-rival-backed-by-githubs-founders/,Nhost is an open source Firebase rival backed by GitHub’s founders,"In a world where technical talent is at a premium, businesses have to tap into the broader technology ecosystem to help build and scale their digital products. In truth, most companies probably don’t care much how their software is constructed — as long as it has all the features and functionality needed to satisfy their target market. Against this backdrop, Swedish startup Nhost is setting out to expedite the development process with an open source backend-as-a-service (BaaS) platform that lets developers forget about the infrastructure and focus purely on the customer-facing frontend. With Nhost, companies can automate their entire backend development and cloud infrastructure spanning file storage, databases, user authentication, APIs, and more. “We remove a considerable amount of ongoing effort, time, and resources for tasks that are not directly related to the product our customers want to build,” Nhost CEO and cofounder Johan Eliasson told VentureBeat. “With Nhost, they can start building their customer-facing products after only one minute.” To help fund its next stage of growth, Nhost today announced it has raised $3 million in a round of funding led by Nauta Capital, with participation from some prominent angel investors, including GitHub founders Scott Chacon and Tom Preston-Werner and Netlify founders Christian Bach and Mathias Biilmann Christensen. Existing investor Antler also participated in the round. Even the biggest technology giants with the deepest pockets look externally to boost their technology stack. Open source software, for example, allows them to benefit from the scalability of community-driven projects. And using third-party APIs (application programming interfaces) also saves them having to develop every component of their application internally. Nhost and its backend infrastructure are a different proposition, but the idea is the same — to help companies offload some of their requirements to a third party with domain-specific expertise. The Stockholm-based startup was created in late 2019 with Eliasson as the sole founder, though he soon realized that building what is effectively an open source alternative to Google’s Firebase would be a tall order. After he met software engineer Nuno Pato at a startup accelerator program in early 2020, the duo officially became cofounders. The global backend-as-a-service market was pegged at $1.6 billion in 2020, according to some estimates, a figure that’s projected to rise to nearly $8 billion by 2027. Existing players such as Firebase claim major clients like Alibaba, the New York Times, Duolingo, Venmo, and Trivago, highlighting that it’s not just cash-strapped startups that want to outsource their backend management. One of Nhost’s major selling points is that it’s an open source project, meaning companies can do with it as they please, though Eliasson notes that the main benefits of its open source status are around “collaboration and transparency.” There are, of course, other players in the open source BaaS space, such as Back4App, Parse, Kinvey, and Kuzzle. But Nhost considers itself distinct on a number of grounds, chief among them the scope of its single-platform offering. Nhost offers all the required building blocks for modern software, including a PostgreSQL database, real-time GraphQL API (which is available for most major front-end frameworks, such as React, Flutter, and Vue), authentication, storage, and serverless functions that allow companies to deploy custom code. On top of that, Nhost offers a managed cloud incarnation with plans spanning hobbyists, professionals, and enterprises. “Our tech stack offers a unique combination of open source tools we haven’t seen anywhere else, plus a tremendous focus on the developer experience,” Eliasson said. “We believe that building robust and highly scalable applications should be fun, fast, and easy for everyone.” For now, Eliasson said most Nhost customers are “indie-hackers, startups, and agencies,” and it has around 110 paying customers. However, it has aspirations on the enterprise front, something its seed round should help support. “Our approach is bottom-up — indies, developers, startups, small and medium-sized teams first,” Eliasson explained. “Enterprise will have its own sales channel when the right time comes.” Nhost is in the process of rolling out enterprise-grade features, including support for single sign-on (SSO), audit logs, and ISO certificates, which have “already been requested by larger customers,” according to Eliasson. It’s easy to see why Nhost could prove popular for developer teams looking to spin up a quick prototype or minimal viable product (MVP), given that it removes much of the friction involved in launching even a semi-functional app. However, it’s worth noting that prototypes or MVPs are how most modern software starts out — which puts Nhost in a favorable position when the time comes for developers to ramp things up. “Nhost really shines for MVPs because the stack we chose makes that easy,” Eliasson explained. “That is important for us because there is very low friction for developers to start building, while the platform is scalable, flexible, and performant enough for when their apps get successful and need to scale.”"
https://venturebeat.com/2021/04/22/hubspot-adds-operations-hub-to-drive-revenue-ops-shift/,HubSpot adds Operations Hub to drive revenue ops shift,"HubSpot kicked off an ambitious effort to unify sales, marketing, and service processes with the launch of Operations Hub on Wednesday. The offering is directed at companies that are beginning to unify various operations teams within sales, marketing, and services groups in order to create a single revenue operations (Revenue Ops) organization, HubSpot VP Andy Pitre said. Operations Hub is an extension of the HubSpot customer relationship management (CRM) application through which those cross-functional teams can manage business workflows on an end-to-end basis for each customer, Pitre added. It’s available in Free, Starter, and Professional versions. In addition, the platform bi-directionally synchronizes data across applications such as Microsoft Dynamics and NetSuite. Citizen developers and power users can also extend those processes using low-code tools based on a JavaScript framework that HubSpot makes available. It also supports a Lambda serverless computing framework from Amazon Web Services (AWS) to orchestrate event-driven workflows. As those processes evolve, however, professional developers will have the option of working with their preferred development tool to further customize them, Pitre said. In effect, Operations Hub makes it possible to unify a flywheel of processes that all tie into the same content management system, Pitre added. “We’re opening up the hood of HubSpot,” he said. HubSpot is also working toward adding additional analytics and data management capabilities within each of the marketing, sales, and service hubs it provides. In the longer term, it may open up AI models it has created to end users that might wish to extend them, Pitre noted. Historically, sales, marketing, and customer service have been managed by organizations in isolated silos. But that is changing. Now it has become more common for customer service representatives to upsell customers. Sales representatives are also increasingly crafting their own content marketing campaigns to drive leads. Meanwhile, marketing teams are running ecommerce sites that enable end customers to acquire products and services without ever engaging a sales representative. Revenue operations break down the silos to work with a single pipeline. The HubSpot CRM application provides a system of record that enables all those customers to be engaged in a consistent fashion, Pitre said. Just about every provider of a CRM platform is now trying to drive some level of unification across sales, marketing, and service functions. As a provider of a CRM platform that many organizations already rely on to drive marketing automation, HubSpot sees an opportunity to expand its reach. Many organizations are just coming to terms with Revenue Ops as a concept, and it’s not always clear who is in charge of the area. However, the title chief revenue officer is starting to supplant more mundane titles, such as vice president of sales. Regardless of how or even to what degree organizations embrace Revenue Ops, the whole sales and marketing motion within organizations is being transformed. Much of the impetus for that shift is being driven by the need to either launch or accelerate digital business transformation initiatives in the wake of the economic downturn brought on by the COVID-19 pandemic. It wasn’t always easy to get an appointment with customers prior to the pandemic. These days, it’s often next to impossible. The HubSpot operations effort comes as enterprises continue moving to online channels. Organizations of all sizes now depend on online engagements with customers in place of sales teams that travel to visit a customer. Not only are many of the individuals that sales teams used to regularly visit in person now working from home, such visits are discouraged due to the pandemic."
https://venturebeat.com/2021/04/22/yseop-recognized-as-best-ai-product-enterprise-at-the-technical-analyst-awards-2021/,Yseop Recognized as “Best AI Product: Enterprise” at The Technical Analyst Awards 2021," Yseop has won the Best Enterprise AI Product award, courtesy of The Technical Analyst Awards. Founded in 2008, Yseop is a pioneer in Natural Language Generation (NLG), intelligent report automation and a world-leading AI software company.  PARIS–(BUSINESS WIRE)–April 22, 2021– The Technical Analyst Awards are the only awards devoted to technical analysis research, data and trading software for the institutional market. Now in its thirteenth year, the Awards are highly regarded within the financial markets, attracting participation from hundreds of banks, research houses and software companies across the globe. Award categories are assessed by an independent panel of judges, who identify the winners based on excellence. “We are delighted to receive the recognition from these awards, which is testament to the hard work of our colleagues and best in class intelligent report automation solution – Augmented Financial Analyst,” said Emmanuel Walckenaer, CEO of Yseop. Financial controllers and analysts spend 45% of their time interpreting and analyzing data, 48% building and writing reports, and the remaining 7% communicating and interacting with the business. Yseop has over 50,000 users who use its no-code and AI-based Intelligent Report Automation, Augmented Financial Analyst (AFA) to save time. AFA automatically transforms complex data sets into high-quality narrative reports with actionable insights – all reliably, efficiently and at scale. With just a few clicks, analysts can seamlessly create and automate text-based reporting from all structured financial data such as balance sheets, profit and loss, financial statements, and more. Thanks to Yseop’s solution that removes the risk of human error or any room for interpretation by providing automated written and explained reports on achieved results, companies can gain greater productivity and lower their costs. Augmented Financial Analyst is designed for large-scale enterprise-level deployment, making it easy to generate hundreds of financial reports on demand. With an immediate and high return on investment (ROI), businesses use Yseop’s solution to solve many pain points across different departments. ENDS About Yseop: Founded in 2008 and based in North and South America, and Europe, Yseop specializes in artificial intelligence (AI) and is a recognized pioneer in Natural Language Generation (NLG) technology. Yseop is rapidly expanding globally, providing enterprise-level automation solutions for some of the world’s largest companies in a variety of industries including finance (Credit Agricole, Factset, BNP Paribas), pharmaceuticals (Sanofi) and computer software company (Oracle). Yseop also partners with strategic consulting firms and system integrators including CapGemini, Accenture and LTI, who support the adoption and deployment of Yseop’s NLG solution. With its multi award-winning Language AI technology, Yseop is revolutionizing the way analysis and reporting is done. Yseop’s powerful and user-friendly Augmented Analyst platform allows business users to seamlessly and quickly build and automate the generation of text reports from any structured data. At Yseop, we exist to support companies through this digital transformation. We believe that our cutting-edge artificial intelligence technology allows businesses to increase the efficiency of their operations and enables people to accomplish less tedious tasks and allows them to use that saved time to do more added-value and creative work. Find out more at https://yseop.com Find out more about Augmented Financial Analyst here: https://www.yseop.com/solutions/augmented-financial-analyst  View source version on businesswire.com: https://www.businesswire.com/news/home/20210422005578/en/ Press contact – Lise Grant – +33 6 99 65 71 91 – lgrant@yseop.com"
https://venturebeat.com/2021/04/22/deep-instincts-neural-networks-for-cybersecurity-attract-110m/,Deep Instinct’s neural networks for cybersecurity attract $100M,"The increasingly rich data companies are collecting makes them a more tantalizing target for attacks. But Deep Instinct wants to turn that same data into an enterprise’s greatest defensive asset. Deep Instinct is applying end-to-end deep learning to cybersecurity, an approach that allows it to predict and prevent cyberattacks across a company’s network, according to CEO Guy Caspi. Today, Deep Instinct announced it has raised $100 million in a round led by BlackRock. Other investors include Untitled Investments, The Tudor Group, Anne Wojcicki, Millennium, Unbound, and Coatue Management. The company has now raised a total of $200 million. The New York-based company is part of a growing wave of startups turning to machine learning and artificial intelligence to combat the rising number of cyberattacks. The industry is optimistic that this ability to automate defenses will help companies gain an edge against increasingly sophisticated and well-funded hackers. But Deep Instinct is trying to go a step beyond the way others are using AI and machine learning for security. The company has created deep neural networks that allow it to avoid using feature processing that can add an additional step and slow reaction time. With traditional machine learning, Caspi explained, executable files cannot be processed directly. Instead, they must be converted into a list of features that are then fed into a machine learning model. Deep Instinct’s end-to-end deep learning system uses the raw data as input without needing to convert it. The company trains its model in its own labs, rather than on the customer’s premises, by feeding it hundreds of millions of malicious and legitimate files. This huge-scale training workload relies on Nvidia GPUs. Once the training is finished, Deep Instinct creates a standalone neural network that can be deployed to an organization, where it starts protecting every device connected to the network. Because the system doesn’t require agents, it can be rapidly installed, including covering all applications currently running. And it can recognize previously unknown types of attacks without needing to be constantly updated. As a result, Deep Instinct claims it can identify and stop attacks within 20 milliseconds while reducing false positives by 99%. Caspi said he wants to use the latest funding to accelerate growth with an eye toward an IPO in the next couple of years. For now, that means ramping up sales and marketing, with about 30% of the money being reserved for product development."
https://venturebeat.com/2021/04/22/software-product-planning-platform-productboard-raises-72m/,Software product planning platform Productboard raises $72M,"Productboard, a startup developing a DevOps orchestration system for enterprises, today announced that it raised $72 million in a series C round. The company says that the funds will be put toward expanding its team and customer base while supporting product research and development. An estimated 19% to 23% of software development projects fail, with that statistic holding steady for the past couple of decades, according to data compiled by Ask Wonder. Standish Group found that “challenged” projects — i.e., those that fail to meet scope, time, or budget expectations — account for about 52% of software projects. Often, a lack of user involvement, executive support, and clear requirements are to blame for missed benchmarks. Productboard was founded in 2014 by Hubert Palan and Daniel Hejl, who sought to build a service that could tackle some of the most common DevOps challenges. The platform enables companies to create single product feedback repositories and prioritize what to build next, ostensibly helping to mitigate bottlenecks in the creation process. “Product teams often use an assortment of PowerPoint, email, Post-It notes, Slack, and other generic task management and engineering tools,” Palan told VentureBeat via email. “While a lot of these tools are free, they fall short when it comes to delivering the structure, best practices, and flexibility provided by tools built specifically for product management. There are a few other roadmapping tools out there in the market, but they don’t offer a full-fledged, customer-centric product management platform. Productboard is the first enterprise-ready, customer-centric product management platform that organizes the product development process around customer insights, creates alignment between product, and go-to-market teams.” With Productboard, companies can consolidate support tickets, Slack messages, and sales conversations in a single dashboard.  Moreover, they can categorize product ideas, requests, and feedback to route back to product teams for resurfacing down the road. Insights can be highlighted in each piece of user research or feedback and linked to a related feature idea. Once approved by a manager, developers can indicate the importance of these features, rating them on a 0-3 scale. Productboard also lets teams share product roadmaps to which they can add features and custom filters. Each roadmap can be tailored to different audiences with leadership-, company-, delivery-, and customer-focused views. And roadmaps can be integrated with existing workflows in Trello, GitHub, Jira, and other DevOps platforms. Productboard’s other headlining feature is its product portal, which lets companies show which features are planned, what’s been launched, and user feedback. Product boards hosted by Productboard can surface top-requested features and update customers about features that they requested, or serve as an internal brainstorming board for developers. “Productboard uses its own product … to showcase planned features and receive feedback from customers and prospects, which includes ways the company is incorporating AI and machine learning into the product,” Palan said. “For example, Productboard is currently working on machine learning algorithms to auto-suggest existing features relevant to the customer feedback a user is processing within the platform. The keywords found in the features’ names and descriptions or in other insights and notes already linked to these features can drive these suggestions and the algorithm is constantly trained by product managers using the platform and users submitting structured feedback via the customer portal.” Productboard, which employs over 200 people, has indirect competition in a number of startups vying for a slice of the more than $3.42 billion DevOps tools market. For example, there’s Tasktop, which recently nabbed $100 million to turn DevOps metrics into visualizations at scale. Meanwhile, Harness coordinates DevOps and cloud spending across multiple platforms. But Productboard claims to have over 4,000 customers including Zoom, Microsoft, UiPath, and 1-800 Contacts. “In today’s incredibly fast-paced world, companies are in a race to bring their customers the best products possible. Yet, most product teams still don’t have a system of record that brings together customer insights to inform and drive their product strategy, prioritization, and roadmapping process,” Palan continued. “Productboard is building the first customer-centric product management platform and working to help companies worldwide deliver better digital product experiences.” Tiger Global Management led Productboard’s latest funding round with participation from existing investors Sequoia Capital, Kleiner Perkins, Index Ventures, and Bessemer Venture Partners. It brings the San Francisco-based company’s total raised to date to over $137 million."
https://venturebeat.com/2021/04/22/rapid7-bolsters-open-source-security-with-velociraptor-acquisition/,Rapid7 bolsters open source security with Velociraptor acquisition,"Cybersecurity company Rapid7 yesterday announced it has acquired Velociraptor, an open source platform focused on endpoint monitoring, digital forensics, and incident response. Terms of the deal were not disclosed. Founded in 2000, Rapid7 provides a range of security-focused tools spanning applications and the cloud, including vulnerability management, orchestration and automation, and detection and response. With clients such as Autodesk, First Republic Bank, Kimberly-Clark, Hilton, and Univision and the pandemic driving digital transformation across industries, Rapid7 has been on a tear over the past 12 months. In fact, its share value has nearly doubled. Australian company Velocidex developed Velociraptor as an open source endpoint visibility tool in 2018. It’s designed to help digital forensics and incident response (DFIR) security teams proactively search for malicious activities across all devices and entry points to a network. With this deal, Velociraptor will be better positioned to receive direct and continued investment from a billion-dollar cybersecurity giant. Velocidex founder Mike Cohen added that Velociraptor will also receive greater exposure through conference and community events, which should increase participation in the project globally. “Rapid7 will enable Velociraptor to graduate to the ‘next level’ in terms of scale, development velocity, stability, and capability by drawing on a wide range of capable and experienced people to support the project,” Cohen wrote in a blog post. Boston-based Rapid7 has something of a track record in the open source security sphere, having acquired Metasploit back in 2009. There are benefits to pursuing a community-driven ethos in cybersecurity — essentially, the more eyeballs tethered to a piece of software, the more chances flaws or vulnerabilities will be found promptly. And the threat is urgent. In the past few months alone, at least two prominent security software providers have fallen victim to exploits. Fireye was reportedly hacked in a state-sponsored attack, and just this week cybersecurity company Sonicwall confirmed that some of its customers were targeted using a previously undisclosed vulnerability in its email security product. In truth, all software — open source or otherwise — can become vulnerable if it’s neglected. But open source holds greater potential for robust security, given that it draws on the collective wisdom of a community. This is why companies invest significant resources in supporting and maintaining mission-critical open source software. The Linux Foundation, for example, has set up the The Core Infrastructure Initiative (CII) with support from Amazon, Google, Microsoft, Intel, and others to ensure open source projects are sufficiently supported. And earlier this year, Google announced it would be funding the salaries for two developers to improve Linux’s security. In cybersecurity, specifically, attackers only need to get lucky once when searching for a weakness to exploit, whereas defenders have to cover all entry points to a network at all times. The fact that new vulnerabilities come to light on a daily basis highlights why a community-led (i.e. open source) approach to cybersecurity makes sense. With Velociraptor on board, Rapid7 said it will continue to build and work with the community around it, and — as you might expect — “leverage its technology and insights” to improve Rapid7’s own incident response abilities. According to Cohen, who now joins Rapid7 to continue leading the Velociraptor project, there are no immediate plans to commercialize Velociraptor directly."
https://venturebeat.com/2021/04/22/cluedin-raises-15m-to-grow-its-data-prep-and-analytics-platform/,CluedIn raises $15M to grow its data prep and analytics platform,"Data management startup CluedIn today announced the closure of a $15 million series A funding round led by Dawn Capital, which brings its total raised to over $16 million. The company says that the proceeds will be used to build out its platform, expand its sales and marketing team, and drive partnerships and expansion, particularly in the U.S. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why the corporate sector has taken an interest in solutions that ingest, understand, organize, and act on digital content from multiple digital sources. Gartner says that data integration and preparation are among the top three technologies organizations seek to automate by the end of 2022. Copenhagen-based CluedIn, which was founded in 2015 by former Sitecore engineers Tim Ward, Pierre Derval, and Martin Hyldahl, aims to streamline the process of making data ready for insights. The company leverages a graph database that sits between data sources and applications, offering solutions for data integration, governance, and management.  CluedIn integrates with existing systems and delivers a view of what data needs fixing at the global, source, entity, and property level. Data arbitration is built into the platform — CluedIn automatically reconciles records like addresses, company names, and more via reconfigurable rules. CluedIn’s data prep engine removes duplicate entries from databases, leveraging AI and machine learning for metadata management system. According to a Forbes survey, data scientists spend 80% of their time on data preparation, and 76% view it as the least enjoyable part of their work. It’s also expensive. Trifacta pegs the collective data prep cost for organizations at $450 billion. Beyond time and cost savings, CluedIn says that its platform, which can share data with third-party applications, enables enterprises to meet core regulatory and compliance requirements. For example, CluedIn can track data lineage and mask any personally identifiable information that it encounters. It also features templates for retention policies designed to help customers align with business- and government-level compliance rules. Thirty-employee CluedIn has customers in a range of verticals including Nordea, SAP, and Ticketmaster. For Pfizer and Coca-Cola, it’s providing customer insights analytics, according to Ward. “For some sectors, for example retail and consumer industries, the shift to online sales channels during the pandemic added urgency to the need to know the customer and sales patterns through data,” a spokesperson told VentureBeat via email. “In terms of master data management use cases, the top vendors in the space are the likes of Informatica, SAP and IBM, however the disruptive nature and unique approach of CluedIn’s product mean it offers a very different experience for the end user than these providers.” In addition to Dawn Capital, Collibra cofounder Stijn Christiaens and existing investor Nordic Makers participated in CluedIn’s latest funding round."
https://venturebeat.com/2021/04/21/automation-software-maker-uipath-shares-rise-over-23-in-nyse-debut/,Automation software maker UiPath shares rise over 23% in NYSE debut,"(Reuters) — Shares of automation software provider UiPath jumped 23.21% in their New York Stock Exchange debut on Wednesday, underscoring investors’ appetite for high-growth tech stocks. The stock closed its first day on the stock market at $69 a share, up from its $56 IPO price on Tuesday, giving the company a market capitalization of $35.82 billion. “This is just a milestone,” CEO and cofounder Daniel Dines told Reuters in an interview. “Starting from tomorrow, our focus is posting a good quarter, and we’re really marching on our vision of empowering everyone through automation.” Backed by the likes of Accel, Dragoneer, and Coatue Management, UiPath uses artificial intelligence and low-code tools to help large corporations and government agencies automate repetitive and routine tasks in areas such as accounting and human resources. Several richly valued startups, including cryptocurrency exchange operator Coinbase Global and South Korean ecommerce startup Coupang, have already cashed in on the record run in U.S. capital markets this year. Unicorns such as electric-vehicle startup Rivian and Microsoft-backed DataBricks are also set to go public later in 2021. Started in 2005 in Romania by Dines, a former Microsoft executive, UiPath recorded a surge in demand for its services during the COVID-19 pandemic from businesses shifting to remote working and digitalizing workflows. The New York-based company reported $607.6 million in revenue in the year ended January 31, 2021, an 81% jump year over year. “It took them 10 years to go from zero to a couple million in revenue. And then in five years, they went from a couple to 600 million,” said Rich Wong, partner at Accel who first invested in UiPath in 2017. The company partnered with Cleveland Clinic, one of the largest hospitals in the United States, to cut the waiting time at drive-thru COVID-19 testing sites from three minutes to around 15 seconds, Dines said. Morgan Stanley and J.P. Morgan were the lead underwriters for the IPO."
https://venturebeat.com/2021/04/21/adversarial-machine-learning-underrated-threat-data-poisoning/,Adversarial machine learning: The underrated threat of data poisoning,"Most artificial intelligence researchers agree that one of the key concerns of machine learning is adversarial attacks, data manipulation techniques that cause trained models to behave in undesired ways. But dealing with adversarial attacks has become a sort of cat-and-mouse chase, where AI researchers develop new defense techniques and then find ways to circumvent them. Among the hottest areas of research in adversarial attacks is computer vision, AI systems that process visual data. By adding an imperceptible layer of noise to images, attackers can fool machine learning algorithms to misclassify them. A proven defense method against adversarial attacks on computer vision systems is “randomized smoothing,” a series of training techniques that focus on making machine learning systems resilient against imperceptible perturbations. Randomized smoothing has become popular because it is applicable to deep learning models, which are especially efficient in performing computer vision tasks. But randomized smoothing is not perfect. And in a new paper accepted at this year’s Conference on Computer Vision and Pattern Recognition (CVPR), AI researchers at Tulane University, Lawrence Livermore National Laboratory, and IBM Research show that machine learning systems can fail against adversarial examples even if they have been trained with randomized smoothing techniques. Titled “How Robust are Randomized Smoothing based Defenses to Data Poisoning?,” the paper sheds light on previously overlooked aspects of adversarial machine learning. One of the known techniques to compromise machine learning systems is to target the data used to train the models. Called data poisoning, this technique involves an attacker inserting corrupt data in the training dataset to compromise a target machine learning model during training. Some data poisoning techniques aim to trigger a specific behavior in a computer vision system when it faces a specific pattern of pixels at inference time. For instance, in the following image, the machine learning model will tune its parameters to label any image with the purple logo as “dog.” Other data poisoning techniques aim to reduce the accuracy of a machine learning model on one or more output classes. In this case, the attacker would insert carefully crafted adversarial examples into the dataset used to train the model. These manipulated examples are virtually impossible to detect because their modifications are not visible to the human eye. Research shows that computer vision systems trained on these examples would be vulnerable to adversarial attacks on manipulated images of the target class. But the AI community has come up with training methods that can make machine learning models robust against data poisoning. “All previous data poisoning methods assume that the victim will use the standard training procedure of minimizing the empirical error on the training data,” Akshay Mehra, Ph.D. student at Tulane University and lead author of the paper, told TechTalks. “However, the adversarial robustness community has highlighted that minimizing the empirical error is not suitable for model training since models trained with it are vulnerable to adversarial attacks. Several works have been published that try to improve the adversarial robustness of the models. Of these works, training procedures that can produce certifiably robust models are of the most interest due to the adversarial robustness guarantees of the models, trained using these methods.” Random smoothing is a technique that cancels out the effects of data poisoning by establishing an average certified radius (ACR) during the training of a machine learning model. If a trained computer vision model classifies an image correctly, then adversarial perturbations within the certified radius will not affect its accuracy. The larger the ACR, the harder it becomes to stage an adversarial attack against the machine learning model without making the adversarial noise visible to the human eye. Experiments show that deep learning models trained with random smoothing techniques maintain their accuracy even if their training dataset contains poisoned examples. In their research, Mehra and his coauthors assumed that a victim has used random smoothing to make the target robust against adversarial attacks. “In our work, we explored three popular training procedures (Gaussian data augmentation, smooth adversarial training, and MACER) which have been shown to increase certified adversarial robustness of the models as measured by the state-of-the-art certification method based on randomized smoothing,” Mehra says. Their findings show that even when trained with certified adversarial robustness techniques, machine learning models can be compromised through data poisoning. In their paper, the researchers introduce a new data poisoning method called Poisoning Against Certified Defenses (PACD). PACD uses a technique known as bilevel optimization, which achieves two goals: create poisoned data for models that have undergone robustness training, and pass the certification procedure. PACD produces clean adversarial examples, which means the perturbations are not visible to the human eye. “A few previous works have shown the effectiveness of solving the bilevel optimization problem to achieve better poisoning data,” Mehra says. “The difference in the formulation of the attack in this work is that instead of using the poison data to reduce the model accuracy we are targeting certified adversarial robustness guarantees obtained from state-of-the-art certification procedure based on randomized smoothing.” The bilevel optimization process takes a set of clean training examples and gradually adds noise to them until they reach a level that can circumvent the target training technique. The ingenuity behind this data poisoning technique is that researchers were able to create a machine learning algorithm that optimizes the adversarial noise for the specific type of robustness training method used in the target model. The algorithm that creates the adversarial example is called ApproxGrad, and it can be adjusted for different robustness training methods. Once the target model is trained on the tainted dataset, its ACR will be reduced considerably, and it will be highly vulnerable to adversarial attacks. “In our approach, we explicitly generated poison data that when used for training, will lead to models with low certified adversarial robustness,” Mehra says. “To do this we used the training procedures that produce models with high certified adversarial robustness as our lower-level problem. The attacker’s objective (upper-level problem) is to lower the guarantees produced by the certification procedure. By approximately solving this bilevel optimization problem we were able to generate poison data that could significantly hurt the certified adversarial robustness guarantees of the models. The lowered guarantees lead to a loss of trust in the model’s prediction at test-time.” The researchers applied PACD to the MNIST and CIFAR datasets and tested it on neural networks trained with all three popular adversarial robustness techniques. In all cases, PACD data poisoning resulted in a considerable decrease in the average certified radius of the trained model, making it vulnerable to adversarial attacks. The AI researchers also tested to see whether a poisoned dataset targeted at one adversarial training technique would prove to be effective against others. Interestingly, their findings show that PACD transfers across different training techniques. For instance, even if a poisoned dataset has been optimized for gaussian data augmentation, it will still be effective on machine learning models that will go through the MACER and smooth adversarial training processes. “We demonstrate, through transfer learning experiments, that the generated poison data works to reduce the certified adversarial robustness guarantees of models trained with different methods and also models with different architectures,” Mehra says. But while PACD has proven to be effective, it comes with a few caveats. Adversarial attacks that assume full knowledge of the target model, including its architecture and weights, are called “white box attacks.” Adversarial attacks that only need access to the output of a machine learning model are “black box attacks.” PACD stands somewhere in between the two ends of the spectrum. The attacker needs to have some general knowledge of the target machine learning model before formulating the poisoned data. “Our attack is a grey box attack since we are assuming knowledge of victim’s model architecture and training method,” Mehra says. “But we don’t assume knowledge of the particular weights of the network.” Another problem with PACD is the cost of producing the poisoned dataset. ApproxGrad, the algorithm that generates the adversarial examples, becomes computationally expensive when applied to large machine learning models and complicated problems. In their experiments, the AI researchers focused on small convolutional neural networks trained to classify the MNIST and CIFAR-10 datasets, which contain no more than 60,000 training examples. In their paper, the researchers note, “For datasets like ImageNet where the optimization must be performed over a very large number of batches, obtaining the solution to bilevel problems becomes computationally hard. Due to this bottleneck we leave the problem of poisoning ImageNet for future work.” ImageNet contains more than 14 million examples. A machine learning model that can perform well on the ImageNet dataset requires a convolutional neural network with dozens of layers and millions of parameters. Accordingly, creating PACD data would require large resources. “Solving bilevel optimization problems can be computationally expensive, especially when using very large datasets and deep models,” Mehra says. “However, in our paper, we show that attacks generated against moderately deep models transfer well to much deeper models. It would be interesting to see if attacks generated against a portion of the large training data also work well on the entire training data.” Today, machine learning applications have created new and complex attack vectors in the millions of parameters of trained models and the numerical values of image pixels, audio samples, and text documents. Adversarial attacks are presenting new challenges for the cybersecurity community, whose tools and methods are centered on finding and fixing bugs in source code. The PACD technique shows that poisoned data can render proven adversarial defense methods ineffective. Mehra and his coauthors warn that data quality is an underrated factor in assessing adversarial vulnerabilities and developing defenses. For instance, a malicious actor can develop a tainted dataset and deploy it online for others to use in training their machine learning models. Alternatively, the attacker can insert poisoned examples into crowdsourced machine learning datasets. The adversarial perturbations are imperceptible to the human eye, which makes it extremely difficult to detect them. And automated tools that vet software security can’t detect them. PACD has important implications for the machine learning community. Machine learning engineers should be more careful about the datasets they use to train their models and make sure the source is trustworthy. Organizations that curate datasets for machine learning training should be more careful about the provenance of their data. And companies such as Kaggle and GitHub that host datasets and machine learning models should start thinking about ways to verify the quality and security of their datasets. We still don’t have complete tools to detect adversarial perturbations in training datasets. But securing the pipeline for accessing and managing machine learning training datasets can be a good first step in preventing the kind of data poisoning measures Mehra and his coauthors describe in their paper. The Adversarial ML Threat Matrix, introduced last October, provides solid guidelines on finding and fixing possible holes in the training and deployment pipeline of machine learning models. But a lot more needs to be done. Another useful tool is a series of deep learning trust metrics developed by AI researchers at the University of Waterloo, which can find classes and areas where a computer vision system is underperforming and might be vulnerable to adversarial attacks. “Through this work, we want to show that advances in certified adversarial robustness are dependent on the quality of the data used for training the models,” Mehra says. “Current methods for detecting data poisoning attacks may not be sufficient when attacker adds imperceptibly distorted data. We need more sophisticated methods to deal with this and is a direction for our future research.” Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/21/proglove-promotes-worker-well-being-with-human-digital-twin-technology/,ProGlove promotes worker well-being with human digital twin technology,"ProGlove, the company behind an ergonomic barcode scanner, has developed new tools for analyzing human processes to build a human digital twin. “We have always been driven to have our devices narrate the story of what is really happening on the shop floor, so we added process analytics capabilities that allow for time-motion studies, visualization of the shop floor, and more,” ProGlove CEO Andreas Koenig told VentureBeat. The company’s newest process analytics tools can complement the typical top-down perspective of applications by adding a process-as-seen view to the conventional process-as-wanted view. Most importantly, it can also provide insights that improve well-being. Koenig said, “We are building an ecosystem that empowers the human worker to make their businesses stronger.” The market for barcode scanning is still going strong and is often taken for granted, given how old it is. “You have technologies like RFID that have been celebrated for being the next big thing, and yet their impact thus far hasn’t been anywhere near where most pundits expected it,” Koenig said. Companies like Zebra, Honeywell, and Datalogic have lasted for decades by building out an ecosystem of tools to address industry needs. “What sets us apart is that we looked beyond the obvious and started with the human worker in mind,” Koenig said. Not only is the company providing a form factor designed to meet requirements for rugged tools, this shift to analytics could further promote efficiency, quality, and ergonomics on the shop floor. ProGlove’s cofounders participated in Intel’s Make It Wearable Challenge, with the idea of designing a smart glove for industries. Today, ProGlove’s MARK scanner can collect six-axis motion data, including pitch, yaw, roll, and acceleration, along with timestamps, a step count, and camera data (such as barcode reading speed and the scanner ID). Koenig’s vision goes beyond selling a product to establish the right balance between businesses’ need for profits and their obligation to ensure worker well-being. Koenig estimates that human hands deliver 70% of added value in factories and on warehouse floors. “There is no doubt that they are your most valuable resource that needs protection. Even more so since we are way more likely to experience a shortage of human workers in the warehouses across the world than having them replaced by robots, automation, or AI.” ProGlove Insight contextualizes the collected data and lets users compare workstations and measure the workload and effort necessary to complete the tasks. Users can also visualize their shop floor, look at heatmaps, and identify best practices or efficiency blockers. After a recent smart factory lab experiment with users, DPD and Asics realized efficiency gains by as much as 20%, Koenig said. ProGlove’s vision of the human digital twin is built on three pillars: a digital representation of onsite workers, a visualization of the shop floor, and an industrial process engineer. “The human digital twin is all about striking the right balance between businesses’ needs for profitability, efficiency, and worker well-being,” Koenig said. At the same time, it is important that the human digital twin complies with data privacy regulations and provides transparency to frontline workers around what data is being transmitted."
https://venturebeat.com/2021/04/21/koch-industries-embraces-networking-as-a-service/,Koch Industries embraces networking as a service,"Koch Industries has joined a growing list of enterprises where the IT organization is evolving into becoming a general manager of IT that is consumed as a service rather than via platforms built and maintained by an internal IT team. That shift in the case of the privately held conglomerate not only encompasses cloud platforms but now also network services delivered via Alkira, a provider of a wide-area network (WAN) service that Koch Industries is starting to rely on to interconnect a highly distributed computing environment that spans the globe. Rather than deploy, secure, and maintain routers, switches, and firewalls, along with all the hardware and software required to deliver enterprise-class networking services, it’s becoming more economically efficient to simply consume networking services much like any other cloud service, said Koch Industries CTO Matt Hoag. Alkira constructed an Alkira Cloud Services Exchange (CSX) using $76 million it raised from Koch Disruptive Technologies (KDT), the venture capital arm of Koch Industries, along with Sequoia Capital, Kleiner Perkins, GV (formerly Google Ventures), and others. That WAN aggregates multiple points-of-presence that can be provisioned via a console that Alkira customers access as a cloud service. The security capabilities of the Alkira WAN are provided by firewalls and other offerings from Palo Alto Networks that Alkira also manages on behalf of customers. The Alkira approach eliminates both the need to acquire networking infrastructure and the need to hire the specialists required to maintain it, noted Hoag. The decision by Koch Industries to employ the Alkira networking service is part of a larger series of initiatives to reduce the amount of IT that needs to be directly managed by Koch Industries personnel, said Hoag. “We don’t have own or manage all the underlying infrastructure,” he said. “It’s not our business.” Koch Industries has made a strategic decision to focus its internal resources on building applications that make a strategic difference to the business, added Hoag. Everything else going forward will be consumed as a service as much as possible. The plan is to elevate the role of the internal IT team organization to one focused on the management of services rather than, for example, installing networking equipment, noted Hoag. The conglomerate is joining the ranks of an increasing number of enterprise IT organizations that have grown more comfortable consuming IT as a service. A recent report published by Information Services Group (ISG) noted demand for technology and business services continues to rise sharply as the economy recovers from the downturn brought on by the COVID-19 pandemic. In the first quarter of 2021 the annual contract value (ACV) for as-a-service and managed services offerings that exceed $5 million reached a record $17.1 billion, up 11% over last year and 4% from the previous quarter. Overall, the cloud-based as-a-service market rose 15% to a record $9.9 billion in the first quarter. Managed services reached $7.2 billion in the first quarter, up 7% year over year. It’s not clear at what rate IT will be consumed as a service by enterprises that have historically preferred to deploy and maintain their own infrastructure. Consuming IT as a service not only provides more flexibility to scale services up and down as required, but it enables organizations to finance IT as an operational rather than capital expense. The financing option frees up more capital that can be invested in, for example, a manufacturing plant rather than servers, switches, and storage systems. For the time being, the bulk of IT systems still reside in on-premises IT environments that are managed by internal IT teams. However, even on-premises IT environments are being increasingly managed by vendors such as Hewlett-Packard Enterprise (HPE) and Dell Technologies or third-party managed service providers (MSPs). It’s not clear how the rank and file that make up IT teams will ultimately be impacted by this shift. Many of them will wind up working for IT services providers. Others will evolve into managers of services provided by those third-party providers. However, as IT services become more automated, it’s clear the number of IT professionals required to manage IT may not be as large as it is today."
https://venturebeat.com/2021/04/21/eu-proposes-strict-ai-rules-with-fines-up-to-6-for-violations/,"EU proposes strict AI rules, with fines up to 6% for violations","(Reuters) — The European Commission on Wednesday announced tough draft rules on the use of artificial intelligence, including a ban on most surveillance, as part of an attempt to set global standards for a technology seen as crucial to future economic growth. The rules, which envisage hefty fines for violations and set strict safeguards for high-risk applications, could help the EU take the lead in regulating AI, which critics say has harmful social effects and can be exploited by repressive governments. The move comes as China moves ahead in the AI race, while the COVID-19 pandemic has underlined the importance of algorithms and internet-connected gadgets in daily life. “On artificial intelligence, trust is a must, not a nice to have. With these landmark rules, the EU is spearheading the development of new global norms to make sure AI can be trusted,” European tech chief Margrethe Vestager said in a statement. The Commission said AI applications that allow governments to do social scoring or exploit children will be banned. High risk AI applications used in recruitment, critical infrastructure, credit scoring, migration and law enforcement will be subject to strict safeguards. Companies breaching the rules face fines up to 6% of their global turnover or 30 million euros ($36 million), whichever is the higher figure. European industrial chief Thierry Breton said the rules would help the 27-nation European Union reap the benefits of the technology across the board. “This offers immense potential in areas as diverse as health, transport, energy, agriculture, tourism or cyber security,” he said. However, civil and digital rights activists want a blanket ban on biometric mass surveillance tools such as facial recognition systems, due to concerns about risks to privacy and fundamental rights and the possible abuse of AI by authoritarian regimes. The Commission will have to thrash out the details with EU national governments and the European Parliament before the rules can come into force, in a process that can take more than year. ($1 = 0.8333 euros)"
https://venturebeat.com/2021/04/21/gartner-predicts-public-cloud-spending-to-reach-332b-in-2021/,Gartner predicts public cloud spending to reach $332B in 2021,"Worldwide spending on public cloud services is expected to reach $332.3 billion in 2021, an increase of 23.1% from 2020, according to the latest Gartner forecast. Public cloud spending in 2020 was $270 billion. Cloud spending is getting a boost because emerging technologies such as containerization, virtualization, and edge computing are becoming more mainstream, Sid Nag, research vice president at Gartner, said in the forecast. Software-as-a-service (SaaS) remains the largest market segment and is forecast to reach $122.6 billion in 2021, spurred by the demand for composable applications. “The events of last year allowed CIOs to overcome any reluctance of moving mission critical workloads from on-premises to the cloud,” Nag said. There was an expectation that some workloads had to stay on-premises, but the past year showed those concerns were unfounded. But even if there hadn’t been a pandemic, Nag said there was a “loss of appetite” for datacenters. As organizations mobilize for a massive global effort to produce and distribute COVID-19 vaccinations, SaaS-based applications that enable essential tasks such as automation and supply chain are critical. The fact that these applications demonstrate reliability in scaling vaccine management will help CIOs validate the ongoing shift to cloud, Gartner said. Desktop-as-a-service (DaaS) will see the highest growth in 2021, growing 67.7% to reach $2 billion, followed by infrastructure-as-a-service (IaaS) at 38.5% to reach $82 billion. CIOs will boost IaaS and DaaS spending as they continue to face pressure to scale infrastructure that supports complex workloads and meets the demands of a hybrid workforce, Gartner said. Growth in IaaS and DaaS will slow down 30% in 2022, with spending to reach $106.8 billion and $2.7 billion, respectively. Cloud spending will grow across all areas in 2021, Gartner said in its forecast. Business process services (business-product-as-a-service, or BPaaS) is forecast to reach $50.1 billion, and application infrastructure services (platform-as-a-service, or PaaS) will reach $59.4 billion. Cloud management and security services will reach $16 billion.  The public cloud market fared well over the past year as enterprises relied on cloud services to maintain business continuity in light of business disruptions and shift to a remote workforce. However, cloud spending will look different in 2021 and 2022 as enterprises shift away from infrastructure and application migration and toward innovative applications combining cloud with technologies such as AI, internet of things, and 5G. “Cloud will serve as the glue between many other technologies that CIOs want to use more of, allowing them to leapfrog into the next century as they address more complex and emerging use cases,” Nag said. Companies depend on the cloud to adopt emerging technologies at scale, and the resulting applications and workloads encourage more cloud spending. For example, a company building an IoT application could use virtual machines and containers to build at scale, and then buy more cloud services to manage the data that is generated. Gartner is not the only one noting that emerging technologies such as edge computing is growing rapidly. International Data Corporation (IDC) said the worldwide edge computing market is expected to reach $250.6 billion in 2024. The services market should account for 46.2% of all edge spending by 2024, followed by hardware at 32.2% and edge software at 21.6%, IDC said. Earlier this month, Gartner predicted worldwide IT spending will reach $3.8 trillion in 2021, an increase of 4% from 2020. The biggest growth is expected in enterprise software, fueled by enterprises accelerating their digital transformation plans to deliver virtual services such as distance learning, telehealth, and automation. Other areas of spending include datacenter systems, communications, IT services, and devices. PC spending is also up this year."
https://venturebeat.com/2021/04/21/google-rolls-out-new-ai-powered-features-for-meet/,Google rolls out new AI-powered features for Meet,"Google today announced a number of updates to Google Meet, including a refreshed user interface, features powered by AI, and tools that aim to make meetings more engaging. Among the highlights is a data saver mode that limits data usage on mobile networks, and Autozoom, which uses AI to zoom in and position participants in front of their cameras. The pandemic has supercharged video chat usage. Eighty-three percent of businesses with over 250 employees are likely to purchase video calling tools in the near future, according to a survey by Commercial Integrator. Underlining the trend, Meet had over 50 million users as of May 2020, a 900% increase from last March. Meet’s new Data Saver feature, which launches this month, automatically limits data usage on mobile and other bandwidth-constrained networks. It will arrive alongside improvements to low-light mode for Meet, a capability that leverages AI to brighten video in poorly lit environments. Previously only available for smartphones and tablets, low-light mode will soon work on the web, detecting when users are underexposed and enhancing quality to improve their visibility. Meanwhile, Autozoom, which will arrive for paid Google Workspace customers in the coming months, is a feature that taps AI to zoom in and position users squarely in-camera. Autozoom intelligently readjusts as users move, ensuring they remain in the frame, Google says. Beyond Data Saver, low-light mode on the web, and Autozoom, Meet will soon allow users to replace their background with an image of a classroom, party, or forest — with more on the way. In the future, Meet for web will also offer greater control over how users see themselves in meetings. For example, they’ll be able to shrink their video feed to a tile in a grid or a floating picture, with the latter resizable, as well as moveable. And if they prefer not to see themselves at all, users will be able to minimize their feed and hide it from view, optionally disabling their self-feed across calls. Meet will also offer a way to pin and unpin content that will shrink the presentation tile down to the size of the other tiles. And the bottom bar will become easier to navigate — meeting dial-in codes, attachments, the participants’ list, chat, and other activities will move to the bottom right to create more vertical space for participants and content. The “leave call” button will also move away from the camera and microphone buttons to prevent accidental hang-ups, Google says. This week, Google also updated Google Maps with over 100 AI-powered improvements, like upgraded Live View navigation that enables users to get turn-by-turn directions indoors. Other enhancements include a new routing model that optimizes for lower fuel consumption based on factors like road incline and traffic congestion, as well as a layer that shows current and forecasted weather and temperature conditions."
https://venturebeat.com/2021/04/21/workflow-automation-platform-aisera-raises-40m/,Workflow automation platform Aisera raises $40M,"Aisera, a company developing a platform that automates operations and support tasks across IT, sales, and customer service, today announced it has raised $40 million in a series C round led by Icon Ventures. The startup says the funds, which bring its total raised to $90 million, will support product expansion and deployment, as well as go-to-market, marketing, and software development efforts. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. According to market research firm Fact.MR, small and medium-sized enterprises are expected to adopt business workflow automation at scale, creating a market opportunity of more than $1.6 billion between 2017 and 2026. Aisera offers products that auto-complete actions and workflows by integrating with existing enterprise apps, like Salesforce and ServiceNow. The company was founded in 2017 by Muddu Sudhakar, who previously launched e-discovery vendor Kazeon (which was acquired by EMC in 2009), big data startup Cetas (acquired by VMware in 2012), and cybersecurity firm Caspida (acquired by Splunk in 2015). Aisera claims its platform can continuously learn to resolve issues through a combination of conversational AI, robotic process automation, and reinforcement learning. For example, Aisera can predict outages and send notifications to DevOps teams and customers. Moreover, the company claims its platform can detect patterns to predict service disruptions.  Aisera customers can choose from a library of prebuilt workflows and intents built for IT, HR, facilities, sales operations, and customer service applications. The platform offers out-of-the-box reports and dashboards for auditing, including auto-resolution metrics and the ability to discover the most-requested knowledge articles. Aisera has a number of competitors in a global intelligent process automation market that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere and UiPath have secured hundreds of millions of dollars in investments at multibillion-dollar valuations. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. That’s not counting newer startups like WorkFusion, Indico, Tray.io, Tonkean, AirSlate, Workato, Camunda, and Automation Hero. But the funding comes at a time of significant expansion for Aisera. In addition to achieving year-over-year growth of 300% and a base of over 65 million users, the company says it has secured a number of new enterprise customers, including Autodesk, Dartmouth College, McAfee, and Zoom. Aisera’s success is perhaps unsurprising, given the value proposition of automation. Ninety-five percent of IT leaders are prioritizing automation, and 70% of execs are seeing the equivalent of over four hours saved per employee, per week, according to Salesforce’s recent Trends in Workflow Automation report. Moreover, market research firm Fact.MR says the adoption of business workflow automation at scale could create a market opportunity of over $1.6 billion between 2017 and 2026. Palo Alto, California-based Aisera’s latest funding round saw participation from new investor World Innovation Lab, as well as existing backers True Ventures, Menlo Ventures, Norwest Venture Partners, Khosla Ventures, First Round Capital, Webb Investment Network, and Sherpalo."
https://venturebeat.com/2021/04/21/security-operations-platform-cyrebro-raises-15m-to-expand-its-customer-base/,Cyrebro raises $15M to expand its security operations platform,"Cyrebro, a cloud-based security operations center (SOC), today announced it has raised $15 million in a series B round led by Prytek. CEO Nadav Arbel says the capital, which brings the company’s total raised to $22 million, will be used to support development of the Cyrebro platform and strengthen the startup’s reach in the small and medium-sized business (SMB) market. Email messages and files have become top attack vectors for hackers looking to steal data, particularly as the pandemic necessitates novel work-from-home arrangements. Cybersecurity Ventures anticipates that the damage related to cybercrime will hit $6 trillion annually this year. Corresponding with this rise, Gartner reports that worldwide spending on cybersecurity is expected to reach $133.7 billion in 2022. Cyrebro aims to address this with a platform that integrates cybersecurity tools in a single dashboard. Cyrebro shows critical incidents across business operations and security solutions, allowing response teams to conduct investigations by type, severity, and status. With Cyrebro, security analysts can drill down into events and see which assets were impacted, as well as recommended actions and real-time status. Beyond this, Cyrebro delivers insights about where most alerts are generated and where attention should be focused for preemptive steps. “Global businesses of all sizes are facing the new threats of cyberattacks brought on by the pandemic, and companies now need simplified solutions to see, understand, and respond to their cybersecurity needs. With this understanding in mind, we created Cyrebro, a real-time, live security operation platform to enable online operations of the entire security stack,” Arbel told VentureBeat via email. “It is the only platform to address the full scope of cybersecurity needs in the most effective, powerful, and cost-efficient way. More than ever, organizations need holistic detection and response mechanisms while covering the complete suite of security solutions, including monitoring, threat hunting, response, and compliance.” Cyrebro collects and processes data from clouds, networks, and endpoints, including laptops, desktops, and servers. A feature shows the geographic location of hosts with their coverage, connectivity state, and related alerts, and reporting functions enable users to generate audits with visualizations like pie charts. Competition is fierce in a cybersecurity market that’s anticipated to reach $199.98 billion in value by 2025, according to Market Research Future. Securiti.ai, a developer building a platform designed to automate cybersecurity and compliance processes, recently emerged from stealth with $31 million. There’s also Swimland and Tines, a cybersecurity startup that helps enterprise security teams automate repetitive workflows. But Arbel, which has 80 employees, says it has “hundreds” of paying customers, including casinos, global retailers, banks, insurance companies, and other Fortune 500 companies. “Over the past year, our customer base has grown by 100% and our employees by 20%. Since the start of COVID, SMBs experienced new challenges within their IT and cybersecurity departments. We realized the critical need to develop an accessible platform that simplifies proactive cyber operations for the average small to mid-sized business owner, allowing them to gain the posture of Fortune 500 enterprises,” he added. “Our business has grown exponentially, and we took on this round of funding to meet this new market demand for comprehensive online security solutions.” InCapital, Mizrahi Bank, and previous investor Mangrove also participated in Tel Aviv, Israel-based Cyrebro’s latest round of funding."
https://venturebeat.com/2021/04/21/pipedrives-smart-docs-automates-sales-workflows-and-helps-close-deals/,Pipedrive’s Smart Docs automates sales workflows and helps close deals,"Pipedrive, a customer relationship management (CRM) platform for salespeople, has launched new smart documentation features designed to automate the process of creating, sending, and tracking sales documents. The rollout comes as much of the world has retreated to remote work, which means teams across industries need new tools to carry out their day-to-day tasks. Founded in 2010, Pipedrive helps teams plan all their sales activities, including recording their customer conversation history and tracking their deals from inception. It also integrates with more than 200 third-party apps, such as Zoom, Microsoft Teams, Trello, and MailChimp. The company, which counts Salesforce and Hubspot as competitors, has amassed a number of notable clients, including the mighty Amazon, leading to Pipedrive’s acquisition by Vista Equity Partners a few months back. Pipedrive quietly launched Smart Docs earlier this month as a rebrand of a beta product called Sales Docs it launched last year. Smart Docs enables teams to send trackable quotes, contracts, and proposals to clients and prospects so they can know when a customer opens a document and follow up at a suitable time. More than that, sales teams can use Smart Docs to create templates that automatically pull in Pipedrive field data to reduce the amount of manual work required to generate new documents from scratch. For example, rather than creating largely identical sales quotes every day, companies can use the Smart Docs template editor to construct a standardized document by adding the relevant Pipedrive fields (such as company name, job titles, and addresses) from a panel on the right. Then all a salesperson has to do is choose the template to auto-fill the document with data from Pipedrive — it’s all about being able to send quotes faster, reducing the need to copy/paste from other documents. Smart Docs is actually split into two components — Smart Docs Basics, which is available to subscribers on the Advanced pricing plan, and Smart Docs Pro, which is available to Professional and Enterprise users. With Smart Docs Basics, users can create document templates and quote tables that automatically pull in product data related to the field, receive notifications when a third party views their document, and automatically share new document links with recipients if they update that document. On the Pro plan, users can remove the Pipedrive logo from the document before it’s shared with a third party. The plan also offers support for e-signatures, which is particularly useful in a remote-working world. This plan includes a feature that automatically requests signatures from all relevant stakeholders, who receive an emailed request for their signature — as well as a transparent audit trail that shows the sender who has signed and who hasn’t. Smart Docs Pro also ships with granular permission settings so that only specified individuals at a company can access specific templates. While Smart Docs Pro is currently restricted to Pipedrive’s Professional and Enterprise subscribers, it will also be made available for Advanced plan users as an add-on later this year."
https://venturebeat.com/2021/04/21/google-launches-ai-powered-document-processing-services-in-general-availability/,Google launches AI-powered document processing services in general availability,"Google today announced that several of its cloud-based, AI-powered document processing products have become generally available after launching in preview last year. DocAI platform, Lending DocAI, and Procurement DocAI, which have been piloted by thousands of businesses to date, are now open to all customers and include new features and resources. Companies spend an average of $20 to file and store a single document, by some estimates, and only 18% of companies consider themselves paperless. An IDC report revealed that document-related challenges account for a 21.3% productivity loss, and U.S. companies waste a collective $8 billion annually managing paperwork. With the launch in general availability, Lending DocAI, which processes loan applicants’ asset documents, now offers a set of specialized AI models for paystubs and bank statements. The service also now benefits from DocAI platform’s Human-in-the-Loop AI capability, which provides a workflow to manage human data review tasks. As Google explains, Human-in-the-Loop AI enables human reviewers to verify data captured by Lending DocAI, Procurement DocAI, and other offerings in DocAI platform. The system shows a percentage score of how “sure” it is that the AI ingested the document correctly, and it’s customizable, with the flexibility to set different thresholds and assign groups of reviewers to stages of a workflow. Developers can choose reviewers to assign to tasks either from within their own company or from partner organizations. Lending institutions like banks and brokers process hundreds of pages of paperwork for every loan. It’s a heavily manual process that adds thousands of dollars to the cost of issuing a loan. While hardly flawless, automated processing can give customers a degree of confidence they can afford the property they’re interested in, and some lenders are able to complete the ordeal within minutes, as opposed to the weeks it once took. Procurement DocAI, which performs document processing for invoices, receipts, and more, has gained an AI parser for electric, water, and other utility bills. The latest release taps Google’s Knowledge Graph to validate information, a system that understands over 500 facts about 5 billion entities from the web, as well as from open and licensed databases. Google claims that Knowledge Graph can help increase document parsing accuracy by identifying, for example, that “Angelina” correlates to “Angelina Paris,” a bakery identified using geodata. Google also today announced a partnership with mortgage servicing firm Mr. Cooper, following a collaboration with home loan company Roostify last October. Google says Mr. Cooper will offer its customers greater automation and workflow tools by connecting them with the DocAI platform. “Over the last few years, we have made substantial investments in our proprietary servicing technology and core mortgage platform that have revolutionized the customer experience while providing dramatic efficiencies in operating cost. By joining forces with Google … we are able to build on those advances and help make these technologies available for the mortgage industry to deploy through Google Cloud,” Mr. Cooper CEO Jay Bray said in a press release. The general release of the DocAI platform comes after Google launched PPP Lending AI, an effort to help lenders expedite the processing of applications for the since-exhausted U.S. Small Business Administration’s (SBA) Paycheck Protection Program. As Google explained at the time in a whitepaper, AI can automate the handling of volumes of loan applications by identifying patterns that would take a human worker longer to spot."
https://venturebeat.com/2021/04/21/saas-app-management-and-security-startup-appomni-nabs-new-funding/,SaaS app management and security startup AppOmni nabs $40M,"AppOmni, a provider of software-as-a-service (SaaS) security management, today announced the closing of a $40 million round led by Scale Venture Partners. The company says it will use the funding to grow its footprint and bolster product R&D as it looks to acquire new customers. Estimates show that 92% of web apps have security flaws that can be exploited, and some 16.2% of U.S. companies have two or more web apps that don’t securely store personally identifiable information. The problem is often less negligence than ignorance, as companies lack the expertise required to lock down their services and platforms. A recent survey by ESG and the Information Systems Security Association found that 70% of organizations believe they’re impacted by a cybersecurity skills shortage. AppOmni’s SaaS platform allows customers to scan, triage, and monitor apps using firewalls and access controls. With AppOmni, teams can define security rules and use continuous monitoring for alerts of data exposures. They’re also able to apply role-based policies across clouds, business units, environments, and apps. “While some companies have taken steps to secure their SaaS environments, they commonly use a cloud access security broker (CASBs) or a penetration test. Unfortunately, neither of these tools is sufficient to fully secure today’s SaaS platforms,” AppOmni cofounder and CEO Brendan O’Connor told VentureBeat via email. “CASBs focus on network traffic between employee devices and the cloud. Pentests can be helpful but only measure a single moment in time and often are limited in scope. Alternatively, AppOmni scans cloud APIs and implements data access policies within the SaaS application, governs third-party access integrations, and continuously monitors SaaS applications and activities.” IT teams using AppOmni get a configuration management dashboard that lets them snapshot security policies, access profiles, and review changes before they’re promoted to production. Beyond this, teams can export access reviews and reports or classify data according to type and business requirements. AppOmni isn’t alone in the app management services market, which is anticipated to be worth $32.5 billion by 2024. Among others, it competes with DoControl and Sqreen, which raised $14 million in April 2019 for its cybersecurity suite that helps developers monitor and protect web apps from vulnerabilities and attacks. And Productiv, which develops software that helps enterprises manage SaaS apps, secured $45 million in March. The sector’s growth is partly due to the pandemic, which accelerated the adoption of SaaS applications and remote work. According to Gartner, enterprises are investing in SaaS at a record rate, with 95% of new enterprise app purchases cloud-based. And cloud spending rose 37% to $29 billion during the first quarter of 2020, a May 2020 report from Synergy Research Group revealed. AppOmni, which employs over 50 people, claims to have an advantage over rivals in its momentum. The company has several Fortune 500 organizations as clients, including Dropbox and Accenture, and backing from Salesforce Ventures and ServiceNow Ventures via its recent series B round. And in 2020, AppOmni’s annual recurring revenue increased by over 900%. “The COVID pandemic accelerated the move to cloud and SaaS … Unfortunately, we anticipate that companies’ rush to install and expand SaaS platforms over the past year — without making the accompanying investment in SaaS security — has created a significant security gap,” O’Connor said. “What makes AppOmni unique, aside from its technology, is that it was founded by security practitioners who have real-world experience securing SaaS applications at scale.” San Francisco-based AppOmni has raised over $53 million to date, after closing a $10 million series A in January 2020 and a $3 million seed round in April 2019. Existing backers, including ClearSky, Costanoa Ventures, Inner Loop Capital, and Silicon Valley Data Capital, also participated in AppOmni’s series B announced today."
https://venturebeat.com/2021/04/21/rapidapi-capitalizes-on-booming-api-economy-with-60m-in-funding/,RapidAPI capitalizes on booming API economy with $60M in funding,"RapidAPI, a platform that helps developers find, manage, and test application programming interfaces (APIs), has raised $60 million in a series C round of funding. The raise comes amid a boom in the API economy, as organizations strive to improve their efficiency (and profitability) by moving away from tightly woven, monolithic applications to microservices-based applications. By splitting apps into smaller, function-based components, companies are afforded more agility, and they gain domain-specific expertise from third parties and circumvent the need to develop everything from scratch themselves. Founded out of San Francisco in 2014, RapidAPI helps developers discover and connect to thousands of public APIs, covering everything from file storage and currency conversion to flight search and COVID-19 statistics. The company also offers Enterprise Hub, which is essentially a white label version of RapidAPI that businesses use to build private marketplaces for internal and external APIs. RapidAPI has amassed a number of notable enterprise clients, including SAP, Cisco, Tata, and Hyatt. RapidAPI had raised around $63 million after closing a $50 million series B last year from lead investors that included Microsoft’s M12 and Andreessen Horowitz. With today’s $60 million cash injection, spearheaded by Green Bay Ventures, the company plans to invest in several key areas “based on direct feedback from our enterprise customers,” according to RapidAPI founder and CEO Iddo Gino. This will include the continued platformization of the RapidAPI Enterprise Hub to include additional tooling spanning the entire API development lifecycle. This is evidenced by the company’s recent acquisition of Paw, a platform that helps developers design and test web APIs. “Over the course of this year, we will continue to build out the platform to add other key capabilities, including things like API Security, and increase the integration between our API Marketplace and Enterprise Hub, testing, and design products,” Gino told VentureBeat. RapidAPI intends to continue opening up to new API types beyond SOAP and REST, having recently extended support to Kafka and GraphQL APIs. “We will continue to add support for other API types, such as asynchronous APIs and webhooks,” Gino added. The company is also setting out to solve the problem of “multicloud, multi-gateway integration” for its customers. “As customers continue to deploy more API gateways across a variety of hybrid cloud environments, we will continue to invest in becoming the aggregation layer across multiple gateways and clouds, providing customers with the ability to integrate their workflows across different platforms and cloud environments,” Gino explained. Other investors in the company’s series C round include existing backers M12, Andreessen Horowitz, DNS Capital, Viola Growth, and Grove Ventures, in addition to new investor Stripes. There has been a flurry of activity across the API realm of late. MessageBird, SendBird, Postman, and Kong have all pulled in large sums of cash at multi billion-dollar valuations, not to mention investments for smaller API-focused companies such as Nylas and Skyflow. Twilio, meanwhile, acquired Segment for $3.2 billion, and Idera acquired Apilayer, a cloud-based API provider for big-name companies like Amazon, Apple, and Facebook. The API onslaught is nothing new — over the past five years, Google shelled out $625 million for Apigee and Salesforce snapped up Mulesoft for a cool $6.5 billion. The API economy is unquestionably thriving. But with the proliferation of APIs, many methods companies previously used to manage them all have become outdated, according to Gino. “One of our customers is a major financial institution that, prior to RapidAPI, used a spreadsheet to manage their APIs,” he said. “This spreadsheet was updated by distributing it manually to various executives throughout the company once a year. They had very lagging visibility into their APIs.” Internal tools such as Google Docs and Excel spreadsheets offer little in the way of transparency or collaboration in terms of being able to share code and documentation. “They lack key features such as deep search, tagging, and so on — in fact, in some cases, teams could be developing the same API in different organizations or subscribe to the same APIs without ever realizing it,” Gino said. “This leads to inefficiencies and lowered productivity.” The pandemic has also played a part in the continued rise of the API. Many businesses that relied on more traditional offline mechanisms to drive revenues were forced to embrace digital alternatives. And even major companies with long-established digital components have seen those efforts boosted massively by COVID-19 — for example, Starbucks’ mobile orders increased from 17% to 24% of its total transactions in 2020.  RapidAPI itself has benefited greatly from this surge in demand, adding more than 750,000 new developers to its platform over the past 12 months and doubling the number of available APIs to 35,000. Digitalization has “gone from being a nice-to-have to an imperative to survival,” according to Gino. “To react this quickly and accelerate development, companies have to rely on APIs in their software development. It really puts the onus on companies to get APIs right — and fast.” Put another way, the fact that every company is now a software company is one reason APIs are in such hot demand."
https://venturebeat.com/2021/04/21/perception-point-raises-28-million-in-series-b-funding-to-advance-messaging-and-collaboration-protection-for-enterprise-users/,Perception Point Raises $28 Million in Series B Funding to Advance Messaging and Collaboration Protection for Enterprise Users," Company triples recurring revenue amid increased customer demand for advanced cyber threat protection  TEL-AVIV, Israel–(BUSINESS WIRE)–April 21, 2021– Perception Point, a leading email and collaboration security company, offering fast interception of content-based attacks as a service, announced today it has raised $28 million in Series B, bringing the total funding to $48 million. The new funding round was led by Red Dot Capital Partners and joined by global investor NGP Capital along with existing investors Pitango Venture Capital and State of Mind Ventures (SOMV). Yoram Oron and Atad Peled from Red Dot Capital Partners, and Bo Ilsoe from NGP Capital will be joining the company’s board of directors. Funds from this round will be used to fuel rapid growth, expand to new markets, accelerate product innovation and grow the team to support customer demand. In 2020, Perception Point recorded its most successful year since its inception. The company tripled its recurring revenue and expanded its customer portfolio to include users from multiple industries, such as telecom, tech, retail, food and beverage, healthcare, financial services, and more. The company also doubled its number of Fortune 500 customers and developed a strong network of partners, including global resellers and MSSPs. The need for enterprises to be agile, data savvy, and responsive to any change has only intensified since the COVID-19 pandemic outbreak. As a result, companies now deploy on average 6 different communication and collaboration solutions to support their business operations, including email, cloud storage platforms, messaging apps, CRM, and more to help them communicate with internal and external stakeholders. These growing numbers of channels are fertile ground for cybercriminals to launch content-based attacks. Perception Point has formulated a comprehensive three-pillar approach for protecting businesses from content-based attacks, which provides the most effective detection, supports admins and end-users with full Incident Response services, and covers all communication channels. In addition, the solution is offered through easily implementable API technology, enabling clients to get immediate protection. This combination enables Perception Point to not only intercept more incidents prior to compromising end-users, but also ensures that all events are fully and rapidly contained and remediated. It is a cybersecurity service that works for its users, and not the other way around. “Perception Point is perfectly positioned to capture the rapidly growing yet untapped messaging and collaboration market. We are pleased to join forces with the company and the other investors to help the company expand to new verticals and other parts of the world,” said Yoram Oron, chairman and managing partner at Red Dot Capital Partners. “The beauty of Perception Point is that it addresses challenges that many companies encounter today, offering a 360-degree, SaaS solution that enhances enterprise security and allows users to become more agile and responsive.” “We are strong believers in the Israeli cybersecurity ecosystem and its vision,” commented Bo Ilsoe, Partner at NGP Capital. “Perception Point is a great example of a startup solving complex problems with innovative ‘hacks’ in the development of its core IP, leveraging rapid API-based integration and the cloud to provide instant value to customers while continuing to gain strong commercial and technological momentum. We are glad to be part of the company’s journey.” “Our prevention-as-a-service approach solves the customers’ most advanced messaging and collaboration security challenges, showing value immediately,” said Yoram Salinger, CEO of Perception Point. “We combine a 7-layer platform that easily and quickly integrates with any application along with a comprehensive Incident Response service to ensure flawless prevention, monitoring, and remediation of any attack.” About Perception Point: Perception Point is a Prevention-as-a-Service company, offering fast interception of any content-based attack across all collaboration channels, including email, cloud storage, CRM apps, and messaging platforms. The company prevents phishing, BEC, spam, malware, Zero-days, and N-days well before it reaches enterprise users. Deployed in minutes with no change to the enterprise’s infrastructure, the Perception Point solution confirms with any policy and requires zero fuss from IT teams. The company’s Incident Response team serves as a force multiplier to the enterprise’s SOC team. To learn more about Perception Point, visit our website, or follow us on LinkedIn, Facebook, and Twitter.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210421005370/en/ Georgiana ComsaSilicon Valley Public Relationsgeorgiana@siliconvalleypr.com (650) 800-7084"
https://venturebeat.com/2021/04/21/salesforce-introduces-new-sales-cloud-features-to-boost-automation-and-remote-collaboration/,Salesforce introduces new Service Cloud features to boost automation and remote collaboration,"Salesforce today rolled out the next generation of Service Cloud, including enhancements to Cloud Voice and Einstein Bots. The company says that the features and products are intended to address the new reality brought about by the coronavirus pandemic. Over the past year, service agents moved quickly to work from home but were forced to rely on legacy technology that wasn’t designed to manage distributed workforces. Today, parts of the world are beginning to reopen, but these reopenings are raising questions around updated policies, protocols, and safety measures. This adds a new level of challenge for agents, who are already contending with increased workloads. Service Cloud Voice, Salesforce’s service that brings together phone, digital channels, and customer relationship management data, can now connect with existing phone systems via a new product called Service Cloud Voice for Partner Telephony. With Service Cloud Voice for Partner Telephony, enterprises with landlines can benefit from features including real-time call transcription and AI-powered guidance on recommended next steps. Beyond Service Cloud Voice for Partner Telephony, Salesforce is introducing Service Cloud Workforce Engagement, a workforce planning product that uses AI to predict how many requests will come into a contact center and on which channels (for example, phone, email, web chat, text, and social). First announced in December, Service Cloud Workforce Engagement provides agents with a workspace that integrates data, as well as real-time coaching and on-demand training with Salesforce’s MyTrailhead online learning portal. Salesforce also today previewed Pre-built Einstein Bots, a collection of chatbots powered by Einstein Bots, the company’s platform for conversational bots that resolve issues like processing a return or checking a flight. And the company furthered detailed Visual Remote Assistant, a service launched last year that helps companies deliver service without coming on site by walking customers through solutions remotely. Visual Remote Assistant, an offshoot of Salesforce Field Service, provides tools like annotations, a live pointer, and more. Leveraging AI-powered character recognition and scalable video, the service launches sessions in a browser, integrating customer service data from Service Cloud, Field Service, and third-party systems. Service Cloud Workforce Engagement and Service Cloud Voice for Partner Telephony are expected to be generally available in June 2021. Pricing information will be made available at general availability. Visual Remote Assistant is generally available today, while Pre-Built Einstein Bots are currently in beta and are expected to be generally available in October 2021. According to Kate Leggett, VP and principal analyst at Forrester, the new Service Cloud features align with trends accelerated by the pandemic. For example, according to Salesforce, 61% of salespeople believe their roles have changed since the pandemic began. Even when salespeople are able to return to the road and in-person workplaces, 51% expect to travel less than they did before the pandemic — and fewer than half expect to go back to an office. “Customer service leaders must stay abreast of three megatrends in 2021 as they weather the storm,” Leggett wrote in a recently published report. “They are: AI-fueled digital experiences underpin great customer service, modern agent desktops empower agents to best serve customers, and customer service technology enables resilience and sustainability.”"
https://venturebeat.com/2021/04/21/hives-cloud-hosted-machine-learning-models-draw-85m/,Hive’s cloud-hosted machine learning models draw $85M,"While cloud computing continues to gain favor, only a limited number of companies have embraced machine learning based in the cloud. Hive wants to change this by allowing enterprises to access hosted machine learning models via APIs. Hive has had particular success in the area of content moderation, thanks to its deep learning models that help companies interpret unstructured data, like images, videos, and audio. But it’s also expanding into areas like advertising and sponsorship measurement as it seeks to find other areas that would benefit from intelligent automation. In an interview with VentureBeat, Hive CEO Kevin Guo said the company kept relatively quiet as it sought to prove its models work. But its growth has started to accelerate, and the company is getting ready to make more noise. “Now that we have enough tracking points and we have over 100 customers, we are quite confident what we have in the market does actually work,” Guo said. “Now we’re ready to scale up.” Investors are also excited by what they see. Today, Hive announced it has raised $85 million in funding over two rounds that put its total raised at $121 million and bring its valuation to $2 billion. Glynn Capital led a $50 million round, which followed a $35 million round the company had not previously disclosed. Guo said that when the company was founded in 2014, he and cofounder Dmitriy Karpman were at Stanford studying computer vision. They originally began building consumer apps that used AI to improve things like content recommendations. But along the way, they encountered issues around content moderation and couldn’t find models that solved them. As they started building a solution, other companies heard about it and asked if they could try it. By the end of 2017, the company had become enterprise-focused and the current incarnation of Hive was born. Even then, Guo said the founders took a slow and steady approach. They continued to deploy the service to partners who are now using it to monitor every piece of content shared by users. If Hive spots a problem, something like a video stream can be instantly shut down. Guo said Hive has fewer false positives than some alternatives on the market, which lowers the risk of blocking a non-violating piece of content. “If your models are inaccurate and you’re banning 30% of your users’ content incorrectly, that’s a real problem,” Guo said. Guo said the key to Hive’s accuracy is the massive amount of data fed into the models as they have been developed. To do that, Hive has built a distributed workforce of data labelers. “They have fed in now billions of human judgments,” Guo said. “And that’s what makes this model work so well. At this point, our clients basically view that our models are pretty much at or even above human accuracy, which is why they can trust [them] 100% and use [them] in real-time in production.” Hive claims its models have been trained on 1 billion pieces of human-labeled data, the largest such public dataset. That allows Hive to screen for 40 classes of content categories, such as sexual content, violence, and hate speech. This work has put the company in the middle of the debate over supervised versus semi-supervised and unsupervised learning. Guo said the right answer really depends on the nature of the company’s service. But for Hive, the human element is essential. “There’s nothing quite like humans, truthfully,” he said. “Humans are really good at generating labeling data, finding patterns, and solving hard problems. And so that’s ground truth data for models. We’ve always believed that human training is necessary. Our approach has been generally to stick with [the] tried and true path of using humans to train our models, first and foremost.” The ability to offer the service as an API means clients just have to drop a few lines of code into their service to be up and running, Guo said. That ease of use has helped adoption. According to Guo, the company has seen 300% revenue growth over the past year. Customers include NBCUniversal, Interpublic Group, Reddit, Walmart, Visa, Anheuser-Busch InBev, Comscore, and Cognizant. Hive intends to use the funding to continue development of the company’s cloud-based deep learning models. It also plans to invest in building out its sales and marketing teams. “Up until now, we’ve really been operating with a bit more of a conservative mindset,” Guo said. “We didn’t want to over-invest in sales and marketing until we knew our product [worked]. It took a while. It takes time to prove these models out.”"
https://venturebeat.com/2021/04/21/blockchain-startup-digital-asset-nabs-120m-to-knock-down-data-silos/,Blockchain startup Digital Asset nabs $120M to knock down data silos,"Blockchain-based app development platform Digital Asset today announced that it raised $120 million in series D funding led by 7Ridge and Eldridge. The company plans to use the funding to expand its team and product portfolio with a protocol that enables data to interface across blockchains and databases. Digital Asset was cofounded 2014 by Sunil Hirani, Don Wilson, Yuval Rooz, Shaul Kfir, and Eric Saraniecki, and its flagship product is a distributed ledger technology for banks and other financial, health care, and insurance institutions. The company asserts that the blockchain can transform disparate data silos into synchronized networks, minimizing latency and errors by ensuring that data remains consistent. “I’ve always been passionate about technology and innovation. While I was working at DRW, I had the great opportunity to get out from behind the trading desk and work side by side with Don,” Rooz told VentureBeat via email. “He challenged me to find interesting companies in which to invest on behalf of DRW. At that time, Bitcoin and cryptocurrency were gaining traction and we were able to make some rewarding investments in that space. On the technology side, we developed an investment thesis and spent nearly two years trying to find companies that fit the bill so to speak, but no such company existed. In October 2014, with the support of Don, I left DRW to start Digital Asset.” Digital Asset developed an open source smart contracting language called DAML that’s designed to run on various ledgers. The company hosts a marketplace featuring premade DAML scripts from Accenture, Wipro, and other companies, addressing uses cases like repackaging mortgage-backed securities and reducing credit risk in repo markets. DAML can also run on top of non-blockchain solutions like Amazon Aurora, which works with MySQL, Postgres, and other conventional database technologies. Digital Asset also offers DABL, a hosting platform for developers to deploy their DAML scripts into production. DABL helps to abstract away the deployment, management, and scaling of distributed apps, functioning as a live environment for apps that involve flows between people, companies, and markets. Digital Asset’s highest-profile project is perhaps the distributed ledger system scheduled to be adopted by the Australian Securities Exchange (ASX) in 2023. The system, which was built using DAML and the ISO 20022 protocol, a messaging standard adopted by the Reserve Bank of Australia, is expected to offer better performance, resilience, and security as well as new functions compared with ASX’s existing CHESS system. “We continue to build our customer base,” CFO and COO Emnet Rios said. “We are working with 5 of the 10 global stock exchanges, including Hong Kong Exchanges and Singapore Exchange, as well as companies across financial services, like Broadridge and BNP Paribas, health care, like Change Healthcare, and a variety of customers across multiple industries, including insurance, that have not yet been announced.” Buoyed by recent events, blockchain adoption is on the rise across industries. Thirty-nine percent of respondents to a 2020 Deloitte survey said they’d already put blockchain into production, compared to 23% in 2019. The production figure was even higher, at 46%, for organizations with more than $1 billion in revenues. Gartner forecasts that blockchain will generate an annual business value of more than $3 trillion by 2030. “Our vision is for a world of countless systems, each powered by infrastructure that suits their own unique requirements,” Rios said. “We call this vision the global economic network. To date, there is nothing of its kind that exists today. With the additional funding we can take Daml to the next level, creating a global ecosystem of interconnected, heterogeneous technologies tied together with a common protocol. We are trailblazing a path for the future of how disparate systems will interoperate regardless of the underlying technology.” Digital Asset’s latest round of funding brings the New York-based company’s total raised to over $300 million."
https://venturebeat.com/2021/04/21/marketing-automation-startup-activecampaign-raises-240m/,Marketing automation startup ActiveCampaign raises $240M,"Customer experience automation startup ActiveCampaign today announced it has raised $240 million at an over $3 billion valuation. The capital comes as the startup surpasses $165 million in annual recurring revenue, up from $90 million a year ago. CEO Jason VandeBoom created the company in 2003 to help businesses orchestrate email marketing, customer relationship management, and ad campaign processes. ActiveCampaign’s automations are powered in part by AI and machine learning algorithms and customized with flows that inform hundreds of unique experiences generated dynamically for each customer. The flows live within a visual automation map that reveals which are connected to each other, active, or in need of adjustment. ActiveCampaign’s platform enables brands to broadcast emails or configure triggers that send messages based on purchase intent, site visits, and engagement. Segmenting tools let managers group audiences and orchestrate email autoresponders, funnels, and scheduled emails. Meanwhile, an integrated drag-and-drop email designer enables features like revision histories, geotracking, analytics, image hosting, and social sharing. ActiveCampaign supports split testing — up to five emails with different subject lines, content, images, and calls to action can be tested at once — and records metrics like conversion rate, opens, and more to bubble the best-performing options to the top. A split actions feature allows customers with a certain number of products to make offers until they sell out, for example, before sending other contacts down a different automation path. And extensive event tracking saves logins, plays, clicks, video views, orders, and more across apps, membership sites, and online portals. Beyond email, ActiveCampaign features a text message marketing platform that supports scheduling, notifying, and automating, as well as social media audience targeting. Companies can target people with Facebook Ads and draw on data to automatically retarget based on visits to websites, product interests, form submissions, custom fields, and other collected information. A brand can set up a welcome series and other automations and view them from within a single dashboard using ActiveCampaign. Or they can tap migration services that move emails, forms, and contacts from other platforms to ActiveCampaign’s own. They’re also able to implement site tracking that follows up on customer interactions with triggered messages and tracks goals and conversions to evaluate the effectiveness of ongoing campaigns. And they can employ lead scoring to pick out top engaged contacts and offer what they want. “[We] rely on machine learning to help our customers better understand their own customers,” a spokesperson told VentureBeat via email. “One example is sentiment analysis, which looks at the emails of a customer or prospect and analyzes the sentiment of that communication to trigger automations. This means we can easily automate a happy customer receiving a thank you note, while a frustrated customer can receive a support call — all without manual intervention.” ActiveCampaign says it achieved record usage in 2020, including 4 billion weekly automated experiences, 150 million monthly automated campaigns, and 2 million daily predictions. The company also launched functionality like pages and web personalization, plus a predictive feature that helps to navigate its over 500 automation recipes. The platform’s ecosystem grew to over 850 technology partners, up from 200 as of January 2020. And ActiveCampaign announced plans to grow to over 1,000 employees by the end of 2021, after adding 300 in 2020. The global campaign management software market was valued at approximately $1.85 billion in 2017 and is anticipated to grow at 15.65% from 2018 to 2025. Competitors abound. There’s Pyze, which raised $4.6 million in July 2019 for its AI-driven analytics and marketing campaign orchestration tool, and Clari, which recently nabbed $60 million to further develop its AI sales pipeline toolset. That’s not to mention TapClicks, which raised $10 million in August 2019; Boomtrain; Albert Technologies; 6Sense; Appier; Highspot; and Panoramic. But VandeBoom notes that ActiveCampaign’s client list is swiftly growing, with over 145,000 customers in more than 170 countries — 50,000 of which were added in the last year. “The need for enhanced customer experiences is fueling growth at ActiveCampaign and the growth of businesses around the globe. Companies are becoming increasingly frustrated with legacy marketing automation tools and customer relationship management platforms that only solve for one part of the customer experience,” he told VentureBeat via email. “Our customer experience automation solution helps companies grow by automating 1:1 communication during the entire customer lifecycle, from awareness to acquisition to advocacy, and creates experiences that turn customers from first-time purchasers into long-term brand advocates.” Tiger Global led the series C round announced today, which included new investor Dragoneer. Existing investors Susquehanna Growth Equity and Silversmith Capital Partners also participated. The round brings the Chicago, Illinois-based company’s total raised to date to $360 million."
https://venturebeat.com/2021/04/21/hackers-exploit-sonicwall-email-security-vulnerability/,Hackers exploit SonicWall email security vulnerability,"(Reuters) — Hackers have targeted customers of California-based network services firm SonicWall via a previously undisclosed vulnerability in its email security product, the company and cybersecurity firm FireEye said Tuesday. In a statement, SonicWall said that the vulnerability had been “exploited in the wild,” meaning hackers had already used the flaw to break into target systems. SonicWall urged customers to “immediately upgrade” to a version that patched the hole. The intrusions are the latest in a string of hacks using third-party-provided software and hardware in the United States. The most notable — the compromise of SolarWinds by alleged Russian hackers last year — has raised concerns about the ability of end users to vet the security of their devices and their programs. Last month, it was disclosed that an unknown number of Microsoft customers had been compromised after an allegedly Chinese hacking group made use of serious vulnerabilities in the company’s email server software. Just last week, a breach with potentially serious knock-on consequences was reported at San Francisco-based software auditing firm Codecov. Earlier on Tuesday, hackers were outed for exploiting a serious vulnerability in VPN devices made by Utah-based IT firm Ivanti. In SonicWall’s case, hackers could have used the weakness to easily gain “a pretty significant foothold” in their targets’ networks, said Charles Carmakal, a senior VP of Mandiant, an arm of FireEye. He said his firm didn’t have a clear idea of who the hackers were and said that he was aware of “fewer than five” victims. SonicWall did not immediately respond to a Reuters’ call for comment."
https://venturebeat.com/2021/04/20/only-6-of-organizations-have-adopted-ai-powered-solutions-study-finds/,"Only 6% of companies have adopted AI, study finds","In a new survey of over 700 C-suite executives and IT decision-makers examining AI adoption in the enterprise, Juniper Networks found that 95% of respondents believe their organization would benefit from embedding AI into their daily operations. However, only 6% of those respondents reported adoption of AI-powered solutions across their business. The findings agree with other surveys showing that, despite enthusiasm around AI, companies struggle to deploy AI-powered services in production. Enterprise use of AI grew a whopping 270% over the past several years, Gartner recently reported, while Deloitte says 62% of respondents to its corporate October 2018 study adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of businesses that have seen half their AI projects fail will tell you. According to the Juniper survey, top challenges around AI remain ingesting, processing and managing data. In other tech stack trends, 39% of respondents said that they were likely to collect telemetry data to improve “user experience” AI embedded in products, while 34% noted that AI tool capabilities are the most critical in order to enable AI adoption. Juniper’s is the second recent report to peg data issues as the reason organizations fail to successfully deploy AI. Data scientists spend the bulk of their time cleaning and organizing data, according to a 2016 survey by CrowdFlower. And respondents to Alation’s latest quarterly State of Data Culture Report said that inherent biases in the data being used in their AI systems produce discriminatory results that create compliance risks for their organizations. A majority (73%) of Juniper survey respondents said that their organizations were struggling with expanding their workforce to integrate with AI systems. C-level executives reported that they feel it’s more of a priority to hire people than to develop AI capabilities within their business. Laments over the AI talent shortage have become a familiar refrain from private industry. O’Reilly’s 2021 AI Adoption in the Enterprise paper found that a lack of skilled people and difficulty hiring topped the list of challenges in AI, with 19% of respondents citing it as a “significant” barrier. In 2018, Element AI estimated that of the 22,000 Ph.D.-educated researchers globally working on AI development and research, only 25% are “well-versed enough in the technology to work with teams to take it from research to application.” Tencent says that there are about 300,000 AI professionals worldwide but “millions” of roles available. And a 2019 Gartner survey found that 54% of chief information officers view this skills gap as the biggest challenge facing their organization. On the topic of AI governance, 87% of executives told Juniper that they believe organizations have a responsibility to implement policies that minimize the negative impacts of AI. Despite this, executives ranked establishing AI governance, policies, and procedures as one of their lowest priorities. And only 7% of survey takers said their organizations have established a company-wide leader who oversees AI strategy and governance. Growing evidence suggests that organizations are implementing AI less responsible than they internally assume. According to a recent Boston Consulting Group survey of 1,000 enterprises, less than half that achieved AI at scale had fully mature, responsible AI implementations, according to the same report. The lagging adoption of responsible AI belies the value that these practices can bring to bear. A study by Capgemini found that customers and employees will reward organizations that practice ethical AI with greater loyalty, more business, and even a willingness to advocate for them — and in turn, punish those that don’t. The study suggests that there’s both reputational risk and a direct impact on the bottom line for companies that don’t approach the issue thoughtfully."
https://venturebeat.com/2021/04/20/why-a-cedars-sinai-hospital-and-bp-use-facial-recognition-exclusive/,Why a Cedars-Sinai hospital and BP use facial recognition,"(Reuters) — Deployments of facial recognition from Israeli startup AnyVision show how the surveillance software has gained adoption across the United States even as regulatory and ethical debates about it rage on. The technology finds certain faces in photos or videos, with banks representing one sector that has taken interest in systems from AnyVision or its many competitors to improve security and service. Organizations in other industries are chasing similar goals. Los Angeles hospital Cedars-Sinai and oil giant BP are among several previously unreported users of AnyVision. Cedars-Sinai’s main hospital uses AnyVision facial recognition to give staff a heads-up about individuals known for committing violence or drug fraud or using different names at the emergency room, three sources said. Cedars said it “does not publicly discuss our security programs” and could not confirm the information. Meanwhile, BP has used facial recognition for at least two years at its Houston campus to help security staff detect people on a watchlist because they had previously trespassed or issued threats, two sources said. BP declined to comment. AnyVision declined to discuss specific clients or deals. Gaining additional clients may be difficult for AnyVision amid mounting opposition to facial recognition from civil liberties advocates. Critics say the technology compromises privacy, targets marginalized groups, and normalizes intrusive surveillance. Last week, 25 social justice groups, including Demand Progress and Greenpeace USA, called on governments to ban corporate use of facial recognition in an open letter. AnyVision CEO Avi Golan, a former SoftBank Vision Fund operating partner who joined the startup in November, sees a bright future. He told Reuters that AnyVision has worked with companies across retail, banking, gaming, sports, and energy on uses that should not be banned because they stop crime and boost safety. “I am a bold advocate for regulation of facial recognition. There’s a potential for abuse of this technology both in terms of bias and privacy,” he said. “[But] blanket bans are irresponsible.” The startup has faced challenges in the past year. AnyVision laid off half of its staff, with deep cuts to research and sales, according to people who have worked for the company, as well as customers and partners, all speaking on the condition of anonymity. The slashing followed the onset of COVID-19 shrinking clients’ budgets, sources said, with investor Microsoft in March 2020 saying it would divest its stake over ethical concerns. AnyVision announced raising an additional $43 million last September. Macy’s installed AnyVision in 2019 to alert security when known shoplifters entered its store in New York’s Herald Square, five sources said. The deployment expanded to around 15 more New York stores, three sources said, and if not for the pandemic would have reached an additional 15 stores, including on the West Coast. Macy’s told Reuters it uses facial recognition “in a small subset of stores with high incidences of organized retail theft and repeat offenders.” Menards, a U.S. home improvement chain, has used AnyVision facial recognition to identify known thieves, three sources said. Its system has also alerted staff to the arrival of design center clients and reidentified them on future visits to improve service, a source said. Menards said its current face mask policy has rendered “any use of facial recognition technology pointless.” In an online video, and without naming Menards, AnyVision has touted its results, and two sources said the companies struck a deal for 290 stores. In 2019, Menards apprehended 54% more potential threats and recovered over $5 million, according to the video. The U.S. financial services unit of automaker Mercedes-Benz said it has used AnyVision at its Fort Worth, Texas offices since 2019 to authenticate about 900 people entering and exiting daily before the pandemic, adding a layer of security on top of building access cards. Such employee-access applications are a common early use of AnyVision, including at Houston Texans’ and Golden State Warriors’ facilities, sources said. The sports teams declined to comment. Several deals have failed to materialize, however. Among organizations that considered AnyVision early last year were Amazon’s grocery chain Whole Foods to monitor workers at stores, Comcast to enable ticketless experiences at Universal theme parks, and baseball’s Dodger Stadium for suite access, sources said. Talks with airports in the Dallas and San Francisco areas referenced in public records have not led to contracts either. Universal Parks, the Los Angeles Dodgers, and the airports all declined to comment on their interest. And Whole Foods did not respond to a request for comment. Government requirements for surveillance at casinos have made the gaming industry a big purchaser of facial recognition. Las Vegas Sands, for instance, is using AnyVision, three sources said. Sands declined to comment. MGM Resorts International and Cherokee Nation Entertainment also use AnyVision, representatives of the casino operators said last month in an online presentation seen by Reuters. Ted Whiting of MGM said the software, deployed in 2017 and used at 11 properties, including the Aria in Las Vegas, has detected vendors not wearing masks and helped catch patrons accused of violence. MGM said its “surveillance system is designed to adhere to regulatory requirements and support ongoing efforts to keep guests and employees safe.” Cherokee’s Joshua Anderson said in addition to security uses, AnyVision has accelerated coronavirus contact tracing as the Oklahoma company rolls out the technology across 10 properties."
https://venturebeat.com/2021/04/20/shine-phoenix-merger-focused-on-advancing-fusion-technology/,"SHINE, Phoenix merger focused on advancing fusion technology","JANESVILLE & FITCHBURG, Wis.–(BUSINESS WIRE)–April 20, 2021– SHINE Medical Technologies LLC and Phoenix LLC today announced that the companies have completed a merger under which Phoenix has become a wholly owned subsidiary of SHINE. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210420006038/en/ SHINE is a next-generation nuclear technology company focused on unlocking the power of fusion technologies to benefit the planet and humankind. The company’s goal is to deliver on the long-term promise of clean fusion energy by advancing fusion technology starting with the commercialization of medical isotopes. Phoenix designs and manufactures the world’s strongest steady-state fusion neutron generators used for advanced industrial imaging and other applications for improving safety and quality in the aerospace, defense, medical and energy sectors. The combined company represents the first two phases of the long-term vision of Greg Piefer, the founder of both companies, for producing clean energy from fusion (see “SHINE’s Four-Phase Progression to Clean Energy Production” below). The goal of each phase of SHINE’s approach is to build additional capacity and capability, and deepen scientific understanding of fusion technology as it progresses to clean fusion energy production. Each step through the four phases is expected to provide further proof of the technology’s robustness, a foundation for ongoing innovation in the next phase and the creation of value for the company, its customers, and shareholders. “SHINE and Phoenix have shared a common long-term vision and operated in close collaboration during the past 11 years, but it’s always been inefficient to operate as separate companies,” said Greg Piefer, CEO of SHINE. “Coming together will enable us to advance fusion technology more quickly by aligning interests and combining complementary core competencies. Through the four phases, we are taking a deliberate approach to building a company that can ultimately deliver cost-effective, clean fusion energy to billions, while serving important near-term market needs like advanced industrial imaging and medical isotopes, along the way.” For a video of additional comments from Greg Piefer, please click here (:46 broadcast-quality available for the media). Phoenix has developed a strong track record of commercialization and revenue generation by applying its fusion-based technology to applications such as advanced industrial imaging, which can image modern materials in great detail, addressing quality assurance and safety needs in the aerospace, defense, energy, and other industries. These applications are part of Phase 1 of the four-phase approach. The second phase of the approach involves applications of nuclear fusion to replace nuclear reactors used in the production of life-saving medical isotopes for diagnostic imaging, like molybdenum-99 (Mo-99), and with potential use as cancer therapeutics like lutetium-177 (Lu-177). This month, SHINE kicked off Phase 2 commercialization when it began producing Lu-177. In 2022, SHINE expects to commence production of up to 20 million doses of Mo-99 per year in its fusion-powered production facility in Janesville, Wis. The facility is expected to be the world’s largest-capacity medical isotope production plant. “This merger is a natural evolution of our strong existing partnership with SHINE, rooted in our common origin and shared mission,” said Evan Sengbusch, general manager of SHINE’s Phoenix division. “Phoenix’s track record of successfully deploying our core neutron generation technology across multiple demanding market sectors has provided important commercial validation and risk reduction for critical technologies that underpin execution in Phase 2. We are excited to join with SHINE and leverage our complementary nuclear capabilities to advance towards clean fusion energy production.” For a video of additional comments from Evan Sengbusch, please click here (1:24 broadcast-quality available for the media). Phoenix was founded in 2005 by Piefer to develop and commercialize a unique technology that generated neutrons through fusion. He spun SHINE out of Phoenix in 2010 to apply that technology to medical isotope production and other applications through the four-phase approach. Evercore Group L.L.C. served as exclusive financial advisor to SHINE. Foley & Lardner served as lead legal counsel to SHINE. SVB Leerink served as exclusive financial advisor to Phoenix. Godfrey & Kahn S.C. served as lead legal counsel to Phoenix. SHINE’s Four-Phase Progression to Clean Energy Production About SHINE Medical Technologies SHINE is a nuclear technology company committed to improving the lives of people and the planet. The company is focusing its fusion-based technology initially on advanced industrial imaging and the production of diagnostic and therapeutic isotopes. These isotopes include molybdenum-99, a diagnostic isotope used to diagnose heart disease, cancer, and other conditions, and lutetium-177, a therapeutic isotope that holds the promise of significantly improving the outcome of some cancer patients. SHINE has a long-term strategy to solve some of humanity’s biggest problems, including nuclear waste recycling and the production of clean fusion energy, in addition to advanced industrial imaging and medical isotopes, by pursuing our vision for progressively broad and impactful uses of fusion technology. For more information about SHINE, please visit our website at www.shinemed.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420006038/en/ MALLORY PROUTYCORPORATE COMMUNICATIONS PROJECT MANAGER, MBA P: 608-530-5606 | M: 630-945-2379mallory.prouty@shinemed.com ROD HISEDIRECTOR OF STRATEGIC COMMUNICATIONS P 608-530-5659 | M 608-770-7850rod.hise@shinemed.com"
https://venturebeat.com/2021/04/20/cerebras-systems-launches-new-ai-supercomputing-processor-with-2-6-trillion-transistors/,Cerebras launches new AI supercomputing processor with 2.6 trillion transistors,"Cerebras Systems has unveiled its new Wafer Scale Engine 2 processor with a record-setting 2.6 trillion transistors and 850,000 AI-optimized cores. It’s built for supercomputing tasks, and it’s the second time since 2019 that Los Altos, California-based Cerebras has unveiled a chip that is basically an entire wafer. Chipmakers normally slice a wafer from a 12-inch-diameter ingot of silicon to process in a chip factory. Once processed, the wafer is sliced into hundreds of separate chips that can be used in electronic hardware. But Cerebras, started by SeaMicro founder Andrew Feldman, takes that wafer and makes a single, massive chip out of it. Each piece of the chip, dubbed a core, is interconnected in a sophisticated way to other cores. The interconnections are designed to keep all the cores functioning at high speeds so the transistors can work together as one. In 2019, Cerebras could fit 400,000 cores and 1.2 billion transistors on a wafer chip, the CS-1. It was built with a 16-nanometer manufacturing process. But the new chip is built with a high-end 7-nanometer process, meaning the width between circuits is seven billionths of a meter. With such miniaturization, Cerebras can cram a lot more transistors in the same 12-inch wafer, Feldman said. It cuts that circular wafer into a square that is eight inches by eight inches, and ships the device in that form. “We have 123 times more cores and 1,000 times more memory on chip and 12,000 times more memory bandwidth and 45,000 times more fabric bandwidth,” Feldman said in an interview with VentureBeat. “We were aggressive on scaling geometry, and we made a set of microarchitecture improvements.” Now Cerebras’ WSE-2 chip has more than twice as many cores and transistors. By comparison the largest graphics processing unit (GPU) has only 54 billion transistors — 2.55 trillion fewer transistors than the WSE-2. The WSE-2 also has 123 times more cores and 1,000 times more high performance on-chip high memory than GPU competitors. Many of the Cerebras cores are redundant in case one part fails. “This is a great achievement, especially when considering that the world’s third largest chip is 2.55 trillion transistors smaller than the WSE-2,” said Linley Gwennap, principal analyst at The Linley Group, in a statement. Feldman half-joked that this should prove that Cerebras is not a one-trick pony. “What this avoids is all the complexity of trying to tie together lots of little things,” Feldman said. “When you have to build a cluster of GPUs, you have to spread your model across multiple nodes. You have to deal with device memory sizes and memory bandwidth constraints and communication and synchronization overheads.” The WSE-2 will power the Cerebras CS-2, the industry’s fastest AI computer, designed and optimized for 7 nanometers and beyond. Manufactured by contract manufacturer TSMC, the WSE-2 more than doubles all performance characteristics on the chip — the transistor count, core count, memory, memory bandwidth, and fabric bandwidth — over the first generation WSE. The result is that on every performance metric, the WSE-2 is orders of magnitude larger and more performant than any competing GPU on the market, Feldman said. TSMC put the first WSE-1 chip in a museum of innovation for chip technology in Taiwan. “Cerebras does deliver the cores promised,” Patrick Moorhead, an analyst at Moor Insights & Strategy. “What the company is delivering is more along the lines of multiple clusters on a chip. It does appear to give Nvidia a run for its money but doesn’t run raw CUDA. That has become somewhat of a de facto standard. Nvidia solutions are more flexible as well as they can fit into nearly any server chassis.” With every component optimized for AI work, the CS-2 delivers more compute performance at less space and less power than any other system, Feldman said. Depending on workload, from AI to high-performance computing, CS-2 delivers hundreds or thousands of times more performance than legacy alternatives, and it does so at a fraction of the power draw and space. A single CS-2 replaces clusters of hundreds or thousands of graphics processing units (GPUs) that consume dozens of racks, use hundreds of kilowatts of power, and take months to configure and program. At only 26 inches tall, the CS-2 fits in one-third of a standard datacenter rack. “Obviously, there are companies and entities interested in Cerebras’ wafer-scale solution for large data sets,” said Jim McGregor, principal analyst at Tirias Research, in an email. “But, there are many more opportunities at the enterprise level for the millions of other AI applications and still opportunities beyond what Cerebras could handle, which is why Nvidia has the SuprPod and Selene supercomputers.” He added, “You also have to remember that Nvidia is targeting everything from AI robotics with Jenson to supercomputers. Cerebras is more of a niche platform. It will take some opportunities but will not match the breadth of what Nvidia is targeting. Besides, Nvidia is selling everything they can build.” And the company has proven itself by shipping the first generation to customers. Over the past year, customers have deployed the Cerebras WSE and CS-1, including Argonne National Laboratory; Lawrence Livermore National Laboratory; Pittsburgh Supercomputing Center (PSC) for its Neocortex AI supercomputer; EPCC, the supercomputing center at the University of Edinburgh; pharmaceutical leader GlaxoSmithKline; Tokyo Electron Devices; and more. Customers praising the chip include those at GlaxoSmithKline and the Argonne National Laboratory. Kim Branson, senior vice president at GlaxoSmithKline, said in a statement that the company has increased the complexity of the encoder models it generates while decreasing training time by 80 times. At Argonne, the chip is being used for cancer research and has reduced the experiment turnaround time on cancer models by more than 300 times. “For drug discovery, we have other wins that we’ll be announcing over the next year in heavy manufacturing and pharma and biotech and military,” Feldman said. The new chips will ship in the third quarter. Feldman said the company now has more than 300 engineers, with offices in Silicon Valley, Toronto, San Diego, and Tokyo."
https://venturebeat.com/2021/04/20/study-finds-that-detoxified-language-models-might-marginalize-minority-voices/,"‘Detoxified’ language models might marginalize minorities, says study","AI language models like GPT-3 have an aptitude for generating humanlike text. A key factor is the large datasets, scraped from the web, on which they’re trained. But because the datasets are often too large to filter with precision, they contain expletives, slurs, and other offensive and threatening speech. Language models unavoidably learn to generate toxic text when trained on this data. To address this, research has pivoted toward “detoxifying” language models without affecting the quality of text that they generate. Existing strategies employ techniques like fine-tuning language models on nontoxic data and using “toxicity classifiers.” But while these are effective, a new study from researchers at the University of California, Berkeley, and the University of Washington finds issue with some of the most common detoxification approaches. According to the coauthors, language model detoxification strategies risk marginalizing minority voices. Natural language models are the building blocks of apps including machine translators, text summarizers, chatbots, and writing assistants. But there’s growing evidence showing that these models risk reinforcing undesirable stereotypes, mostly because a portion of the training data is commonly sourced from communities with gender, race, and religious prejudices. Detoxification has been proposed as a solution to this problem, but the coauthors of this latest research — as well as research from the Allen Institute — found that the technique can amplify rather than mitigate biases. In their study, the UC Berkeley and University of Washington researchers evaluated “detoxified” language models on text with “minority identity mentions” including words like “gay” and “Muslim,” as well as surface markers of African-American English (AAE). AAE, also known as Black English in American linguistics, refers to the speech distinctive to many Black people in the U.S. and Canada. The researchers — who used GPT-2, the predecessor to GPT-3, as a test model — showed that three different kinds of detoxification methods caused a disproportionate increase in language model perplexity on text with African-American English and minority identity mentions. In machine learning, perplexity is a measurement of the quality of a model’s outputs — lower is generally better. Using a curated version of English Jigsaw Civil Comments for training, a dataset from Alphabet-owned anti-cyberbullying firm Jigsaw, the researchers found that perplexity increased by a factor of 2.1 on nontoxic “white-aligned English” data and a factor of 4.3 on minority identity mention data. Increasing the strength of the detoxification worsened the bias. Why might this happen? The coauthors speculate that toxicity datasets like English Jigsaw Civil Comments contain spurious correlations between the presence of AAE and minority identity mentions and “toxic” labels — the labels from which the language models learn. These correlations cause detoxification techniques to steer models away from AAE and minority identity mentions because the models wrongly learn to consider these aspects of language to be toxic. As the researchers note, the study’s results suggest that detoxified language models deployed into production might struggle to understand aspects of minority languages and dialects. This could force people using the models to switch to white-aligned English to ensure that the models work better for them, which could discourage minority speakers from engaging with the models to begin with. Moreover, because detoxified models tend to avoid certain topics mentioning minority identity terms, like religions including Islam, they could lead to ostracization and a lack of informed, conscious discussion on topics of identity. For example, tailoring an language model for white-aligned English could stigmatize AAE as incorrect or “bad” English. In the absence of ways to train accurate models in the presence of biased data, the researchers propose improving toxicity datasets as a potential way forward. “Language models must be both safe and equitable to be responsibly deployed in practice. Unfortunately, state-of-the-art debiasing methods are still far from perfect,” they wrote in the paper. “We plan to explore new methods for debiasing both datasets and models in future work.” The increasing attention on language biases comes as some within the AI community call for greater consideration of the role of social hierarchies like racism. In a paper published last June, Microsoft researchers advocated for a closer examination and exploration of the relationships between language, power, and prejudice in their work. The paper also concluded that the research field generally lacks clear descriptions of bias and fails to explain how, why, and to whom that bias is harmful."
https://venturebeat.com/2021/04/20/is-nvidias-arm-deal-really-a-u-k-national-security-threat/,Is Nvidia’s Arm deal really a U.K. national security threat?,"A British government official added a new wrinkle to United Kingdom’s investigation of Nvidia’s proposed $40 billion Arm acquisition by directing the agency to probe the national security implications of the buyout. The U.K.’s Competition and Markets Authority, which scrutinizes mergers and acquisitions on anti-competition and monopoly grounds, launched an investigation into the deal’s impact on competition back in January. On Monday, the U.K.’s Secretary of State for Digital, Culture, Media and Sport directed the CMA to consider the national security component. The directive is ostensibly because the deal would transfer ownership of Arm, a crown jewel of the U.K.’s high-tech portfolio, to a foreign-owned (in this case, American) entity. The Secretary asked the CMA, whose role is similar to that of the U.S. Federal Trade Commission, to report its findings by July 30. Arm ecosystem partners I’ve spoken with hope the latest development is a sign regulators are waking up to the notion that the buyout is bad for just about everybody but Nvidia. It’s an understandable sentiment. The fortunes of hundreds of licensees, developers, and others are so inexorably enmeshed with the Arm platform that they have no choice but to accept the deal and stick with the relationship post-acquisition. Many of them view the acquisition — a transfer of ownership from a neutral third party that benefits when everyone is successful to an aggressive competitor that stands to gain at their expense — as an existential threat. But they are unwilling to speak on the record for fear of retribution from Nvidia, should regulators approve the deal over their objections. The FTC is reportedly taking a second, more intense look at the deal as well. Which it should. Arm is a foundational building block for myriad electronics markets, including smartphones, wearables, automobiles, industrial robotics, IoT, and — increasingly — the datacenter, as I said in today’s Feibus Tech report, Nvidia and Arm: The Perils of Technology Platform Acquisitions. If Nvidia owned Arm, it could, for example, focus resources on the market it cares most deeply about — specifically, the datacenter, where its high-profit GPUs are a leading source of processing power for artificial intelligence applications. It could forcibly link its own GPUs to Arm cores — whether applications needed them or not — and rationalize much higher licensing fees as a result. To allow any company with a vested interest to take the reins of an organization that has to date functioned as a neutral standards-setting body is blatantly anti-competitive. The company underscored its interest in the datacenter last week at its annual GTC developer conference, when it announced a new Arm-based datacenter processor that will feature its own proprietary high-speed link to its GPUs. Project Grace, which is based on a new Arm datacenter platform unveiled last September, won’t hit the market before 2023. Grace is widely seen as a shot across the bow at Intel, which dominates the datacenter CPU market. But it is far more than that. Indeed, given that Nvidia hopes to be the eventual owner of Arm, the news of Project Grace should raise questions for regulators. For example, why does Nvidia feel it needs to buy Arm, when it can develop a new CPU as a licensee for a fraction of the cost? Nvidia also raised eyebrows at GTC 21 when it revealed that Project Grace would feature NVLink, the company’s proprietary high-speed data connection between the CPU and its market-leading GPUs for AI in the datacenter. This means none of the growing number of AI alternatives would work with the new CPU. As an Arm licensee, Nvidia’s design decision makes absolute sense. But coming from the hopeful owner of the platform, the move smacks of anticompetitive behavior. Once it owned Arm, would Nvidia carry the design choice over to that side of the house, thereby locking AI alternatives out of Arm-based datacenter systems? Would it weave NVLink into an Arm license, raising the price as a result? Might the company also cut competitive graphics designs out of the smartphone market? All legitimate questions, and with potentially unsettling answers. The U.S. government should follow Britain’s lead on this investigation. When you get right down to it, allowing a company to buy what is effectively a standards-setting body that so many companies entrust with their livelihoods is a national security issue. We usually evoke national security concerns when a foreign company acquires an American company. But the concerns are just as valid when it’s the reverse, when the target is a foreign-based asset being acquired by a U.S. company. Mike Feibus is president and principal analyst of FeibusTech, a Scottsdale, Arizona-based technology market research and consulting firm. Reach him at mikef@feibustech.com. Follow him on Twitter @MikeFeibus."
https://venturebeat.com/2021/04/20/charm-embraces-open-source-to-make-command-line-interfaces-glamorous/,Charm embraces open source to make command line interfaces ‘glamorous’,"Before the slick bells and whistles of the modern graphical user interface (GUI), the humble command line interface (CLI) ruled the roost — purely text-based commands typed via a keyboard to communicate with the operating system. But even in a world now dominated by fancy GUIs, command lines remain a popular option for developers. They can be a lot quicker (for those who know the commands, at least) and also consume less memory and computing resources than GUIs. Microsoft even launched a new application for command line users last year called Windows Terminal. Against this backdrop, Charm is setting out to apply “modern product thinking to one of the original forms of human-computer interaction” and make the command line “glamorous.” Charm was founded in 2019 by Toby Padilla, a former engineer at Apple, Last.fm, and TweetDeck; and Christian Rocha, formerly head of voice at Snap-acquisition Zenly. In a nutshell, the company is developing next-gen CLIs fit for the 21st century, with tools to improve visual appearance and store data, such as user profiles. To advance that effort, the company today announced it has raised $3 million in a round of funding led by Cavalry Ventures. Charm currently has three products on the market, including an open source terminal-based markdown reader it calls Glow that enables developers to view documentation such as readme files directly on the command line. It also renders the markdown to make the text easier to read and allows users to build a private, encrypted library of documentation. A typical use case might involve a developer who has cloned a project from GitHub and can run Glow in the project directory to discover all the documentation for that project. “The developer can read that documentation and/or stash it for later reference on this machine, or any other machine linked to their Charm account,” Padilla explained. As an open source tool, Charm tacitly acknowledges that most of the tools that contain markdown documentation are open source. Command line aficionados have a “strong desire to use open source software,” Padilla said. “It also allows Glow to be bundled and distributed with other open source tools, like Linux distributions and package managers. Since the majority of our users are developers, being open source also helps with adoption.” Elsewhere, Charm also offers a command-line-first account management tool called Charm, which leans on secure shell (SSH) keys to create an invisible account system. “This means that on first run of any software built with Charm accounts, users are not prompted to create an account,” Padilla said. “It happens invisibly behind the scenes, thus lowering the barrier of entry to using any tool built with Charm.” And then there’s Charm Cloud, which offers a suite of online services spanning identity, authentication, data storage, and encryption. It is a little like Firebase or Parse, except “focused on the command line as a platform,” Padilla said. Charm launched Glow in late 2019, though in its original guise it only displayed documentation — it didn’t allow a user to stash or build a private library. In October, the company introduced Charm Cloud, alongside a new version of Glow, replete with Charm Cloud integration. Additionally, Charm has open-sourced a bunch of libraries and tools it used to make Glow and Charm, such as Bubble Tea, Lip Gloss, Termenv, Glamour, and Bubbles, each designed to encourage an ecosystem of products built on Charm’s technologies. Glamour, for example, offers stylesheet-based markdown rendering for CLI apps, and GitHub has already integrated this into its official command line tool, according to Padilla.  Charm’s suite of products is currently free to use, and the company plans to keep it that way for “basic non-commercial use.” However, it intends to charge for enterprise-grade solutions and perhaps resource usage on individual accounts — “for example, if you need to store more than the free limit of data,” Padilla said. “Our business model is selling developer experience solutions built on our suite of products to enterprises.” Charm is also planning to later launch an open source self-hosted version of Charm Cloud that will be free for non-commercial use."
https://venturebeat.com/2021/04/20/datarails-nabs-18-5m-to-automate-financial-reporting-for-excel-users/,"DataRails, which automates financial reporting for Excel users, nabs $18.5M","DataRails, an Israeli startup that wants to help businesses understand their financial data better — and more quickly — has raised $18.5 million in funding as it looks to double down on its enterprise integrations and invest in its AI capabilities. The raise comes amid a flurry of activity across the financial planning and analytics sphere, with OneStream this month raising $200 million at a $6 billion valuation, shortly after Jedox locked down more than $100 million. “Businesses typically spend between 10 to 14 days every month on manually gathering data from different sources and bringing it together to understand the current status of the organization and try to predict future performance,” DataRails cofounder and COO Eyal Cohen told VentureBeat. “Despite their efforts, the results tend to be difficult to analyze, error-prone, and lacking insights. DataRails shortens the time spent on this to a few hours and allows organizations to get better insights into their business.” Founded in 2015, DataRails has entered a space that includes legacy players such as Anaplan; Hyperion, which Oracle bought for more than $3 billion in 2007; and Adaptive Insights, which Workday acquired for north of $1.5 billion in 2018. But the biggest incumbent DataRails is up against is arguably trusty old Excel. This is particularly true for small to medium-sized businesses, according to Cohen, as they use Microsoft’s omnipresent spreadsheet software for all their month-close and management reports, budgets, forecasts, and more. “Although there are well-established solutions in the FP&A [financial planning and analysis] market, more than 80% of small and medium-sized organizations conduct their routine processes manually, using Microsoft Excel and PowerPoint,” Cohen said. “The reason that this persists as the status quo is due to the fact that existing solutions require redesigning existing models and processes, leaving behind years’ worth of invested time in analyses, models, and reporting templates.” Excel’s persistence is one of the reasons we’ve seen a slew of newer players enter the FP&A market — such as Cube Software and Vena Solutions — taking a more modern approach that works on top of familiar spreadsheets. DataRails takes a similar path insofar as it seeks to supplement — rather than replace — Excel. It has created what it calls an “elastic database technology,” one that can transform spreadsheets into a structured database. Underpinning this are AI and machine learning algorithms that can take both structured and unstructured spreadsheet data (e.g. cell values, formulas, formats, and macros) to develop a “logical, centralized database.” “With this technology, DataRails automates existing Excel-based processes by leveraging existing models and templates to create one unified database,” Cohen said. “DataRails combines the flexibility of Excel with the power of a cloud-based database and a web-based dashboard.” DataRails’ customers, which include businesses from across the medical, transport, manufacturing, and cybersecurity spheres, can access the platform as a layer directly on top of Microsoft Excel and PowerPoint, which perhaps is how a financial analyst is most likely to use it. But it can also be accessed through a dedicated web interface, where management, executives, and board members would be more inclined to go to access data and insights. It’s worth noting that DataRails can also glean data from sources such as enterprise resource planning (ERP), customer relationship management (CRM), and human resource information systems (HRIS), including Netsuite, Quickbooks, SAP, Salesforce, and Microsoft Dynamics. And given that its interface is based on Excel, DataRails can connect with pretty much any tool capable of exporting data as a CSV file. “DataRails has very strong analysis capabilities, thanks to the fact that cross-organizational data, from all financial and operational systems, is housed under one roof in our unified database,” Cohen said. “With all organizational data centralized in one place, customers can conduct variance analyses [and] drill-downs for full-scale granularity and quickly design ad-hoc reports.” DataRails had previously raised $10 million, and with another $18.5 million from Zeev Ventures Fund, Vertex Ventures Israel, and Innovation Endeavors, the company is well-financed to add more data and analytics tooling to its platform. “We’re looking to add stronger analysis capabilities, as well as newer prediction capabilities and better insights,” Cohen said."
https://venturebeat.com/2021/04/20/bushel-closes-47-million-investment-round-led-by-lewis-clark-agrifood-and-continental-grain-company/,Bushel® Closes $47 Million Investment Round Led by Lewis & Clark AgriFood and Continental Grain Company," Investment captures significant market share of grain supply chain and accelerates digital infrastructure for sustainability initiatives, fintech offerings and expanded product suites  FARGO, N.D.–(BUSINESS WIRE)–April 20, 2021– Bushel, an independently owned software company and leading provider of software technology solutions for growers, grain buyers, ag retailers, protein producers and food companies, announced today closings on its $47 million Series C investment round. The oversubscribed round was led by Lewis & Clark AgriFood, a St. Louis-based food and agriculture focused investment firm, and Continental Grain Company, a global holding company focused on agriculture, food and protein production. Participation from new and existing investors include Cargill, Scoular, Germin8 Ventures and others. In addition, Consolidated Grain and Barge Co. is expected to close their investment in the round in the coming weeks. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210420005377/en/ The infusion of growth capital and continued partnership of the syndicate will introduce payments, credit, trading offerings and increased data services, including capabilities to help consumer brands sustainably source commodities in the global grain supply chain. The funding will also accelerate Bushel’s current product offerings that currently deliver value to growers, commodity buyers, ag retailers and consumer packaged goods companies (CPGs). “We are excited to invest further into Bushel alongside some of our high-value partners like Cargill and Scoular,” said Chris Abbott, Co-Head of Continental Grain Ventures. “Bushel is one of the leading independent software companies in agriculture, and serves as the critical link connecting growers to grain buyers, processors, brands and consumers. This funding accelerates deployment of new offerings and embedded capabilities throughout the food and ag value chain. We believe it results in better outcomes for growers and the supply chain.” Bushel’s platform now reaches 40% of grain origination in the United States, resulting in inarguably the largest technology network effect among growers and grain buyers in the U.S. today. $22 billion of grain is contracted annually within Bushel’s ecosystem. Bushel’s collaborative ethos will continue to accelerate adding new participants throughout the value chain, particularly CPGs and consumer brands, that will benefit from upstream and midstream data insights and help deliver on sustainability promises. “Bushel is here to win for our customers and their growers,” said Jake Joraanstad, CEO and Co-founder of Bushel. “We are here to build on the agriculture industry’s century of infrastructure investment. We’re here not to disrupt but instead, to collaborate and help lead the industry into the digital age with strategies that make sense and provide value for all stakeholders in the ag and food value chain. We serve as the independent integration hub that saves businesses significant manual effort, time and expense, while also strengthening the relationships between growers and agribusinesses through secure, proven and easy to use software products and services.” “As Food and Agriculture specialist investors, we see Bushel as the leader in connecting disparate data systems in the ‘messy middle’ of the food and agriculture supply chain,” said Larry Page, Managing Director of Lewis & Clark Agrifood. “Digital innovation has landed on the farm, but few companies are focused on the digital side of agriculture (or lack thereof) from the farm gate and beyond. We hear so much noise in the space with very little real traction. As Bushel has consistently gained market share, it has become clear that the ability to connect data with physical grain has significant value for many stakeholders in our food system.” Monthly, 60,000 growers use Bushel’s products and services and nearly 2,000 grain buying locations across the U.S. and Canada trust Bushel to power their grower-facing and internal software products. Bushel’s platform integrates into multiple grain accounting systems, trading desks, farm management systems, insurance companies and market feeds to allow different software systems to work with each other. Current grain data-sharing processes are manual and fragmented, resulting in lost productivity and revenue potential. Bushel’s technologies aim to solve some of the industry’s biggest challenges and bring the grain supply chain together through connected, standardized and properly-permissioned data. “As a grower myself, and a daily user of Bushel’s products and services, it’s been rewarding to have a front row seat to the work our team is doing to create a great platform for the existing food industry,” said Ryan Raguse, President and Co-founder of Bushel. “This also comes with a deep responsibility to be stewards of the data that flows through the digital infrastructure we’re building. In addition to the responsibility of carrying the data of our customers and their growers, Bushel’s platform also carries the data of many of our employees’ farming operations, including my own. We will continue to set high standards for data security, ownership and transparency in order to protect and empower our customers, our families and the broader agriculture industry.” In addition to their investment, Cargill has committed to using the Bushel platform to power select internal and external digital tools. Scoular has been using the Bushel platform since 2018. Consolidated Grain and Barge Co. is currently exploring opportunities for utilization of Bushel’s suite of services. Bushel’s product suite includes its flagship mobile app, websites, trading tools, market feeds, API services and a custom software division focused on agriculture. Bushel has been focused on building software since the company was founded in 2011. Learn more: To learn more about how Bushel works, view demo videos, read testimonials from customers or to get started, visit bushelpowered.com. About Bushel Bushel is an independently owned software company and leading provider of software technology solutions for growers, grain buyers, ag retailers, protein producers and food companies, headquartered in Fargo, N.D. Since launching in 2017, Bushel’s platform has grown rapidly, now powering nearly 2,000 grain facilities across the U.S. and Canada with real-time business information for their producers. Monthly, 60,000 producers utilize Bushel products and services. Bushel is focused on bringing innovative software products and solutions to the agriculture industry.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420005377/en/ Bushel contact:Camille Grade, Chief Market Officercgrade@bushelpowered.com 701.730.0694"
https://venturebeat.com/2021/04/20/synthesia-raises-12-5m-for-ai-that-generates-avatar-videos/,Synthesia raises $12.5M for AI that generates avatar videos,"Synthesia, a startup using AI to create synthetic videos of avatars for marketing, today announced it has raised $12.5 million. In a press release, the company said the funding will be put toward expanding its workforce as it invests in product R&D. As the pandemic makes virtual meetups a regular occurrence, the concept of “personal AI” is rapidly gaining steam. Startups creating virtual beings, or artificial people powered by AI, have collectively raised more than $320 million in venture capital to date. As my colleague Dean Takahashi points out, these beings are a kind of precursor to the metaverse, a universe of virtual worlds that are all interconnected, as in novels such as Snow Crash and Ready Player One. Synthesia’s immediate goals are less ambitious. Like rivals Soul Machines, Brud, Wave, Samsung-backed STAR Labs, and others, the company employs a combination of machine learning techniques to create visual chatbots, product videos, and sales videos for clients without actors, film crews, studios, or cameras. “We’ve still only scratched the surface of the video economy. In 10 years, we believe most of our digital experiences will be powered by video in some way or form,” CEO Victor Riparbelli told VentureBeat. Riparbelli cofounded Synthesia in 2017 alongside Steffen Tjerrild and computer vision professors Lourdes Agapito and Matthias Niessner, who is behind some of the better-recognized research projects in the field of synthetic media, such as Deep Video Portraits and Face2Face.  “Today, video production is costly, complex, and unscalable. It requires studios, actors, cameras, and post-production. It’s an incredibly long and multidisciplinary process, rooted in physical space and sensors,” Riparbelli continued. “To truly realize the video-first internet, we need a more scalable and accessible way to make video.” Synthesia customers choose from a gallery of in-house, AI-generated presenters or create their own by recording voice clips and then uploading them. After typing or pasting in a video script, Synthesia generates a video “in minutes,” making it available for translation into dozens of languages. As pandemic restrictions make conventional filming tricky and risky, the benefits of AI-generated video have been magnified. According to Dogtown Media, an education campaign under normal circumstances might require as many as 20 different scripts to address a business’ worldwide workforce, with each video costing tens of thousands of dollars. Synthesia’s technology can pare the expenses down to a lump sum of around $100,000. Synthesia says that client CraftWW used its platform to ideate an advertising campaign for JustEat in the Australian market featuring an AI-manipulated Snoop Dogg. The company also worked with director Ridley Scott’s production studio to create a film for the nonprofit Malaria Must Die, which translated David Beckham’s voice into over nine languages. And it partnered with Reuters to develop a prototype for automated video sport reports. “We’re building an application layer that turns code into video, allowing for video content to be programmed with computers rather than recorded with cameras and microphones. Once video production is abstracted away as code, it has all the benefits of software: infinite scale, close to zero marginal costs, and it can be made accessible to everyone,” Riparbelli said. “This is now quickly becoming a reality. We launched our software-as-a-service product just six months ago … [and we] have essentially reduced the entire video production process to a single API call or a few clicks in our web app.” In the near future, Synthesia plans to make generally available a product that personalizes videos to specific customer segments. It’s called Personalize, and Synthesia says it can automatically translate videos featuring actors or staff members into over 40 languages. “We have been overwhelmed by the response in the last six months since our beta launch: We now have thousands of users, and our customers range from small agencies to Fortune 500 companies,” Riparbelli said. “They use Synthesa primarily for internal training and corporate communications. But now we are seeing more and more companies starting to use it for external communications, incorporating personalized video into every step of the customer journey through our personalized video API.” Some experts have expressed concern that tools like Synthesia’s could be used to create deepfakes, or AI-generated videos that take a person in an existing video and replace them with someone else’s likeness. The fear is that these fakes might be used to do things like sway opinion during an election or implicate a person in a crime. Deepfakes have already been abused to generate pornographic material of actors and defraud a major energy producer. For its part, Synthesia has posted ethics rules online and says it vets its customers and their scripts. It also requires formal consent from a person before it will synthesize their appearance and refuses to touch political content. “We are trying to solve a very complex and technical problem,” Riparbelli recently told the Telegraph. “We are not releasing any software to the public … There is a wider discussion to be had about the malevolent use of this kind of stuff.” Synthesia’s series A funding round announced today was led by FirstMark Capital, with participation from Christian Bach; Michael Buckley; and existing investors, including Mark Cuban. The London, U.K.-based company has 30 employees, and its total raised is now over $16.6 million."
https://venturebeat.com/2021/04/20/ci-security-closes-first-tranche-of-series-b-financing-and-announces-record-q1/,CI Security Closes First Tranche of Series B Financing and Announces Record Q1," MDR Leader Accelerates Presence in Healthcare as American Hospital Association Names Company Preferred Provider  SEATTLE–(BUSINESS WIRE)–April 20, 2021– CI Security (CI), the Seattle-based company mission-focused on defending critical services from cyberattack, has announced the initial close of a Series B extension funding round led by Alan Frazier’s East Seattle Partners. San Francisco technology-focused investment bank Capital Clarity is advising CI. Due to strong investor interest, CI and Capital Clarity will continue accepting investments into the round. CI will use these funds to continue developing its vertical-specific cybersecurity and MDR platform and to support its sales and marketing efforts. The company’s announcement comes after recording record-breaking sales in Q1. The first quarter results doubled the Company’s prior quarterly sales record, demonstrating significant positive momentum in sales velocity and closed sales. Additionally, CI just announced it is an American Hospital Association preferred provider to protect and defend hospitals. The AHA vetted cybersecurity providers in a nationwide search and chose CI among the select group. CI is the only AHA preferred provider for Managed Detection and Response as well as the Critical Insight Healthcare Security Program, which includes HIPAA Risk Assessments, Vulnerability scanning along with MDR. “The exciting Q1 results, along with the news from the AHA means we are fulfilling our mission to defend critical service organizations from cyberattack,” said Garrett Silver, CI Security’s CEO. “Our combination of Managed Detection and Response, coupled with our suite of Assessment and Testing services, support small security teams in building out their total security program.” “CI’s investors are enthusiastic about supporting and growing a cybersecurity company with this kind of focus,” said John Cooper, Managing Partner of Capital Clarity. “We’re keeping the round open to support significant additional investment interest.” About CI Security CI Security provides Managed Detection and Response services, combining purpose-built technology with expert security analysts to perform full-cycle threat detection, investigation, response, and recovery. CI Security is focused on defending critical systems in healthcare, the public sector, and other industries while helping customers gain critical insight into their security posture through the MDR platform and Information Security Consulting Services. Find out more at https://ci.security. About Capital Clarity Capital Clarity offers a refined approach to investment banking that emphasizes long-term partnership with investors and management teams. Our leadership team has a combined experience of 80 years in financial advisory, mergers & acquisitions, corporate development and private equity. We combine advisory expertise with deep industry knowledge and long-standing buyer and investor relationships to create successful outcomes for our clients. Cybersecurity is one of Capital Clarity’s many areas of technology focus and expertise.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420005203/en/ Susan BlancoCapital Clarity(415) 320-1582susan@capital-clarity.com John CooperCapital Clarity(415) 683-0339coop@capital-clarity.com Jake MilsteinCI Security206-347-0588jake.milstein@ci.security"
https://venturebeat.com/2021/04/20/cape-privacy-raises-20m-to-enable-data-science-operations-on-encrypted-data/,Cape Privacy raises $20M to enable data science operations on encrypted data,"Cape Privacy, which is developing a privacy-preserving platform for collaborative data science, today announced that it closed a $20 million series A led by Evolution Equity Partners. CEO Ché Wijesinghe says that the proceeds will be used to support growth as Cape Privacy develops new technologies for secure machine learning. AI promises to transform — and has transformed — entire industries, from civic planning and health care to cybersecurity. But privacy remains an unsolved challenge, particularly where compliance and regulation are concerned. Banks, health providers, and even retailers can run into problems when collaborating on AI and machine learning research involving sensitive or proprietary data, like patient records, financial documents, and supply chain details. Cape was founded in 2018 by Gavin Uhma, the cofounder and CTO of GoInstant, which was acquired by Salesforce in 2012. Cape’s combination of privacy, machine learning, and cryptography enables encrypted data-sharing, helping teams in compliance, legal, and risk management work with each other and third-party vendors. “Today many financial institutions access the same publicly available data from data providers like Nielsen and Bloomberg — but they all want a better edge,” a spokesperson told VentureBeat via email. “Non-public data sources such as those from retail and credit card companies would greatly enrich their models. Yet concerns around confidentiality on both sides have prevented this collaboration. Many data providers are interested in finding new channels to monetize their data, but they can rarely get it past their internal legal and compliance teams.” Cape’s open source software integrates with data science and AI infrastructure to provide a workflow guiding contributors toward building custom projects and policies. Cape enables developers to decide on the placement of tools in relation to data storage and pipelines, ensuring data access, privacy, and monitoring meet each product’s requirements. Moreover, it allows stakeholders to set monitoring and auditing configurations so that all parties receive logs for review, approval, or amendment. “Cape Privacy’s platform … ensures privacy by default. With Cape as the broker, data providers are only renting data instead of selling it. This is a significant point because companies that lose control of their data can get in trouble,” the spokesperson said. “Once a data model is enriched using encrypted data on the Cape cloud, the transaction between buyer and seller ends and the data is returned. Now the data subscriber can enrich its data for better business outcomes, and the data provider can securely monetize its data.” Cape’s platform is underpinned by tf-encrypted, the company’s suite for experimenting with private machine learning on top of Google’s TensorFlow framework. Tf-encrypted enables training, validation, and prediction over encrypted data. The data remains encrypted during the workflow, meaning that AI models can be hosted in the cloud without decrypting the training data or outputs. Seventeen-employee Cape, which claims to have two major clients and “half a dozen” in the pipeline, isn’t the first to advance a privacy-preserving data science approach. Companies including Enveil, Cosmian, Duality Technologies, and Intel are investigating homomorphic encryption, a form of cryptography that enables computation on file contents encrypted using an algorithm so that the generated encrypted result exactly matches the result of operations that would’ve been performed on unencrypted file. Using homomorphic encryption, a “cryptonet” can perform computation on data and return the encrypted result back to a client, which can then use the encryption key to decrypt the returned data and get the actual result. Homomorphic encryption libraries don’t yet fully leverage modern hardware and are at least an order of magnitude slower than conventional methods. That said, newer projects like the accelerated encryption library cuHE claim speedups of 12 to 50 times on various encrypted tasks over previous implementations. And HE-Transformer, a backend for nGraph (Intel’s neural network compiler), delivers leading performance on some cryptonets. New investors Tiger Global Management, Ridgeline Partners, and Downling Lane participated in Cape Privacy’s series A together with existing investors Boldstart Ventures, Version One Ventures, Haystack, Radical Ventures, and Jevon MacDonald. Additional investment came from Coinbase cofounder and board member Fred Ehrsam, the Tokyo Black Fund, and Sand Hill East. To date, New York-based Cape Privacy has raised over $25 million. “We are excited at reaching this company milestone,” Wijesinghe told VentureBeat via email. “Cape’s technology will be a defacto standard for privacy preserving machine learning. Building on our success in the financial services industry, we have already had great interest from Health and Life Sciences companies for potential drug discovery and genomics research use cases. In addition, there is clear demand for this technology for collaboration on machine learning model development across government agencies for counter-terrorism programs.”"
https://venturebeat.com/2021/04/20/grip-security-aims-to-simplify-and-automate-saas-endpoint-security/,Grip Security aims to simplify and automate SaaS endpoint security,"As SaaS platforms explode in popularity, developers are finding it easier than ever to write applications that plug in and reach big user bases. But this has created a growing security headache as the number of applications running on enterprise SaaS platforms soars. Idan Fast, Lior Yaari, and Alon Shenkler joined forces last year when they saw an opportunity to address this problem. Their new startup, Grip Security, is the latest member of the red-hot Israeli cybersecurity ecosystem. In an interview with VentureBeat, Yaari said Grip aims to revolutionize SaaS security with an enforceable endpoint-centric approach that secures all SaaS application access, regardless of device or location. “SaaS is becoming a problem, with the ever-growing landscape of SaaS within enterprises,” he said. “Hundreds of applications are being used, and there is a lack of tools to control that.” Grip is just at the start of that journey. Today, the company officially unveiled itself and announced it has raised $6 million in a funding round led by YL Ventures. It also boasts an impressive roster of business angels, including CrowdStrike CEO George Kurtz, former Akamai CSO Andy Ellis, former Zscaler CISO Michael Sutton, and former Bank of America chief security scientist Sounil Yu. According to Yaari, existing SaaS security tools have failed to offer comprehensive solutions and lack visibility and control for users. Because an enterprise may not see all their SaaS applications, they remain vulnerable. He puts secure access service edge (SASE) in this category of approaches with limitations. That’s big talk, considering SASE has been showing tremendous momentum with both users and investors. In this respect, Grip will be competing against fellow Israeli startup Cato Networks, which pioneered SASE and raised rounds of $130 million and $77 million last year. Boston-based SASE startup Iboss raised $145 million in January. Endpoint security company Lookout in March acquired CipherCloud, a cloud-native cybersecurity startup focused on SASE. And Cisco recently announced its own SASE offering. But Yaari said such SaaS security solutions tend to approach the problem by trying to secure the network or by securing the applications. “Both of them have blind spots in either the availability to cover the vast amount of applications that exist or the ability to cover things that are coming out of the corporate network,” he said. Yaari and his cofounders are veterans of Israel’s security ecosystem, and their fast start is a testament to just how robust this beehive of security startups has become. Despite facing some well-funded rivals, they are betting Grip can build a superior solution by creating a platform that automatically tracks all applications, no matter which device they are running on or where they’re located. By mapping data flows, the Grip platform will be able to apply security policies across a SaaS network and all applications. Eventually, the Grip platform will work alone or in tandem with a cloud access security broker. “The platform itself does two things,” he said. “It discovers what applications are being used, which is a very big problem in the security space. And then for each application it discovers, it adds an additional security layer, allowing for access and data governance to those applications.” Among the features the company is promising are greater visibility into all applications and risk profiles and protection against sensitive data flows. Grip has already deployed an initial product for medium-sized tech companies that it’s learning from to further develop its platform. The product is currently only available to a limited number of companies as an early-access version. Grip will use the funding to increase its customer count in order to accelerate development while expanding up to 30 employees by next year."
https://venturebeat.com/2021/04/20/log-management-database-startup-era-software-raises-15-25m/,Log management database startup Era Software raises $15.25M,"Era Software, creator of the EraDB time-series database architecture for log management, today announced it has raised $15.25 million in a round led by Playground Global. The funding, which brings the company’s total raised to over $22 million, will be put toward supporting product development and bolstering the launch of its EraSearch Cloud product in private beta. A 2018 report from Domo estimated that humans are creating 2.5 exabytes of data per day, a number that’s increasing. Perhaps unsurprisingly, some companies are tapping the deluge more effectively and efficiently than others. A recent PricewaterhouseCoopers survey of over 1,800 business leaders found that 43% obtained little tangible benefit from their data. And over 20% reported deriving no benefit whatsoever. While Seattle, Washington-based Era has a number of rivals in a database market estimated to be worth tens of billions (e.g., Splunk, ChaosSearch, and Sumo Logic), the company differentiates itself with decoupled storage and compute, indexing powered by machine learning, and schema-free data storage. Era automatically indexes on every dimension in logs and works with arbitrarily shaped data, with support for third-party frontend and backend systems, as well as query-based performance optimizations. Era can adjust to different log data rates dynamically, without fine-tuning. And the architecture can ingest data that isn’t timestamped, allowing developers to filter and aggregate pieces of information. Todd Persen, who cofounded Era with Robert Winslow, previously launched InfluxData, originator of the InfluxDB time-series database. He started Era to tackle challenges around managing large volumes of data in logs, with a focus on data that’s becoming increasingly complex. High-dimensionality data is emerging at a rate faster than traditional databases can keep up, he says, leaving organizations to pay exponentially increasing costs for ill-designed solutions. “While Elasticsearch became the de facto open source choice for storing logs, the world was moving to a place where the average log volumes exploded from gigabytes per day to terabytes per day,” an Era spokesperson told VentureBeat via email. “What began as simple, single-node deployments of Elasticsearch grew into complex clusters that had skyrocketing hardware costs and burdensome operational demands. Elasticsearch was never designed to be an optimal solution for the high-volume demands of the log management use case. While still hugely popular, users are battling the costs of running it at scale — but there are not many alternatives, hence the creation of Era’s log management solution.” Persen claims Era’s fully managed offering, EraSearch Cloud, allows customers to deploy log management infrastructure that’s less complex and performant but more cost-effective. EraSearch Cloud ensures copies of data live within object storage, like on Amazon’s Simple Storage Service, Google Cloud Storage, or Microsoft Azure Blob Storage, ostensibly without sacrificing performance. Beyond Playground Global, existing investors Foundation Capital, Array Ventures, Global Founders Capital, and angel backers also participated in Era’s series A. This follows a $7 million seed round in 2020 that was led by Foundation Capital."
https://venturebeat.com/2021/04/20/xilinx-launches-kria-chips-to-handle-ai-for-edge-applications/,Xilinx launches Kria chips to handle AI for edge applications,"Xilinx has introduced its Kria programmable chips and boards for holding AI applications at the edge of the network. This should come in handy for visual applications like smarter cameras. San Jose, California-based Xilinx, which is in the process of being acquired by Advanced Micro Devices (AMD) for $35 billion, has a group of products dubbed the Kria portfolio of adaptive system-on-module offerings for AI at the edge. These are production-ready small form factor embedded boards that enable rapid deployment in edge-based applications. Coupled with a complete software stack and prebuilt, production-grade accelerated applications, Kria adaptive modules are a new method of bringing adaptive computing to AI and software developers. The first product available in the Kria SOM portfolio, the Kria K26 SOM, specifically targets vision AI applications in smart cities and smart factories. The Xilinx SOM roadmap includes multiple products, from cost-optimized SOMs for size- and cost-constrained applications to higher-performance modules that will offer developers more real-time compute capability per watt. Xilinx said industry reports put market growth at 11% per year and total market revenue at $2.3 billion by 2025. “I think there’s solid potential here,” HotTech Vision and Analysis analyst Dave Altavilla said in an email to VentureBeat. “Similar to the way Nvidia enabled its graphics processing unit (GPU)-powered edge AI solutions with Jetson and Jetson Nano, Xilinx’s Kria SOM is a complete shrink-wrapped solution for developers, engineers, and makers to get their feet wet with adaptable field-programmable gate array (FPGA)-based acceleration, with little to no experience required in programmable logic (FGPAs).” He added, “In addition, Xilinx’s embedded app store for edge AI will help foster accelerated solutions with faster time to market in anything from facial recognition to natural language processing applications and more. In short, it’s what the company needs to demystify the FGPA for the masses, and in conjunction with Xilinx Vitis software development tools, will allow engineers to work in their own native frameworks and programming languages like PyTorch, TensorFlow, Caffee, C++, OpenCL, and Python.” He added, “It’s a clear indication that the adaptive nature of FPGAs doesn’t need to be relegated to just the power user-programmable logic engineer anymore. And with Ubuntu support on the way, these dev kits could go mainstream in a hurry.” Kria SOMs use Xilinx adaptable hardware, delivered as production-deployable, adaptive modules. Kria SOMs can be deployed rapidly using end-to-end board-level solutions with a prebuilt software stack. By allowing developers to start at a more evolved point in the design cycle compared to chip-down design, Kria SOMs can reduce time to deployment by up to nine months, Xilinx said. The Kria K26 SOM is built on top of the Zynq UltraScale+ MPSoC architecture, which features a quad-core ARM Cortex A53 processor, more than 250,000 logic cells, and an H.264/265 video codec. The SOM also features 4GB of DDR4 memory and 245 input-output paths that allow it to adapt to virtually any sensor or interface. With 1.4 teraflops of AI compute, the Kria K26 SOM enables developers to create vision AI applications offering more than 3 times higher performance at lower latency and power compared to GPU-based SOMs. This is critical for smart vision applications, including security cameras, city cameras, traffic cameras, retail analytics, machine vision, and vision-guided robotics. Xilinx has invested heavily in its tool flows to make adaptive computing more accessible to AI and software developers who lack hardware expertise. The Kria SOM portfolio takes this accessibility to the next level by coupling the hardware and software platform with production-ready vision accelerated applications. These applications eliminate all the FPGA hardware design work and only require software developers to integrate their custom AI models and application code. They can optionally modify the vision pipeline — using their familiar design environments, such as TensorFlow, Pytorch, or Caffé frameworks, as well as C, C++, OpenCL, and Python programming languages — enabled by the Vitis unified software development platform and libraries. Xilinx offerings are open source accelerated applications, provided at no charge, and range from smart camera tracking and face detection to natural language processing with smart vision. The Kria KV260 Vision AI starter kit is priced at a low $199. When customers are ready to move to deployment, they can seamlessly transition to the Kria K26 production SOM, including commercial and industrial variants priced at $250 or $350, respectively. The KV260 Vision Starter Kit is available immediately, with the commercial-grade Kria K26 SOM shipping in May of 2021 and the industrial-grade K26 SOM shipping this summer. Ubuntu Linux on Kria K26 SOMs is expected to be available in July."
https://venturebeat.com/2021/04/20/how-vertical-location-improves-operational-efficiency/,How vertical location improves operational efficiency,"Presented by NextNav From food delivery services to safety apps, 3D location technology is about to transform the 2D technology we’ve long accepted as the limit of location services. Join VB’s Dean Takahashi and others to learn what it means for any app relying on location services — and how it’s now possible to enable accurate vertical location at scale in vertical environments such as skyscrapers, apartment buildings, or malls. Register here for free. Mobile apps of all kinds use location to increase efficiency. From deliveries to ridesharing, family safety to parking, mobile apps depend on location infrastructure to deliver the just-in-time services that we all depend on — the services which power billions of dollars a day in revenue. While location services play a key role in driving operational efficiency, there’s a significant missing piece that keeps most location-enabled services from reaching their full potential. Today’s location services are all constrained by 2D technology. They can optimize for flat environments, but that’s it. Buildings, underground locations, and complex urban landscapes are a significant challenge. For a delivery application, an app can get you to an address, but can’t identify which floor the recipient ordered from. Parking apps can’t direct you to your car in a four-story underground garage. Family safety apps can tell you that your child is in a skyscraper somewhere, but you’d have to search fifty floors to find out exactly where. A quick-serve restaurant on the ground floor of a building can’t predict when someone from the twentieth floor above them will arrive to pick up their food. Unfortunately, most companies don’t realize what they’re missing. We’re all so used to the flat, 2D technology we rely on today that we don’t think about how much better it could be with a whole new dimension of data. Or we think about vertical location as a neat feature, but not something that actually has an impact on the bottom line. We discount that “last mile” of the user experience, not making the connection to increased revenue, higher LTV, and improved customer retention. In fact, vertical location is a major step forward in operational efficiency, with downstream effects that have the potential to revolutionize a wide range of location-based applications. If the 2D applications we have today made us this efficient, just imagine what we could accomplish with a whole new data set? It’s worth noting that the efficiency gains of vertical location are likely to be felt the most by the very customers that most app companies crave:  urban residents.  Dense cities — the places where people live, work, and play in multi-story buildings — are both the largest market for mobile apps and the places where 2D location falls down on the job.  The companies that fill this gap first will naturally reap outsize dividends. NextNav’s vertical location technology helps companies increase operational efficiency in the urban markets where it matters most. We’ve built our Pinnacle service for vertical location to focus specifically on built-up urban areas — 4,400 cities at last count.  We cover over 90% of buildings three stories and above, knowing that these are the places where deliveries, directions, and other precise location-based services are the most critical. How can vertical location improve the operational efficiency of your location app? Don’t miss this VB Live event. Register for free here. Attendees will learn: Speakers: More to come!"
https://venturebeat.com/2021/04/20/korean-ai-based-diagnostic-software-dr-answer-helps-countries-with-a-shortage-of-medical-professionals/,Korean AI-based Diagnostic Software Dr. Answer Helps Countries With a Shortage of Medical Professionals," -Saudi Arabia and Africa are in clinical verification of Dr. Answer while discussing its adoption  SEOUL, South Korea–(BUSINESS WIRE)–April 20, 2021– Dr. Answer, an innovative artificial intelligence (AI) based diagnostics application, is expected to significantly help countries with insufficient healthcare professionals make more efficient diagnosis. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210420005568/en/ Dr. Answer is a suite of AI-powered precision diagnostic software that supports healthcare. The suite is comprised of 21 AI-based software programs that assist diagnosis of 8 major diseases selected in consideration of mortality, the proportion of medical costs and public interest. 26 medical organizations developed Dr. Answer in collaboration with 22 information and communication technology companies in Korea for three years, and 38 medical organizations verified its stability and effectiveness in improving accuracy in diagnosis and reducing diagnosing time. “Healthcare professionals at large general hospitals can easily make reference to opinions of other professionals when they involve in diagnosis and therapeutic decisions as a large number of professionals in diverse disciplines are working within the same hospitals. But a few medical professionals diagnose patients in many small hospitals or clinics where Dr. Answer can help professionals make an accurate diagnosis,” explained Kim Jong-jae, Director of Asan Institute for Life Sciences. “Dr. Answer can be usefully used in Europe, and countries with insufficient medical professionals such as Africa may achieve substantial outcomes when they use Dr. Answer,” added Kim. Recognizing the excellence of Dr. Answer, hospitals affiliated with the Ministry of National Guard-Health Affairs of Saudi Arabia are currently conducting its clinical verification while discussing the possibility of its adoption.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210420005568/en/ Dr. AnswerLee Junyoung+82-43-931-5500leejy@nipa.kr"
https://venturebeat.com/2021/04/20/codecov-hackers-gained-access-to-hundreds-of-restricted-customer-networks/,Codecov hackers gained access to hundreds of restricted customer networks,"(Reuters) — Hackers who tampered with a software development tool from a company called Codecov used that program to gain restricted access to hundreds of networks belonging to the San Francisco firm’s customers, investigators told Reuters. Codecov makes software auditing tools that allow developers to see how thoroughly their own code is being tested, a process that can give the tool access to stored credentials for various internal software accounts. The attackers used automation to rapidly copy those credentials and raid additional resources, the investigators said, expanding the breach beyond the initial disclosure by Codecov on Thursday. The hackers put extra effort into using Codecov to get inside other makers of software development programs, as well as companies that themselves provide many customers with technology services, including IBM, one of the investigators said on condition of anonymity. The person said both methods would allow the hackers to potentially gain credentials for thousands of other restricted systems. IBM and other companies said their code had not been altered but did not address whether access credentials to their systems had been taken. “We are investigating the reported Codecov incident and have thus far found no modifications of code involving clients or IBM,” an IBM spokesperson said. The FBI’s San Francisco office is investigating the compromises, and dozens of likely victims were notified on Monday. Private security companies were already beginning to respond to assist multiple clients, employees said. Codecov did not respond to Reuters’ request for comment on Monday. Security experts involved in the case said the scale of the attack and the skills needed to execute it compared to last year’s SolarWinds attack. The compromise of that company’s widely used network management program allowed hackers inside nine U.S. government agencies and about 100 private companies. It is unclear who is behind the latest breach or if they are working for a national government, as was the case with SolarWinds. Others among Codecov’s 19,000 customers, including big tech services provider Hewlett Packard Enterprise, said they were still trying to determine if they or their customers had been affected. “HPE has a dedicated team of professionals investigating this matter, and customers should rest assured we will keep them informed of any impacts and necessary remedies as soon as we know more,” said HPE spokesperson Adam Bauer. Even Codecov users who had seen no evidence of hacking were taking the breach seriously, a corporate cybersecurity official told Reuters. He said his company was busy resetting its credentials and that his counterparts elsewhere were doing the same, as Codecov recommended. Codecov earlier said hackers began tampering with its software on January 31. The hack was only detected earlier this month, when a customer raised concerns. Codecov’s website says its customers include consumer goods conglomerate Procter & Gamble, web hosting firm GoDaddy, the Washington Post, and Australian software firm Atlassian. Atlassian said it had not yet seen any impact or signs of a compromise. The Department of Homeland Security’s cybersecurity arm and the FBI declined to comment."
https://venturebeat.com/2021/04/20/devops-automation-platform-octopus-deploy-nabs-172-5m/,DevOps automation platform Octopus Deploy nabs $172.5M,"Octopus Deploy, a Brisbane, Australia-based company developing a continuous integration and delivery (CI/CD) platform, today announced that it closed a $172.5 million round. The funding, led by Insight Partners for a minority stake in the company, will be used to expand Octopus Deploy’s footprint and accelerate its go-to-market efforts. In software engineering, CI/CD ideally bridges gaps between development and operations by enforcing automation in building, testing, and deployment. But execution remains a challenge. In a recent survey, when asked whether their organizations have software release problems, 77% of respondents said that they did. Lack of expertise, poor training, and other challenges can lead to the inefficient implementation of CI/CD pipelines. Octopus Deploy began in 2011 as a nights-and-weekends project for software developer Paul Stovell and his wife, Sonia. It became a profitable business in 2012, and until now, it’s remained entirely self-funded. Octopus Deploy is designed to integrate with existing source control systems and build servers, orchestrating the DevOps automation that happens after a software build completes. By reusing configurations, API keys, connection strings, permissions, and automation logic, Octopus Deploy lets teams work together from a single platform.  From a dashboard, developers can use Octopus Deploy to facilitate releases, approvals, and notes across environments. Over 400 step templates help to ensure processes remain consistent across dev, test, and production environments. Runbook automation, meanwhile, affords control over infrastructure and apps. With runbooks, developers can automate tasks like maintenance and emergency incident recovery. Each automation includes permissions for the infrastructure it runs on so that on a team can be granted permission to execute a runbook with an audit trail. Octopus Deploy, which employs over 100 people, says that more than 25,000 organizations (7,200 of which are paying customers) use its platform, including Disney, NASA, Microsoft, Xero, and Stack Overflow. With offices in the U.S. and U.K., the company plans to open a larger headquarters in Brisbane in the near future. “Octopus Deploy makes it easy for software teams to automate complex software deployment processes. One of the big four accountancy firms uses the solution to deploy over 50 different internal products,” Stovell said. “Prior to using Octopus Deploy, its deployments used to be manual or semi-automated and took 3 to 4 hours to complete, and failure was common because of human error; now, deployments take 10 to 15 minutes, and humans are only involved for approvals. With less downtime and with their confidence increased, the firm can now do thousands of deployments even during the busy tax season.” According to Markets and Markets, the CI tools market alone is anticipated to reach $1.1393 billion in value by 2023. Among other rivals, Octopus Deploy competes with CI/CD platform Harness, which in January raised $115 million at a $1.7 billion valuation."
https://venturebeat.com/2021/04/20/mantl-raises-40m-to-help-legacy-banks-transition-to-digital/,Mantl raises $40M to help legacy banks transition to digital,"Mantl, a New York-based digital account-opening solution for legacy banks and credit unions, today announced that it closed a $40 million series B funding round. The company says it’ll use the capital to hire new talent and expand its product suite, with a particular eye toward developing solutions that improve and digitize the onboarding experience for businesses. It’s estimated that community banks and credit unions make up 95% of all banking institutions. Currently, they rely on third-party technology providers whose systems sometimes lack the capabilities deployed by larger, established banks. This impedes them when competing online and limits the options available to customers who prefer banking digitally. For example, one study found most millennials prefer to have interactions with financial brands through social media, and millennials make up the highest percentage of mobile banking users. Founded in 2016, Mantl provides a platform that integrates with core banking systems to enable customers to open accounts through white-labeled web and mobile portals. The system automates application decisioning for over 90% of cases while ostensibly reducing fraud, leading to deposit growth while eliminating the need to build new branches. “We originally set out to build a challenger bank, but we realized that the bigger opportunity was in helping existing banks modernize,” CEO Nathaniel Harley, who cofounded Mantl with Benjamin Conant and Raj Patel, told VentureBeat via email. “We enable banks and credit unions to open deposit accounts online, often for the first time. This includes products like checking accounts, savings accounts, certificates of deposit, and money market accounts. We estimate that less than half of all banks in the US give their customers the ability to open deposit accounts online today.”  With Mantl, banks can customize the look, feel, and messaging on the platform using a no-code editor. A console aggregates data points and turns them into insights to show which marketing channels are driving conversions. Mantl’s campaign management tool, meanwhile, automates deposit operations including payment settlement. And the company’s data connectors work with over 50 systems including Plaid, Stripe, SendGrid, and Twilio. “Currently, 43% of legacy banks are still running their core banking services on platforms that were designed with COBOL, a programming language that’s now over 60 years old,” Harley said. “This is why the gap keeps widening between the community banks who rely on these legacy players and the big banks who don’t: the top 15 banks hold 56.2% deposit market share today versus 16% 25 years ago. MANTL is fixing the legacy infrastructure problem, which is the biggest obstacle limiting modernization in the U.S. banking system today.” Mantl says that its clients, which include Cross River Bank, Quontic, and Midwest BankCentre, have attracted hundreds of thousands of new customers and raised billions in core deposits to date. In the past year, revenue increased by 213%. And banks on the platform saw 300% growth in deposit volumes. For Midwest BankCentre, Mantl helped raise more than $180 million through a digital-only branch called Rising Bank. And in the case of Flushing Bank, Mantl estimates that 20% of all new accounts are now coming from digital channels that didn’t previously exist. In what might be a boon for Mantl, eMarketer predicts that banks will increasingly partner with companies to offer “banking-as-a-service” products. For example, in August 2019, HSBC partnered with startup Amount to launch a digital lending product — and more partnerships like that could be on the way.  A PricewaterhouseCoopers survey of banks worldwide found that among those that wanted to collaborate with other sectors for growth, 47% were likely to work with a fintech firm. “As the world prepares to fully open again, MANTL has rounded out our account opening suite with a fully omni channel platform that improves the customer experience across all bank channels — in-branch, online, mobile and through relationship managers in the field. The next step is to bring similar efficiencies to business onboarding, and we’re excited to introduce a first-of-its-kind business account opening solution to market,” Harley said. “We are launching it later this year and it will be met with significant demand as community institutions are urgently looking for ways to transition their [short-term] relationships into long-term deposit relationships.” Mantl’s series B investment was led by Google parent company’s Alphabet’s growth fund, CapitalG, with participation from D1 Capital Partners, BoxGroup, and existing investors Point72 Ventures, Clocktower Technology Ventures, and OldSlip Group. It brings Mantl’s total raised to date to over $60 million following a $19 million series A round that closed in July 2020."
https://venturebeat.com/2021/04/20/hypr-raises-35m-to-grow-its-passwordless-authentication-platform/,Hypr raises $35M to grow its passwordless authentication platform,"Hypr, a cloud multifactor authentication platform, today announced it has raised $35 million in a series C round led by Advent International, doubling the company’s total funding to over $72 million. Hypr says it will leverage the funds to bolster its go-to-market strategy and grow its support organization globally. According to a recent study, passwordless authentication — which Hypr provides — is seeing heightened interest among enterprises. A survey of executives commissioned by LogMeIn’s LastPass found that 92% believe passwordless authentication is the future of their organization. That’s perhaps because users prefer it. The Ponemon Institute reports that 57% of users would would choose a passwordless method of protecting their identity if given the choice. But the rise of passwordless authentication is also likely attributable to an increased understanding of the risks associated with passwords. It’s estimated that 51% of people use the same passwords for both work and personal accounts and that 53% have a password that’s easy enough to memorize. Hypr offers a platform that’s powered by “open standards,” the company claims, with a “mobile-first” and public-key cryptography-based login experience for workstations. Using a branded smartphone app, customers can deploy a passwordless solution to users whether they’re online or not. Once an employee registers their smartphone, they gain access to their PCs via Hypr’s desktop client for Windows, macOS, and Linux. From Hypr’s web dashboard, admins can manage, provision, and deploy passwordless authentication policies across upwards of millions of users. The platform integrates with existing online fraud detection and risk engines, offering plugins and extensions for third-party identity and single sign-on providers, including Azure AD, Okta, and ForgeRock. And with an SDK, companies can integrate Hypr’s login flow with existing mobile and web apps. Hypr CEO George Avetisov notes that an estimated 300 billion passwords are used worldwide. This being the case, passwords remain the top cause of cybersecurity breaches — over 80% of which are due to compromised credentials. The pandemic and a swift transition to work-from-home arrangements expanded the attack surface for cybercriminals, highlighting the security risk, high costs, and user friction caused by passwords and password-based multifactor authentication. “It’s 2021 and passwords are still the top cause of breaches. And despite millions of dollars invested in multi-factor authentication (MFA), the whole world still relies on passwords,” Avetisov told VentureBeat via email. “As if the password problem wasn’t loud enough, the pandemic has turned the volume up to an 11. Businesses of all sizes have been dealing with credential reuse, fraud, phishing, and MFA bypass attacks at a scale never seen before. Just last year, the number of remote login attacks jumped more than 700%. Passwordless MFA is a new way to solve this problem that combines the security of strong encryption with the speed and convenience of the smartphone that users love.” Since announcing its previous funding round in October 2019, Hypr says its annual recurring revenue grew by more than 300%, driven by the doubling of the company’s customer base to brands including Norwegian Airlines, Point72, Rakuten, Otis Worldwide, Takeda Pharmaceuticals, CVS Health, Fiserv, and City National Bank. To date, Hypr has sold more than 100 million licenses, and it now serves organizations with up to “millions” of customers in more than 20 verticals, primarily financial services, energy, automotive, health care, education, and government. Hypr is headquartered in New York City, with teams in California, as well as Boston, London, and Tokyo."
https://venturebeat.com/2021/04/20/chargebee-boosts-subscription-and-recurring-revenue-tools-with-125m-round/,Chargebee boosts subscription and recurring revenue tools with $125M round,"Subscription and recurring billing tools provider Chargebee today said that it raised $125 million in series G funding co-led by Sapphire Ventures, Tiger Global, and Insight Venture Partners. With the fresh round of capital, which values Chargebee at $1.4 billion post-money, the company says that it plans to increase its investments in  expansion and partnerships, setting the stage for an IPO. The pandemic accelerated the shift to software-as-a-service (SaaS) and subscription-based business models, with companies expecting a 12% compound annual growth rate from recurring revenue over the next five years. By some estimates, 40% of ecommerce revenue comes from repeat purchasers — streaming music subscriptions alone generated $19.1 billion in 2018. But only 32% of online bills are made on a recurring basis, while the remaining 68% are one-time payments. Chargebee’s platform automates things like funneling users toward plans and collecting payment information, as well as executing upgrade or downgrade billing adjustments and facilitating recurring subscription renewals. For tasks it can’t handle automatically, Chargebee enlists human agents through customer relationship management dashboards from Zendesk, Salesforce, and NetSuite while guiding customers through a customizable checkout experience. Courtesy of integrations with Stripe, Braintree, WorldPay, and PayPal payment products, customers using Chargebee can pay with digital wallets like Amazon Pay and Apple Pay; with credit or debit cards; or directly through their bank accounts. Chargebee supports over 480 recurring billing use cases, with more than 20 payment gateways across over 50 countries. And the platform is available for upwards of 120 currencies and payment methods in dozens of languages. Chargebee features a range of pricing schemes including variable and usage-based pricing, and the platform is able to renew billing cycles based on sign-up or other dates. Chargebee can also selectively route payments and currencies in keeping with predefined rules. The platform’s optional Smart Dunning feature algorithmically susses out retry logic across days and times for failed payments. On Chargebee’s backend, managers get a visual customer organizational chart that allows them to define payment and invoicing responsibilities. They can also access templatized reports and KPI dashboards with metrics such as subscription revenue, discount, bad debt, and add-on metrics, all of which feed into accounting platforms like NetSuite, Intacct, and Xero. Teams receive real-time notifications if any tracked goals meet or exceed expectations or the team is in danger of falling behind. “Chargebee is the only solution in the space to include a ‘simulation engine’ as part of the product called TimeMachine. TimeMachine allows a business to simulate their subscription and billing configurations and analyze the potential response to external conditions,” a spokesperson told VentureBeat via email. “With capabilities like this businesses are able to assess the impact of their pricing, revenue recovery, and billing logic proactively. Chargebee also offers features like smart dunning capabilities which automate revenue recovery processes to maximize collections, while minimizing the cost of payment failures due to hard declines.” Chargebee competes with publicly traded Zuora and with ReCharge and Recurly, which create and maintain software subscription-based service solutions for businesses. But Chargebee is evidently doing something right — its customers include Okta, Freshworks, Calendly, and Study.com. And Chargebee claims to have a net retention rate exceeding $150. In fact, Chargebee claims to have the largest footprint of any revenue management provider in its segment, with businesses in 160 countries across North America, Europe, Asia, and Australia processing billions of dollars in revenue. The company’s user base has grown to more than 3,000 paying customers and over 20,000 companies across SaaS, direct-to-consumer ecommerce, over-the-top streaming, elearning, and publishing, and Chargebee says it is continuing to benefit from a “global surge” in subscription services deployments. A report from Zuora suggests 22% of companies have seen their subscription growth accelerate since the start of the pandemic, particularly in categories like video streaming and digital news and media. “The pandemic has opened up opportunities for us as a business.  Some of our customers have even been successful in pivoting their business models and offerings to the market, [and] we are proud to have enabled these shifts in decisions and support our customers through these turbulent times, only opening more doors of opportunity,” the spokesperson said. Chargebee’s latest investment round was led by Insight Venture Partners, with participation from existing investors. It brings Chargebee’s total funding to date to $230 million, following a $55 million round in October 2020."
https://venturebeat.com/2021/04/19/mastercard-bets-on-security-and-digital-identity-with-850m-ekata-deal/,Mastercard bets on security and digital identity with $850M Ekata deal,"(Reuters) — Mastercard said on Monday it had agreed to buy digital identity verification company Ekata in a deal valued at $850 million, as the global payments processor bets on a boom in demand for companies in the digital security space. Ekata’s products allow businesses to separate fraudsters from legitimate customers during digital interactions like opening an online account or making digital payments. It operates in three industries: ecommerce, payments, and financial services, according to its website. “The acceleration of online transactions has thrust global digital identity verification to the forefront as one of the biggest opportunities to build digital trust and combat global fraud,” Ekata CEO Rob Eleveld said in a statement. Seattle-headquartered Ekata counts more than 2,000 companies as its partners, including credit reporting company Equifax and software firm Intuit, its website showed. Its products, which include Ekata Identity Graph and Ekata Identity Network, allow companies to combat online fraud, it said. In February, Ekata said its revenue had surged in 2020, as the COVID-19 pandemic accelerated the adoption of ecommerce, boosting demand for services to safeguard against cyber fraud. Ekata added 300 new customers last year, including food and grocery-delivery startup Postmates, which was acquired by Uber Technologies last year. The payments processor said the deal is expected to close in the next six months, adding that it does not expect the deal to be a drag on its business for more than two years."
https://venturebeat.com/2021/04/19/cloud-strength-pushes-ibm-to-sales-growth-after-a-year-of-declines/,Cloud strength pushes IBM to sales growth after a year of declines,"(Reuters) — International Business Machines returned to sales growth in the first quarter after a year of declines and beat Wall Street targets on Monday, boosted by its bets in the high-margin cloud computing business. Shares of the Dow component, which have gained nearly 6% so far this year, were up more than 4% in extended trading. Finance chief James Kavanaugh said cloud spending by clients in retail, manufacturing, and travel industries in the United States was picking up after the initial pandemic-driven slump. Sales from its cloud computing services jumped 21% to $6.5 billion in the quarter. The 109-year-old firm is preparing to split itself into two public companies, with the namesake firm narrowing its focus on the so-called hybrid cloud, where it sees a $1 trillion market opportunity. Big Blue recorded a sales decline in global technology services, its largest unit, but that was largely offset by a rise in revenue in the remaining three units, including a surprise growth in the business that hosts mainframe computers. Mainframe saw strong traction from the financial services industry, where its banking clients shopped for more capacity as trading volumes soared during the retail trading frenzy, Kavanaugh said. Total revenue rose nearly 1% to $17.73 billion in the quarter, beating analysts’ average estimate of $17.35 billion, according to IBES data from Refinitiv. Net income fell to $955 million, or $1.06 per share, in the quarter ended March 31, from $1.18 billion, or $1.31 per share, a year earlier. Excluding items, the company earned $1.77 per share, beating market expectations of $1.63."
https://venturebeat.com/2021/04/19/the-ai-arms-race-has-us-on-the-road-to-armageddon/,The AI arms race has us on the road to Armageddon,"It’s now a given that countries worldwide are battling for AI supremacy. To date, most of the public discussion surrounding this competition has focused on commercial gains flowing from the technology. But the AI arms race for military applications is racing ahead as well, and concerned scientists, academics, and AI industry leaders have been sounding the alarm. Compared to existing military capabilities, AI-enabled technology can make decisions on the battlefield with mathematical speed and accuracy and never get tired. However, countries and organizations developing this tech are only just beginning to articulate ideas about how ethics will influence the wars of the near future. Clearly, the development of AI-enabled autonomous weapons systems will raise significant risks for instability and conflict escalation. However, calls to ban these weapons are unlikely to succeed. In an era of rising military tensions and risk, leading militaries worldwide are moving ahead with AI-enabled weapons and decision support, seeking leading-edge battlefield and security applications. The military potential of these weapons is substantial, but ethical concerns are largely being brushed aside. Already they are in use to guard ships against small boat attacks, search for terrorists, stand sentry, and destroy adversary air defenses. For now, the AI arms race is a cold war, mostly between the U.S., China, and Russia, but worries are it will become more than that. Driven by fear of other countries gaining the upper hand, the world’s military powers have been competing by leveraging AI for years — dating back at least to 1983 — to achieve an advantage in the balance of power. This continues today. Famously, Russian President Vladimir Putin has said the nation that leads in AI will be the “ruler of the world.” According to an article in Salon, diverse and ideologically-distinct research organizations including the Center for New American Security (CNAS), the Brookings Institution, and the Heritage Foundation have argued that America must ratchet up spending on AI research and development. A Foreign Affairs article argues that nations who fail to embrace leading technologies for the battlefield will lose their competitive advantage. Speaking about AI, former U.S. Defense Secretary Mark Esper said last year, “History informs us that those who are first to harness once-in-a-generation technologies often have a decisive advantage on the battlefield for years to come.” Indeed, leading militaries are investing heavily in AI, motivated by a desire to secure military operational advantages on the future battlefield. Civilian oversight committees, as well as militaries, have adopted this view. Last fall, a U.S. bipartisan congressional report called on the Defense Department to get more serious about accelerating AI and autonomous capabilities. Created by Congress, the National Security Commission on AI (NSCAI) recently urged an increase in AI R&D funding over the next few years to ensure the U.S. is able to maintain its tactical edge over its adversaries and achieve “military AI readiness” by 2025. In the future, warfare will pit “algorithm against algorithm,” claims the new NSCAI report. Although militaries have continued to compete using weapon systems similar to those of the 1980s, the NSCAI report claims: “the sources of battlefield advantage will shift from traditional factors like force size and levels of armaments to factors like superior data collection and assimilation, connectivity, computing power, algorithms, and system security.” It is possible that new AI-enabled weapons would render conventional forces near obsolete, with rows of decaying Abrams tanks gathering dust in the desert in much the same way as mothballed World War II ships lie off the coast of San Francisco. Speaking to reporters recently, Robert O. Work, vice chair of the NSCAI said of the international AI competition: “We have got … to take this competition seriously, and we need to win it.” Work to incorporate AI into the military is already far advanced. For example, militaries in the U.S., Russia, China, South Korea, the United Kingdom, Australia, Israel, Brazil, and Iran are developing cybersecurity applications, combat simulations, drone swarms, and other autonomous weapons.  Caption: The Russian Uran-9 is an armed robot.  Credit: Dmitriy Fomin via Wikimedia Commons. CC BY 2.0. A  recently completed “global information dominance exercise” by U.S. Northern Command pointed to the tremendous advantages the Defense Department can achieve by applying machine learning and artificial intelligence to all-domain information. The exercise integrated information from all domains including space, cyberspace, air, land, sea, and undersea, according to Air Force Gen. Glen D. VanHerck. Gilman Louie, a commissioner on the NSCAI report, is quoted in a news article saying: “I think it’s a mistake to think of this as an arms race” — though he added, “We don’t want to be second.” West Point has started training cadets to consider ethical issues when humans lose some control over the battlefield to smart machines. Along with the ethical and political issues of an AI arms race are the increased risks of triggering an accidental war. How might this happen? Any number of ways, from a misinterpreted drone strike to autonomous jet fighters with new algorithms. AI systems are trained on data and reflect the quality of that data along with any inherent biases and assumptions of those developing the algorithms. Gartner predicts through 2023, up to 10% of AI training data will be poisoned by benign or malicious actors. That is significant, especially considering the security vulnerability of critical systems. When it comes to bias, military applications of AI are presumably no different, except that the stakes are much higher than whether an applicant gets a good rate on car insurance. Writing in War on the Rocks, Rafael Loss and Joseph Johnson argue that military deterrence is an “extremely complex” problem — one that any AI hampered by a lack of good data will not likely be able to provide solutions for in the immediate future. How about assumptions? In 1983, the world’s superpowers drew near to accidental nuclear war, largely because the Soviet Union relied on software to make predictions that were based on false assumptions. Seemingly this could happen again, especially as AI increases the likelihood that humans would be taken out of decision making. It is an open question whether the risks of such a mistake are higher or lower with greater use of AI, but Star Trek had a vision in 1967 for how this could play out. The risks of conflict had escalated to such a degree in a “Taste of Armageddon” that war was outsourced to a computer simulation that decided who would perish.  Source: Star Trek, A Taste of Armageddon. There is no putting the genie back in the bottle. The AI arms race is well underway and leading militaries worldwide do not want to be in second place or worse. Where this will lead is subject to conjecture. Clearly, however, the wars of the future will be fought and determined by AI more than traditional “military might.” The ethical use of AI in these applications remains an open-ended issue. It was within the mandate of the NSCAI report to recommend restrictions on how the technology should be used, but this was unfortunately deferred to a later date. Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
https://venturebeat.com/2021/04/19/on-the-hunt-for-a-new-job-then-you-need-to-check-these-roles-out/,On the hunt for a new job? Then you need to check these roles out,"There are so many incredible jobs available right now in the tech space, and we want you all to see them! We all deserve to have jobs we love, so we’re going to help you on your way. From Software engineering to designers, there are so many brilliant opportunities. We’ve compiled a list of some really exciting roles available at the moment. As a member of this team, you’ll be joining during exciting times as the company architects, designs, and engineers a distributed streaming pipeline to process billions of market data events each day while producing accurate and repeatable pricing. The successful candidate will take ownership for the full software development life-cycle, from understanding the needs of the business through to coding to deployment and maintenance. They will be thoughtful in testing and making sure robust systems are being developed from the ground up. The ideal candidate will have at least 5 years of software development experience in Java or Scala along with expertise in building high volume, high availability distributed systems. In this role, the successful candidate will work alongside a team of talented software developers that work to make the data engineering experience productive and high quality. The Data Engineering paved path is still taking shape, and Airbnb wants passionate engineers to develop this to support the entire company. They’re looking for someone who has 4+ years working in Data Engineering or Data Infrastructure on building Data Engineering Tools and Frameworks, along with a strong coding ability in one of the following — Scala, Java, Python. This role requires a multi-disciplined Senior Designer to join the Creative Team and execute a range of graphic assets across many areas of the business. Your design work will be viewed by millions of sports fans across North America on items such as homepage takeovers, static advertisements, landing pages, and print campaigns. You will also create assets for major partners across NFL, MLB, NBA, and NHL. You will work closely with Copywriters, Creative Directors, Project Managers, and other Designers in Fanduel’s New York office to ensure projects meet goals and objectives, while also maintaining a high level of quality and brand consistency. This role will have the ability to translate business and marketing objectives into designs that are clear, compelling, and engaging to drive the business forward. Outbrain is seeking a highly motivated, business oriented, self-reliant Product Analyst to join their BI group. In this role, you’ll serve as an analytics domain expert, work closely with the PM team and help drive the adoption, growth, and success of the products. If you’re excited to work in a cutting-edge big data environment, if you’re eager to expand a data-driven culture and drive actionable insights to meet changing business needs, this might be the role for you! The company wants someone who is resourceful, bright, proactive, works well independently and as part of a team, and who will be passionate about what she or he does. For this position, they’re looking for a person with exceptional analytical skills, and who is also a leader with the ability of self-learning and delivering results. As a full-stack engineer at 3Data, you will be responsible for building the future of the 3Data WebXR platform. You’ll work closely with other members of the 3Data Product team to reduce technical debt, ideate on potential features, and implement new features and bug fixes into the platform. Since they’re a startup, you’ll have the opportunity to directly impact the direction of this exciting platform, the creation of an inclusive culture, and will get in at the ground floor of a burgeoning industry. For even more exciting roles, head over to VentureBeat Jobs now."
https://venturebeat.com/2021/04/19/health-care-api-adoption-is-stymied-by-security-concerns-and-skills-gap/,Health care API adoption is slowed by security concerns and skills gap,"APIs may be the cornerstone of a digital transformation strategy, but the health care industry has not really benefited from widespread API adoption because of concerns over interoperability, patient data exchange, and infrastructure challenges, according to a new study. While 9 in 10 health care executives said APIs were important or mission-critical, only 24% of leading organizations currently use APIs, according to the study from Change Healthcare and Engine Group. One reason many health care organizations are not using APIs may be because the main driver for adoption has been regulatory compliance, Change Healthcare VP Gautam Shah told VentureBeat. In contrast, the primary driver in other industries was improving business processes and customer experience. Digital transformation in health care focuses on better patient care, differentiated patient and provider experiences, and increased efficiency. For health care organizations to see the kind of success enjoyed by industries such as banking, fintech, and retail, they need to address differences in API usage, misaligned priorities, and a skills gap. Health care providers (hospitals, health systems, and facilities) and payers (insurance companies, government programs) are not currently aligned on their priorities. Providers want to ensure the security of the data and its appropriate use, so security (52%) and cost (47%) are their biggest barriers to adoption. Payers want to get and operate on health care data in the fastest and most efficient manner possible, but they are hampered by a lack of high-performance technology infrastructure and standards. Payers in the study were concerned about technical infrastructure (45%), privacy (43%), and lack of industry standards (43%). There is also a disconnect between how providers and payers use APIs. Providers use APIs tactically, for eligibility verification and collecting payments. Payers are more strategic, using APIs for insights and engagements. The health care industry is undergoing a third wave of digital transformation, driven in part by changes mandated by the American Restoration and Recovery Act (2014), the 21st Century Cures Act (2016), and the Interoperability and Patient Access Rule (2020). Providers and payers need to implement electronic health record systems, improve data portability and interoperability, and enhance the use of data to make the cost and quality of care more visible and transparent. Making, supporting, and operating these technologies and models requires new or specialized skillsets. There is a steep learning curve because of the unique challenges associated with health care data and operations. There is also a skills gap, and 35% of payers and 29% of providers in the study said knowledge of how to create or use APIs was a barrier to adoption. As the industry consolidates around data interoperability, enhanced patient experiences, and operation experiences, there will be a shift to value-based care, increased consumerization, and adoption of digital technologies in health care, Shah said. New Fast Healthcare Interoperability Resources (FHIR) requirements take effect in July. Providers need to increase their adoption and use of FHIR APIs to power their digital and virtual patient experiences. Payer organizations will need to adopt FHIR APIs to enable greater data portability and transparency for their members. Digital Health companies will need to move quickly to embrace FHIR APIs to enable a wide range of applications, from wellness initiatives to chronic disease management, in order to meet patient and member expectations for richer applications driven by their data. Shah predicted, “Over the next two years, as APIs become more prevalent, as their usage matures, and as the data becomes more liquid, we stand to see real change and tangible benefit to patient care, patient and provider experience, payer operations, and ultimately to a health care system that we, our families and community, interact with daily.”"
https://venturebeat.com/2021/04/19/randori-probes-likely-attack-targets-for-cyberdefense/,Randori probes likely attack targets for cyberdefense,"Organizations spend a lot of time and money on penetration tests and “red team” exercises to identify which vulnerabilities attackers will use to get into the network and to figure out what the attackers will do afterward. Gaining insights into what attackers are most likely to do helps defenders adapt their security decisions appropriately, said Brian Hazzard, CEO and cofounder of security startup Randori. Randori provides red teaming-as-a-service via its Randori Attack Platform so enterprises can test their defenses with real exploits and attack techniques in a safe environment. The company’s Recon product helps organizations find vulnerabilities in their environments, while Attack tests real exploits against production systems to see how they would fare in a real attack. Randori’s Target Temptation engine, which launched last week, helps identify the assets attackers are most likely to target. The Target Temptation engine tells an organization how the attacker sees their infrastructure, which encompasses internet-accessible systems, as well as other services — including third-party services and tools. The assets are ranked by “attackability,” or the likelihood an attacker would want to try to compromise them. This is different from asset inventory, which provides the organization with an internal view of what it has, and vulnerability management, which identifies what is vulnerable. “For every 1,000 exposed assets, there is often only one that’s truly interesting to an attacker,” Hazzard told VentureBeat. It’s important to realize that asset management and attack surface management look at the infrastructure from different directions. “Customers have one view of their infrastructure, but the reality is that the attackers are seeing a totally different view,” Hazzard said. Attackers also care about their return on investment (ROI). They don’t want to waste time targeting systems that are well-defended or won’t lead to anything worth stealing. They look for published proofs-of-concept and exploits because that is cheaper than developing their own attack tools. They will also put more effort into targeting platforms that are widely used. Attackers typically don’t go where the organization is defending, Hazard said. Lionbridge, a company delivering AI-powered translation and localization solutions, uses Randori to help its security team prioritize which security alerts to work on. “First thing you want to do when you get your hands on a security program is to know what you have,” Lionbridge chief trust officer Doug Graham told VentureBeat. “It’s important to know, ‘What does the world see when they look at Lionbridge?'” There are certain properties attackers look for in a potential target: what useful information the attacker would be able to see about the target (enumerability), how valuable the asset is (criticality), whether there are any known vulnerabilities or published proofs of concept (weakness), how well-defended the asset is (post-exploitation), how long it would take to develop an exploit (research), and the ROI for doing so (applicability). “Things that make a software interesting are not always related to vulnerabilities,” Randori cofounder and CTO David Wolpoff told VentureBeat. This is what makes Target Temptation useful. Instead of defenders using severity scores to decide which vulnerability to fix or trying to figure out the firewall rules to protect all their internet-facing systems, organizations can focus on assets that are most likely to be compromised. “The combination helps [the organization] figure out what to do next,” Wolpoff said. Some targets are juicier than others because of where they lead. “I am always going to be interested in a VPN,” Wolpoff said, noting that he also pays attention to remote access technology, credential stores, and authentication systems. Compromising these types of components potentially opens up paths to go deeper into the network. “There is a thing in your perimeter that draws my interest. All things equal, you better have defenses around it,” Wolpoff said. Graham can see everything Randori found in Lionbridge’s environment, and any asset the security team doesn’t recognize is treated as a “risk event” to investigate. The unknown asset may be the result of an incomplete asset inventory, a case of shadow IT, or an unknown system set up by an attacker. Lionbridge found vulnerable assets shortly after signing on with Randori and was able to promptly address the issue, Graham said. The value of a platform like Randori should not be measured in terms of time saved or breaches prevented, but rather reducing attack surface, Graham said. “We measure our attack surface. What’s the vulnerability? What’s the target temptation? What is the priority?” he said. “Can I find a way to shrink that target?” Graham said he has three questions when hearing about a new vulnerability or an attack: “Do I have it [the affected system]? Is it vulnerable? And is it accessible to the internet?” The answers to those questions shape how he would respond to the executive team when they inevitably ask what is being done. “When [the CEO] sends me an email and he says, ‘What are we doing about this?’ I can say, ‘We know about it, and we’re not vulnerable’ or ‘We’re going to jump on it, and we’re going to solve it immediately,'” Graham said. For example, if a vulnerability becomes public, Randori would notify Lionbridge of the fact that it’s present in the company’s environment. But because Randori uses real exploits to test the environment to find weaknesses in the network and provides mitigation controls to fix those issues, the company may already have the controls in place and not actually be vulnerable. Or Randori may provide information about additional controls needed to address the issue. “The difference for me is between a ruined day chasing the latest brand-name vulnerability and just another routine day,” Graham said."
https://venturebeat.com/2021/04/19/meroxas-change-data-capture-service-works-with-apache-kafka-others/,"Meroxa’s change data capture service works with Apache Kafka, others","Meroxa has launched a platform-as-a-service (PaaS) environment with a control plane that leverages machine learning algorithms to manage real-time data. This comes after the company raised a fresh $15 million in its series A round. Meroxa has developed a PaaS platform through which IT organizations invoke a control plane that provides a change data capture service integrated with platforms such as Apache Kafka. That core capability is then extended through a set of rule engines that make it possible to automate repetitive engineering tasks, Meroxa CEO DeVaris Brown told VentureBeat. IT teams will be able to access that control plane via a visual interface or programmatically invoke it through a set of application programming interfaces (APIs) Meroxa has exposed. As organizations look to accelerate digital business transformation initiatives, many have discovered those projects require an ability to regularly shift massive amounts of data between the applications that enable a given process. This has necessitated hiring data engineers with the programming skills to orchestrate the movement of that data. Meroxa is making it possible for the average IT administrator or developer to now orchestrate data flows between applications. Beyond reducing the total cost of digital business initiatives, the rate at which those projects can be completed can now be significantly accelerated, Brown said. “Anybody can be a data engineer,” he said. Meroxa is applying many of the DevOps automation principles that were first applied to application development to the engineering of data pipelines. This has typically been viewed as an IT maintenance task that requires an individual to master the nuance of extract transform and load (ETL) tools. But people with data engineering skills are now among the most sought-after IT specialists, and the number of IT professionals with those skills is limited. The need to orchestrate data pipelines faster is becoming more acute because most digital business transformation initiatives typically require data to be processed and analyzed in near real time. Batch-oriented application processes are being replaced by applications that are capable of consuming stream data directly from platforms such as Kafka. That transition, however, will become extended if every organization needs to find, hire, and retain data engineering specialists. The ability to automate the constriction of data pipelines will also make it feasible for a larger number of organizations to successfully re-engineer processes. Many smaller organizations simply can’t afford to hire a dedicated data engineering specialist or contract an IT services firm to provide one. It may be a little while yet before IT teams are routinely creating data pipelines between applications and processes. However, the history of IT is littered with examples where the domain of an IT specialist has been subsumed into a function that can be handled by an IT generalist using some form of platform that automates a task. The engineering of data pipelines will ultimately be no different. In total, Meroxa has now raised $19.2 million in funding from investors that include Drive Capital, Root, Amplify, Hustle Fund, Village Global, Meritech Capital, Sequoia, Kleiner, Addition, Menlo, and Index Ventures. Other investors include former Heroku CEO Adam Gross; GitHub CTO Jason Warner; former Segment CTO Calvin French-Owen; and Nick Caldwell, VP of engineering at Twitter."
https://venturebeat.com/2021/04/19/zoom-boosts-its-apps-ecosystem-with-100m-fund/,Zoom boosts its app ecosystem with $100M venture fund,"Zoom has announced a new $100 million venture fund designed to “stimulate growth” of its burgeoning ecosystem of third-party app integrations. The announcement comes after a whirlwind 12 months for the video-communications platform company. Zoom has more than doubled in value over the past year, with businesses forced to embrace cloud-based tools as they rapidly transitioned to remote work. Back in October, Zoom launched its new Zoom Apps platform for third-party developers to integrate their apps into Zoom. This is designed to make it easier for teams to collaborate and access data while on video calls — integrations include everything from whiteboarding to cloud storage services. And this is essentially what the new $100 million fund will support. Zoom said it will invest between $250,000 and $2.5 million in growth-stage companies looking to develop tools and products that “will become core to how Zoom customers meet, communicate, and collaborate,” according to a statement. In many ways, Zoom is following the Salesforce playbook in terms of how it’s pushing to develop a vibrant ecosystem built around its core product — first through embracing third-party integrations and then through investing in them directly. Zoom has invested in startups before — in 2019 it backed hardware startup Neat — but this latest fund goes some way toward establishing Zoom as a more formal investor. To qualify for funding, companies must have a market-ready product with evidence of at least some early traction. Perhaps more importantly, their product must be focused on helping improve the Zoom experience in some way, either through Zoom Apps, SDKs, APIs, or even hardware products."
https://venturebeat.com/2021/04/19/affogata-which-helps-brands-manage-their-online-reputation-raises-5m/,"Affogata, which helps brands manage their online reputation, raises $5.5M","Customer intelligence platform Affogata today announced it has raised $5.5 million in series A funding co-led by Mangrove Capital Partners and PICO Venture Partners. Cofounder Sharel Omer said the funds will enable Affogata to invest in its tooling, strengthen its online presence, and support its customer service initiatives. No matter how exceptional a company’s customer service, meeting expectations is a challenge. According to American Express, the U.S. consumers’ board report shows more people talk about poor service than good experiences. Affogata, which was founded in 2019 by Omer, Ran Margaliot, and Itamar Rogel, offers AI-driven technology that provides clients with data related to their brand and competitors from forums, the web, and internal data sources. The platform lets stakeholders — including marketers, product managers, customer success teams, and data analysts — derive insights based on this data, enabling proactive actions to ostensibly ensure better customer experiences. The company surfaces what customers are searching for across various channels to provide a picture of preferences and trends, offering the ability to perform customer segmentation with automated competitor keyword and sentiment tracking. Affogata clients can receive alerts about sentiment changes and connect and engage with customers through the aforementioned channels. And they can keep track of reviews while controlling their messaging by removing bots and spammers with automated, AI-powered moderation.  For example, Affogata can highlight mentions of a company name in app store reviews, on Twitter and Facebook, in online communities such as Discord, and from internal systems like Zendesk and Wix Answers. Users can funnel these mentions into customized reports and dashboards or export the data for further analysis and ingestion by analytics and business intelligence systems. “The unique challenge faced by our platform is how to fuse data from structured, unstructured, and half-structured sources, coming from sources as diverse as support conversation, reviews, surveys, social media posts, and more,” Omer told VentureBeat via email. “This means we had to build a proprietary analysis pipeline, leveraging a broad set of AI approaches that we developed in-house, which is able to look at very different ‘signals’ from a vast array of sources. After those signals are taken into account, we then come up with a sensible, unified view of what customers care about, what warrants attention, and what is relevant to which stakeholder, and what requires immediate action.” Omer believes that there’s a vital need for brands like Wix, eToro, MyHeritage, Playtika, and Lemonade, which Affogata counts among its customers, to gain visibility and take action regarding sentiment. A company’s ability to implement customer feedback, monitor, and respond to online communities is increasingly a determinant in their success or failure, he claims. “Our technology shines when customers integrate it as a key part of their ops. For example, gaming companies have really short feedback loops — for example, overly aggressive game mechanics are introduced, leading to immediate churn and user complaints about ‘pay to play,'” Omer said. “Our tool helps them get instant feedback and adapt their games accordingly, reducing churn and increasing engagement, both directly contributing to their bottom-line revenues.” One study found that nearly 3 out of 4 consumers trust a company to a greater degree if the reviews for that company are positive. And according to research from Reputation X, consumers read an average of seven reviews before trusting a business. “The criteria for the insights we produce is whether they’re actionable. Our insights are relevant for customer success, product managers, marketing, developers, executives, crisis managers, security, and more,” Omer said. “That cross-organizational relevancy enables strategic initiatives that involve multiple teams at the company, giving them the power to utilize customer insights to execute and adjust initiatives.” Wix Capital and Fiverr Founder and CEO Micha Kaufman also participated in Affogata’s funding round — its first. the Tel Aviv, Israel-based company currently has 25 employees. Updated at 7:35 a.m. Pacific: This article has been updated to note that the funding round totaled $5.5 million, not $5 million, as was earlier reported. We regret the error."
https://venturebeat.com/2021/04/19/survey-finds-talent-gap-is-slowing-enterprise-ai-adoption/,Survey finds talent gap is slowing enterprise AI adoption,"AI’s popularity in the enterprise continues to grow, but practices and maturity remain stagnant as organizations run into obstacles while deploying AI systems. O’Reilly’s 2021 AI Adoption in the Enterprise report, which surveyed more than 3,500 business leaders, found that a lack of skilled people and difficulty hiring topped the list of challenges in AI, with 19% of respondents citing it as a “significant” barrier — revealing how persistent the talent gap might be. The findings agree with a recent KPMG survey that revealed a large number of organizations have increased their investments in AI to the point that executives are now concerned about moving too quickly. Indeed, Deloitte says 62% of respondents to its corporate October 2018 report adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of companies that have seen half their AI projects fail will tell you. The O’Reilly report suggests that the second-most significant barrier to AI adoption is a lack of quality data, with 18% of respondents saying their organization is only beginning to realize the importance of high-quality data. Interestingly, participants in Alation’s State of the Data Culture Report said the same, with a clear majority of employees (87%) pegging data quality issues as the reason their organizations failed to successfully implement AI. The percentage of respondents to O’Reilly’s survey who reported mature practices (26%) — that is, ones with revenue-bearing AI products — was roughly the same as in the last few years. The industry sector with the highest percentage of mature practices was retail, while education had the lowest percentage. Impediments to maturity ran the gamut but largely centered around a lack of institutional knowledge about machine learning modeling and data science (52%), understanding business use cases (49%), and data engineering (42%). Laments over the AI talent shortage in the U.S. have become a familiar refrain from private industry. According to a report by Chinese technology company Tencent, there are about 300,000 AI professionals worldwide but “millions” of roles available. In 2018, Element AI estimated that of the 22,000 Ph.D.-educated researchers globally working on AI development and research, only 25% are “well-versed enough in the technology to work with teams to take it from research to application.” And a 2019 Gartner survey found that 54% of chief information officers view this skills gap as the biggest challenge facing their organization. While higher education enrollment in AI-relevant fields like computer science has risen rapidly in recent years, few colleges have been able to meet student demand, due to a lack of staffing. There’s evidence to suggest the number of instructors is failing to keep pace with demand due to private sector poaching. From 2006 to 2014, the proportion of AI publications with a corporate-affiliated author increased from about 0% to 40%, reflecting the growing movement of researchers from academia to corporations. One curious trend highlighted in the survey was the share of organizations that say they’ve adopted supervised learning (82%) versus more cutting-edge techniques like self-supervised learning. Supervised learning entails training an AI model on a labeled dataset. By contrast, self-supervised learning generates labels from data by exposing relationships between the data’s parts, a step believed to be critical to achieving human-level intelligence. According to Gartner, supervised learning will remain the type of machine learning organizations leverage most through 2022. That’s because it’s effective in a number of business scenarios, including fraud detection, sales forecasting, and inventory optimization. For example, a model could be fed data from thousands of bank transactions, with each transaction labeled as fraudulent or not, and learn to identify patterns that led to a “fraudulent” or “not fraudulent” output. “In the past two years, the audience for AI has grown but hasn’t changed much: Roughly the same percentage consider themselves to be part of a ‘mature’ practice; the same industries are represented, and at roughly the same levels; and the geographical distribution of our respondents has changed little,” wrote Mike Loukides, O’Reilly VP of content strategy and the report’s author. “[For example,] relatively few respondents are using version control for data and models … Enterprise AI won’t really have matured until development and operations groups can engage in practices like continuous deployment; until results are repeatable (at least in a statistical sense); and until ethics, safety, privacy, and security are primary rather than secondary concerns.”"
https://venturebeat.com/2021/04/19/salesforce-launches-toolkits-and-guides-to-help-businesses-embrace-digitization/,Salesforce launches toolkits and guides to help businesses embrace digitization,"Salesforce today launched Digital 360 for Industries, a set of services built to help companies in market segments like consumer goods, financial services, nonprofit, and the public sector embrace digitization. With prebuilt templates, industry-specific developer toolkits, and customer guides, Salesforce says that Digital 360 is designed to enable businesses to overcome challenges involving data capture, systems integration, and compliance requirements. In a changing, all-digital pandemic and post-pandemic world, consumers are spending 54% more time on digital channels, according to Salesforce — creating major surges across all industries. This has required many businesses to change their operating models, even in industries historically reluctant to make the leap to digital. According to a recent Prophet report, digital transformation is still often perceived as a cost center, and data to prove return on investment remains hard to come by. Digital 360 for Industries aims to ease the transition with a library of webpage layouts, prebuilt portals, and integrations with other technologies. The components address use cases including emergency response management, insurance agents, licensing, permitting and inspections, nonprofit fundraising, and more. For example, the insurance agent template delivers a portal for independent agents to manage their list of clients. The template enables agents to view client policy claims, life events, business milestones, and other key moments to help keep agents organized and deepen client relationships. Digital 360 for Industries also includes the aforementioned toolkits, which contain code and app samples, documentation links, and best practice data for incorporating commerce into digital experiences. The toolkits come with guides for retail, discrete manufacturing, and retail banking that offer implementation blueprints, certifications, and industry best practices. Among other topics, the guides recommend ways to build ecommerce features for companies in the insurance, grocery, communications, and media segments. Digital 360 for Industries is Salesforce’s latest play in the growing digital transformation market, which is projected to grow from $469.8 billion in 2020 to $1 trillion  by 2025, according to Research & Markets. Based on a recent survey of 1,200 mid- to large-scale US companies, the average digital transformation budget was $14 million in 2018, and it is expected to increase. And digital transformation is expected to add $100 trillion to the world economy by 2025, with platform-driven interactions enabling two-thirds of value at stake from digitalization, a report by the World Economic Forum found. Salesforce says that clients including Big Brothers and Big Sisters of America, Deluxe, Fanalca, and Foodstuffs are already using Digital 360 for Industries to deliver goods and services online via tools, websites, and portals. “With changing consumer trends and business models as a result of the COVID-19 pandemic, Foodstuffs North Island, New Zealand’s biggest grocery distributor, was able to use digital channels to extend wholesale relationships,” Foodstuffs chief digital officer Simon Kennedy said in a press release. “We were able to launch a new consumer storefront in only a few weeks, pivoting fast with Salesforce tools for grocery and digital experience.” Digital 360 for Industries is generally available starting this week."
https://venturebeat.com/2021/04/19/the-democratization-of-esports-how-any-size-developer-can-reap-the-benefits-vb-live/,The democratization of esports: How any size developer can reap the benefits (VB Live),"Presented by Xsolla Right now, esports has a 495 million-strong global audience. In this VB Live event, you’ll learn how to leverage the opportunity for an engaged fanbase, including best practices on creating your own online esports and skill-based gaming platform and more. Register for free right here. Esports and competitive gaming isn’t limited to the large events and big cash prizes of an Unreal Tournament. It’s what Niccolo Maisto, co-founder and CEO of FACEIT, calls the ultimate representation of a healthy ecosystem of communities around the game — and it’s where the real opportunity of the esports boom lays. “It’s a strategic product and marketing direction that every game developer of any size should consider, to increase both engagement and reach for their game titles,” Maisto says. Offering a truly competitive experience not only unlocks a number of new sources for engagement and retention through the application of more powerful feedback and reward systems, but also stronger social network effects. When a game title invests in the development of a competitive community that’s healthy and open, players become part of something that goes beyond the in-game experience. The interactions within  this structured ecosystem, created by players, teams, communities, and tournament organizers, create endless storylines and user-generated content for a game title, he explains. “There are many examples of game titles in the market that are 10-plus years old, and have remained almost unchanged in their core game mechanics,” says Maisto, “but since they provide a solid competitive experience and a strong esports ecosystem, they keep generating engagement, growth, and revenues. Unlocking new and more powerful reward and feedback systems within the player experience generates higher levels of engagement and retention, and therefore higher monetization opportunities overall. In particular, no matter the size of a game, a true competitive experience is the most powerful way to effectively engage that cohort of hardcore players who typically tend to play more, spend more, and who tend to act as the backbone of a strong community around every game title. The organic creation and distribution of content of a competitive game title becomes a powerful activation and reactivation strategy for new and existing players with a low, almost null, marginal cost. As a title succeeds and increases its audience, events become the icing on the cake — a great way to not only engage your existing player base but also to accelerate organic growth through the exposure the game gets from the event. “Designing a great multiplayer game is always an incredible challenge that requires the perfect mix of game mechanics, physics, design, technology, psychology, and many other components that many talented game developers deal with every single day,” Maisto says. “Taking a great multiplayer game to the point at which it can become a successful competitive esports title adds even more layers of complexity.” That includes considerations like server latency, ranking systems, matchmaking balancing, tournaments formats, anti-cheat, moderation, skill systems features, and more. “My advice is to address all those considerations early on in the development lifecycle and be open to onboarding partners that can bring product and technical solutions that focus on those specific parts of the experience for the players,” he says. In part that’s because both the provisioning of a solid competitive experience and the development of the esport ecosystems require a set of products and technologies that are usually out of the scope of the typical game design and production cycle. “In the past, this was a luxury of a few large titles supported by deeper pockets,” Maisto says. “They were the only ones accessing those technologies, an issue that we decided to address as FACEIT.” Since 2012, the company has focused on building a platform to support game developers and their communities in creating competitive experiences, and developing stable esport ecosystems. The depth of integration between the FACEIT platform and a game title can vary, from the creation and development of competitive ecosystems within an experience that is entirely served outside the game client, or an integrated competitive engine inside the game client. Their most popular game on FACEIT is Counter-Strike: Global Offensive, which currently hosts nearly a million daily active users. From there, once a robust competitive experience is built, it’s a matter of supporting and empowering the creation of that ecosystem where content creators, teams, communities and players can interact with the game title to generate other forms of engagement through events and content generation, which drive more and more organic awareness for the game. “The activation of this ecosystem can take very different routes and it depends on multiple factors including the type of game, and the game-mode being played competitively,” he says. “However, providing the community with a set of accessible and effective tools to create and manage different types of competitive experiences, including tournaments, in-house leagues, and clans, is always a good way to start a snowball effect.” To learn more about the acquisition, engagement, and retention opportunities an esports strategy holds, how to design an effective platform for your game and more, don’t miss this VB Live event! Register for free here. Attendees will learn about: Speakers:"
https://venturebeat.com/2021/04/19/backup-firm-druva-protects-data-in-the-cloud-with-147m-in-new-funding/,Backup firm Druva protects data in the cloud with $147M in new funding,"Druva, a cloud data protection and backup startup based in Sunnyvale, California, today announced it has raised $147 million, pushing the company’s valuation to over $2 billion post-money. Druva says the capital will bolster a range of initiatives spanning product development and geographic expansion, as well as hiring, delivery, and customer support. Enterprises are managing nearly 40% more data than a year ago, and the stakes have arguably never been higher. Gartner predicts that at least 75% of IT operations will face one or more cyberattacks by 2025, and the University of Texas found 94% of companies suffering from a catastrophic data loss do not survive. Those statistics are more alarming in light of high-profile outages like that of OVHCloud earlier this year, an attack that took down 3.6 million websites ranging from government agencies to financial institutions and computer gaming companies. Druva, which was founded in 2008 by Jaspreet Singh, Milind Borate, and Ramani Kothandaraman, provides software-as-a-service-based data protection and management products for over 4,000 organizations, including Zoom, NASA, and Pfizer. In 2008, Singh, Borate, and Kothandaraman, who met working together at Veritas Software, formally launched Druva in Pune, India. (In Sanskrit, “druva” translates to “North Star.”) The company initially focused on providing management software to financial companies before shifting to general enterprise data management. In 2018, Druva acquired Letterkenny-based CloudRanger, a backup and disaster recovery company. The following year, Druva purchased CloudLanes to supplement its on-premises to cloud performance. Today, Druva offers services that aggregate enterprise data from endpoints, datacenters, and cloud workloads for backup and restore, disaster recovery, archival and retention, compliance monitoring, data forensics, and other uses. For example, Druva’s InSync product supports data backup on endpoint devices like laptops, smartphones, and tablets, in addition to platforms such as G Suite and Office 365. Druva Phoenix is the company’s solution for physical and virtual file servers, while Druva CloudRanger addresses Amazon Web Services  (AWS) environments and workloads. All of Druva’s offerings run on the Druva Cloud Platform, a cloud-native backup platform built on AWS that provides a centralized backup repository. Druva occupies a data backup and recovery market anticipated to be worth $11.59 billion by 2022, according to Markets and Markets. It competes to a degree with San Francisco-based Rubrik, which has raised hundreds of millions in venture capital for its live data access and recovery offerings. There’s also Cohesity and Clumio, which raked in $51 million for its cloud-hosted backup and recovery tools, as well as data recovery companies Veeam, Acronis, and HYCU. But Druva, which has over 800 employees, believes it can continue to stand out in a crowded field. In December 2019, the company surpassed $100 million in annual recurring revenue, and it claims to have grown since then, with a 26% uptick in a customer base of thousands of companies over the last year. In March, Druva crossed 2.5 billion annual backups, experiencing a 40% increase in daily backup activity over the last 12 months alone. Singh says the platform now performs over 7 million backups per day. “The global pandemic and unprecedented events of 2020 have ushered in a generational cloud transformation for businesses, with data’s increasing value at the heart of it,” Singh told VentureBeat via email. “Businesses today need a new approach to data protection, which can be deployed from anywhere, protect data everywhere, and securely scale on-demand. Only solutions built natively in the cloud are able to deliver all this functionality.” Caisse de dépôt et placement du Québec led Druva’s latest round of fundraising, with a significant investment by Neuberger Berman. It brings the company’s total raised to date to $475 million, following a $130 million series G round in June 2019."
https://venturebeat.com/2021/04/19/u-s-banks-deploy-visual-ai-tools-to-monitor-customers-and-workers/,U.S. banks deploy visual AI tools to monitor customers and workers,"(Reuters) — Several U.S. banks have started deploying camera software that can analyze customer preferences, monitor workers, and spot people sleeping near ATMs, even as the banks remain wary about possible backlash over increased surveillance, more than a dozen banking and technology sources told Reuters. Previously unreported trials at City National Bank of Florida and JPMorgan Chase, as well as earlier rollouts at banks such as Wells Fargo, offer a rare view into the potential U.S. financial institutions see in facial recognition and related artificial intelligence systems. Widespread deployment of such visual AI tools in the heavily regulated banking sector would be a significant step toward their becoming mainstream in corporate America. Bobby Dominguez, chief information security officer at City National, said smartphones that unlock via a face scan have paved the way. “We’re already leveraging facial recognition on mobile,” he said. “Why not leverage it in the real world?” City National will begin facial recognition trials early next year to identify customers at teller machines and employees at branches, aiming to replace clunky and less secure authentication measures at its 31 sites, Dominguez said. Eventually, the software could spot people on government watch lists, he said. JPMorgan said it is “conducting a small test of video analytic technology with a handful of branches in Ohio.” Wells Fargo said it works to prevent fraud but declined to discuss how. Civil liberties issues loom large. Critics point to arrests of innocent individuals following faulty facial matches, disproportionate use of the systems to monitor lower-income and non-white communities, and the loss of privacy inherent in ubiquitous surveillance. Portland, Oregon, as of January 1 banned businesses from using facial recognition “in places of public accommodation,” and drugstore chain Rite Aid shut down a nationwide face recognition program last year. Dominguez and other bank executives said their deployments are sensitive to the issues. “We’re never going to compromise our clients’ privacy,” Dominguez said. “We’re getting off to an early start on technology already used in other parts of the world and that is rapidly coming to the American banking network.” Still, the big question among banks, said Fredrik Nilsson, vice president of the Americas at Axis Communications, a top maker of surveillance cameras, is “what will be the potential backlash from the public if we roll this out?” Walter Connors, chief information officer at Brannen Bank, said the Florida company had discussed but not adopted the technology for its 12 locations. “Anybody walking into a branch expects to be recorded,” Connors said. “But when you’re talking about face recognition, that’s a larger conversation.” JPMorgan began assessing the potential of computer vision in 2019 by using internally developed software to analyze archived footage from Chase branches in New York and Ohio, where one of its two Innovation Labs is located, said two people — including former employee Neil Bhandar, who oversaw some of the effort at the time. Chase aims to gather data to better schedule staff and design branches, three people said, and the bank confirmed. Bhandar said some staff even went to one of Amazon’s cashierless convenience stores to learn about its computer vision system. Preliminary analysis by Bhandar of branch footage revealed more men would visit before or after lunch, while women tended to arrive mid-afternoon. Bhandar said he also wanted to analyze whether women avoided compact spaces in ATM lobbies because they might bump into someone, but the pandemic halted the plan. Testing facial recognition to identify clients as they walk into a Chase bank, if they consented to it, has been another possibility considered to enhance their experience, a current employee involved in innovation projects said. Chase would not be the first to evaluate those uses. A bank in the Northeast recently used computer vision to identify busy areas in branches with newer layouts, an executive there said, speaking on the condition the company not be named. A Midwestern credit union last year tested facial recognition for client identification at four locations before pausing over cost concerns, a source said. While Chase developed custom computer vision in-house using components from Google, IBM Watson, and Amazon Web Services, it also considered fully built systems from software startups AnyVision and Vintra, people including Bhandar said. AnyVision declined to comment, and Vintra did not respond to requests for comment. Chase said it ultimately chose a different vendor, which it declined to name, out of 11 options considered, and began testing that company’s technology at a handful of Ohio locations last October. The effort aims to identify transaction times, how many people leave because of long queues, and which activities are occupying workers. The bank added that facial, race, and gender recognition are not part of this test. Using technology to guess customers’ demographics can be problematic, some ethics experts say, because it reinforces stereotypes. Some computer vision programs are also less accurate on people of color, and critics have warned that could lead to unjust outcomes. Chase has weighed ethical questions. For instance, some internally called for reconsidering planned testing in Harlem, a historically Black neighborhood in New York, because it could be viewed as racially insensitive, two of the people said. The discussions emerged about the same time as a December 2019 New York Times article about racism at Chase branches in Arizona. Analyzing race was not part of the eventually tabled plans, and the Harlem branch had been selected because it housed the other Chase Innovation Lab for evaluating new technology, the people said, and the bank confirmed. Security uses for computer vision have long stirred banks’ interest. Wells Fargo used software from the company 3VR over a decade ago to review footage of crimes and see if any faces matched those of known offenders, said John Honovich, who worked at 3VR and founded video surveillance research organization IPVM. Identiv, which acquired 3VR in 2018, said banking sales were a major focus, but it declined to comment on Wells Fargo. A security executive at a mid-sized Southern bank, speaking on condition of anonymity to discuss secret measures, said over the last 18 months the bank has rolled out video analytics software at nearly every branch to generate alerts when doors to safes, computer server rooms, and other sensitive areas are left open. Outside, the bank monitors for loitering, such as the recurring issue of people setting up tents under the overhang for drive-through ATMs. Security staff at a control center can play an audio recording politely asking those people to leave, the executive said. The issue of people sleeping in enclosed ATM lobbies has long been an industry concern, said Brian Karas, vice president of sales at Airship Industries, which develops video management and analytics software. Systems that detected loitering so staff could activate a siren or strobe light helped increase ATM usage and reduce vandalism for several banks, he said. Though companies did not want to displace people seeking shelter, they felt this was necessary to make ATMs safe and accessible, Karas said. City National’s Dominguez said the bank’s branches use computer vision to detect suspicious activity outside. Sales records from 2010 and 2011 reviewed by Reuters show that Bank of America purchased iCVR cameras that were marketed at the time as helping organizations reduce loitering in ATM lobbies. Bank of America said it no longer uses iCVR technology. The Charlotte, North Carolina-based bank’s interest in computer vision has not abated. Its officials met with AnyVision on multiple occasions in 2019, including at a September conference during which the startup demonstrated how it could identify the face of a Bank of America executive, according to records of the presentation seen by Reuters and a person in attendance. The bank said, “We are always reviewing potential new technology solutions that are on the market.”"
https://venturebeat.com/2021/04/19/national-grid-partners-raises-150-million-to-invest-in-energy-and-information-crossovers/,National Grid Partners raises $150 million to invest in energy and tech crossovers,"National Grid Partners, the venture capital arm of a big utility provider in the northeastern U.S., has raised $150 million in fresh capital to invest in startups at the intersection of energy and information technology. The funding comes a couple of years after the Los Gatos, California-based investment division raised $250 million to try to disrupt the energy business. Of that amount, National Grid Partners has invested more than $227 million into 29 startups in about 30 months. Now the utility industry’s first Silicon Valley-based corporate venture and innovation group will invest even more money in the cause of energy transition amid greater overall investment activity. Just last week, Energy Transition Ventures raised $75 million to invest in getting the U.S. energy industry to move to renewable energy. Lisa Lambert, chief technology and innovation officer of National Grid and president of National Grid Partners, said in a statement that the new money is a vote of confidence from National Grid’s senior leadership. She noted that this week — which is Earth Week — National Grid is kicking off its sponsorship of the United Nations’ COP26 climate conference. COP26 is the biggest assembly of global environmental policy and industry leaders since the 2015 Paris Agreement. As part of its COP26 activities in the United Kingdom this fall, NGP plans to hold its first annual summit of the Next Grid Alliance. This consortium of more than 60 worldwide utility companies provides senior-level executives a platform to share industry best practices and solve some of the biggest challenges facing the energy sector. I asked NGP how it measures the success of its investments, and Lambert responded via email. “We measure ourselves on both strategic success and financial success,” she said. “Almost 78% of our portfolio companies are strategic — which means they have a proof-of-concept, pilot, or deployment with National Grid’s business units. These are technologies that are helping keep our networks more secure and reliable for customers, helping National Grid become more efficient and cost-effective, and helping onboard more clean energy to the grid.” She added, “On the financial side, we’ve seen significant valuation increases throughout the portfolio. With our exits, Pixeom and Aporeto, we saw an internal rate of return of over 150% for each company. And we expect more exits this year. Our top eight portfolio companies right now are expected to produce minimum combined proceeds of nearly half a billion dollars in the next one to three years. If you think about a venture capital fund’s lifecycle, it typically takes 10 years to return a fund. In another three years, we’ll likely have returned our initial $250 million fund twice over.” Lambert also announced today that NGP has invested $7.5M in two Silicon Valley companies that help enterprise customers protect physical and cyber infrastructure. These are: These companies will help National Grid serve its customers more safely and securely, Lambert said. While more than 70% of NGP’s portfolio companies have strategic engagements with National Grid — such as LineVision, which also joined the portfolio this month — seed stage startups like AccuKnox and Pathr reflect NGP’s plan to invest in every stage of the innovation ecosystem, Lambert said. This stage-agnostic approach includes NGP’s Incubation office in San Francisco and an Innovation team to turn ideas into prototypes that can be deployed in National Grid’s business units or spun out as standalone companies. NGP has led more than 60% of its startup investment rounds, with two merger and acquisition exits. Lambert spent 19 years at Intel Capital, investing in a variety of software and other kinds of tech companies. She was a leader in the company’s investments in diversity and was directly involved in the creation of a $125 million diversity fund at Intel. Asked how far along the company is in terms of disrupting the energy industry, Lambert said:"
https://venturebeat.com/2021/04/18/cambridge-quantum-pushes-nlp-quantum-computing-new-head-ai/,Cambridge Quantum pushes into NLP and quantum computing with new head of AI,"Cambridge Quantum Computing (CQC) hiring Stephen Clark as head of AI last week could be a sign the company is boosting research into ways quantum computing could be used for natural language processing. Quantum computing is still in its infancy but promises such significant results that dozens of companies are pursuing new quantum architectures. Researchers at technology giants such as IBM, Google, and Honeywell are making measured progress on demonstrating quantum supremacy for narrowly defined problems. Quantum computers with 50-100 qubits may be able to perform tasks that surpass the capabilities of today’s classical digital computers, “but noise in quantum gates will limit the size of quantum circuits that can be executed reliably,” California Institute of Technology theoretical physics professor John Preskill wrote in a recent paper. “We may feel confident that quantum technology will have a substantial impact on society in the decades ahead, but we cannot be nearly so confident about the commercial potential of quantum technology in the near term, say the next 5 to 10 years.” CQC has been selling software focused on specific use cases, such as in cybersecurity and pharmaceutical and drug delivery, as the hardware becomes available. “We are very different from the other quantum software companies that we are aware of, which are primarily focused on consulting-based revenues,” CQC CEO Ilyas Khan told VentureBeat. For example, amid concerns that improvements in quantum hardware will make it easier to break existing algorithms used in modern cryptography, CQC devised a method to generate quantum-resistant cryptographic keys that cannot be cracked by today’s methods. CQC partners with pharmaceutical and drug discovery companies to develop quantum algorithms for improving material discovery, such as working with Roche on drug development, Total on new materials for carbon capture and storage solutions, and CrownBio for novel cancer treatment biomarker discovery. The addition of Clark to CQC’s team signals the company will be shifting some of its research and development efforts toward quantum natural language processing (QNLP). Humans are good at composing meanings, but this process is not well understood. Recent research established that quantum computers, even with their current limitations, could learn to reason with the uncertainty that is part of real-world scenarios. “We do not know how we compose meaning, and therefore we have not been sure how this process can be carried over to machines/computers,” Khan said. QNLP could enable grammar-aware representation of language that makes sense of text at a deeper level than is currently available with state-of-the-art NLP algorithms like Bert and GPT 3.0. The company has already demonstrated some early success in representing and processing text using quantum computers, suggesting that QNLP is within reach. Clark was previously senior staff research scientist at DeepMind and led a team working on grounded language learning in virtual environments. He has a long history with CQC chief scientist Bob Coecke, with whom he collaborated 15 years ago to devise a novel approach for processing language. That research stalled due to the limitations of classical computers. Quantum computing could help address these bottlenecks, and there are plans to continue that research program, Clark said in a statement. “The methods we developed to demonstrate this could improve a broad range of applications where reasoning in complex systems and quantifying uncertainty are crucial, including medical diagnoses, fault-detection in mission-critical machines, and financial forecasting for investment management,” Khan said."
https://venturebeat.com/2021/04/18/is-boston-dynamics-becoming-boring-robotics-company/,Is Boston Dynamics becoming a boring robotics company?,"Boston Dynamics has made a name for itself through fascinating videos of biped and quadruped robots doing backflips, opening doors, and dancing to “Uptown Funk.” Now, it has revealed its latest gadget: a robot that looks like a huge overhead projector on wheels. It’s called Stretch, and it doesn’t do backflips, it doesn’t dance, and it’s made to do one task: moving boxes. It sounds pretty boring. But this could, in fact, become Boston Dynamics’ most successful commercial product and turn it into a profitable company.  Stretch has a box-like base with a set of wheels that can move in all directions. On top of the base are a large robotic arm and a perception mast. The robotic arm has seven degrees of freedom and a suction pad array that can grab and lift boxes. The perception mast uses computer vision–powered cameras and sensors to analyze its surroundings. While we have yet to see Stretch in action, according to information Boston Dynamics provided to the media, it can handle boxes weighing up to 23 kilograms and make 800 displacements per hour, and its battery can last eight hours. The video Boston Dynamics posted on its YouTube channel suggests the robot can reach the 800-cases-per-hour speed if everything remains static in its environment. Traditional industrial robots must be installed in a fixed location, which puts severe limits on the workflows and infrastructure of the warehouses where they are deployed. Stretch, on the other hand, is mobile and can be used in many different settings with little prerequisite beyond a flat ground and a little bit of training (we still don’t know how the training works). This could be a boon for many warehouses that don’t have automation equipment and infrastructure. As Boston Dynamics’ VP of business development Michael Perry told The Verge, “You can take this capability and you can move it into the back of the truck, you can move it into aisles, you can move it next to your conveyors. It all depends what the problem of the day is.” At first glance Stretch seems like a step back from the previous robots Boston Dynamics has created. It can’t navigate uneven terrain, climb stairs, jump on surfaces, open doors, and handle objects in complicated ways. It did manage to do some amusing feats on its intro video, but we can’t expect it to be as entertaining as Spot, Atlas, and Handle. But that’s exactly what real-world applications of robotics and artificial intelligence are all about. We still haven’t figured out how to create artificial general intelligence, the kind of AI that can mimic all aspects of the cognitive and physical abilities of humans and animals. Current AI systems are robust when performing narrow tasks in stable environments but start to break when they’re forced to tackle various problems in unpredictable settings. Therefore, the success of AI systems is to find the right balance between versatility and robustness, especially in physical settings where safety and material damage are major concerns. And Stretch exactly fits that description. It does a very specific task (picking up and displacing boxes) in a predictable environment (flat surfaces in warehouses). Stretch might sound boring in comparison to the other things that Boston Dynamics has done in the past. But if it lives up to its promise, it can directly result in reduced costs and improved production for many warehouses, which makes it a viable business model and product. As Perry told The Verge last June, “[A] lot of the most interesting stuff from a business perspective are things that people would find boring, like enabling the robot to read analogue gauges in an industrial facility. That’s not something that will set the internet on fire, but it’s transformative for a lot of businesses.” Boston Dynamics is not alone in working on autonomous mobile robots for warehouses and other industrial settings. There are dozens of companies competing in the field, ranging from longstanding companies such as Honeywell to startups such as Fetch Robotics. And unloading boxes is just one of the several physical tasks that are ripe for automation. There’s also a growing market for sorting robots, order-picking robots, and autonomous forklifts. What would make Boston Dynamics a successful contender in this competitive market? The way I see it, success in the industrial autonomous mobile robots market will be defined by versatility/robustness threshold on the one hand and cost efficiency on the other. In this respect, Boston Dynamics has two factors working to its advantage. First, Boston Dynamics will leverage its decades of experience to push the versatility of its robots without sacrificing their robustness and safety. Stretch has inherited technology and experience from Handle, Atlas, Spot, and other robots Boston Dynamics has developed. It also contains elements of Pick, a computer vision­–based depalletizing solution mentioned in the press release that declared Hyundai’s acquisition of Boston Dynamics. This can enable Stretch to work in a broader set of conditions than its competitors.  Second, the company’s new owner, Hyundai, is one of the leading companies in mobile robot research and development. Hyundai has already made extensive research in creating autonomous robots and vehicles that can navigate various environments and terrains. Hyundai also has a great manufacturing capacity. This will enable Boston Dynamics to reduce the costs of manufacturing Stretch and sell it at a competitive price. Hyundai’s manufacturing facilities will also enable Boston Dynamics to deliver new parts and props for Stretch at a cost-efficient price. This will further improve the versatility of the robot in the future and allow customers to repurpose it for new tasks without making large purchases. Stretch is the second commercial product of Boston Dynamics, the first one being the quadruped robot Spot. But Spot’s sales were only covering a fraction of the company’s costs, which were at least $150 million per year when Hyundai acquired it. Stretch has a greater potential for making Boston Dynamics a profitable company. How will the potential success of Stretch affect the future of Boston Dynamics? Here’s an observation I made last year after Hyundai acquired Boston Dynamics: “Boston Dynamics might claim to be a commercial company. But at heart, it is still an AI and robotics research lab. It has built its fame on its advanced research and a continuous stream of videos showing robots doing things that were previously thought impossible. The reality, however, is that real-world applications seldom use cutting-edge AI and robotics technology. Today’s businesses don’t have much use for dancing and backflipping robots. What they need are stable solutions that can integrate with their current software and hardware ecosystem, boost their operations, and cut costs.” How will Stretch’s success affect Boston Dynamics’ plans for humanlike robots? It’s hard to remain committed to long-term scientific goals when you’re owned by a commercial enterprise that counts profits by the quarter. But it’s not impossible. In the early 1900s, Albert Einstein worked as an assistant examiner at the Swiss patent office in Bern because physics research didn’t put food on his family’s table. But he remained a physicist at heart and continued his research in his idle time while his job as patent clerk paid the bills. His passion eventually paid off, earning him a Nobel prize and resulting in some of the greatest contributions to science in history. Will Stretch and its successors become the norm for Boston Dynamics, or is this the patent-clerk job that keeps the lights on while Boston Dynamics continues to chase the dream of humanoid robots that push the limits of science? This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/17/why-microsofts-new-ai-acquisition-is-a-big-deal/,Why Microsoft’s new AI acquisition is a big deal,"Microsoft’s recent shopping spree reached a new climax this week with the announcement of its $19.7 billion acquisition of Nuance, a company that provides speech recognition and conversational AI services. Nuance is best known for its deep learning voice transcription service, which is very popular in the health care sector. The two companies had already been working together closely before the acquisition. Nuance had built several of its products on top of Microsoft’s Azure cloud. And Microsoft had been using Nuance’s Dragon service in its Cloud for Healthcare solution, which launched last year in the midst of the pandemic. The acquisition is Microsoft’s biggest since the $26 billion purchase of LinkedIn. And it tells a lot about Microsoft’s AI strategy. Most of the focus in the announcement was on AI in health care, which makes sense because Nuance is a leading provider of AI services in the sector. “Nuance provides the AI layer at the health care point of delivery and is a pioneer in the real-world application of enterprise AI,” Microsoft CEO Satya Nadella said. “AI is technology’s most important priority, and health care is its most urgent application.”  One thing I like about Nuance is its laser focus, which is in line with the current limits and capabilities of deep learning algorithms. Deep learning might not be very good at general problem-solving or causal inference, but it can be extremely efficient at narrow tasks. Nuance has chosen one application (voice transcription) and has narrowed down its focus to one domain (clinical settings). This has enabled the company to train its machine learning models on tons of data in that specific field and make sure that its AI solutions have peak performance and reliability. Nuance has a series of AI products tailored for clinical settings, including a virtual assistant for electronic health records, a multi-party conversation transcription service, and a deep learning language model that converts clinical conversations into structured notes for integration into health records. Documentation is one of the main pain points for clinics and one of the lowest-hanging fruits for AI in health care. Nuance’s AI technology is helping save time and improve the patient experience. According to the acquisition announcement, Nuance’s AI solutions are currently used by more than 55% of physicians and 75% of radiologists in the U.S. and used in 77% of U.S. hospitals. The company has also seen a 37% year-over-year growth in the revenue of its cloud service, though that is probably due to the shifts caused by the COVID-19 pandemic. “The acquisition will double Microsoft’s total addressable market (TAM) in the health care provider space, bringing the company’s TAM in health care to nearly $500 billion,” according to Microsoft’s announcement. Nuance’s reach in the health care market suggests Microsoft will recoup its $19.7 billion investment in a relatively short term. But being able to address this market is not a simple feat. Other big tech companies, such as Apple and Google, already have health care initiatives that are much older than Microsoft’s. But Microsoft is especially well-positioned to take advantage of this new acquisition because of its business model. Google and Apple are consumer companies. Microsoft, on the other hand, gets most of its revenue from enterprise customers. Its Office suite and its collaboration tools were already being used in many hospitals even before it announced its health care solution. That’s why it was already in a good spot to penetrate the market. And if you look over at the Cloud for Healthcare page, the company has done a great job of integrating its health solution into tools that many health care workers were already used to working with, such as Outlook, Teams, Office, and messaging apps. The real advantage is the infrastructure Microsoft has built, the integration of all these services with clinical applications, and terrific data engineering that makes it possible to deploy machine learning models and data analytics tools that span various data sources. This is the perfect infrastructure on top of which Microsoft can build an AI factory, where it creates machine learning models that provide ways to improve existing products and build new ones. The acquisition will enable Microsoft to accelerate its growth by leveraging Nuance’s reach in the health care sector. Now every Nuance customer will also be a Microsoft customer. Before the acquisition, Microsoft was already using Nuance’s Dragon AI technology in its health care solution, transcribing virtual visits, taking notes, and integrating information into patients’ health records. Now, with the acquisition of Nuance, Microsoft will also have full access to its technology and will be able to take its new AI transcription power beyond health care. “Beyond health care, Nuance provides AI expertise and customer engagement solutions across Interactive Voice Response (IVR), virtual assistants, and digital and biometric solutions to companies around the world across all industries,” Microsoft says in its blog. It will be interesting to see how Nuance’s technology will be integrated into other Microsoft enterprise products. One thing that is also worth watching is how Microsoft will be able to combine Nuance’s AI with other technologies it’s experimenting with. For instance, Microsoft already has an exclusive license to OpenAI’s GPT-3 language model. Nuance’s transcription technology and GPT-3 might become a powerful combination for the enterprise. Microsoft might not be able to predict which company will be successful in five years’ time, especially in a field as volatile as AI. But it’s banking on the one constant that is always needed in the field: compute power. Microsoft uses its huge Azure platform to develop ties with companies, often providing them with subsidized access to its cloud-based machine learning tools. It also makes many of its investments in Azure credits, ensuring companies it invests in will be locked into its platform. This puts Microsoft in a position to both help those companies grow and learn from them. And the investment pays off when the company’s technology and business model mature. Earlier this year, I wrote about Microsoft’s investment in the self-driving car startup Cruise, which also made Microsoft Azure the preferred cloud of Cruise and its owner General Motors. I noted at the time that Microsoft’s success is in maintaining a safe distance from developing sectors. Instead of making one big acquisition, Microsoft casts a wide net by making smaller investments in several companies. This gives it a good foothold into many innovative sectors. As these sectors mature, Microsoft is gradually entering partnerships with the more successful startups. And when the time is right, it will acquire the company that gives it the best leverage in the market. We can see this exact cycle with Nuance as Microsoft evolved from being Nuance’s cloud provider to its partner to its owner. And this evolution tells us a lot about Microsoft’s AI strategy, which I think is very smart, given how fast things can change in the AI industry. The enterprise AI sector has come a long way toward creating applications that can solve real-world problems. But we still haven’t figured out many things. And as new technologies and companies continue to develop, Microsoft will be watching and picking winners. Ben Dickson is a software engineer and the founder of TechTalks, a blog that explores the ways technology is solving and creating problems. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/17/jensen-huang-interview-from-the-grace-cpu-to-engineers-metaverse-of-the-omniverse/,Nvidia CEO Jensen Huang interview: From the Grace CPU to engineer’s metaverse,"Nvidia CEO Jensen Huang delivered a keynote speech this week to 180,000 attendees registered for the GTC 21 online-only conference. And Huang dropped a bunch of news across multiple industries that show just how powerful Nvidia has become. In his talk, Huang described Nvidia’s work on the Omniverse, a version of the metaverse for engineers. The company is starting out with a focus on the enterprise market, and hundreds of enterprises are already supporting and using it. Nvidia has spent hundreds of millions of dollars on the project, which is based on 3D data-sharing standard Universal Scene Description, originally created by Pixar and later open-sourced. The Omniverse is a place where Nvidia can test self-driving cars that use its AI chips and where all sorts of industries will able to test and design products before they’re built in the physical world. Nvidia also unveiled its Grace central processing unit (CPU), an AI processor for datacenters based on the Arm architecture. Huang announced new DGX Station mini-sucomputers and said customers will be free to rent them as needed for smaller computing projects. And Nvidia unveiled its BlueField 3 data processing units (DPUs) for datacenter computing alongside new Atlan chips for self-driving cars. Here’s an edited transcript of Huang’s group interview with the press this week. I asked the first question, and other members of the press asked the rest. Huang talked about everything from what the Omniverse means for the game industry to Nvidia’s plans to acquire Arm for $40 billion. Jensen Huang: We had a great GTC. I hope you enjoyed the keynote and some of the talks. We had more than 180,000 registered attendees, 3 times larger than our largest-ever GTC. We had 1,600 talks from some amazing speakers and researchers and scientists. The talks covered a broad range of important topics, from AI [to] 5G, quantum computing, natural language understanding, recommender systems, the most important AI algorithm of our time, self-driving cars, health care, cybersecurity, robotics, edge IOT — the spectrum of topics was stunning. It was very exciting. Question: I know that the first version of Omniverse is for enterprise, but I’m curious about how you would get game developers to embrace this. Are you hoping or expecting that game developers will build their own versions of a metaverse in Omniverse and eventually try to host consumer metaverses inside Omniverse? Or do you see a different purpose when it’s specifically related to game developers? Huang: Game development is one of the most complex design pipelines in the world today. I predict that more things will be designed in the virtual world, many of them for games, than there will be designed in the physical world. They will be every bit as high quality and high fidelity, every bit as exquisite, but there will be more buildings, more cars, more boats, more coins, and all of them — there will be so much stuff designed in there. And it’s not designed to be a game prop. It’s designed to be a real product. For a lot of people, they’ll feel that it’s as real to them in the digital world as it is in the physical world. Omniverse enables game developers working across this complicated pipeline, first of all, to be able to connect. Someone doing rigging for the animation or someone doing textures or someone designing geometry or someone doing lighting, all of these different parts of the design pipeline are complicated. Now they have Omniverse to connect into. Everyone can see what everyone else is doing, rendering in a fidelity that is at the level of what everyone sees. Once the game is developed, they can run it in the Unreal engine that gets exported out. These worlds get run on all kinds of devices. Or Unity. But if someone wants to stream it right out of the cloud, they could do that with Omniverse, because it needs multiple GPUs, a fair amount of computation. That’s how I see it evolving. But within Omniverse, just the concept of designing virtual worlds for the game developers, it’s going to be a huge benefit to their work flow. Question: You announced that your current processors target high-performance computing with a special focus on AI. Do you see expanding this offering, developing this CPU line into other segments for computing on a larger scale in the market of datacenters? Huang: Grace is designed for applications, software that is data-driven. AI is software that writes software. To write that software, you need a lot of experience. It’s just like human intelligence. We need experience. The best way to get that experience is through a lot of data. You can also get it through simulation. For example, the Omniverse simulation system will run on Grace incredibly well. You could simulate — simulation is a form of imagination. You could learn from data. That’s a form of experience. Studying data to infer, to generalize that understanding and turn it into knowledge. That’s what Grace is designed for, these large systems for very important new forms of software, data-driven software. As a policy, or not a policy, but as a philosophy, we tend not to do anything unless the world needs us to do it and it doesn’t exist. When you look at the Grace architecture, it’s unique. It doesn’t look like anything out there. It solves a problem that didn’t used to exist. It’s an opportunity and a market, a way of doing computing that didn’t exist 20 years ago. It’s sensible to imagine that CPUs that were architected and system architectures that were designed 20 years ago wouldn’t address this new application space. We’ll tend to focus on areas where it didn’t exist before. It’s a new class of problem, and the world needs to do it. We’ll focus on that. Otherwise, we have excellent partnerships with Intel and AMD. We work very closely with them in the PC industry, in the datacenter, in hyperscale, in supercomputing. We work closely with some exciting new partners. Ampere Computing is doing a great ARM CPU. Marvell is incredible at the edge, 5G systems and I/O systems and storage systems. They’re fantastic there, and we’ll partner with them. We partner with Mediatek, the largest SOC company in the world. These are all companies who have brought great products. Our strategy is to support them. Our philosophy is to support them. By connecting our platform, Nvidia AI or Nvidia RTX, our raytracing platform, with Omniverse and all of our platform technologies to their CPUs, we can expand the overall market. That’s our basic approach. We only focus on building things that the world doesn’t have. Question: I wanted to follow up on the last question regarding Grace and its use. Does this signal Nvidia’s perhaps ambitions in the CPU space beyond the datacenter? I know you said you’re looking for things that the world doesn’t have yet. Obviously, working with ARM chips in the datacenter space leads to the question of whether we’ll see a commercial version of an Nvidia CPU in the future. Huang: Our platforms are open. When we build our platforms, we create one version of it. For example, DGX. DGX is fully integrated. It’s bespoke. It has an architecture that’s very specifically Nvidia. It was designed — the first customer was Nvidia researchers. We have a couple billion dollars’ worth of infrastructure our AI researchers are using to develop products and pretrain models and do AI research and self-driving cars. We built DGX primarily to solve a problem we had. Therefore it’s completely bespoke. We take all of the building blocks, and we open it. We open our computing platform in three layers: the hardware layer, chips and systems; the middleware layer, which is Nvidia AI, Nvidia Omniverse, and it’s open; and the top layer, which is pretrained models, AI skills, like driving skills, speaking skills, recommendation skills, pick and play skills, and so on. We create it vertically, but we architect it and think about it and build it in a way that’s intended for the entire industry to be able to use however they see fit. Grace will be commercial in the same way, just like Nvidia GPUs are commercial. With respect to its future, our primary preference is that we don’t build something. Our primary preference is that if somebody else is building it, we’re delighted to use it. That allows us to spare our critical resources in the company and focus on advancing the industry in a way that’s rather unique. Advancing the industry in a way that nobody else does. We try to get a sense of where people are going, and if they’re doing a fantastic job at it, we’d rather work with them to bring Nvidia technology to new markets or expand our combined markets together. The ARM license, as you mentioned — acquiring ARM is a very similar approach to the way we think about all of computing. It’s an open platform. We sell our chips. We license our software. We put everything out there for the ecosystem to be able to build bespoke, their own versions of it, differentiated versions of it. We love the open platform approach. Question: Can you explain what made Nvidia decide that this datacenter chip was needed right now? Everybody else has datacenter chips out there. You’ve never done this before. How is it different from Intel, AMD, and other datacenter CPUs? Could this cause problems for Nvidia partnerships with those companies, because this puts you in direct competition? Huang: The answer to the last part — I’ll work my way to the beginning of your question. But I don’t believe so. Companies have leadership that are a lot more mature than maybe given credit for. We compete with the ARM GPUs. On the other hand, we use their CPUs in DGX. Literally, our own product. We buy their CPUs to integrate into our own product — arguably our most important product. We work with the whole semiconductor industry to design their chips into our reference platforms. We work hand in hand with Intel on RTX gaming notebooks. There are almost 80 notebooks we worked on together this season. We advance industry standards together. A lot of collaboration. Back to why we designed the datacenter CPU, we didn’t think about it that way. The way Nvidia tends to think is we say, “What is a problem that is worthwhile to solve, that nobody in the world is solving and we’re suited to go solve that problem and if we solve that problem it would be a benefit to the industry and the world?” We ask questions literally like that. The philosophy of the company, in leading through that set of questions, finds us solving problems only we will, or only we can, that have never been solved before. The outcome of trying to create a system that can train AI models, language models, that are gigantic, learn from multi-modal data, that would take less than three months — right now, even on a giant supercomputer, it takes months to train 1 trillion parameters. The world would like to train 100 trillion parameters on multi-modal data, looking at video and text at the same time. The journey there is not going to happen by using today’s architecture and making it bigger. It’s just too inefficient. We created something that is designed from the ground up to solve this class of interesting problems. Now this class of interesting problems didn’t exist 20 years ago, as I mentioned, or even 10 or five years ago. And yet this class of problems is important to the future. AI that’s conversational, that understands language, that can be adapted and pretrained to different domains, what could be more important? It could be the ultimate AI. We came to the conclusion that hundreds of companies are going to need giant systems to pretrain these models and adapt them. It could be thousands of companies. But it wasn’t solvable before. When you have to do computing for three years to find a solution, you’ll never have that solution. If you can do that in weeks, that changes everything. That’s how we think about these things. Grace is designed for giant-scale data-driven software development, whether it’s for science or AI or just data processing. Question: You’re proposing a software library for quantum computing. Are you working on hardware components as well? Huang: We’re not building a quantum computer. We’re building an SDK for quantum circuit simulation. We’re doing that because in order to invent, to research the future of computing, you need the fastest computer in the world to do that. Quantum computers, as you know, are able to simulate exponential complexity problems, which means that you’re going to need a really large computer very quickly. The size of the simulations you’re able to do to verify the results of the research you’re doing to do development of algorithms so you can run them on a quantum computer someday, to discover algorithms — at the moment, there aren’t that many algorithms you can run on a quantum computer that prove to be useful. Grover’s is one of them. Shore’s is another. There are some examples in quantum chemistry. We give the industry a platform by which to do quantum computing research in systems, in circuits, in algorithms, and in the meantime, in the next 15-20 years, while all of this research is happening, we have the benefit of taking the same SDKs, the same computers, to help quantum chemists do simulations much more quickly. We could put the algorithms to use even today. And then last, quantum computers, as you know, have incredible exponential complexity computational capability. However, it has extreme I/O limitations. You communicate with it through microwaves, through lasers. The amount of data you can move in and out of that computer is very limited. There needs to be a classical computer that sits next to a quantum computer, the quantum accelerator if you can call it that, that pre-processes the data and does the post-processing of the data in chunks, in such a way that the classical computer sitting next to the quantum computer is going to be super fast. The answer is fairly sensible, that the classical computer will likely be a GPU-accelerated computer. There are lots of reasons we’re doing this. There are 60 research institutes around the world. We can work with every one of them through our approach. We intend to. We can help every one of them advance their research. Question: So many workers have moved to work from home, and we’ve seen a huge increase in cybercrime. Has that changed the way AI is used by companies like yours to provide defenses? Are you worried about these technologies in the hands of bad actors who can commit more sophisticated and damaging crimes? Also, I’d love to hear your thoughts broadly on what it will take to solve the chip shortage problem on a lasting global basis. Huang: The best way is to democratize the technology, in order to enable all of society, which is vastly good, and to put great technology in their hands so that they can use the same technology, and ideally superior technology, to stay safe. You’re right that security is a real concern today. The reason for that is because of virtualization and cloud computing. Security has become a real challenge for companies because every computer inside your datacenter is now exposed to the outside. In the past, the doors to the datacenter were exposed, but once you came into the company, you were an employee, or you could only get in through VPN. Now, with cloud computing, everything is exposed. The other reason why the datacenter is exposed is because the applications are now aggregated. It used to be that the applications would run monolithically in a container, in one computer. Now the applications for scaled out architectures, for good reasons, have been turned into micro-services that scale out across the whole datacenter. The micro-services are communicating with each other through network protocols. Wherever there’s network traffic, there’s an opportunity to intercept. Now the datacenter has billions of ports, billions of virtual active ports. They’re all attack surfaces. The answer is you have to do security at the node. You have to start it at the node. That’s one of the reasons why our work with BlueField is so exciting to us. Because it’s a network chip, it’s already in the computer node, and because we invented a way to put high-speed AI processing in an enterprise datacenter — it’s called EGX — with BlueField on one end and EGX on the other, that’s a framework for security companies to build AI. Whether it’s a Check Point or a Fortinet or Palo Alto Networks, and the list goes on, they can now develop software that runs on the chips we build, the computers we build. As a result, every single packet in the datacenter can be monitored. You would inspect every packet, break it down, turn it into tokens or words, read it using natural language understanding, which we talked about a second ago — the natural language understanding would determine whether there’s a particular action that’s needed, a security action needed, and send the security action request back to BlueField. This is all happening in real time, continuously, and there’s just no way to do this in the cloud because you would have to move way too much data to the cloud. There’s no way to do this on the CPU because it takes too much energy, too much compute load. People don’t do it. I don’t think people are confused about what needs to be done. They just don’t do it because it’s not practical. But now, with BlueField and EGX, it’s practical and doable. The technology exists. The second question has to do with chip supply. The industry is caught by a couple of dynamics. Of course one of the dynamics is COVID exposing, if you will, a weakness in the supply chain of the automotive industry, which has two main components it builds into cars. Those main components go through various supply chains, so their supply chain is super complicated. When it shut down abruptly because of COVID, the recovery process was far more complicated, the restart process, than anybody expected. You could imagine it, because the supply chain is so complicated. It’s very clear that cars could be rearchitected, and instead of thousands of components, it wants to be a few centralized components. You can keep your eyes on four things a lot better than a thousand things in different places. That’s one factor. The other factor is a technology dynamic. It’s been expressed in a lot of different ways, but the technology dynamic is basically that we’re aggregating computing into the cloud, and into datacenters. What used to be a whole bunch of electronic devices — we can now virtualize it, put it in the cloud, and remotely do computing. All the dynamics we were just talking about that have created a security challenge for datacenters, that’s also the reason why these chips are so large. When you can put computing in the datacenter, the chips can be as large as you want. The datacenter is big, a lot bigger than your pocket. Because it can be aggregated and shared with so many people, it’s driving the adoption, driving the pendulum toward very large chips that are very advanced, versus a lot of small chips that are less advanced. All of a sudden, the world’s balance of semiconductor consumption tipped toward the most advanced of computing. The industry now recognizes this, and surely the world’s largest semiconductor companies recognize this. They’ll build out the necessary capacity. I doubt it will be a real issue in two years because smart people now understand what the problems are and how to address them. Question: I’d like to know more about what clients and industries Nvidia expects to reach with Grace, and what you think is the size of the market for high-performance datacenter CPUs for AI and advanced computing. Huang: I’m going to start with I don’t know. But I can give you my intuition. 30 years ago, my investors asked me how big the 3D graphics was going to be. I told them I didn’t know. However, my intuition was that the killer app would be video games, and the PC would become — at the time the PC didn’t even have sound. You didn’t have LCDs. There was no CD-ROM. There was no internet. I said, “The PC is going to become a consumer product. It’s very likely that the new application that will be made possible, that wasn’t possible before, is going to be a consumer product like video games.” They said, “How big is that market going to be?” I said, “I think every human is going to be a gamer.” I said that about 30 years ago. I’m working toward being right. It’s surely happening. Ten years ago someone asked me, “Why are you doing all this stuff in deep learning? Who cares about detecting cats?” But it’s not about detecting cats. At the time I was trying to detect red Ferraris, as well. It did it fairly well. But anyway, it wasn’t about detecting things. This was a fundamentally new way of developing software. By developing software this way, using networks that are deep, which allows you to capture very high dimensionality, it’s the universal function approximator. If you gave me that, I could use it to predict Newton’s law. I could use it to predict anything you wanted to predict, given enough data. We invested tens of billions behind that intuition, and I think that intuition has proven right. I believe that there’s a new scale of computer that needs to be built, that needs to learn from basically Earth-scale amounts of data. You’ll have sensors that will be connected to everywhere on the planet, and we’ll use them to predict climate, to create a digital twin of Earth. It’ll be able to predict weather everywhere, anywhere, down to a square meter, because it’s learned the physics and all the geometry of the Earth. It’s learned all of these algorithms. We could do that for natural language understanding, which is extremely complex and changing all the time. The thing people don’t realize about language is it’s evolving continuously. Therefore, whatever AI model you use to understand language is obsolete tomorrow, because of decay, what people call model drift. You’re continuously learning and drifting, if you will, with society. There’s some very large data-driven science that needs to be done. How many people need language models? Language is thought. Thought is humanity’s ultimate technology. There are so many different versions of it, different cultures and languages and technology domains. How people talk in retail, in fashion, in insurance, in financial services, in law, in the chip industry, in the software industry. They’re all different. We have to train and adapt models for every one of those. How many versions of those? Let’s see. Take 70 languages, multiply by 100 industries that need to use giant systems to train on data forever. That’s maybe an intuition, just to give a sense of my intuition about it. My sense is that it will be a very large new market, just as GPUs were once a zero billion dollar market. That’s Nvidia’s style. We tend to go after zero billion dollar markets, because that’s how we make a contribution to the industry. That’s how we invent the future. Question: Are you still confident that the ARM deal will gain approval by close? With the announcement of Grace and all the other ARM-relevant partnerships you have in development, how important is the ARM acquisition to the company’s goals, and what do you get from owning ARM that you don’t get from licensing? Huang: ARM and Nvidia are independently and separately excellent businesses, as you know well. We will continue to have excellent separate businesses as we go through this process. However, together we can do many things, and I’ll come back to that. To the beginning of your question, I’m very confident that the regulators will see the wisdom of the transaction. It will provide a surge of innovation. It will create new options for the marketplace. It will allow ARM to be expanded into markets that otherwise are difficult for them to reach themselves. Like many of the partnerships I announced, those are all things bringing AI to the ARM ecosystem, bringing Nvidia’s accelerated computing platform to the ARM ecosystem — it’s something only we and a bunch of computing companies working together can do. The regulators will see the wisdom of it, and our discussions with them are as expected and constructive. I’m confident that we’ll still get the deal done in 2022, which is when we expected it in the first place, about 18 months. With respect to what we can do together, I demonstrated one example, an early example, at GTC. We announced partnerships with Amazon to combine the Graviton architecture with Nvidia’s GPU architecture to bring modern AI and modern cloud computing to the cloud for ARM. We did that for Ampere computing, for scientific computing, AI in scientific computing. We announced it for Marvell, for edge and cloud platforms and 5G platforms. And then we announced it for Mediatek. These are things that will take a long time to do, and as one company we’ll be able to do it a lot better. The combination will enhance both of our businesses. On the one hand, it expands ARM into new computing platforms that otherwise would be difficult. On the other hand, it expands Nvidia’s AI platform into the ARM ecosystem, which is underexposed to Nvidia’s AI and accelerated computing platform. Question: I covered Atlan a little more than the other pieces you announced. We don’t really know the node side, but the node side below 10nm is being made in Asia. Will it be something that other countries adopt around the world, in the West? It raises a question for me about the long-term chip supply and the trade issues between China and the United States. Because Atlan seems to be so important to Nvidia, how do you project that down the road, in 2025 and beyond? Are things going to be handled, or not? Huang: I have every confidence that it will not be an issue. The reason for that is because Nvidia qualifies and works with all of the major foundries. Whatever is necessary to do, we’ll do it when the time comes. A company of our scale and our resources, we can surely adapt our supply chain to make our technology available to customers that use it. Question: In reference to BlueField 3, and BlueField 2 for that matter, you presented a strong proposition in terms of offloading workloads, but could you provide some context into what markets you expect this to take off in, both right now and going into the future? On top of that, what barriers to adoption remain in the market? Huang: I’m going to go out on a limb and make a prediction and work backward. Number one, every single datacenter in the world will have an infrastructure computing platform that is isolated from the application platform in five years. Whether it’s five or 10, hard to say, but anyway, it’s going to be complete, and for very logical reasons. The application that’s where the intruder is, you don’t want the intruder to be in a control mode. You want the two to be isolated. By doing this, by creating something like BlueField, we have the ability to isolate. Second, the processing necessary for the infrastructure stack that is software-defined — the networking, as I mentioned, the east-west traffic in the datacenter, is off the charts. You’re going to have to inspect every single packet now. The east-west traffic in the data center, the packet inspection, is going to be off the charts. You can’t put that on the CPU because it’s been isolated onto a BlueField. You want to do that on BlueField. The amount of computation you’ll have to accelerate onto an infrastructure computing platform is quite significant, and it’s going to get done. It’s going to get done because it’s the best way to achieve zero trust. It’s the best way that we know of, that the industry knows of, to move to the future where the attack surface is basically zero, and yet every datacenter is virtualized in the cloud. That journey requires a reinvention of the datacenter, and that’s what BlueField does. Every datacenter will be outfitted with something like BlueField. I believe that every single edge device will be a datacenter. For example, the 5G edge will be a datacenter. Every cell tower will be a datacenter. It’ll run applications, AI applications. These AI applications could be hosting a service for a client or they could be doing AI processing to optimize radio beams and strength as the geometry in the environment changes. When traffic changes and the beam changes, the beam focus changes, all of that optimization, incredibly complex algorithms, wants to be done with AI. Every base station is going to be a cloud native, orchestrated, self-optimizing sensor. Software developers will be programming it all the time. Every single car will be a datacenter. Every car, truck, shuttle will be a datacenter. Every one of those datacenters, the application plane, which is the self-driving car plane, and the control plane, that will be isolated. It’ll be secure. It’ll be functionally safe. You need something like BlueField. I believe that every single edge instance of computing, whether it’s in a warehouse, a factory — how could you have a several-billion-dollar factory with robots moving around and that factory is literally sitting there and not have it be completely tamper-proof? Out of the question, absolutely. That factory will be built like a secure datacenter. Again, BlueField will be there. Everywhere on the edge, including autonomous machines and robotics, every datacenter, enterprise or cloud, the control plane and the application plane will be isolated. I promise you that. Now the question is, “How do you go about doing it? What’s the obstacle?” Software. We have to port the software. There’s two pieces of software, really, that need to get done. It’s a heavy lift, but we’ve been lifting it for years. One piece is for 80% of the world’s enterprise. They all run VMware vSphere software-defined datacenter. You saw our partnership with VMware, where we’re going to take vSphere stack — we have this, and it’s in the process of going into production now, going to market now … taking vSphere and offloading it, accelerating it, isolating it from the application plane. Number two, for everybody else out at the edge, the telco edge, with Red Hat, we announced a partnership with them, and they’re doing the same thing. Third, for all the cloud service providers who have bespoke software, we created an SDK called DOCA 1.0. It’s released to production, announced at GTC. With this SDK, everyone can program the BlueField, and by using DOCA 1.0, everything they do on BlueField runs on BlueField 3 and BlueField 4. I announced the architecture for all three of those will be compatible with DOCA. Now the software developers know the work they do will be leveraged across a very large footprint, and it will be protected for decades to come. We had a great GTC. At the highest level, the way to think about that is the work we’re doing is all focused on driving some of the fundamental dynamics happening in the industry. Your questions centered around that, and that’s fantastic. There are five dynamics highlighted during GTC. One of them is accelerated computing as a path forward. It’s the approach we pioneered three decades ago, the approach we strongly believe in. It’s able to solve some challenges for computing that are now front of mind for everyone. The limits of CPUs and their ability to scale to reach some of the problems we’d like to address are facing us. Accelerated computing is the path forward. Second, to be mindful about the power of AI that we all are excited about. We have to realize that it’s a software that is writing software. The computing method is different. On the other hand, it creates incredible new opportunities. Thinking about the datacenter not just as a big room with computers and network and security appliances, but thinking of the entire datacenter as one computing unit. The datacenter is the new computing unit. 5G is super exciting to me. Commercial 5G, consumer 5G is exciting. However, it’s incredibly exciting to look at private 5G, for all the applications we just looked at. AI on 5G is going to bring the smartphone moment to agriculture, to logistics, to manufacturing. You can see how excited BMW is about the technologies we’ve put together that allow them to revolutionize the way they do manufacturing, to become much more of a technology company going forward. Last, the era of robotics is here. We’re going to see some very rapid advances in robotics. One of the critical needs of developing robotics and training robotics, because they can’t be trained in the physical world while they’re still clumsy — we need to give it a virtual world where it can learn how to be a robot. These virtual worlds will be so realistic that they’ll become the digital twins of where the robot goes into production. We spoke about the digital twin vision. PTC is a great example of a company that also sees the vision of this. This is going to be a realization of a vision that’s been talked about for some time. The digital twin idea will be made possible because of technologies that have emerged out of gaming. Gaming and scientific computing have fused together into what we call Omniverse."
https://venturebeat.com/2021/04/17/the-sase-wave-why-cloud-native-edge-security-is-gathering-huge-momentum/,The SASE wave: Why cloud-native edge security is gathering huge momentum,"Secure Access Service Edge (SASE), commonly pronounced “sassy,” is less than two years old and is already moving the needle when it comes to forging a new market. SASE brings network and security capabilities to the edge, making it possible for distributed workforces to access corporate applications and resources with the same ease and security as they would have at a central office. Of course, the massive shift to working from home — and learning from home — during the pandemic has been a major driver of SASE adoption and deployment. SASE enables network security tools to transition away from private data centers into the public cloud or global cloud network. As a result, all users, regardless of physical location, have the same access and network flow efficiency. This means that remote user traffic no longer has to be backhauled to the corporate LAN, resulting in decreased network traffic. And this reduction in traffic can result in lower costs by allowing companies to downsize corporate Internet broadband and private WAN throughput capacity. SASE platforms are designed to provide exceptionally granular organization-wide defense that considers factors such as user location, user identification, resources used, sensitive data patterns, and any other environmental aspects that impact security integrity. In essence, SASE changes the security spotlight from traditional site-centric models to an agile user-centric approach. We’re seeing a lot of enterprises considering and adopting SASE platforms. Concurrently, we’re seeing cloud providers and communications service providers investing more in delivering SASE capabilities. To understand the SASE market, you have to know a bit about the Software-defined Wide Area Network (SD-WAN) market, since SD-WAN is typically a key SASE component. SD-WAN is a virtual WAN architecture that enables organizations to administer any combination of transport services, including MPLS, 4G/5G, and broadband Internet connectivity to securely connect users to applications. The SD-WAN uses a centralized control function to intelligently and securely direct traffic across the WAN. SD-WAN is well-suited to extending enterprise VPNs to remote sites, especially where MPLS VPN is unavailable or is too costly. So SD-WAN helps organizations connect remote sites, including work-from-home locations, on the VPN wherever MPLS could not work. SASE expands the SD-WAN mission in a couple of ways. First, it brings a very granular level of security to SD-WAN. Second, it is fully cloud-native, whereas many SD-WAN solutions can require a physical presence. As such, SASE forms a logical complement to emerging network-as-a-service (NaaS) offerings that deliver personalized network-slice services to customers on an on-demand basis. We concur with market data that supports SASE’s market momentum in 2021 and beyond. Gartner, for example, projects that by 2024 at least 40% of all enterprises will have explicit strategies to adopt SASE, up from less than 1% at year-end 2018. In addition, the global SASE market is expected to reach a compound annual growth rate of 10.8% by 2026 (according to Market Insight Reports). Such data points suggest SASE is the real deal and will avoid the marketing hype that frequently accompanies the emergence of new technologies, markets, and innovations. Ongoing standards-development initiatives by key industry bodies such as the MEF are also proving crucial in advancing and fortifying SASE acceptance across the ecosystem. Specifically, the MEF SASE security reference architecture includes: The inclusion of SASE services in the MEF standards is vital to fulfilling the open-source priorities of communications service providers, cloud service providers,, and enterprises. Without such standards-backing near its inception, SASE likely would face limited traction or would take longer than anticipated to achieve market impact. SASE creates less complex management and reduces costs of multiple separate services when an organization has various networking security solutions that are integrated into one service. A single suite of security capabilities managed by a single unified solution can also deliver better threat detection and data protection. In addition, an integrated solution helps organizations to unify identity management and authentication policies across all their locations. SASE enables organizations to activate, manage, monitor, and enforce policies across all applications, devices, locations, and users though a single portal, mitigating the need to run around administering disparate policies for separate solutions. SASE relies substantially on the zero-trust network access (ZTNA) approach, which denies users access to data and applications until their identity has been verified, including internal users inside the perimeter of a private network. When establishing access policies, the SASE model takes more than an entity’s identity into account by also factoring in security facets such as enterprise security requirements, user location, time of day, and continuous assessment of trust/risk factors. Organizations are adopting and evaluating SASE to improve the performance of any service where latencies diminish the user experience, such as conference collaboration tools, video monitoring/surveillance, and AR/VR training. SASE mitigates latency by routing traffic across a global edge network designed to assure traffic is processed as close as possible to where it will be used. It uses routing optimization to find the fastest network path based on network traffic conditions and other factors. Since SASE merges single-point security solutions into a single cloud-based service, organizations reduce integration overhead and implementation complexity. This can result in savings related to phasing out manual configuration and maintenance of traditional network and security infrastructure. Communications and cloud service providers are expanding their SASE offerings to increase revenues and boost customer retention as organizations look to impose network-wide security and data-traffic policies across their headquarters, branch offices, and remote workforce. There is growing enterprise interest in combining SD-WAN with private backbones, primarily to avoid the hacker pitfalls and security breaches that pervade the public internet. Enterprises are also increasingly interested in transitioning away from their existing MPLS VPN services, which are typically more restrictive and expensive than SD-WAN. SASE blends the functions of network and security point solutions into a unified, ubiquitous cloud-native service. As such, the SASE market segment is attracting a wide variety of competitors, including players with broad SD-WAN/security/networking portfolios, such as Cisco, HPE/Silver Peak, Oracle, Juniper/128T, and Nokia/Nuage; SD-WAN/cloud portfolios such as VMware, Cloudflare and zScaler; and SD-WAN/security suites such as Fortinet, Versa, and Palo Alto Networks. Additional high-profile players with security portfolio acumen that are targeting the SASE market include Akamai, Forcepoint, McAfee, Mushroom Networks, Netskope, Proofport, and Symantec. Clearly the SASE competitive stakes are sizzling on the supplier side. The competitors that stand out at this early stage of the SASE market all offer solutions aimed at simplifying the SASE adoption process. They include a top-tier networking supplier (Cisco), a content delivery network specialist (Cloudflare), a SASE-oriented startup (Cato Networks), and a broadband network gateway supplier (Benu Networks). Cisco. Among the established network infrastructure suppliers, we see Cisco gaining a competitive edge in the early SASE market. The company already has the SD-WAN, security, and cloud portfolio assets (e.g., the new Cisco Plus hybrid cloud solutions) to directly address the SASE space. What makes Cisco stand out are the SASE-related announcements it made at the 2021 Cisco Live event. Cisco offers its SASE package as a single, unified bundle available on a subscription basis, making it easy to procure, activate, and manage through an intuitive cloud dashboard. The package includes Cisco’s Meraki and Viptella SD-WAN software packages, Duo and Any Connect remote access, Umbrella security, the newly available Duo zero trust security, and additional security components. Cato Networks. Among new players, Cato looks like it will compete successfully long-term in the SASE market segment. Cato Cloud provides a clearly differentiated SASE solution by purpose-designing SD-WAN, network security, and ZTNA into a worldwide, cloud-native offering. Cato connects and secures the full range of enterprise edges, including sites, cloud-resources, and mobile users, with a single worldwide cloud-native platform that is distributed across more than 60 points of presence (PoPs), a clear differentiator. As testament to Cato’s ability to stand out in the nascent SASE market, the company raised $130 million in November 2020, bringing its total funding to $332 million. So it is in a good position to fund strategic business objectives such as building more PoPs and broadening ecosystem partnerships. Cloudflare. Cloudflare also combines SASE with private IP backbones to strengthen its security credentials. Its SASE model extends to both Cloudflare for Infrastructure and Cloudflare for Teams, both of which are backed by one worldwide network that services approximately 25 million internet properties. Cloudflare’s experience as a content delivery network (CDN) network provider gives the company the global network resources key to driving SASE adoption. Already, Cloudflare offers a platform of integrated network and security services across each of its 200+ distributed cities in multiple regions, mitigating the need for organizations to purchase and manage a complex collection of point solutions in the cloud. Cloudflare’s annual 2020 revenues were $431 million and its market cap is registering at $22.3 billion during April 2021, suggesting the company’s SASE proposition is boosting its market momentum. Benu Networks. Benu recently upgraded its BNG portfolio, bringing integration of SASE and 5G Access Gateway Function (AGF) capabilities to its Virtual Broadband Network Gateway (vBNG). vBNG fulfills the burgeoning carrier demand for cloud-native edge solutions. As a result, service providers can swiftly deploy SASE services to their subscribers and provide a unified experience across both mobile and fixed network environments. Benu Networks’ support of SD-defined SASE services, designed to run inside a carrier network, is a clear differentiator by giving operators the comprehensive control and ability to run organization-wide security across business sites, branch offices, and the distributed work-from-home workforce under a unified policy. Through Benu’s implementation of SASE at the service edge, carriers avoid VPN clients, use existing WiFi access points (APs), avoid low performance tunnels, and can support all devices across a distributed network. This approach takes direct aim at SD-WAN/SASE solutions that entail customer on-premise implementations such as Juniper’s 128T session-aware routing technology, which requires the deployment of the 128T Session Smart Router at the customer site. Moreover, Benu offers the combination of SASE with SD-LAN, enabling 5G-like services for fixed connections, device-level network slicing, and streamlined customer premise equipment management. In tandem, Benu’s SD-Edge Platform and vBNG solutions provide the 5G Wireless-Wireline Convergence (WWC) capabilities required to unify the use experience across fixed and mobile implementations, assuring consistent treatment of business traffic through application prioritization and holistic security policies. Overall, the SASE market is showing tangible, long-term momentum in just its second year as a new technology segment. SASE is changing the way enterprises evolve their security implementations, emphasizing cloud-first highly automated solutions that overcome the limitations and costs of traditional security approaches. Service providers of all types are prioritizing SASE as a key capability to expand their influence across the digital ecosystem and win more enterprise business. Competition across the SASE realm is intensifying, with clearly differentiated solutions already available on the market. For enterprises, adopting the SASE approach provides long-term assurances for unified security across the entire organization including the distributed WFH workforce. Through SASE, enterprises gain built-in benefits such as positive return on investment from streamlining complex security and WAN implementations and assimilating user-centric security frameworks while also taking advantage of enduring ecosystem-wide support, including industry-wide standards backing and fast expanding service provider and supplier investments in SASE. It will be exciting to watch this market continue to develop in 2021 and beyond. Daniel Newman is the principal analyst at Futurum Research, which provides research, analysis, advising, and/or consulting to high-tech companies in the tech and digital industries. Ron Westfall is a Senior Analyst at Futurum Research."
https://venturebeat.com/2021/04/16/ai-weekly-data-analytics-keeps-attracting-investment-through-the-pandemic/,AI Weekly: Data analytics keeps attracting investment through the pandemic,
https://venturebeat.com/2021/04/16/harness-coordinates-devops-and-cloud-spending-across-multiple-platforms/,Harness coordinates DevOps and cloud spending across multiple platforms,"Harness.io, a software delivery platform with cloud cost management capabilities, yesterday launched a variety of integrations that expand its services across Amazon GovCloud, Azure, and Google Cloud Platform. The new integrations make it easier for engineers to weave cost considerations into engineering decisions when working across multiple cloud platforms. Harness has long offered basic capabilities across all three of the major cloud services, but it had the most advanced features on Amazon Web Services (AWS). On Azure, Harness has now added support for Azure native deployments and cloud bill analysis. On Google Cloud Platform, it has added critical integrations to bring the features closer to what was available on AWS. Enterprises are starting to develop more scalable and resilient applications using containers and microservices on top of Kubernetes. All the major cloud platforms support Kubernetes, and in theory developers could write an application and deploy it to whichever cloud platform fits their requirements. In practice, however, engineers have to know the differences between the APIs and features of each platform and have the specific deployment for that tool. “Developers shouldn’t need to know the APIs for every container deployment service now or in the future,” Harness senior product manager Rohan Gupta told VentureBeat. Harness allows them to focus on the business functionality, which can be deployed to the appropriate cloud vetted by security and engineering teams and prioritized by cost factors. DevOps and finance teams have traditionally relied on different tools to deploy new apps and analyze cloud spending. Finance receives a bill at the end of the month and has to figure out where money is being spent and whose budget it should be charged back to. With Harness, finance and engineering teams can work together to find ways to optimize costs that address the constraints of business needs and technical infrastructure. In general, billing systems provided by each cloud vendor only go to the granularity of the cloud service, and it’s up to each consumer to define and analyze the relationship between business and cloud service. In an ideal world, these relationships are automatically built, applied, and analyzed. When businesses use a combination of CI/CD and cloud cost management tools together, the business service to cloud service relationships can be automatically added as tags. This requires an impeccable tag hygiene to be sustained for any period, however. “When it comes to cost management, a major challenge is understanding what business applications or services were responsible for costs that are rolled up at the cloud service level,” Gupta said."
https://venturebeat.com/2021/04/16/open-grid-alliance-aims-to-support-cloud-computing-at-the-edge/,Open Grid Alliance aims to support cloud computing at the edge,"VMware and Vapor IO this week kicked off an ambitious effort to rearchitect the internet via a vendor-neutral Open Grid Alliance (OGA) that aims to make network services easier for app developers to consume. Other founding members of the OGA include Dell Technologies, DriveNets, MobiledgeX, and PacketFabric. But the OGA is not seeking to define any specific technology implementations and is open to all interested parties, Vapor IO CEO Cole Crawford told VentureBeat. In fact, the OGA is deliberately staying away from creating any type of formal foundation structure, Crawford added. “A lot of people are suffering foundation fatigue,” he said. Vapor IO today provides its own framework for integrating edge computing platforms and distributed datacenters to enable machine-to-machine communications that are the core of many internet of things (IoT) applications. OGA will embrace technologies that distribute the economics and flexibility of cloud computing platforms out to the network edge. That approach will make it possible for developers and other members of an IT organization to declaratively describe their intent, which will then be used to automatically configure network services. This shift is required because, for example, next-generation wireless technologies will not be able to achieve the level of scale required by billions of intelligent devices that will be connected to the internet, Crawford said. Many of those devices will be running highly distributed applications based on microservices that will not only consume a lot of data but are also latency-sensitive, Crawford noted. Many of the applications will need to run in real time as digital business processes continue to evolve and expand, he added. The internet in its current form is designed around a core-out model that needs to be replaced by an edge-in approach that enables network services to be delivered more efficiently. It could do this by making sure packets travel more directly to where they need to arrive, instead of being widely broadcasted, Crawford added. Some members of OGA are using network virtualization overlays and smart contracts currently associated with distributed ledgers based on blockchain databases to enable that goal. But rather than relying on low-level application programming interfaces (APIs), the idea is to allow platforms to describe their capabilities to an application, VMware VP Kaniz Mahdi said. As the amount of compute horsepower and storage capacity deployed at the network edge increases, it becomes possible to reimagine how the internet could be constructed, Mahdi noted. “More automation and abstraction is required,” she said, adding “More telematics will also be necessary.” The OGA plans to define key principles for the Open Grid and identify open interoperable technologies that adhere to those principles as they emerge. It will also document how these technologies will impact cloud providers, developers, vendors, communication service providers (CSPs), internet service providers (ISPs), and end users. Intent-based networking is not a new idea, of course. Networking vendors have been using that phrase to describe the next era of networking for several years. It will, however, be several years before we see the level of scale the OGA envisions for applying those concepts. In that sense, the OGA is focused on starting a conversation about how the internet needs to change. It’s already apparent the internet in its current incarnation will need to evolve as the world becomes more interconnected. The issue that needs to be determined now is under what type of framework that goal can be achieved in the absence of a single governing body. That body would need to be empowered to define a set of interoperable internet standards in the way the U.S. Defense Advanced Research Projects Agency (DARPA) did some six decades ago."
https://venturebeat.com/2021/04/16/the-deanbeat-a-big-bang-week-for-the-metaverse/,The DeanBeat: A Big Bang week for the metaverse,"The metaverse had a couple of Big Bangs this week that should put it on everyone’s radar. First, Epic Games raised $1 billion at a $28.7 billion valuation. That is $11.4 billion more valuable than Epic Games was just nine months ago, when it raised $1.78 billion at a $17.3 billion value. And it wasn’t raising this money to invest more in Fortnite. Rather, Epic explicitly said it was investing money for its plans for the metaverse, the universe of virtual worlds that are all interconnected, like in novels such as Snow Crash and Ready Player One. Epic Games CEO Tim Sweeney has made no secret of his ambitions for building the metaverse and how it should be open. And while that might sound crazy, he received $200 million from Sony in this round, on top of $250 million received from Sony in the last round. I interpret this to mean that Sony doesn’t think Sweeney is crazy, and that it too believes in his dream of making the metaverse happen. And if Sony believes in the metaverse, then we should expect all of gaming to set the metaverse as its North Star. Epic’s $1 billion in cash is going to be spent on the metaverse, and that amount of money is going to look small in the long run. Epic Games has a foothold to establish the metaverse because it has the users and the cash. It has 350 million-plus registered users for Fortnite. And it has been investing beyond games into things like social networks and virtual concerts, as Sweeney knows that the metaverse — a place where we would live, work, and play — has to be about more than just games. Games are a springboard to the metaverse, but they’re only a part of what must be built. One of the keys to the metaverse will be making realistic animated digital humans, and two of Epic’s leaders — Paul Doyle and Vladimir Mastilović — will speak on that topic at our upcoming GamesBeat Summit 2021 conference on April 28 and April 29. This fits squarely with the notion of building out the experience of the metaverse. We need avatars to engage in games, have social experiences, and listen to live music, according to my friend Jon Radoff (CEO of Beamable) in a recent blog post. Meanwhile, this morning Nvidia announced something called GanVerse, which can take a 2D picture of a car and turn it into a 3D model. It’s one more tool to automate creation for the metaverse. To make the metaverse come to life, we need so many more layers, including discovery tools, a creator economy, spatial computing to deliver us the wow 3D experience, decentralization to make commerce between worlds seamless and permission-less, human interface and new devices that make the metaverse believable, and infrastructure too. And when you think about those things, that is what we got in another Big Bang this week as Nvidia announced its enterprise version of the Omniverse, a metaverse for engineers. By itself, that doesn’t sound too exciting. But drilling deep on it, I learned a lot about how important the Omniverse could be in providing the foundational glue for the metaverse. “The science fiction metaverse is near,” said Nvidia CEO Jensen Huang in a keynote speech this week at the company’s GTC 21 online event. First, Nvidia has been working on the Omniverse — which can simulate real-world physics — for four years, and it has invested hundreds of millions of dollars in it, said Nvidia’s Richard Kerris in a press briefing. Nvidia started this as “Project Holodeck,” using proprietary technology. But it soon discovered the Universal Scene Description language that Pixar invented for describing 3D data in an open, standardized way. Pixar invented this “HTML of 3D” and shared it with its vendors because it didn’t want to keep reinventing 3D tools for its animated movies. “The way to think about USD is the way you would think about HTML for the internet,” Huang said. “This is HTML for 3D worlds. Omniverse is a world that connects all these worlds. The thing that’s unique about Omniverse is its ability to simulate physically and photorealistically.” It open sourced USD about eight years ago, and it has spread to multiple industries. One of the best things about it is that it enable remote collaboration, where multiple artists could work on the same 3D model at once. Nvidia made USD the foundation for the Omniverse, adding real-time capabilities. Now BMW Group, Ericsson, Foster + Partners, and WPP are using it, as are 400 enterprises. It has application support from Bentley Systems, Autodesk, Adobe, Epic Games, ESRI, Graphisoft, Trimble, Robert McNeel & Associates, Blender, Marvelous Designer, Reallusion, and Wrnch. That’s just about the entire 3D pipeline for tools used to make things like games, engineering designs, architectural projects, movies, and advertisements. BMW Group is building a car factory in the Omniverse, replicating exactly what it would build in the real world but doing it first in a “digital twin” before it has to commit any money to physical construction. I saw a demo of the Omniverse, and Nvidia’s engineers told me you could zip through it at 60 frames per second using a computer with a single Nvidia GeForce RTX card (if you can get one). “You could be in Adobe and collaborate with someone using Autodesk or the Unreal Engine and so on. It’s a world that connects all of the designers using different worlds,” Huang said. “As a result, you’re in a shared world to create a theme or a game. With Omniverse you can also connect AI characters. They don’t have to be real characters. Using design tools for these AI characters, they can be robots. They can be performing not design tasks, but animation tasks and robotics tasks, in one world. That one world could be a shared world, like the simulated BMW factory we demonstrated.” Nvidia hopes to test self-driving cars — which use Nvidia’s AI chips — inside the Omniverse, driving them across a virtual U.S., from California to New York. It can’t do that in the real world. Volvo needs the Omniverse to create a city environment around its cars so that it can test them in the right context. And its engineers can virtually sit in the car and walk around it while designing it. The Omniverse is a metaverse that obeys the laws of physics and supports things that are being created by 3D creators around the world. You don’t have to take a Maya file and export it in a laborious process to the Omniverse. It just works in the Omniverse, and you can collaborate across companies — something that the true metaverse will require. Nvidia wants tens of millions of designers, engineers, architects and other creators — including game designers — to work and live in the Omniverse. “Omniverse, when you generalize it, is a shared simulated virtual world. Omniverse is the foundation platform for our AR and VR strategies,” Huang said. “It’s also the platform for our design and collaboration strategies. It’s our metaverse virtual world strategy platform, and it’s our robotics and autonomous machine AI strategy platform. You’ll see a lot more of Omniverse. It’s one of the missing links, the missing piece of technology that’s important for the next generation of autonomous AI.” By building the Omniverse for real-time interaction, Nvidia made it better for game designers. Gamers zip through worlds at speeds ranging from 30 frames per second to 120 frames per second or more. With Nvidia’s RTX cards, they can now do that with highly realistic 3D scenery that takes advantage of real-time ray tracing, or realistic lighting and shadows. And Kerris said that most what you see doesn’t have to be constantly refreshed on every user’s screen, making the real-time updating of the Omniverse more efficient. Tools like Unreal or Unity can plug into the Omniverse, thanks to USD. They can create games, but once the ecosystem becomes mature, they can also absorb assets from other industries. Games commonly include realistic replicas of cities. Rockstar Games built copies of New York and Los Angeles for its games. Ubisoft has built places such as Bolivia, Idaho, and Paris for its games. Imagine if they built highly realistic replicas and then traded them with each other. The process of creating games could be more efficient, and the idea of building a true metaverse, like the entire U.S., wouldn’t seem so crazy. The Omniverse could make it possible. Some game companies are thinking about this. One of the studios playing with Omniverse is Embark Studios. It’s founder is Patrick Soderlund, the former head of studios for Electronic Arts. Embark has backing from Nexon, one of the world’s biggest makers of online games. And since the tools for Omniverse will eventually be simplified, users themselves might one day be able to contribute their designs to the Omniverse. Huang thinks that game designers will eventually feel more comfortable designing their worlds while inside the Omniverse, using VR headsets or other tools. “Game development is one of the most complex design pipelines in the world today,” Huang said. “I predict that more things will be designed in the virtual world, many of them for games, than there will be designed in the physical world. They will be every bit as high quality and high fidelity, every bit as exquisite, but there will be more buildings, more cars, more boats, more coins, and all of them — there will be so much stuff designed in there. And it’s not designed to be a game prop. It’s designed to be a real product. For a lot of people, they’ll feel that it’s as real to them in the digital world as it is in the physical world.” Omniverse enables game developers working across this complicated pipeline, allowing them to be connected, Huang said. “Now they have Omniverse to connect into. Everyone can see what everyone else is doing, rendering in a fidelity that is at the level of what everyone sees,” he said. “Once the game is developed, they can run it in the Unreal engine that gets exported out. These worlds get run on all kinds of devices. Or Unity. But if someone wants to stream it right out of the cloud, they could do that with Omniverse, because it needs multiple GPUs, a fair amount of computation.” He added, “That’s how I see it evolving. But within Omniverse, just the concept of designing virtual worlds for the game developers, it’s going to be a huge benefit to their work flow. The metaverse is coming. Future worlds will be photorealistic, obey the laws of physics or not, and be inhabited by human avatars and AI beings.” On a smaller scale, Roblox also did something important. It cut a deal with Hasbro’s Nerf brand this week, where some new blasters will come to the game. Roblox doesn’t make the blasters itself. Rather, it picks some talented developers to make them, so that it stays true to its user-generated content mantra. That Roblox can partner with a company like Hasbro shows the brands have confidence in Roblox, as it has demonstrated in deals with Warner Bros. Usually, user-generated content and brands don’t mix. The users copy the copyrighted brands, and the brands have to take some legal action. But Roblox invests a lot in digital safety and it doesn’t seem to have as big a problem as other entities. That’s important. We know that Roblox is a leading contender for turning into the metaverse because it has the users — 36 million a day. But the real test is whether the brands will come and make that metaverse as lucrative as other places where the brands show up, like luxury malls. And FYI, we’ve got a panel on Brands and the Metaverse at our GamesBeat Summit 2021 event on April 28 and April 29. Kudos for Steven Augustine of Intel for planting that thought in my brain months ago. I feel like the momentum for the metaverse is only getting stronger, and it is embedding itself in our brains as a kind of Holy Grail — or some other lost treasure in other cultures — that we must find in order to reach our ultimate goals."
https://venturebeat.com/2021/04/16/gamesbeat-summit-2021-growing-the-next-generation-our-leader-packed-agenda/,GamesBeat Summit 2021: Growing the next generation — Our leader-packed agenda,"GamesBeat Summit 2021 is a digital online-only event taking place on April 28 and April 29 as our third virtual conference in a year. We’re looking forward to getting together in the physical world or in the metaverse one day, but in the meantime, we’ve learned how to deliver a good experience with an online event. I’ve included the full agenda below and will update it as we fill in the final names. We’ve got 87 speakers at the moment and 45 of them — about 52% — come from diverse backgrounds. We’ve got top speakers like Bobby Kotick of Activision Blizzard, Phil Spencer of Microsoft, and Laura Miele of Electronic Arts. And we have others you haven’t heard about who are making a difference in the ranks. This is our most diverse event ever, and it will be the first one with a member of the U.S. Congress — Yvette Clarke, Democrat from New York’s 9th District. She will join Stanley Pierre-Louis of the Entertainment Software Association and Laila Shabir, CEO of Girls Make Games for a talk on the importance of STEAM (science, technology, engineering, art, and math) education for gaming. We’ve also confirmed our awesome emcees Kahlief Adams of Spawn on Me and Andrea Rene of What’s Good Games. Rene will moderate our second annual Women in Gaming Breakfast with speakers that include Samantha Ryan of Electronic Arts and Brenda Romero of Romero Games. Since our last post, we’ve added a lot of speakers including Karthik Bala of Velan Studios, Samir Agili of Tilting Point, tech futurist Cathy Hackl, Ken Martin of GreenPark Sports, Kimberly Voll of the Fair Play Alliance, James Zhang of Fifth Era and Concept Art House, Tim O’Brien of Scopely, Amir Rahimi of Scopely, Eric Seufert of Mobile Game Dev Memo, former NBCUniversal Games head Chris Heatherly, Jon Radoff of Beamable, Michael Pachter of Wedbush Securities, Tammy McDonald, and Gabby Dizon of Yield Guild Games. Diversity, inclusion, and mental health challenges are going to be big topics for discussion. And we’ll explore how games can keep their historic growth going and at the same time explore new parts of the business including blockchain and nonfungible tokens, the post-IDFA world, augmented and virtual reality, and the metaverse. Michael Pachter of Wedbush Securities will open with a talk on the explosion of opportunities that come from having an unprecedented amount of money coming into the industry through investments, public offerings, and acquisitions. We want to continue our reputation as the most intimate gaming event where business meets passion. Our event will include fireside chats, panels, and small-group roundtables. We’ll provide Q&A sessions for VIP attendees, and a way for attendees to network with each other and make new connections. We have a wide range of partners including the International Game Developers Association and Women in Gaming International. And our sponsors include Lego Ventures, Anzu, Xsolla, Jam City, Adjust, Accenture, Rogue Games, Epic Games, Scopely, Singtel, the Entertainment Software Association, Wildlife, Perforce, Outfit7, and more. For attendees, you’ll be getting invitations to join using the email you used to register. If you upgrade to VIP, you’ll be able to join things like our GamesBeat Slack (which we’ve already started), one-on-one meetings in Grip, roundtables, and our Zoom Q&A sessions with our speakers. 8 a.m. Tutorial for watching and participating in the event 8:10 a.m. – 8:30 a.m. Introduction Emcee Kahlief Adams, Spawn on Me Dean Takahashi of GamesBeat 8:30 a.m. – 8:40 a.m. Gaming’s time to shine Michael Pachter, managing director at Wedbush Securities and an analyst for the games industry for 20 years, will talk about
what it means to have so much money coming into the game business at all levels. Michael Pachter, Wedbush Securities 8:40 a.m – 9:00 a.m. Scaling creativity through the Scopely Operating System Join Scopely Chief Business Officer Aaron Loeb and moderator Dean Takahashi for a conversation on how game companies can maximize creativity among rapidly growing global teams. Aaron Loeb, Scopely Moderator: Dean Takahashi, GamesBeat 9:00 a.m. – 9:30 a.m. Enabling the next generation of leaders Game director Brenda Romero of Romero Games and technical director Sushama Chakraverty of Prodigy Education examine the role of leadership in the game industry through the lens of their own intertwining careers. Brenda Romero, Romero Games Moderator: Sushama Chakraverty, Prodigy Education 9:30 a.m. – 10:00 a.m. Diving into digital humans with Epic Games Join Vladimir Mastilovic and Paul Doyle of Epic Games and Wanda Meloni of M2 Insights for a discussion on the evolution of digital humans and how MetaHuman Creator simplifies the creation of unique, convincing digital humans. Paul Doyle, Epic Games Vladimir Mastilović, Epic Games Moderator: Wanda Meloni, M2 Insights 10:00 a.m. – 10:30 a.m. How learning through play intersects with gaming Rob Lowe, Lego Ventures Karsten Lund, Director, Light Brick Studio Moderator: Keza MacDonald, Guardian 10:30 a.m. – 11:00 a.m. Magic Meets the Measured: striking the right balance to make great games Chris DeWolfe, CEO of Jam City, talks about striking the right balance in taking care of employees, taking advantage of growth opportunities during the pandemic, and doing the right things for gamers. Chris DeWolfe, Jam City Moderator: Dean Takahashi, GamesBeat 11:00 a.m. – 11:30 a.m. Game VCs who bring the value of operating experience to investing Game VCs who are former operators at companies talk about the value — and the challenges — that they can bring to the table in helping startups grow. Andrew Sheppard, Transcend Fund Jens Hilgers, Bitkraft Ventures Ed Fries, 1Up Ventures Moderator: Eric Goldberg, Playable Worlds 11:30 a.m. – 12:00 p.m. What IDFA changes mean for the game industry Apple is requiring users to opt-in for sharing their private info on iOS, and that’s going to affect the ability for mobile game makers to target ads at gamers. What comes next? Eric Seufert, Mobile Game Dev Memo Moderator: Dean Takahashi, GamesBeat 12:00 pm -12:30 pm The next generation of mobile game development With more and more individuals turning to games amidst a global pandemic, the importance of mobile gaming is at an unprecedented height. But how does mobile gaming keep up with other platforms? Publishers talk about empowering the next generation of developers. Victor Lazarte, Wildlife Studios Ken Martin, GreenPark Sports Samir Agili, Tilting Point Moderator: Joost van Dreunen, author of One Up 12:30 p.m. -1:10 p.m. The post-pandemic world of gaming Bobby Kotick, chairman and CEO of Activision Blizzard, will speak at one of our events for the first time in a fireside chat about the acceleration of games into the top tier of all entertainment. But what comes next? And how will we navigate 2021 and beyond? Bobby Kotick, Activision Blizzard Moderator: Dean Takahashi, GamesBeat 12:30 p.m.-1:30 p.m. Roundtable (VIP attendees eligible) 12:30 p.m.-1:30 p.m. Your Culture isn’t what you think it is Caroline Stokes, Forward Roundtable (VIP attendees eligible) 1230 pm to 1:30 pm Life in a post-IDFA world Faith Price, Double Down Interactive Dan Barnes, N3twork 1:10 p.m.-1:40 p.m. Team Xbox on gaming for everyone Microsoft’s Phil Spencer will lead a discussion about the company’s journey to bring the joy and community of gaming to everyone on the planet. The panelists will explore the challenges of ensuring that gaming is inclusive for all players and that the game industry welcomes all creators. Agnes Kim, Microsoft Esteban Lora, Microsoft Cierra McDonald, Microsoft Moderator: Phil Spencer, head of Xbox at Microsoft 1:40 p.m. -2 p.m. The future of accessibility in the game industry Daniel Melville was born without an arm. But he has managed to become a gamer as well as an ambassador for Open Bionics, which recently produced a Metal Gear-themed bionic arm for him. He’ll discuss this and other themes about accessibility and inclusion in the game industry with Keisha Howard of Sugar Gamers. Daniel Melville, Open Bionics Moderator: Keisha Howard, Sugar Gamers 2 p.m. – 2:30 p.m. Changing the game Robert Antokol, CEO of Playtika, and Michael Metzger, partner at Drake Star Partners, will talk about Playtika’s journey from launch, to expansion into casual games and recent public offering.  Robert will discuss Playtika’s Boost Platform and acquisition of independent gaming studios as well as share his vision for Playtika in the years to come. Robert Antokol, Playtika Moderator: Michael Metzger, Drake Star Partners 2:30 p.m. – 3 p.m. STEAM, why it matters to video games, diversity, the future workforce, and the economy STEAM (science, technology, engineering, art, and math) education is not only critical to the success of video game industry, but also to creating meaningful opportunities for people from different backgrounds and to American competitiveness. This panel will discuss the increasingly important role of STEAM in the overall American economic landscape, its impact on the video game industry, and youth leadership. Laila Shabir, Girls Make Games Yvette Clarke, U.S. Congresswoman from New York’s 9th District Moderator: Stanley Pierre-Louis, Entertainment Software Association 3 p.m. – 3:30 p.m. What to expect in a post-pandemic gaming world Join Frank Azor, AMD’s Chief Architect of Gaming Solutions and Marketing, as he shares thoughts on how the COVID-19 pandemic has and will continue to impact the gaming industry, and how companies and developers are adapting to that change. Frank Azor, AMD Moderator: N’Gai Croal, Hit Detection 3:30 p.m. – 3:50 p.m. Burnout: What it is and what we can do A lot of people are suffering from burnout as the pandemic and quarantine continue into year two. What is burnout (beyond exhaustion and overwork)? This game-focused talk will detangle and explain the aspects of burnout, and provide actionable, practical steps for addressing it inside your studio and workplace. Raffael Boccamazzo, Take This 3:50 p.m. – 4:20 p.m. A strategy for a global gaming business Mike Vorhaus of Vorhaus Advisers will interview Simon Zhu, general manager at NetEase, about the company’s approach to the gaming industry. They will talk about NetEase’s focus on quality as well as its investments in Bungie, Theorycraft, Behaviour Interactive, and Quantic Dream. Simon Zhu, NetEase Moderator: Mike Vorhaus 4:20 p.m. – 4:50 p.m. Designing blockbuster games infused with diversity What happens when you start designing a major video game and try to infuse diversity throughout the characters and environment? Halley Gross — a seasoned Hollywood writer on shows such as Westworld participated in such as project as the co-writer for The Last of Us Part II. The game won more than 215 awards for Game of the Year. She shares some of the lessons she learned about writing blockbuster narratives with an inclusive lens. Halley Gross Moderator: Dean Takahashi 4:50 p.m. to 5:20 p.m. Why gaming M&A and investments have exploded and show no signs of slowing M&A, public offerings, and investments have exploded for games during the pandemic. We’ll have a fireside chat with Lars Wingefors, CEO of Embracer Group, which has been the most active of all game company acquirers. Lars Wingefors, Embracer Group Moderator: Nick Tuosto, Liontree 5:20 p.m.-6:20 p.m. Reception and networking on Clubhouse Hosts Jon Radoff and Dean Takahashi 8:00 a.m. – 9:30 p.m. Women in gaming breakfast Samantha Ryan, EA Brenda Romero, Romero Games Emily Greer, Double Loop Games Moderator: Andrea Rene 9:30 am – 9:40 am Introduction Andrea Rene, What’s Good Games 9:40 am – 9:45 am Opening comments Mike Minotti 9:45 a.m. – 10:05 a.m. It takes a village The game industry hasn’t always been kind to its own. And as a result, it has lost members to suicide. Mark Chandler of the International Games Summit on Mental Health Awareness and Jason Docton of Rise Above the Disorder will talk about what could be done so the game industry and gamers take care of their own better. Mark Chandler, The International Games Summit on Mental Health Awareness Jason Docton, Rise Above the Disorder 10:05 a.m. – 10:25 a.m. How to do remote game development When the world went remote, game development companies knew how to make it work. Many have been coordinating remote contributors for years. But with everyone at home, demands for media and games have dramatically increased. Our panel will discuss how to overcome challenges and build better games. Mark James, Striking Distance Studios Karthik Bala, Velan Studios Moderator: Brad Hart, CTO of Perforce 10:25 a.m. to 10:45 a.m. Pulling the future from the cloud Tim Guhl of Singtel and Lisa Cosmas Hanson of Niko Partners talk about the opportunities in gaming that will depend on 5G networks, including augmented reality, low-latency multiplayer games, and esports. Tim Guhl, Singtel
Moderator: Lisa Cosmas Hanson, Niko Partners 10:45 a.m. – 11:15 a.m. Making great games is only going to get more complicated Laura Miele started her career in a game studio 25 years ago and today she is leading one of the largest collectives of game creators in the world at Electronic Arts. EA’s 20+ global studios. Geoff Keighley, creator of The Game Awards, will talk with Miele about the ever-changing world of game development and how she navigates through the internal and external challenges to deliver the best possible games to players. Laura Miele, Electronic Arts Moderator: Geoff Keighley 11:15 a.m. – 11:45 a.m. Advertising in games: Tapping into the next wave Brands are finally realizing the potential of gaming and are ready to treat it as an advertising medium. This panel will explain the evolution of advertising in games and why brands are now finally looking at games as an advertising category and what is on the horizon. Ronnie Nelis, Lion Castle Itamar Benedy, Anzu Gabrielle Heyman, Zynga Moderator: Steve Peterson, StoryPhorce 11:45 a.m. to 12:15 p.m. Representation in-game matters, and it starts with developers Nearly half of all gamers now identify as female. But in the development community, female representation is still sorely lacking. With such a skewed balance among the people who make our games, how can we make sure we’re meeting the needs of all our players, who are increasingly expressing their needs to see themselves represented in the games they play? Hear from top female development leads across Activision Blizzard about addressing the gender gap in the games, and on the teams who make them. Jennifer Oneal, Blizzard Entertainment Lydia Bottegoni, Blizzard Entertainment Nour Polloni, Studio Head, Beenox Moderator: Eunice Lee, Activision 12:15 p.m. to 1:15 p.m. Roundtable (for VIP attendees) NFTs and games 101 Nonfungible tokens have taken the art and music collectible worlds by storm, and now they’re ready to disrupt games. Are NFTs the next big thing to monetize games, or are they a flash in the pan? Chris Heatherly Jon Radoff, Beamable James Zhang, Fifth Era Gabby Dizon, Yield Guild Games 12:15 p.m. to 1:15 p.m. Roundtable (for VIP attendees) Understanding the future of consolidation, expanding capital pools, and hyperactivity in gaming M&A Hemal Thakur, Goldman Sachs Alina Soltys, Quantum Tech Partners 12:15 p.m. to 1:15 p.m. Roundtable (for VIP attendees) Expanding the esports audience Daniel Evans, Reely Robyn St. Germain, Houston Outlaws Rebecca Longawa, Rokkr, Version1, GameHers Heather Garozzo, VP Talent at Dignitas 12:15 p.m. – 12:35 p.m. How to do M&A right Scopely acquired FoxNext Games in January 2020. The companies set a goal of a seamless merging of the two teams. It turns out that, during the pandemic, M&A has exploded in the game industry. Two Scopely leaders will talk about the lessons learned. Tim O’Brien, Scopely Amir Rahimi, Scopely Moderator: Nick Tuosto, Liontree 12:35 p.m. – 12:55 p.m. Scaling the right way: When a startup is no longer a startup Katie Jansen, chief marketing officer at AppLovin, and Katie Madding, chief product officer at Adjust, share similar stories in their respective careers, with both having joined their current companies early on, and then having witnessed the trajectories of those companies skyrocket in a relatively short amount of time. Learn about their unique journeys in navigating these exciting, yet uncharted periods of hyper-growth. Katie Jansen, AppLovin Moderator: Katie Madding, Adjust 1:15 p.m. – 1:45 p.m. The past, present, and future of XR and mental health Since the onset of COVID-19, 53% of adults in the United States reported that their mental health has been negatively impacted. This is particularly evident for front line workers, where one-third have reported elevated levels of mental distress. This panel will bring together professionals in multiple sectors—Kelli Dunlap (Kentlands Psychotherapy, Take This), Noah Falstein (The Inspiracy), and Brennan Spiegel (Cedars-Sinai Medical Center)—to delve deeper into how this rapidly evolving technology is being used to directly help mental health. Kelli Dunlap, American University Noah Falstein, The Inspiracy Brennan Spiegel, Cedars-Sinai Moderator: Susanna Pollack, Games for Change 1:45 p.m. – 2:15 p.m. How Iron Galaxy Studios avoids crunch One of Iron Galaxy’s Studios’ many core values is people – the strength of their team is the foundation of their success. They focus first and foremost on the well-being of their employees, making sure staff feel cared for and valued. Co-CEO’s Adam Boyes and Chelsea Blasko can share their leadership philosophies and how they continue to offer a healthy work/life balance. Chelsea Blasko, Iron Galaxy Adam Boyes, Iron Galaxy Moderator: Eve Crevoshay, Take This 2:15 p.m. – 2:45 p.m. A game changing experience for everyone This panel will discuss issues of digital civility and the challenges to getting through to the next level as games become even more mainstream. What challenges need to be addressed in order for the gaming industry to seize the exponential opportunity at hand? Christian Kelly, Accenture Laura Higgins, Roblox Kimberly Voll, Fair Play Alliance Moderator: Seth Schuler, Accenture 2:45 p.m. – 3:15 p.m. How global publishers can differentiate themselves? What are the challenges of global publishing? How do you use IP? How can you adapt to new platforms, app stores, business models, and regional markets? Chris Hewish, Xsolla Matt Casamassina, Rogue Games Anthony Crouts, Tencent Moderator: Lisa Cosmas Hanson, Niko Partners 3:15 p.m. – 3:35 p.m. When is it time to launch a new IP? Outfit7 has one of the biggest intellectual properties for games and entertainment with Talking Tom. It has created lots of apps and games based on it. It has endless runner games, virtual life simulations, videos, and others that tap into Tom’s personality. But the company also needs to create new IPs for the future. What’s the right time to do that and how? Ante Odić, Senior VP of Product, at Outfit7 Shawn Layden, former head of Sony Worldwide Studios Marty O’Donnell, Highwire Games Moderator: Dean Takahashi, GamesBeat 3:35 p.m. – 3:55 p.m. Brands x Creators: Collaborating to scale economic opportunity for everyone Celebrity influencers have come out of the woodworks in the past decade, but not everybody can be the next Ninja. What can the industry do to create the next generation of streamers who can make a living at their trade? We’ll explore this in a panel on getting paid as a streamer. Stu Grubbs, Lightstream Natasha Zinda, ZombaeKillz Moderator: Andrea Rene, What’s Good Game 3:55 p.m. – 4:25 p.m. Brands and the metaverse We’ve been stuck in the Zoomverse during the pandemic, and we can’t wait for an actual metaverse to arrive. Gaming will likely be the big draw for consumers. And if the consumers come, the brands will follow. We’ll ask some brands if they believe in the metaverse. Ryan Mullins, Aglet Ian Fitzpatrick, New Balance Perry Nightingale, WPP Moderator: Cathy Hackl 4:25 p.m. – 4:40 p.m. Visionary Awards Tammy McDonald 4:40 p.m. – 4:45 p.m. Closing session 4:45 p.m. – 5:45 p.m. Networking on Clubhouse Hosts Jon Radoff and Dean Takahashi"
https://venturebeat.com/2021/04/16/device42-extends-aiops-reach-of-it-infrastructure-discovery-tool/,Device42 extends AIOps reach of IT infrastructure discovery tool,"Device42 today announced it has added support for additional platforms to an AIOps tool that enables organizations to discover IT resources residing in the cloud and in on-premises IT environments. As enterprise IT environments become more extended, platforms are being added in a way that is not always immediately discernible to a centralized IT team. Developers, for example, now routinely spin up virtual machines on public clouds without any intervention on the part of an internal IT team required. AIOps tools from Device42 employ machine learning algorithms to create device and application topologies and impact charts that surface all the IT infrastructure being employed across a hybrid cloud computing environment. The latest update adds the ability to also discover cloud databases, along with other types of cloud services — in addition to storage resources in on-premises IT environments. Those assets are discovered using the application programming interfaces (APIs) that have been exposed by cloud services providers and IT vendors that provide infrastructure for on-premises IT environments. Once all those assets are mapped, it becomes possible for IT teams to employ a search function to uncover, for example, dependencies between multiple services being employed, Device42 cofounder and CEO Raj Jalan told VentureBeat. Not every IT organization may need an AI platform to keep track of its IT assets. But the larger an organization becomes, the more challenging it is to monitor what infrastructure is being employed where and for what purpose. Cloud services, especially, tend to be dynamically consumed, which makes trying to keep track of usage manually nearly impossible. It’s not uncommon for IT organizations to find themselves being billed for cloud services they had no idea where being employed until the invoice from the cloud service provider arrived. At the same time, more application workloads are now starting to be pushed out to the network edge in places centralized IT teams often have no way to physically reach. With the addition of each new platform to an IT environment, the probability an organization is going to run afoul of one compliance regulation or another increases, Jalan noted. The Device42 platform makes it easier for IT teams to uncover potential issues long before any audit might, he said. This capability can also play a role in helping IT teams discover application workloads they may want to move from on-premises IT environments to the cloud or vice versa. Overall, the rate of change in IT environments is accelerating as organizations add, for example, Kubernetes clusters to run microservices-based applications that are being rolled out by a line-of-business unit. IT organizations now need AIOps capabilities to keep track of the changes and updates being made across an extended enterprise. “IT has become too complex to run without it,” he said. Longer-term, it’s not clear to what degree AIOps will remain a discrete concept or simply become part of the IT service management (ITSM) firmament. At some point, every ITSM tool is going to be infused with machine learning algorithms to some degree. Of course, many IT professionals are dubious about the capabilities of AIOps platforms. But the longer machine learning algorithms are employed, the more accurate they become since they continuously learn from their environment. As the overall size of the IT environment continues to expand, increasing the size of the IT staff needed to manage it become financially impractical. It’s now only a question of to what degree AI will be employed to augment IT teams that have no other viable way of managing modern IT environments."
https://venturebeat.com/2021/04/16/paxafe-which-offers-visibility-into-b2b-supply-chains-raises-2-25m/,"Paxafe, which offers visibility into B2B supply chains, raises $2.25M","Paxafe, a platform using AI and machine learning to classify and contextualize supply chain data, today announced it has raised $2.25 million. The startup says it will leverage the funds to support the rollout of its platform and further develop its AI technology. Historically, a lack of visibility throughout the supply chain has resulted in product losses and operational inefficiencies. The problem is quite the opposite today, with an abundance of data from sensors and aggregator platforms providing shippers, carriers, and insurers with key information. But this data doesn’t always drive smarter decision-making, mainly because it tends to lack context and structure. Paxafe aims to address this with a platform designed for predictive routing, time to arrival estimation, and adverse event prediction for business-to-business shipments. The company delivers the status of goods transported via ocean cargo, air freight, or parcel, relying on a combination of sensors and modeling to provide pricing and insurance coverage for shipments. The software automatically converts sensor data into contextual insights, sending alerts if a shipment experiences potentially harmful deviations. “While there is no shortage of asset tracking and visibility platforms available, cargo losses keep rising year over year, and cargo loss ratios are not improving,” a spokesperson told VentureBeat via email. “Current solutions in the market are the equivalent of fire detectors — once a problem occurs, they wake up and notify stakeholders that something is wrong.  What they cannot do is actually diagnose the ‘how’ and ‘why’ behind that particular event of interest. Without a precise and automated diagnosis, business-to-business shippers find it virtually impossible to build accurate and consistent prediction models that enable supply chain risk mitigation across future shipments.” Paxafe claims its “adaptive insurance” calculations factor in critical supply chain factors, as well as confounding variables like weather and traffic. The platform also considers things like the product being shipped, the number of stops along the route, the time of the shipment, and the mode of transportation. Supply chain challenges have been amplified during the pandemic. For retail, it’s estimated that inventory is accurate just 63% of the time, on average. But stakeholders with superior visibility into their supply lines consistently outperform the competition. Seventy-nine percent of companies with high-performing supply chains achieve revenue growth greater than the mean within their industries, according to Logistics Bureau. Despite this, Paxafe says that over the past six months it has embarked on a series of pilots with enterprises across health care, perishables, oil and gas, logistics, manufacturing, jewelry, and insurance verticals. The company also launched a commercial version of its platform, converting a number of its pilot customers — about 10 so far — to commercial partners. “Paxafe was prelaunch when the pandemic hit, so it didn’t have a material impact on customer operations — more so on planning, fundraising, and go-to-market strategy,” cofounder and CEO Ilya Preston told VentureBeat in an email interview. “The pandemic has only reinforced the demand for technology visibility platforms like ours, given the pandemic’s impact on various supply chains — from product integrity and temperature control requirements for the vaccine itself to all of the lead time increases and delays that [are] impacting customer inventories, product availability, [and] time of arrival.” Ubiquity Ventures led Paxafe’s seed round announced today, bringing the Milwaukee, Wisconsin-based company’s total raised to date to over $3 million."
https://venturebeat.com/2021/04/16/facebook-claims-ai-can-predict-drug-combinations-to-treat-complex-diseases/,Facebook claims AI can predict drug combinations to treat complex diseases,"Facebook today detailed what it claims is the first single AI model capable of predicting the effects of drug combinations, dosages, timing, and other types of interventions like gene deletion. Developed in collaboration with Helmholtz Zentrum München, Facebook says the model could accelerate the process of identifying combinations of medications and other treatments that might lead to better outcomes for diseases. Discovering ways to repurpose existing drugs has proven to be a powerful tool to treat diseases including cancer. In recent years, doctors have seen success with “drug cocktails” to combat malignant conditions and continue to explore personalized treatments for patients. But finding an effective combination of existing drugs at the right dose is extremely challenging, in part because there are nearly infinite possibilities. Researchers would have to try from 5,000 to 19 billion solutions to find the optimal regimen given a pool of 100 drugs. Facebook’s open source model — Compositional Perturbation Autoencoder (CPA) — ostensibly addresses this with a self-supervision technique that observes cells treated with drug combinations and predicts the effect of new combinations. Unlike supervised models that learn from labeled datasets, Facebook’s generates labels from data by exposing the relationships between the data’s parts, a step believed to be critical to achieving human-level intelligence. CPA’s predictions take hours as opposed to the years that might elapse with conventional methods, allowing researchers to select the most promising results for validation and follow-up, according to Facebook. In biology, RNA sequencing is used as a way to measure the gene expressions of cells at the molecular level and study the effects of perturbations including drug combinations. Academia and industry have released RNA sequencing datasets containing up to millions of cells and 20,000 readouts per cell to facilitate biomedical research. Facebook leveraged these datasets to train CPA using an approach called auto-encoding, in which data is compressed and decompressed until summarized into patterns useful for prediction. CPA first separates and learns the key attributes about a cell, such as the effects of a certain drug, combination, dosage, time, gene deletion, or cell type. It then independently recombines the attributes to project their effects on the cell’s gene expressions. For example, if one of the datasets had information on how drugs affect different types of cells A, B, C, and A+B, CPA would learn the impact of each drug in a cell-type specific fashion and then recombine each in order to extrapolate interactions between A+C, B+C, and A+B. To test CPA, Facebook says it applied the model to five publicly available RNA sequence datasets with measurements and outcomes of drugs, doses, and other confounders on cancer cells. Benchmarked in terms of the R2 metric, which represents the accuracy of the gene expression predictions, Facebook claims that CPA “stayed consistent” between training and testing — an indication of robustness. Moreover, CPA’s predictions of the effects of drug combinations and doses on cancer cells matched those found in the testing dataset “reliably.” Facebook believes that CPA can “dramatically” accelerate the process of identifying optimal combinations of treatments, as well as pave the way for new opportunities in the development of medications. Toward this end, the company is making available APIs and a software package designed to let researchers plug in datasets and run through predictions. “Our hope is that pharmaceutical and academic researchers as well as biologists will utilize [CPA] to accelerate the process of identifying optimal combinations of drugs for various diseases,” Facebook program manager Anna Klimovskaia and research scientist David Lopez-Paz wrote in a blog post. “In the future, [CPA] could not only speed up drug repurposing research, but also — one day — make treatments much more personalized and tailored to individual cell responses, one of the most active challenges in the future of medicine to date.” While Facebook claims that CPA is novel in its architecture, it isn’t the first algorithm engineered to predict drug interactions. In July 2018, Stanford researchers detailed an AI system that can anticipate the effects of drug combinations by modeling the more than 19,000 proteins in the body that interact with each other and with medications. Researchers at the MIT-IBM Watson AI Lab, Harvard School of Public Health, Georgia Institute of Technology, and IQVIA more recently created an AI tool called CASTER that estimates potentially harmful and unsafe drug-to-drug interactions. A separate Harvard group has proposed applying AI to identify candidates for drug repurposing in Alzheimer’s disease. And researchers at Aalto University, University of Helsinki, and the University of Turku in Finland created a machine learning model that projects how combinations of drugs might kill various cancer cells."
https://venturebeat.com/2021/04/15/marketplacer-takes-another-step-in-growth-strategy/,Marketplacer Takes Another Step in Growth Strategy," Deepens integration with Salesforce Commerce Cloud  SAN JOSE, Calif.–(BUSINESS WIRE)–April 15, 2021– Marketplacer, a global technology Platform as a Service (PaaS) company that builds successful and scalable online marketplaces, announces the acceleration of its growth strategy through a new funding round including Salesforce Ventures and closer alignment with Salesforce Commerce Cloud. In addition to a capital investment, Marketplacer is certified for the Salesforce partner marketplace, adding capability to Salesforce Commerce Cloud’s enterprise customers to sell third-party products through their Salesforce Commerce Cloud instance. Marketplacer and Salesforce’s go-to-market strategies are tightly aligned, giving both companies the opportunity to innovate and work with customers more closely, and making it easier for Salesforce Commerce Cloud customers to grow through a marketplace strategy. Marketplacer Co-Founder and Executive Chairman, Jason Wyatt, is thrilled to count Salesforce Ventures amongst its partners and investors in the Marketplacer journey; “It is fantastic to be able to include Salesforce, an iconic name in the world of digital innovation, in the evolving Marketplacer story. We have a shared vision to connect business and its customers in new ways to enable them to grow faster, and with this partnership we are both able to deliver on these objectives even further.” With the newly launched Marketplacer cartridge for Salesforce Commerce Cloud, enterprises around the world now have the ability to accelerate their growth with ease. From implementing strategies such as driving sales from dropship sellers, adding new categories or third-party range extension – Marketplacer makes it possible. Marketplacer is used in both B2C and B2B environments and has helped over 90 enterprises build and deploy their own successful marketplace strategies, connecting over 20,000 businesses worldwide. “Building relationships and knowing your customer is key to the success of any commerce business,” said Lidiane Jones, EVP & GM, Salesforce Commerce Cloud. “For many customers, this requires a suite of integrations made possible by our partners. With Marketplacer’s cartridge for Salesforce Commerce Cloud, companies will be able to accelerate growth with a leading marketplace solution that takes businesses beyond physical stores and inventory holdings.” Rob Keith, Head of Australia, Salesforce Ventures adds, “Marketplacer is tapping into one of the most consequential trends for the industry. Marketplacer and Salesforce Commerce Cloud are well positioned to support our joint customers as consumers increase their digital spend and sellers seek new direct routes to reach their customers.” The investment follows a $20 million capital raise in Q4 of 2020 and the recent announcement of establishing U.S. operations and appointment of Jim Stirewalt as North American President. Across its portfolio, Marketplacer clients include some of the largest and most recognised brands today, such as Myer, BikeExchange, Metcash, FishBrain, Nokia, Surfstitch, Petstock, Providoor and Bob Jane T-Marts to name a few. For more information or to enquire about Marketplacer’s services, visit www.marketplacer.com. To view employment opportunities with Marketplacer Australia or US, please visit Marketplacer’s LinkedIn page for further details. About Marketplacer: Established in 2016 in Melbourne, Australia, Marketplacer is a global technology Platform as a Service (PaaS) company equipped with all the tools and functionality needed to build a successful and scalable online marketplace, at speed. To date, Marketplacer has helped over 90 businesses execute their own successful marketplace strategies and connected over 20,000 businesses worldwide. The Marketplacer platform exists to make growth simple, from implementing strategies such as driving sales from dropship sellers, adding new categories or third-party range extension – Marketplacer makes it possible. Marketplacer is responsible for the business transformations of some of Australia’s largest retail, brand distributor and franchise engines as well as communities, including Myer, SurfStitch, Metcash, Bob Jane T-Marts, FishBrain, Providoor and Petstock to name a few. About Salesforce Ventures: Salesforce is the global leader in Customer Relationship Management (CRM), bringing companies closer to their customers in the digital age. Salesforce Ventures, the global investment arm of Salesforce, invests in the next generation of enterprise technology that extends the power of the Salesforce Platform. Salesforce Ventures is building the world’s largest ecosystem of enterprise cloud companies and extending that technology to customers. Portfolio companies receive funding, strategic advisory, and operating support, and can easily join Pledge 1% to make giving back part of their business model. Salesforce Ventures has invested in more than 400 companies, including DocuSign, GoCardless, Guild Education, nCino, Twilio, Zoom, and others across 22 countries since 2009. For more information, please visit www.salesforce.com/ventures.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210415005967/en/ Nicole JordanRadix Collective for Marketplacer U.S.nicole@radixcollective.com"
https://venturebeat.com/2021/04/15/kryon-throws-down-the-gauntlet-for-better-rpa-governance/,Kryon throws down the gauntlet for better RPA governance,"Robotic process automation (RPA), which mimics human activity and automates mundane tasks, is all the rage. But privacy and governance concerns persist. Recognizing these challenges, Kryon recently became the first RPA vendor to earn ISO 27701 certification. “This framework is essential for any RPA company doing business in Europe, due to GDPR, or any other region with similar data privacy regulations,” Kryon CTO Shay Antebi told VentureBeat. He believes ISO 27701 could become the first widely adopted data privacy standard for RPA vendors. The ISO certification applies to real-time process discovery, as well as bot design, deployment, and management. RPA applications, called bots, are often programmed to access sensitive systems and information as part of process automation projects. An attacker can exploit access to these bots to steal data or gain unauthorized access to systems and applications in a cyberattack. RPA and process mining vendors have addressed several standards and best practices to ensure privacy. While ISO 27001 is an older certification for information security management systems (ISMS), ISO 27701 is an extension standard that builds upon and enhances that with a framework for privacy information management systems (PIMS) to secure and manage personally identifiable information. Kryon had already achieved ISMS certification back in 2019, so catching up with the new extension was a matter of building on this earlier work. Organizations looking to get certified to ISO 27701 will either need to have an existing ISO 27001 certification or implement ISO 27001 and ISO 27701 together as a single implementation audit. Enterprises need to maintain vigilance around industry-specific regulation, particularly in health care and finance, two of the largest markets for RPA. Enterprises using ISO-certified tools like Kryon’s will still need to ensure that their existing systems and applications that interact with RPA tools are compliant. RPA platforms often integrate with other applications on the back end to complete a process. For example, Kryon created a software bot for a health care organization in Israel that automates setting up appointments for patients to receive the two-shot COVID vaccine. That front-end bot, which chats with the patient, also interacts with the organization’s patient record system behind the scenes to complete the process. These applications need to be secured, as well. “This is a great example of when an upfront investment is absolutely necessary to protect yourself from potentially huge losses,” Antebi said. Meeting security certifications requires not only an investment of time and resources but also the right technology, processes, and framework. Security sometimes comes as an afterthought in the software development lifecycle. But it needs to be considered first for RPA to scale. “If the goal is widespread adoption of RPA in the enterprise, then the industry needs to deliver solutions with enterprise-grade security,” Antebi said. Kryon has been investing in solutions to push the envelope of privacy and governance further, such as a way to mask sensitive information in documents and on systems screens without losing the necessary context. Antebi said, “We are always looking for more ways to add value for our customers — offering the best security available is one way to do that.”"
https://venturebeat.com/2021/04/15/google-cloud-partners-with-osisoft-to-simplify-industry-cloud-migration/,Google Cloud partners with OSIsoft to simplify industry cloud migration,"Google Cloud announced a partnership with industrial data platform OSIsoft to make it easier for enterprises to migrate industrial workloads to the cloud. “Ultimately, we want to enable customers to harness the power of the cloud without having to depart from their existing systems,” Google Cloud managing director Dominik Wee told VentureBeat. “Our goal is for manufacturers to be able to scale across thousands of pieces of equipment and dozens of sites with ease — and manage this via a single control pane.” OSIsoft’s PI Integrator for Business Analytics makes it easier to pipe data streams into Google services such as Cloud Storage, BigQuery, and Pub/Sub. As part of the partnership, the companies released GCP deployment scripts for PI Core, the on-premises component moving PI runtime data to the cloud. Data from multiple sites is then consolidated and connected to Google Cloud’s Smart Data Platform for analysis. There are multiple ways to architect and deploy a PI system. OSIsoft created best practice topologies that were designed and tested by its own deployment team to optimize PI system performance. The new deployment scripts are based on OSIsoft’s latest topologies. “Customers’ deployments of PI Core in Google Cloud are significantly simplified with these scripts,” Wee said. OSIsoft provides core infrastructure for two-thirds of industrial Fortune 500 companies and supports over 2 billion data streams in industries as varied as energy, mining, oil and gas, utilities, pharmaceutical, facilities, and manufacturing. The company, which recently merged with United Kingdom-based industrial software vendor Aveva, builds out a special purpose control and management service for operational technology (OT), such as the pumps and motors in factories and plants. Wee said industrial enterprises are starting to move beyond the era of AI “pilot purgatory” to deliver real business value. In manufacturing, getting access to machine-level data is critical to enable AI-based decision-making. Google Cloud has been collaborating with manufacturing companies, independent software vendors, and other vendors — including Ingersoll Rand and Johnson Control — to optimize time-to-market for customers. The partnership with OSIsoft spans industries and enables the data ingestion pipeline to Google Cloud, where enterprises can apply Google’s AI solutions to deliver machine uptime and improve quality and overall equipment effectiveness. Wee said more partnership announcements are on the horizon. “Manufacturing is complex, and it requires deep domain expertise, particularly on the OT layer,” Wee said. “Our aim is to provide simplicity and choice.”"
https://venturebeat.com/2021/04/15/autonomous-trucking-company-plus-will-use-ai-and-billions-of-miles-of-data-to-train-self-driving-semis/,Autonomous trucking company Plus will use AI and billions of miles of data to train self-driving semis,"This article is part of a VB Lab Insight series paid for by Plus. The safest drivers are those with the most experience. Studies show it can take years of practice for automobile drivers to become careful and competent road users. Similarly, the more experience a truck driver has the less likely it is that they will cause a serious crash. What holds true for human drivers holds true for autonomous driving systems — up to a point. The safest self-driving vehicle platforms are those that have accumulated the most experience. Since driving experience is so important, how can technologists make sure computerized driving systems get the training they need to operate safely on the nation’s roads and highways? Solving this challenge is the key to unlocking a fully autonomous future. Thanks to advances in sensor technology and artificial intelligence (AI), an automated truck is capable of analyzing many objects on the road and making a decision about how to respond.  This is accomplished in large part by training so-called “deep learning” algorithms. Repeatedly expose a self-driving system to all kinds of obstacles, from a cut-in vehicle to a construction site, and the system will start to understand how to react when an obstruction appears on the highway. Here it is important to note that unlike people, machines lack common sense and don’t do well handling novel situations. Human drivers know to slow down in the face of an unexpected obstacle — a bear, say — because we can make decisions based on similar situations we have already encountered or extrapolate from other incidents. Unlike humans, however, deep neural networks can only learn from data they have been trained on, whether from public roads, closed courses, or computer simulations. So back to the original question: How do you train the machines so they are exposed to the full range of the driving experience? Plus’s goal is to help truck drivers on long-haul routes, where they encounter a variety of road and weather conditions. In addition to closed-road testing and computer simulations, the company’s PlusDrive system is learning on the open road, where the trucks can be exposed to real-world obstacles and situations. Junk flying from a pickup bed. Ice slicks. A wind turbine blade. A zigzagging motorcycle.  Though these so-called “long tail” phenomena comprise less than 1% of the time behind the wheel, knowing how to safely navigate them is critical for machines. Society expects that a computer-operated machine must be at least an order of magnitude safer than a human driver. Starting this summer, Plus will put its supervised automated driving system into factory production. It is also retrofitting existing trucks with the system. By this time next year, hundreds of automated trucks powered by PlusDrive will be on the road, hauling commercial cargo. Human drivers will be behind the wheel. Like an experienced professional training a new recruit, Plus drivers will monitor the autonomous trucks while teaching them how to handle unexpected obstacles. Plus estimates that its fleet will accumulate billions of collective miles before the company deploys fully driverless vehicles. Taking an evolutionary approach to full autonomy enables the company to rack up miles more quickly, with the assistance of on-board professional drivers who are training and validating the system. To support its global deployment in the U.S., China, Europe, and other markets, Plus recently raised $420 million in new funding. The drivers benefit too. The Plus supervised autonomous trucking solution elevates the role of the truck driver, upskilling them in preparation for an autonomous future. At the same time a digital co-pilot will ease driver exhaustion on long-haul routes, and fleets will spend less on the hiring process. The system yields other gains. Fuel comprises about a third of a trucking company’s operating budget, by far the largest cost for heavy trucks. When an automated system understands the road, pulling in GPS and weather data too, they optimize shifting and braking. Plus has run pilot projects showing that  PlusDrive saves 10% of the tank compared to the most efficient drivers, a win for the bottom line and the environment. Commercial space travel, solar-powered cities, autonomous vehicles — the first two visions of the future depend on specific economic inflection points, while the third is wholly dependent on the amount of data a system has accumulated. Plus is building the necessary feedback loop of information today. Its trucks are accumulating the data. Its drivers, who are among the safest and most efficient Class A drivers, are training the system with their responses. Its engineers are fine-tuning PlusDrive’s algorithms and decisions. And eventually PlusDrive will be one of the safest and most experienced drivers on the road. Plus is applying autonomous trucking technology to trucks today. For more information, please visit www.plus.ai. VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/15/misfits-gaming-esports-group-launches-women-of-misfits-speaker-series/,Misfits Gaming esports group launches Women of Misfits speaker series,"Misfits Gaming Group is leaning into female gamers with the launch of its Women of Misfits speaker series, and the esports company will turn this into a wider platform over time. The Boca Raton, Florida-based company will use this to elevate issues for women in gaming and esports, and it’s happening at a time when problems such as sexual harassment and under-representation of women at game studios and at esports organizations have been in the headlines. Women are a prevalent part of the esports and gaming landscape. Nearly 40% of all gamers are women, with 80% of them being 18 or older. The Women of Misfits initiative will provide a space for them to discuss ideas and be inspired by influential women both inside and outside the organization in addition to supporting the growth and development of women within MGG. We’ll have a Women in Gaming Breakfast at our GamesBeat Summit 2021 on April 28 and April 29. It a series of monthly guest speakers. The first are Chris Evert, 18-time Grand Slam singles champion and tennis legend; GloZell Green, a comedian and YouTuber; Bianca Smith, the first Black woman to serve as a professional baseball coach; Angela Ruggiero, the CEO Sports Innovation Lab and four-time Olympian and Gold Medalist for the U.S. Hockey team; and Maya Enista Smith, the executive director of the Born This Way Foundation. The focus of the Women of Misfits platform will be mentorship, development, network, and advocacy. The platform will be led by female executives within MGG such as chief development officer Hillary Matchett; president of media and branding Ella Pravetz; chief revenue officer Lagen Nash; president of Misfits Agency Amy Palmer; vice president of Communications Becca Henry; chief wellness adviser Carolyn Rubenstein; and cofounder Laurie Silvers.  The Women of Misfits platform includes a monthly speaker series with industry leaders and visionaries that will air on MGG’s YouTube channel. The sessions will be moderated by MGG executives and guest speakers will share topics that matter to them and inspire both the gaming community and women to pursue their dreams. “I am truly inspired and amazed with our women at MGG and their many accomplishments and eager to watch this platform ascend,” said Misfits CEO Ben Spoont in a statement. “The determination and dedication to push one another to break the boundaries as women within the esports industry is remarkable, and I am confident this platform will resonate not only within MGG but also within our wider community.”"
https://venturebeat.com/2021/04/15/user-experience-testing-and-monitoring-startup-userzoom-raises-100m/,User experience testing and monitoring startup UserZoom raises $100M,"UserZoom, a company developing software to measure digital experiences including apps and websites, today announced that it raised $100 million in a venture round led by Owl Rock. The proceeds, which the company says it plans to put toward expansion and hiring, come as UserZoom acquires EnjoyHQ, an experience insights startup that centralizes customer feedback and research findings into a single dashboard. Surveys show that design can be as important for the success of a digital experience as content and functionality. It’s estimated that every $1 invested in user experience nets a $100 return, a 9,900% ROI. And according to Forrester Research, frictionless user experience design could potentially raise customer conversion rates up to 400%. UserZoom lets customers establish criteria for user experience performance, track it over time, and then align it with business results. The metrics the platform records can be used to inform product strategy and information architecture, as well as to continuously test and validate throughout the design and development processes. With UserZoom, companies can evaluate and benchmark products with target users by conducting interviews remotely or funneling them through tests of structure, navigation, and ease of use. These customers can also collect and monitor surveys and invite website visitors or app users to a study, or create shareable reports based on notes, clips, and other aggregated test data. This kind of user experience testing and monitoring can be critical to preventing user frustration. PricewaterhouseCoopers found that 32% of customers leave a brand they loved after just one bad experience. Moreover, 13% of customers tell 15 or more people about their bad experiences, while 72% tell 6 or more people about good experiences. “COVID-19 has led to a significant acceleration in consumers’ rate of digital adoption and their expectations for a seamless, convenient and personalized experience,” cofounder and co-CEO Alfonso de la Nuez said in a press release. “This investment … is proof that businesses are waking up to the fact that traditional ways of designing and measuring digital experiences are no longer good enough. Today’s users and customers demand more, and leaders need the data and insights that UserZoom provides to allow them to quickly make confident, customer- and data-driven decisions to stay in front of the competition.” The user experience monitoring market is large and growing, with a value that’s expected to climb from $1.5 billion in 2019 to $3.7 billion by 2023, Markets and Markets anticipates. UserZoom competes with companies including Usabilla and Qualaroo, as well as UserVoice. But UserZoom claims to count tech giants like Google, Oracle, and half of the Fortune 100 among its clientele. And SVP of product Andrew Jensen believes that the integration of EnjoyHQ’s product will set UserZoom apart from the rest. “Our acquisition of EnjoyHQ is yet another example of our goal to simplify and improve the XIM process,” Jensen said, adding that he expects an integrated UserZoom and EnjoyHQ product to be available in the second half of 2021. “Our vision is to allow our customers to connect project insights from UserZoom with a broader customer experience and user experience integrated experience insights hub, where thousands of users across the organization can conduct analysis, share feedback, and discuss proposals needed to create seamless experiences that drive business outcomes.” This latest funding round brings UserZoom’s total raised to over $130 million. The San Jose, California-based company previously nabbed $34 million in October 2015 in a series A round."
https://venturebeat.com/2021/04/15/digital-transformation-stalwart-bizagi-hires-first-cio-to-boost-enterprise-automation/,Digital transformation stalwart Bizagi hires first CIO to boost enterprise automation,"Bizagi, one of the oldest players in the digital transformation market, recently hired Antonio Vázquez as its first chief information officer (CIO). The company hopes to improve its IT processes to take advantage of the rapidly growing RPA, process mining, and enterprise automation markets. In business since 1989, Bizagi provides business process management integration for all of the major RPA platforms. Bizagi’s tools make it easier to model and simulate processes, define business rules, and automate business processes. “I’m going to be that person who looks at all of the capabilities that are occurring in terms of business and all of the capabilities in terms of IT and defines the gap between them,” Vázquez told VentureBeat. He has also assembled a team of experts who will design business processes to address those gaps. The pandemic has demonstrated the importance of business continuity and resiliency in a rapidly shifting economy. But Vázquez is concerned that the shift to remote work and rise of the hybrid workforce have dispersed teams. He believes that moving to the cloud and eliminating as many legacy systems as possible is the best way to improve resiliency. Since Bizagi sells business architecture, Vázquez believes it’s critical to handle it in the same way internally as externally. He plans to consolidate Bizagi’s core capabilities in client and partner-facing technology into a single system and restructure the approach in terms of customer experience management. He will also look at governance and address cloud and security implications for internal processes. “Because we are focusing on both our own movement to the cloud and our customers’ transition, it’s critical to have a robust governance plan in place to ensure a seamless migration in all aspects,” Vázquez said. Finally, the company is also aligning its current internal infrastructure to make it easier to evaluate IT performance metrics against business goals. “The business side can’t expect IT to speak the business language and vice versa, but it’s necessary in order to compete on a grand scale,” Vázquez said."
https://venturebeat.com/2021/04/15/google-researchers-boost-speech-recognition-accuracy-with-more-datasets/,Google researchers boost speech recognition accuracy with more datasets,"What if the key to improving speech recognition accuracy is simply mixing all available speech datasets together to train one large AI model? That’s the hypothesis behind a recent study published by a team of researchers affiliated with Google Research and Google Brain. They claim an AI model named SpeechStew that was trained on a range of speech corpora achieves state-of-the-art or near-state-of-the-art results on a variety of speech recognition benchmarks. Training models on more data tends to be difficult, as collecting and annotating new data is expensive — particularly in the speech domain. Moreover, training large models is expensive and impractical for many members of the AI community. In pursuit of a solution, the Google researchers combined all available labeled and unlabelled speech recognition data curated by the community over the years. They drew on AMI, a dataset containing about 100 hours of meeting recordings, as well as corpora that include Switchboard (approximately 2,000 hours of telephone calls), Broadcast News (50 hours of television news), Librispeech (960 hours of audiobooks), and Mozilla’s crowdsourced Common Voice. Their combined dataset had over 5,000 hours of speech — none of which was adjusted from its original form. With the assembled dataset, the researchers used Google Cloud TPUs to train SpeechStew, yielding a model with more than 100 million parameters. In machine learning, parameters are the properties of the data that the model learned during the training process. The researchers also trained a 1-billion-parameter model, but it suffered from degraded performance. Once the team had a general-purpose SpeechStew model, they tested it on a number of benchmarks and found that it not only outperformed previously developed models but demonstrated an ability to adapt to challenging new tasks. Leveraging Chime-6, a 40-hour dataset of distant conversations in homes recorded by microphones, the researchers fine-tuned SpeechStew to achieve accuracy in line with a much more sophisticated model. Transfer learning entails transferring knowledge from one domain to a different domain with less data, and it has shown promise in many subfields of AI. By taking a model like SpeechStew that’s designed to understand generic speech and refining it at the margins, it’s possible for AI to, for example, understand speech in different accents and environments. When VentureBeat asked via email how speech models like SpeechStew might be used in production — like in consumer devices or cloud APIs — the researchers declined to speculate. But they envision the models serving as general-purpose representations that are transferrable to any number of downstream speech recognition tasks. “This simple technique of fine-tuning a general-purpose model to new downstream speech recognition tasks is simple, practical, yet shockingly effective,” the researchers said. “It is important to realize that the distribution of other sources of data does not perfectly match the dataset of interest. But as long as there is some common representation needed to solve both tasks, we can hope to achieve improved results by combining both datasets.”"
https://venturebeat.com/2021/04/15/rockset-integrates-real-time-analytics-platform-with-relational-databases/,Rockset integrates real-time analytics platform with relational databases,"Rockset today announced it has integrated its analytics database with both MySQL and PostgreSQL relational databases to enable organizations to run queries against structured data in real time. Rather than having to shift data into a cloud data warehouse to run analytics, organizations can now offload analytics processing to a Rockset database running on the same platform, Rockset CEO Venkat Venkatramani told VentureBeat. The Rockset platform is based on Facebook-developed RocksDB, an open source log structured database engine based on a key/value store that has been extended to support SQL queries. The approach enables organizations to offload queries to an indexing engine that can process sub-second queries while transactions continue to be processed using a relational database, Venkatramani added. The issue many organizations face today is that they already have extensive investments in open source relational databases. Neither MySQL nor PostreSQL are designed to process analytics at scale, which is one reason so many organizations have either adopted a NoSQL database or a data lake in the cloud. Replacing those databases with a proprietary relational database that can also process analytics in real time would be cost-prohibitive for many. Rockset is making a case for an alternative approach based on a Converged Index that can be employed to analyze structured relational data, as well as semi-structured, geographical, and time-series data in real time. Complex analytical queries can be scaled to include JOINS with other databases, data lakes, or event streams. All fields are entered into a converged index that includes an inverted index, a columnar index, and a row index. In addition to integrations with open source relational databases, the company also provides connectors to MongoDB, DynamoDB, Kafka, Kinesis, Amazon Web Services (AWS), and Google Cloud Platform, among others. As organizations collect data in real time, they increasingly also need to analyze it in real time, Venkatramani said. “Batch-based workloads are becoming real-time workloads,” he added. Moving data into a data lake using a batch-oriented process only provides a means to process a larger amount of historical data, Venkatramani said. IT organizations may still have a need for a data lake, but real-time analytics are going to be at the heart of most digital business processes, Venkatramani noted. Rockset earlier this year published the results of a Star Schema Benchmark test showing millisecond-latency query performance against the Star Schema Benchmark (SSB). The company claims it’s the only vendor to publish benchmarks showing it can execute queries up to 9.4 times faster than rivals while simultaneously ingesting 1 billion events a day with one second of data latency. The company last fall raised an additional $40 million to grow its workforce and accelerate product development and research while bolstering its go-to-market efforts. It’s not clear to what degree batch-oriented processes that have dominated IT architecture for decades will give way to real-time platforms. Historically, the data organizations have applied analytics to is usually several hours to a day old because the underlying database has typically been updated overnight using a batch-oriented process. Today organizations want to be able to continuously apply analytics to, for example, clickstream data from social media feeds — in real time, as it’s being processed. Other use cases include supply chain logistics and delivery tracking systems, gaming leaderboards, fraud detection systems, health and fitness trackers, and ecommerce applications. Of course, the days when organizations standardized on one database platform are long over. The challenge now is weaving a polyglot set of databases together in a way that allows an organization to take advantage of the capabilities of multiple platforms optimized for varying classes of workloads."
https://venturebeat.com/2021/04/15/only-13-of-organizations-are-delivering-on-their-data-strategy-survey-finds/,"Only 13% of organizations are delivering on their data strategy, survey finds","A new survey of C-suite data, IT, and senior tech executives finds that just 13% of organizations are delivering on their data strategy. The report, which was based on a survey of 351 respondents at organizations earning $1 billion or more in annual revenue, found that machine learning’s business impact is limited largely by challenges in managing its end-to-end lifecycle. MIT Technology Review Insights and Databricks conducted the survey, which canvassed companies including Total, the Estée Lauder companies, McDonald’s, L’Oréal, CVS Health, and Northwestern Mutual. Among the findings was that only a select group of “high achievers” — the aforementioned 13% — delivered measurable business results across the enterprise. This group succeeded by paying attention to the foundations of sound data management and architecture, which enabled them to “democratize” data and derive value from AI and machine learning technologies, according to the report’s authors. “Managing data is highly complex and can be a real challenge for organizations. But creating the right architecture is the first step in a huge business transformation,” report editor Francesca Fanshawe said in a press release. Every chief data officer interviewed for the study ascribed importance to democratizing analytics and machine learning capabilities. This, they said, will help end users make more informed business decisions — the hallmarks of a strong data culture. The respondents also advocated embracing open source standards and data formats. But what remains the most significant challenge is the lack of a central place to store and discover machine learning models, 55% of executives said. That’s perhaps why 50% are currently evaluating or actively implementing new, potentially cloud-based data platforms. As Broadridge VP of innovation and growth Neha Singh noted in a recent piece, many firms try to develop AI solutions without having clean, centralized data pools or a strategy for actively managing them. Without this critical building block for training AI solutions, the reliability, validity, and business value of any AI solution is likely to be limited. Organizations’ top data priorities over the next two years fall into three areas, all supported by wider adoption of cloud platforms, according to the report. These are: improving data management; enhancing data analytics and machine learning; and expanding the use of all types of enterprise data, including streaming and unstructured data. “There are many models an enterprise can adopt, but ultimately the aim should be to create a data architecture that’s simple, flexible, and well-governed,” Fanshawe continued. The MIT and Databricks findings come after Alation’s latest quarterly State of Data Culture Report, which similarly discovered that only a small percentage of professionals believe AI is being used effectively across their organizations. A lack of executive buy-in was a top reason, Alation reported, with 55% of respondents to the company’s survey citing this as more important than a lack of employees with data science skills. The findings agree with other surveys showing that, despite enthusiasm around AI, enterprises struggle to deploy AI-powered services in production. Business use of AI grew a whopping 270% over the past several years, according to Gartner, while Deloitte says 62% of respondents to its corporate October 2018 report adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of companies that have seen half their AI projects fail will tell you."
https://venturebeat.com/2021/04/15/bigeye-raises-17m-to-algorithmically-monitor-data-quality/,Bigeye raises $17M to algorithmically monitor data quality,"Bigeye, a data quality engineering platform, today announced it has raised $17 million in a series A round led by Sequoia Capital. The company says the funds will be used to improve its platform and help make it available to more data teams. Data is increasingly critical to enterprises and is woven into the products and services that directly affect customers. To keep pace, data engineering has increased in scale, complexity, and automation, leading to a number of significant workflow challenges. A clear majority of employees (87%) peg data quality issues as the reason their organizations failed to successfully implement AI and machine learning, according to a recent Alation report. San Francisco, California-based Bigeye, previously called Toro Data Labs, employs machine learning to enable companies to instrument data lakes and warehouses with thousands of data quality metrics. Founded in 2020, the platform automatically instruments datasets and pipelines with metrics, creating alerts driven by anomaly detection techniques. Bigeye uses connectors and read-only accounts to connect to data sources and record health metrics. Available in fully managed software-as-a-service form or as an on-premises app for enterprises, Bigeye samples objects like tables and generates recommended metrics based on data profiling and semantic analysis. By default, all metrics have automatic thresholds enabled — within 5 to 10 days, Bigeye learns the behavior of the metrics and begins to make adjustments. When those thresholds are reached, the platform sends alerts via email, Slack, and other channels and optionally triggers remediation steps. For one company, Bigeye identified that its customer data had a number of rows in which the values had been written into the wrong columns. The percentage of rows affected was small enough that analysts might not have spotted it, but at the scale that the company was working, it could have led to hundreds of customer support tickets that would have needed to be resolved. Bigeye can draw from Snowflake, Redshift, BigQuery, and other popular sources, and its no-code interface allows teams to create, edit, and read configuration and metric histories. The company says that as a part of its efforts to improve the platform, it recently increased support for service-level agreements, which can help engineers build trust through transparency with users. As processes around data remain a hurdle in adopting AI — 34% of respondents to a 2021 Rackspace survey stated poor data quality as the reason for AI R&D failure — observability solutions like Bigeye are attracting investments. There’s Aporia, Monte Carlo, and WhyLabs, a startup developing a solution for model monitoring and troubleshooting. Another competitor is Domino Data Lab, a company that claims to prevent AI models from mistakenly exhibiting bias or degrading. “Right now, modern data teams are held up by the heroics of data engineers, analysts, and data scientists trying to triage data quality incidents after something has already gone wrong. We’ve been the people who have to stay up until 3 a.m. on a Saturday trying to backfill a pipeline — and it doesn’t feel heroic,” cofounder and CEO Kyle Kirwan told VentureBeat via email. “For companies to realize the value of their data, it needs to be effortless for data teams to measure, improve, and communicate data quality for their organizations.” But Bigeye has already successfully courted large customers, including Instacart, Crux Informatics, and Lambda School. In addition to Sequoia, Costanoa Ventures also participated in Bigeye’s latest funding round. The three-year-old company has 11 employees, and the funds bring its total raised to $21 million."
https://venturebeat.com/2021/04/15/apple-facebook-and-two-different-visions-of-the-internet/,"Apple, Facebook, and 2 different visions of the internet","Presented by AdColony When 98.5% of your business is based on advertising and a genuine threat comes from another business that isn’t a direct competitor of yours, you’d probably consider that a crisis. In response to this crisis, Facebook took out a full-page ad in the New York Times. It’s not just Facebook that made noise and tried to out-engineer Apple’s guidelines. A coalition of major Chinese developers attempted to fingerprint devices via the CAID (Chinese Advertising Identifier), but Apple responded with a fast and detailed slap down of the attempt. Facebook’s full-page ad and larger PR campaign was more subtle than the CAID, but it’s a move that has been seen as desperate, particularly because of the primary argument that they were standing up to Apple “for small businesses everywhere,” when, in fact, the biggest impact will be their own bottom line. For them, nearly billions of dollars is at stake. And it’s all because of one company and their decision to give consumers a choice. Apple claims that users should know when their data is being collected and shared across other apps and websites, and they should have the choice to allow that or not. “App Tracking Transparency in iOS 14 does not require Facebook to change its approach to tracking users and creating targeted advertising, it simply requires that they give users a choice,” Apple said in a statement. What Apple is doing here is calling out the fact that, for lack of federal government regulation that protects consumers from what they believe to be a violation of privacy, Apple will go ahead and do it for them using the sheer power and ubiquity of its own platform. Sure, by touting “Privacy. That’s iPhone,” in a massive ad campaign and hoisting it up on a pedestal like they would a new piece of hardware, it can feel like privacy is their product. Apple has been using privacy as a differentiating factor in its market positioning for the past decade. Now, as part of that, they are hawking consumer choice. But ultimately what this comes down to is that Apple has a different vision of the future of the internet. For Apple, their vision is of a clean, curated web where content — at least the content that they are responsible for distributing — is from trusted sources, high-quality and is primarily paid for up-front or through subscriptions, not through advertising. This isn’t new news. Anyone who follows Apple could see this coming. In 2015, Apple Music became a subscription, then we saw streaming video (Apple TV+) and gaming (Arcade) added to the mix. And now, of course, there’s the bundle option of Apple One. So it’s not surprising that analysts believe this is the road they are heading down. The fast pace of technical innovation means consumers want to own the latest and greatest, and subscriptions offer flexibility to upgrade at a lower upfront cost. Additionally, Millennials and Gen Z tend to have a rent versus buy mentality, which applies not just to cars and homes but music and video streaming. It’s not just about philosophy, of course. Apple can say that they believe in privacy, a clean internet where you pay for premium content via subscriptions. But what it comes down to is they sell hardware, not software. Facebook and Google, on the other hand, are software companies. So of course they believe that the internet — and everything that lives on and around it, including content in mobile apps — should be free. For them, advertising is the “ultimate tax” you pay to access content. And, while you previously paid tax solely with your attention, it’s now paid with data. Thanks to documentaries like The Social Dilemma, as well as the massive increase in malware/spyware on the internet and cybersecurity hacks, consumers are becoming more aware of how deep that cost really is. In many ways, we are going back to the early days of the web where context was king and media was valued when it came from a trustworthy source, but in that world consumers need to pay up…with money. So — when Apple asks you if you want to be tracked across apps and websites, what they are really asking you is “How do you want to pay for your content?” We’re already seeing verticalization from ad networks and MMPs in an effort to combine information under the umbrella of “first party data” so as to not qualify their behavior as tracking under Apple’s App Tracking Transparency (ATT) framework. The other question is whether the CAID will see widespread enough adoption. Alasdair Pressney is Director of Product Strategy – Advertiser Products at AdColony. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/15/ibm-acquires-business-process-automation-startup-myinvenio/,IBM acquires business process automation startup MyInvenio,"IBM this morning announced it has acquired MyInvenio, a process mining software company based in Reggio Emilia, Italy. IBM says the buyout, which is subject to regulatory approval and expected to close by Q3 2021, will help enterprises tackle streamlining backend processes across accounting, sales, and production. Intelligent process automation, or technology that automates monotonous, repetitive chores traditionally performed by human workers, is big business. Forrester estimates that automation and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could eventually be automated in about 60% of occupations. And the pandemic is bolstering the uptake of automation, with nearly half of executives telling McKinsey in a report that their automation adoption has accelerated moderately. IBM’s purchase of MyInvenio builds on its existing OEM agreement with the company, announced in November. As a part of this deal, IBM plans to integrate MyInvenio’s capabilities with its automation portfolio, which spans IBM Cloud Pak for Business Automation, IBM’s hybrid cloud software, and over 600 prebuilt workflows on Red Hat OpenShift. IBM says MyInvenivo’s technologies might also eventually become available to IBM business partners to assist customers in generating insights into their workflows. MyInvenio, which was founded in 2013, offers a platform designed to spotlight inefficiencies, bottlenecks, and tasks in industries that can benefit from automation. MyInvenio can run simulations to assess automation opportunities and measure the benefits of applying automation at the outset. Leveraging historical process execution data and desktop behavior enables the platform to determine exactly where to apply RPA bots, automated decisions, and AI models. Companies can use this to examine the most common IT requests, for instance, and automate resolution. Paired with its automation services, IBM envisions MyInvenio’s software allowing organizations to streamline things like accounts receivable by looking at interactions with enterprise resource planning and invoicing systems. The software can help determine if employees spend a lot of time manually examining invoices and auditing purchase requests to find anomalies, two processes where AI-powered automation can potentially boost efficiency while cutting costs. The MyInvenio acquisition is IBM’s second this year after Salesforce consultancy 7Summits in January, and it follows IBM’s purchase of RPA developer WDG Automation last July — signaling the company’s ambitions in the enterprise automation space. According to Grand View Research, RPA alone is a market opportunity anticipated to be worth $3.97 billion by 2025. “Digital transformation is accelerating across industries as companies face increasing challenges with managing critical IT systems and complex business applications that span the hybrid cloud landscape,” IBM general manager Dinesh Nirmal said in a press release. “With IBM’s planned acquisition of MyInvenio, we are continuing to invest in building the industry’s most comprehensive suite of AI-powered automation capabilities for business automation so that our customers can help employees reclaim their time to focus on more strategic work.”"
https://venturebeat.com/2021/04/15/tetrascience-raises-80m-to-help-life-sciences-companies-analyze-data/,TetraScience raises $80M to help life sciences companies analyze data,"TetraScience, an R&D cloud data management company focused on life sciences and drug discovery, today announced that it raised $80 million in a series B round co-led by Insight Partners and Alkeon Capital. The company says that the funding will allow it to ramp up its go-to-market efforts, expand its technical capabilities, and double the size of its team. The pandemic underscores the need to accelerate scientific discovery toward delivering treatments that improve human life. This arguably starts with enabling breakthroughs in biopharma R&D, where some labs remain dependent upon legacy software and data stacks, resulting in bottlenecks. Fewer than 12% of all drugs entering clinical trials end up in pharmacies, and it takes at least 10 years for medicines to complete the journey from discovery to the marketplace. TetraScience, which was founded by former Harvard and MIT researchers, offers cloud storage, an API, and prebuilt integrations with lab instruments, informatics applications, analytics, and data science partners that pull in and reconcile data. TetraScience can automatically harmonize information across vendors and formats and enable users to share data, reports, analyses, and apps with internal teams. TetraScience offers tools for prototyping, building, testing, and scaling distributed data apps. A growing number of bundled apps are available from TetraScience as well as its network partners, and the platform natively supports data science tools, templates, models, pipelines, and custom configurations. As data and tools become more readily available, life sciences organizations will tap the technologies necessary to boost their output, some experts predict. For example, an IDC report anticipates that this year, 50% of pharmaceutical and biotech companies will use analytics or AI with IoT data to optimize the supply chain. The report also projects that 30% of life science organizations will have achieved data excellence by 2022. “COVID-19 has heightened the urgency to replatform to the cloud to take advantage of advanced data science and AI and machine learning as pharmaceutical companies face enormous societal, governmental, and shareholder pressure to radically accelerate discovery,” CEO Patrick Grady told VentureBeat via email. “Inbound interest in the Tetra R&D data cloud from global pharmaceutical companies has increased dramatically, despite very little investment in marketing.” For one customer — the Johnson Research Group at the MIT Department of Chemistry — TetraScience deployed its product with equipment types within a lab, including freezers and chemical fume hoods. Previously, the lab lacked control over nanoparticle synthesis reaction parameters including temperature monitoring, the ability to turn off a hot plate, and remote syringe pump activation. Now, scientists at the lab can see whether a batch is good based on how much the temperature fluctuated during the synthesis, eliminating the need for manual analysis. Sixty-one-employee TetraScience’s funding comes on the heels of the company’s 10-times revenue growth 1,000% uptick in annual recurring revenue (ARR) in 2020, reflecting additions to its customer base like Novo Nordisk, Merck & Co., and SmartLabs. (ARR is forecast to grow more than 300% in 2021, TetraScience says.) It brings the Boston-based company’s total raised to over $88 million following an $8 million series A in October 2019."
https://venturebeat.com/2021/04/15/keyfactor-125m-acquires-primekey-machine-identity-management-platform/,Keyfactor raises $125M and merges with PrimeKey to create a machine identity management platform,"In a world where machines outnumber people on the internet, identifying and authenticating those devices is becoming increasingly critical to security. Keyfactor, which helps automate the management of certificates for connected devices, today announced two big steps toward seizing this market opportunity. The first is a merger with PrimeKey, which creates the certificates that serve as crucial tools for identifying machines. The second is $125 million in new funding to power growth and comes two years after the company raised $77 million. Keyfactor is dubbing the merged company a “machine identity management platform.” “Now more than ever, enterprises must operate in a zero trust world, and machine identity management can no longer be ignored as part of an identity and access management (IAM) strategy,” Keyfactor CEO Jordan Rackie said in a statement. The company said that Keyfactor alone has seen revenues grow 50% year over year. The merger with PrimeKey will now allow Keyfactor to deliver end-to-end certificate service, which should help simplify identity management, even as systems become more complex. In a press release, Keyfactor executives note that Gartner has been bullish on Identity and Access Management Technologies. Many companies still fail to include machines in their authentication strategy, a major weakness that could create greater vulnerabilities. The ability to address that opportunity drew the $125 million investment from Insight Partners."
https://venturebeat.com/2021/04/15/cado-security-raises-10m-for-cloud-cybersecurity-forensics/,Cado Security raises $10M for cloud cybersecurity forensics,"Digital forensics platform Cado Security today announced a $10 million series A investment led by Blossom Capital, with participation from existing backers. The funds bring the company’s total raised to $11.5 million and will be used to support growth in engineering, customer support, and go-to-market operations. Some experts estimate that legacy forensics tools only provide 5% or less of the data needed to investigate a cloud attack. Forensics analysts often determine that an attack is not worth further investigation, due to the level of effort required to dig deeper. But these attacks aren’t slowing. Some 20% of organizations get hit with cyberattacks six or more times a year, and 80% say they’ve experienced at least one incident in the last year so severe it required a board-level meeting, according to a report from IronNet. James Campbell and Chris Doman founded Cado Security in 2020 with the goal of addressing challenges in cloud security forensics. Campbell, who previously led PricewaterhouseCoopers’ cyber response service and Australia’s national Australian Signals Directorate as associate director, teamed up with ThreatCrowd creator Doman to build a forensics platform that speeds up investigations of cloud attacks. “We founded Cado Security right in the midst of the pandemic in April 2020, as enterprises were shifting to the cloud, to enable their remote workforces to successfully work from anywhere,” Campbell told VentureBeat via email. “This uptick in the cloud introduced new complexities and risks enterprises had never seen before. Security teams didn’t have the time to become experts in the cloud amidst the shift, and hackers noticed.”  Cado Security automatically captures and processes data to visualize and investigate attacks, leveraging an analysis engine that detects malicious files, suspicious events, personally identifiable information, and financial data. Employing a combination of full-content inspection, log parsing, event correlation, and machine learning models, Cado Security’s platform indexes files and logs for later inspection, creating a human-readable timeline of events. “[Our] platform has a unique detection engine that uses machine learning in order to identify financial or personally identifiable data across systems that have been impacted by an event,” Campbell explained. “Many of the existing solutions provide an incident overview, which represents a fraction of the actual data related to the event, meaning you’re more likely to miss something big … [Cado] can see data attempting to be exfiltrated by a hacker, even when they are not using any malicious software to evade detection.” According to Gartner, nearly 70% of enterprises plan to accelerate spending on cloud services in 2021. As more data moves to the cloud, attacks on cloud infrastructures are increasing significantly, putting new pressures on security teams to respond quickly. Cado Security claims it has seen “significant demand” despite competition in the over $34.5 billion cloud security market. Netskope recently raised $340 million at a $3 billion valuation, while Valtix nabbed $14 million in June 2019. There’s also Bitglass, which raked in $70 million for its cloud-native platform that helps companies monitor and secure employee devices. “Data is moving to the cloud at an alarming rate. We founded Cado Security to help enterprises quickly and easily conduct deep forensic investigations across modern cloud environments to stay one step ahead of today’s cybercriminals,” Campbell said. “Our platform is [one of the few solutions] that can capture data across short-term environments, such as containers and auto-scaling infrastructures, enabling security teams to effectively investigate threats.” Ten Eleven Ventures also participated in London-based Cado Security’s latest funding round."
https://venturebeat.com/2021/04/14/dell-finally-spins-off-vmware-stake-in-9-7b-deal/,Dell finally spins off VMware stake in $9.7B deal,"(Reuters) — Dell Technologies said on Wednesday it would spin off its 81% stake in cloud computing software maker VMware to create two standalone public companies in a move that will help the PC maker reduce its pile of debt. VMware is currently Dell’s best-performing unit, as it has benefited from companies looking to cut costs and move to the cloud, a shift that has been accelerated by the COVID-19 pandemic. Shares of Dell rose more than 8% in extended trading. VMware will distribute a special cash dividend of between $11.5 billion and $12 billion to all of its shareholders, including Dell, which will receive between $9.3 billion and $9.7 billion. For Dell, the special dividend will help reduce its long-term debt of $41.62 billion, much of which was taken on during its $67 billion acquisition of VMWare’s then-majority owner EMC in 2016. The companies said the deal will simplify their capital structures. Both companies will also enter into a commercial arrangement to continue to align sales activities and for the co-development of solutions. VMware, whose software helps companies squeeze more work out of datacenter servers, has been looking for a CEO after previous boss Pat Gelsinger was tapped to lead Intel. Dell first announced the spinoff plans in July last year. The deal is expected to close in the fourth quarter."
https://venturebeat.com/2021/04/14/building-a-company-culture-that-directly-impacts-your-bottom-line-vb-live/,Building a company culture that directly impacts your bottom-line (VB Live),"Presented by TriNet How do you create a company culture that empowers your employees to keep growing and gives you a competitive edge? Learn about what makes a successful company culture, how to strengthen your culture to align with your organization’s values and goals and more in this VB Live event. Register here for free. “Company culture is unbelievably instrumental in driving your top-line and bottom-line goals,” says Deepa Gandhi, co-founder and COO of the handbag brand Dagne Dover. “Often people get too focused on hitting KPIs and target revenue goals and forget that the people are the ones who actually drive that.” For Gandhi, it’s a simple equation: If you have happy people, workers who are excited to come to work, and excited to work at your company, then they’re going to be that much more motivated to do their best job. Customers feel that energy, sense when a brand’s external image is matched by the energy of its people, she says. “We started our company saying we wanted to make sure we didn’t just sell great handbags, but we also wanted to build a company with great culture, female-forward, and flexible,” she says. “And we hire for culture.” That, she says, is the number-one rule. Of course, table stakes for recruitment is ensuring a candidate has the requisite skills to do the job. But a job interview should focus on establishing the relationship and culture fit from the start. You want to know who this person, is, what they care about, and what they value. You want them to know about your company, who you are, and how you like to work, and ensure there’s strong cultural affinity. Even somebody who doesn’t have as much experience, but seems really promising can be a strong cultural fit. “You could have the most experienced person out there, and if they’re not a cultural fit, they can end up being toxic for the team,” she says. “Hire for culture, because if you don’t bring in people that support what you’re trying to build internally, it’s all going to break down in the end.” To create a positive company culture that attracts and retains that kind of talent, listening to your employees, to your teammates, to your co-founders, is pivotal. You may have a specific perspective on what you believe is a positive culture or a positive workplace experience, but that might not be what your employee finds important or what they’re ultimately looking for in a career. “Being able to listen and then adjust and evolve your team culture accordingly — that makes a massive difference,” she explains. Another piece is incentivizing people based around culture. For performance evaluations, it’s not enough to look at an employee’s technical or job performance – it’s critical for Gandhi and team to look at how they contributed to the culture. “The ones that go above and beyond to build a better, stronger culture for us, we reward them for that, and it becomes a motivating factor,” she says. This kind of environment, in which your people feel welcome, accepted, and valued, directly contributes to innovation. A flat hierarchy should cast a wide circle in the brainstorming and creative process, where everyone’s voices hold equal weight and purpose – as Gandhi says, you never know where a great idea will come from. Contrast that with organizations that have built stiff barriers between cross- functional partners. “I’ve worked at other companies where the design team did not want to hear from anybody else at the company,” says Gandhi. “Especially in retail companies, your creative and design team hardly interface with the more analytical business and financial side of the company. As a result, you end up often having an imbalance where one overpowers the other, and one feels undervalued.” At Dagne Dover, the finance and analytical side of the business don’t dictate to the design side about what to make and how to sell, but share data and analytics with their counterparts, and make decisions together. Creative partners bring new ideas to the business side for input on making them work from a revenue perspective, and launch products in a way that optimizes not just for creative and brand goals, but also revenue goals. “Our thesis is, let’s empower each other through the decision-making process,” she says. “It’s a great conversation, and one you don’t see often enough.” To learn more about why a good company culture is so essential, what makes a culture great, how to encourage employee passion and buy-in, and more, don’t miss this VB Live event. Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/14/domino-accelerates-mlops-with-new-nvidia-integrations/,Domino accelerates MLOps with new Nvidia integrations,"Domino Data Lab announced new integrations with Nvidia this week to make it easier to adopt AI infrastructure, scale GPU clusters, run more virtual workloads on high-end GPUs, and package AI apps into container infrastructure. Domino’s tools streamline the grunt work associated with building out AI and ML applications. Domino automatically spins up workspaces or models on shared infrastructure so many people can share the same infrastructure. When someone is finished with a workload, Domino spins down that workspace to free up the resources for someone else. Domino also tracks usage, letting IT administrators see consumption and make informed decisions about when to increase computing power. Gartner considers AI orchestration tooling that includes MLOps to be a key trend in 2021. Domino currently supports ephemeral clusters built on Apache Spark and Ray, and the company plans to add support for Dask this fall. Domino strategic partnerships VP Thomas Robinson told VentureBeat that Spark has traditionally excelled at large-scale data processing and transformations. Ray has simplified distributed training and hyperparameter optimizations, and Dask has excellent integration with commonly used Pandas and NumPy libraries. Domino also improved the ability to provision GPU clusters required to run AI training jobs that require more than one Nvidia GPU. Traditionally, it could be difficult and time-consuming to set up machines, ensure network connectivity, and install proper libraries. In addition, it is uncommon for enterprises to give data scientists access and permission to manipulate infrastructure directly. As a result, teams often leave clusters idle between larger projects, rather than reallocate the individual machines for smaller projects. To improve utilization rates, Domino makes it possible to spin up and spin down interactive sessions, batch jobs, or models hosted on Nvidia DGX infrastructure to allow multiple concurrent and consecutive sessions. Previously users depended on email and spreadsheets to coordinate workloads, which was inefficient. Domino will add support for Nvidia’s multi-instance GPU technology in September. MIG allows a single GPU to be sliced up into smaller portions (7 slices per GPU for each of the 8 GPUs in a DGX A100 — a total of 56 slices). This will make it possible to divide the capacity of a larger GPU server or cluster into multiple instances or partitions to host many more predictive models on smaller GPU instances. While many deep learning training workloads require a whole machine or multiple machines in a cluster, research, or inference (prediction), workloads are much less GPU-intensive. “By allowing the GPU to be portioned into pieces, you can have more researchers doing discovery work in notebooks on smaller GPU slices,” Robinson said. Domino also announced immediate support for Nvidia’s new NGC container registry service. This makes it easier to package vetted application and configuration settings into container instances that bake in best practices. This means a data scientist doesn’t have to spend time figuring out how to set up and install all the drivers and tools they need. It also allows organizations to standardize on these containers. NGC currently supports RAPIDS, TensorFlow, PyTorch, and CUDA. Domino additionally supports containers for SAS, MATLAB, Amazon SageMaker, and private container repositories. Finally, Domino worked with Nvidia and NetApp to develop a preconfigured hardware/software package called the ONTAP AI Integration Solution. “This is a specced, tested, and verified packaging of everything you need to accelerate your data science work — so there’s no guesswork and no setup needed for an IT department,” Robinson said."
https://venturebeat.com/2021/04/14/linux-foundation-creates-research-division-to-study-open-source-impact/,Linux Foundation creates research division to study open source impact,"In the latest sign of the growing influence of open source software, the Linux Foundation announced it is creating a new research unit to provide greater insight into the technology, as well as the people creating it. Linux Foundation Research will be a new division with a broad mandate to explore the social and technical aspects of open source in the hopes of expanding the types of people who participate and encouraging more enterprises to adopt the technology. Among the group’s priorities are examining diversity and security. “As open source has become a fundamental part of the global technology supply chain, we started to see a need to really provide deep insight so that we could all collectively make better decisions about how to improve the way development communities work, improve incentives to create better software outcomes, and understand the economics of open source,” Linux Foundation executive director Jim Zemlin said. Discussions about creating the research group had percolated as the Linux Foundation conducted various research projects in collaboration with other institutions over the years. As helpful as those projects were, they also began to highlight knowledge gaps. Eventually, the foundation believed its work with thousands of companies and hundreds of thousands of developers would give it an immense amount of data it could better leverage through its own organization. Hilary Carter, who has made a mark in research at the Blockchain Research Institute, has been recruited to oversee the new research unit. She said she still expects much of the research to be conducted in collaboration with other organizations. Her core objectives include creating awareness of what is happening in open source communities, what important products are being developed, and what vulnerabilities lie in open source ecosystems. “In creating greater awareness, we hope to increase participation in those communities,” Carter said. “And I think one of the ways we do that is by addressing some of the issues within communities, particularly around diversity and inclusion.” She’s also hoping more people will be inspired to join those communities by highlighting how open source software can impact society in terms of diversity, social justice, and climate change. As part of the new initiative, the foundation will also create the Linux Foundation Research Advisory Board, which will gather external experts to help develop the research agenda."
https://venturebeat.com/2021/04/14/nvidia-forms-inception-vc-alliance-to-connect-ai-startups-with-venture-capital/,Nvidia forms Inception VC Alliance to connect AI startups with venture capital,"Nvidia has formed its Inception VC Alliance to connect AI startups with venture capital. The move will help connect more than 7,500 startups in the company’s Inception program for AI tech with venture capital firms. Jeff Herbst, vice president of business development and head of Inception at Nvidia, unveiled the alliance today at the AI Day for VCs event during Nvidia’s annual GTC 21 conference. Nvidia CEO Jensen Huang unveiled the company’s latest products on Monday in a keynote speech where he talked about the company’s new Grace central processing unit (CPU). “We always felt a very strong connection to the ecosystem. We give them technology, we introduce them to our 150 different software development kits, we give them joint marketing, we introduce them to investors,” Herbst said in an interview with VentureBeat. “We give them Cloud Credits. We give them discounts for GPUs.” AI adoption is growing across industries, and startup funding has been booming. Investment in AI companies increased 52% last year to $52.1 billion, according to PitchBook. The Inception AI startups are up 9 times from 2016, Herbst said. The alliance aims to help investment firms identify and support leading AI startups early, as part of their effort to realize meaningful returns down the line. The goal is to educate VCs about AI opportunities and nurture startups, Herbst said. “AI is growing like a weed. We’re over 7500 companies, and it’s not going to be long before we’ve doubled that,” he said. “The ecosystem is clearly exploding. And VCs are a super important part of it. Startups need VCs, and VCs need startups. It’s just that simple fuel for startups to grow. We have thousands of VCs that are already part of our ecosystem, but we’ve never formalized the partnership with them until now.” Founding members of the alliance include venture firms NEA, Acrew, Mayfield, Madrona Venture Group, In-Q-Tel, Pitango, Vanedge Capital, and Our Crowd. More VCs can apply here. The Nvidia Inception VC Alliance is part of the Nvidia Inception program, an acceleration platform for startups working in AI, data science, and HPC. These startups represent every major industry and are located in more than 90 countries. Among its benefits, the alliance offers VCs exclusive access to high-profile events, visibility into top startups actively raising funds, and access to growth resources for portfolio companies. “It’s both a corporate goal and a personal goal to extend this ecosystem around the world,” Herbst said. Nvidia currently counts about 40 companies it has invested in directly. Around 300 Inception companies are making presentations at the GTC 21 event, which is expected to have an online audience of about 150,000. And around 35 of the startups are in emerging markets, Herbst said. “Is there parity in the world with AI startups? No,” Lopez Research analyst Maribel Lopez said on the panel. “Do we have a long way to go? Yes. But I’m seeing exciting things like Cuda, a fintech startup in microfinance in Africa.” These startups are using AI for a wide range of tasks, like figuring out what percentage of fisheries in the world are operating illegally. “Now that Jensen has shown the roadmap, people know that Nvidia is a complete platform, with CPUs, GPUs, DPUs, and everything that enables these startups to do their life’s work.” On Monday, Herbst moderated a panel on investing in startups around the globe and the need to create a more diverse ecosystem for entrepreneurs. He estimated there are 12,000 to 15,000 AI startups around the world and said Nvidia is only in touch with about half of them through Inception. “It’s an open invitation to join our ecosystem,” Herbst said. “Nvidia loves startups.” Herbst said about 16% of Inception members are part of the health care industry. Growth areas include robotics, self-driving cars and trucks, and data science."
https://venturebeat.com/2021/04/14/how-wayfair-and-burts-bees-optimize-digital-creative-for-every-social-platform-vb-live/,How Wayfair and Burt’s Bees optimize digital creative for every social platform (VB Live),"Presented by yellowHEAD In digital marketing, there’s no such thing as a one-size-fits-all creative for all platforms. To really perform, you need data. Learn how to use data to optimize your creatives for every channel, align your strategy with user demographics, and get results in this on-demand VB Live event. Access free on demand right here. Digital marketers face a singular challenge: trying to educate and win customers over and unlock loyalty in an increasingly fragmented media and advertising landscape, says Courtney Lawrie, global head of brand and integrated growth marketing at Wayfair. At Wayfair, they’re well positioned to tackle this challenge, however. “We’ve taken a data-driven and performance-oriented approach in our personalization and relevance initiatives since day one,” Lawrie says. “We make that happen at scale by leveraging our in-house software programs, our machine learning algorithms and analytics, to ensure that we’re serving every customer the optimal content at the right time on any touch point across all different types of platforms.” In the home category, she explains, customers are driven by inspiration, and often have a unique vision for their home. With data, they’re working to anticipate their customers’ needs and to create a shopping experience uniquely tailored to their tastes, which directly informs content and creative asset creation efforts. At Burt’s Bees, says Melissa Culbertson, associate manager of brand engagement and social strategy, they look at engagement and reach from an organic social standpoint. However, they’ve started to pay closer attention to the data on deeper levels of engagement, particularly around comments and shares. It’s easy for people to tap to like a video or an image when you’re scrolling through TikTok or Instagram, but comments and shares take a bit more effort, and that’s a better gauge of engagement. But when measuring content performance, they drill down to content pillars — for instance, a values-based piece of content, such as organic social for Earth Day wouldn’t be measured against a product-based piece of content. They’re also able to compare content formats against one another. They know from data that GIFs and video-based content on Instagram, for example, don’t perform as well as static assets. “That doesn’t mean we don’t do that, because sometimes for the story you need to tell you need a longer-form piece of creative than just an image,” she says. “But we would measure video-based content against video-based content to get a better idea of whether it was successful or not.” At Wayfair, they’re exploring the opportunity to increase awareness in other emerging home categories, like home improvement and renovation, large appliances, and housewares. Those emerging categories lend themselves to some of these subcultures. In the home improvement space, for example, they’re leaning in and looking at DIY enthusiast subcultures, particularly during the pandemic. “Subcultures can spread your brand’s message to a whole new audience with already ingrained loyalty, which is kind of amazing,” says Kinney Edwards, global head of Creative Lab at TikTok. “But you have to do the work. You have to learn their language, speak to them in a real and authentic way. It takes more effort to craft your message creatively. But the reward is in the way in which the subculture community will adopt your messages and become brand ambassadors for you in an organic and passionate way.” There’s an opportunity to tap into these communities and tell engaging stories across social platforms. The key is to remember that each social platform serves a purpose in a user’s life, says Noa Miller, marketing creative strategist at YellowHEAD, and that information is as essential to the creative concept as drilling down to a user’s demographic information. On each platform, a user is consuming content very differently. “We believe that we need to come up with a strong creative concept, and then have that concept be translated to fit what a person is doing once they arrive at these different platforms,” Miller says. For example, on TikTok, the first thing a user sees is a video with sound on. On Facebook, they’ll be scrolling through videos with no sound on. “I need to understand, as a marketer, what’s the way that our audience is consuming their media on each platform, and then create creatives that are a perfect fit for that,” she says. YellowHEAD worked with Tinder to create a campaign around their new video chat feature, creating three different ads with the same concept, but each from a different platform’s perspective: one for Facebook, one for Instagram, and one for TikTok. Facebook scrolling time is about three seconds, which requires a catchy opening to make the audience stick and want to see more of the ad’s story. On Instagram, they cropped the video into three story ads, each highlighting a different fact about why video chatting is a great idea, to keep the consumer skipping from one to the other, and see that there’s a storyline. For TikTok, they filmed the ad with real people, and designed it to look as native as possible to the platform. “We try to instill in brands and marketers that there are best practices to getting to that quality creative,” Edwards says. “Understand the ecosystem by engaging with it, playing around, putting that mindset on, because as you’re creating content for these users, you want it to feel like it’s for them and not for you.” That means building a narrative, and approaching the platform with an audience-first mindset, he adds. The community respects when you’re direct and to the point, but delivering that message in a way that demonstrates you get the platform. However, Culbertson says, her best piece of advice is that while the platform’s best practices are an excellent guide, not to rely on them solely. “Use that as a starting point, but then test different creatives, see what works and what doesn’t. It’s very individualized to your brand, your company and so on,” she explains. “Based on that data, you can iterate and build better creative as you go along.” “Don’t try to be perfect and overthink it,” Edwards adds. “Action drives action. Trends, culture that happens so quickly, you don’t want to get caught up in doing it perfectly. You want to be part of what’s happening. Just dive in.” For the performance-oriented marketers and brands, Lawrie adds, “Don’t get too bogged down by short-term metrics. Make sure you also look at your creative over a long-term horizon. Make sure those short-term metrics are a proxy for long-term outcomes.” And Miller notes that creating different creatives for each platform shouldn’t break your budget. “It doesn’t always have to be a big production — things can be done easily,” she says. “Today everyone is a content creator. Try to make it fit to the platform without additional big productions needed.” For more about creating authentic brand stories that really work, across every platform, accessing and leveraging data to ensure your message is on target, and more, don’t miss the rest of this VB Live event Access free on demand here. You’ll learn: Speakers:"
https://venturebeat.com/2021/04/14/paving-the-yellowbrick-road-to-closer-integration-with-cloud-data-stores/,Paving the Yellowbrick road to closer integration with cloud data stores,"Traditionally, data warehouses meant enterprises had to either commit to one platform or suffer the complexity of managing multiple quasi-compatible infrastructures. With that in mind, Yellowbrick Data recently launched Yellowbrick Manager to make it easier for administrators to manage data warehouses across distributed cloud and on-premises deployments with a simplified interface. Users can control Yellowbrick data warehouses in public clouds, on Yellowbrick hardware instances, in private clouds, and even at the network edge, the company said. Yellowbrick Manager provides a unified control system that uses the Kubernetes container orchestration system to enable users to manage and control both cloud and on-premises deployments with enhanced performance capabilities. “It’s this single unified control plane, together with the adoption of Kubernetes in our database software, that sets us apart,” Yellowbrick CTO Mark Cusack told VentureBeat. The company also added agile data movement capabilities to help customers integrate Yellowbrick with data lakes built on cloud object stores like Amazon S3. A technology preview is expected in early May, and general availability is set for the second half of 2021. Also in early May, there will be an update to Yellowbrick data warehouse release 5, with data lake integration enhancements that include native support for cloud object storage (including Amazon S3 and Azure Data Lake Storage Gen 2). One of the criticisms of hybrid clouds has been around the disjointed set of technologies and user experiences across private and public clouds. There are different identity and access management approaches, which makes it harder to govern access to data across these environments, and they are typically provisioned differently. Yellowbrick is positioning itself as the first data warehouse for the distributed cloud by introducing a unified control plane that works across the most common cloud platforms. This control plane increases the simplicity of running multiple data warehouses in different physical and line-of-business locations across an enterprise. The Kubernetes cloud native architecture provides Yellowbrick Manager with a single unified control panel to provision new data warehouse instances in different clouds, manage existing infrastructure, and monitor deployments. Yellowbrick said Yellowbrick Data Warehouse queries run 3 times faster on Andromeda-optimized instances for private clouds than on the company’s first-generation architecture. The performance improvements are also the result of the company switching to new AMD 64-core CPUs, increasing bandwidth between server notes, and adding dual proprietary Kalidah scan accelerator cards that offload workloads such as filtering, compression/decompression, and row/column transposition from CPUs. “The Andromeda instance is the fastest platform to run Yellowbrick on,” Cusack said. The company is optimizing deployments to take advantage of differences across the big cloud providers. For example, if a public cloud instance has high-performance storage, Yellowbrick can adapt to the underlying hardware to take advantage of those benefits. Yellowbrick is focusing on improving the management aspects for distributed clouds rather than joining them together directly. Competitors tend to fall into two major categories: legacy data warehouses such as Teradata, Oracle, and SQL Server or cloud-only data warehouses like Snowflake, Amazon Redshift, and Microsoft Azure Synapse. Yellowbrick seeks to differentiate itself by taking a unified approach to addressing distributed data challenges like data gravity and sovereignty requirements. It was designed from the ground up to optimize price/performance across bare metal and virtualized infrastructure in public clouds. Yellowbrick product marketing VP Justin Kestelyn says Yellowbrick has an advantage over legacy vendors that have been doubling down on older architectures and have a harder time with real-time analytics. Traditional cloud vendors have not been aggressively pursuing hybrid options for analytics at the edge. “We’re winning business from all of these vendors by providing the best price/performance and the lowest deployment risk,” Cusack said."
https://venturebeat.com/2021/04/14/rocket-software-acquires-asg-technologies-to-boost-infrastructure-management-tools/,Rocket Software acquires ASG Technologies to boost infrastructure management tools,"Rocket Software this week announced it intends to acquire ASG Technologies as part of an ongoing effort to expand the reach of its portfolio. Terms of the deal were not disclosed. ASG Technologies is best known for providing a configuration management database (CMDB) that is widely employed in IBM environments. But in recent years, it has expanded its portfolio to include infrastructure management tools, as well as business process management (BPM) and content management software it gained by acquiring Mowbly in 2018 and the acquisition of Mobius Management Systems in 2007. Rocket Software, which is privately held by Bain Capital, has historically focused on middleware and tools that are employed to modernize mainframe environments. With the acquisition of ASG Technologies, the company will expand the scope of its product offerings to include more infrastructure management tools and software further up the application stack, newly appointed Rocket Software president Milan Shetti told VentureBeat. Rocket Software sees the acquisition of ASG Technologies as part of a larger strategy to expand its reach across the enterprise, Shetti noted. The overall size of the mainframe market opportunity, however, continues to grow as the IT platform becomes more tightly integrated with distributed computing platforms running inside and outside of cloud computing environments, Shetti added. In the months ahead, the company will continue looking to expand its portfolio through organic and inorganic acquisitions, Shetti noted. Over the course of its history, Rocket Software has made more than 45 acquisitions, including Zephyr, Shadow, Aldon, and D3. The acquisition of ASG Technologies will present opportunities across a soon-to-be expanded product portfolio, Shetti noted. “We will continue to be an acquisitive company,” he said. Of course, there is no shortage of rival offerings for managing applications and IT infrastructure in what is becoming an extended enterprise computing environment that reaches from public clouds all the way to the network edge. At the same time, the management of data and the applications employed to create that data are becoming more disaggregated. As that trend continues, the need for more sophisticated tools that can manage what is evolving into multiple centers of data gravity will become more pressing. One of the areas Rocket Software will invest in is developing the machine learning models needed to automate the management of a wide range of IT management tasks, Shetti said. In effect, an arms race to build next-generation tools for managing enterprise IT environments — also known as AIOps — is now well underway. There is no consensus on how sustainable AIOps is, given the degree to which all IT management tools will employ machine and deep learning algorithms. But the next generation of IT management platforms will continuously learn about the IT environment as changes and updates are made. It’s not likely these tools will replace the need for IT administrators as IT environments continue to become more complex. However, the amount of time spent trying to discover the root cause of a specific IT issue should be sharply reduced. In addition, those platforms will enable IT organizations to compensate for a current shortage of IT skills that limits the degree to which such environments can scale. In theory, an IT team should be able to leverage AI platforms to manage several orders of magnitude more workloads as IT becomes more automated. It may be a while before the promise of AIOps is fully realized, but the future of IT management can already be seen in large enterprises. It’s just a question of how long it might take for those AI capabilities to be pervasively applied across all enterprises."
https://venturebeat.com/2021/04/14/gains-in-cloud-digital-transformation-boost-sap-2021-q1-revenue-outlook/,"Gains in cloud, digital transformation boost SAP 2021 Q1 revenue outlook","(Reuters) — German software group SAP on Tuesday nudged its outlook for 2021 revenue higher after reporting first-quarter results showing gains in cloud sales, following the launch of a new business transformation initiative. SAP said it now expects cloud and software revenue this year of between 23.4 billion euros and 23.8 billion euros ($28 billion to $28.4 billion) at constant currency, up 100 million euros from prior guidance and a rise of 1-2% year-on-year. Its forecast for adjusted annual operating profit was unchanged at 7.8 billion euros to 8.2 billion euros, representing a decline of 1-6% from last year’s outturn. The company, based in Walldorf, Germany, prereleased what it called “stellar” first-quarter results that showed CEO Christian Klein’s new focus on selling business transformation as a service via its Rise with SAP package gaining traction. New cloud business, measured as current cloud backlog, rose 19% at constant currencies in the first quarter to 7.63 billion euros — the fastest in five years — while adjusted cloud revenue gained 13% at constant currency. Total revenue, which includes SAP’s traditional mainstays of license sales and service revenues, rose by 2% in the quarter at constant currency to 6.35 billion euros. Reported operating profit was depressed by executive share compensation, which SAP accounts for as a cash expense. After stripping out that effect, adjusted operating profit rose by 24% to 1.74 billion euros at constant currency. SAP prereleased the results, as German stock exchange rules require when results diverge from expectations or management adjusts guidance. The company is due to report full quarterly results on April 22. ($1 = 0.8370 euros.) Reporting by Douglas Busvine, editing by Sam Holmes."
https://venturebeat.com/2021/04/14/how-process-mining-and-analytics-complement-each-other/,How process mining and analytics complement each other,"At the recent ABBYY Reimagine conference, executives discussed how process mining and analytics are distinct technologies that can complement each other to help teams better understand business processes. Process mining helps identify inefficiencies or opportunities for improving how companies do things, while analytics help businesses measure performance and identify opportunities. Together, they can deliver the best of both worlds. Better analytics and data prep workflows allow process mining tools to offer a glimpse inside various business processes. And process mining tools help executives understand and improve data science processes used by applications and improve overall reporting. Process mining applications are optimized for processes that live entirely within ERP or CRM applications but require manual work to handle other applications. With more applications and data moving to the cloud, manual steps become an issue. “Everybody’s got too much data, and everybody’s processes are time-consuming and cumbersome,” Capitalize Analytics managing partner Eric Soden said. “Somehow we’ve got to get better and do more and be faster, and the data isn’t getting any smaller.” Soden found that analytics tools like Alteryx help organize, prepare, and reformat data in a form suited for process analytics. This makes it easier to identify more dependencies or bottlenecks within a process. For example, improved visibility into a driver monitoring app may uncover a manual step that is causing bottlenecks for processing shipping manifest logs. This step can be automated using something like robotic process automation (RPA) technology. When many people are involved, processes can become harder to understand, Soden said. An agreement system may coordinate the marketing and sales teams and also interact with other applications for purchase orders, invoices, and shipping. And an analytics tool can reformat the data used by the system into the format required by the process mining tool. With process mining, it becomes easier to visualize a wider variety of processes, generate better data, and improve business results. Alteryx Global Partner marketing VP Steve Wooledge said scaling up analytics can lead to process bottlenecks in enterprises. Process mining tools like ABBYY Timeline make it easier to understand, streamline, and automate analytics workflows, as well as building analytics modules that can be used in downstream applications. For example, an Alteryx customer ran a monthly process to calculate its fixed assets that took 40 hours and required a team of 10 contract workers to manage. Modeling this with process mining and creating a repeatable workflow allowed them to reduce that to 2.5 hours. Process mining automatically documented the process, which was useful for compliance and governance. Process mining comes in once an enterprise has established repeatable analytic workflows — such as analyzing volume discount opportunities, running fraud detection models, and analyzing a constantly evolving mix of assets — that executives monitor and optimize. Many analytics use cases involve blending data from multiple applications, databases, or spreadsheets — or even documents that may all be maintained by different departments."
https://venturebeat.com/2021/04/14/how-the-human-bot-dynamic-inspires-extraordinary-service-delivery/,How the human / bot dynamic inspires extraordinary service delivery,"Presented by NICE RPA It’s no secret that Covid-19 has put businesses under significant operational pressure to improve their service delivery approaches to stay competitive. Along with the enforced shift to new ways of working — which are likely to continue after the pandemic — one of the main drivers has been evolving customer expectations. Today’s customers expect the same high-level service no matter what the situation. They demand quick and accurate answers to complex questions. And if their needs aren’t met, they’ll look elsewhere. What’s more, they now expect to receive personalized and customized experiences. This is a trend that has increased in prominence as we’ve all become accustomed to operating in a digitally driven reality. The need to be agile and adaptable to customer needs in the moment is now abundantly clear. As a result, enterprises are being forced to adapt and embrace new technologies. One technology that can have a transformative impact is Robotic Process Automation (RPA). More specifically, attended automation. The combination of attended bots with employees can amplify the customer experience, while also driving additional operational efficiencies. Unlike unattended bots that run on backend servers, working on highly structured and repetitive tasks that don’t require human intervention, attended bots live on the employee’s desktop. They go beyond basic process automation, with the ability to guide employees in real time with next-best action advice. This makes attended automation ideally suited to support customer service teams during times of disruption and help them meet customers’ evolving and often complex needs. Equipped with RPA technology, businesses can augment human talent by streamlining processes and automating the more mundane, time-consuming admin tasks. This frees service reps so that they can focus on more complex, higher-value work. Such work may include more creative and strategically driven activities that add value to customers in a relevant and personal way. In today’s competitive landscape, this is what will drive customer satisfaction and long-term loyalty. Attended automation can also provide real-time, context-specific guidance during live customer interactions. From proactively gathering and summarizing data from multiple applications, to providing quick links to relevant data depending on the employee’s needs, this real-time intelligence can take the pressure off reps and give them the support needed to delight customers with tailored advice. Most importantly, it empowers reps to deliver more engaging, memorable, and superior customer service. The real-time guidance component of attended automation prompts employees to interact with customers in the most effective and beneficial way, such as reading a compliance script or presenting a promotional offer at the right moment. As a result, employees will feel empowered to deliver a high-quality service, while customers will get the experience they have come to expect. Augmenting human talent isn’t the only way attended automation can help inspire higher levels of service delivery. With humans and attended bots collaborating, several additional value drivers can be achieved. For example, attended automation solves many of the productivity and processing accuracy challenges facing businesses. Bots can complete the same desktop tasks faster, typically reducing average handle time by 20%, and allowing employees to get more done in their working day and direct more time towards value-added work that enriches the customer experience. Bots also ensure accuracy and compliance across all the desktop tasks they complete, minimizing the risk of data capture errors and compliance breaches down to nearly zero. This can all directly contribute to higher levels of employee satisfaction. Research shows that repetitive work is one of the top 3 reasons for agent attrition. With transactional and process-level tasks taken off the hands of customer service employees, they can focus more on work that requires judgement, empathy, and consulting expertise. As well as increasing their engagement, this will make them happier in their roles and more likely to deliver an exceptional and engaging customer service. Finally, employees can benefit from the enhanced training capabilities offered by attended automation. In fact, a large majority of employees complain about the difficulty they experience in learning new systems. Add to that the need to learn new processes, products, and service offerings, and the time spent in classroom and online training can add up. Real-time process guidance delivers effective on-the-job training that results in meaningful and sustainable learning. Employees get the reassurance of knowing they will be guided every step of the way and supplied with all the support materials they need to provide experiences that keep customers coming back. Ultimately, attended automation exists to amplify human potential and empower extraordinary service delivery. Reducing the admin burden by streamlining internal processes is a key part of the puzzle. But the true value comes from the real-time insights attended bots can provide, inspiring human workers to deliver more engaging, empathetic, and impactful customer service experiences — no matter what’s thrown at them. Oded Karev is General Manager of NICE RPA. With extensive experience in corporate strategy and operations, Oded leads NICE’s global Advanced Process Automation line of business, covering the full spectrum of robotics solutions. Prior to his current role, he served as Director of Corporate Strategy at NICE, leading some of the company’s key growth initiatives. Before joining NICE, Oded specialized in delivering multi-channel strategies, operating model designs and digital transformation projects for Accenture. Oded is a respected industry thought leader and key note speaker in the field of Robotic Process Automation. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/14/oracle-integrates-cloud-service-with-servicenow-itsm-platform/,Oracle integrates cloud service with ServiceNow ITSM platform,"Oracle today revealed it has integrated its cloud services with ServiceNow’s IT service management (ITSM) platform. IT organizations that have standardized on ServiceNow will be able to view Oracle Cloud Infrastructure (OCI) services alongside other public cloud services presented via the ServiceNow Service portal and the ServiceNow ITOM Visibility application, Oracle’s Clive D’Souza said. That significantly increases the chance IT teams will opt to employ OCI to deploy application workloads, D’Souza added. “It’s just like having space on a store shelf,” he said. All Oracle Cloud discoverable cloud resources will now also be extracted and stored in the ServiceNow Configuration Management Database (CMDB) repository, which can then be used to monitor the availability of those resources. That integration will also feed data into the ServiceNow AIOps platform IT teams are employing to monitor workloads. The integration with ServiceNow comes as more IT organizations are routinely employing multiple cloud services. Many IT teams have now determined that different cloud services optimally run different classes of workloads. IT teams are not necessarily forcing cloud service providers to bid to run workloads as much as they are simply trying to determine which service will provide the best performance for a specific class of workloads, D’Souza noted. In the longer term, D’Souza said Oracle will continue to evaluate its options for integrating its services with other ITSM platforms that are widely employed in the enterprise. The effort to make its cloud services more accessible follows Oracle’s move last month to encourage IT teams to employ its OCI platform. Today, Oracle is helping some organizations move a relatively small number of on-premises Oracle workloads to its public cloud at no additional cost. The Oracle Cloud Lift Services initiative provides access to Oracle engineers that are specifically trained to migrate both applications and databases to the Oracle Cloud Infrastructure (OCI) service. IT organizations have been employing cloud services for the better part of a decade, and yet the bulk of enterprise applications continue to run in on-premises IT environments. In theory, many more IT organizations will be centralizing the management of public cloud and on-premises IT environments as they embrace hybrid cloud computing. But for the time being, each cloud computing environment tends to be managed in isolation from the others. As a result, IT teams find themselves hiring multiple specialists to manage each environment, increasing labor costs with each addition. Hybrid cloud computing promises to reduce those costs while making it simpler for IT teams to deploy application workloads more dynamically on various platforms. Eventually, IT teams will also employ that capability to force cloud service providers to compete more aggressively to run those workloads. It may be a while before hybrid cloud computing becomes the new IT normal. But thanks in part to advances in automation and AI, it will soon become simpler for IT organizations to tame what has become a highly complex distributed computing environment. In the meantime, IT organizations will continue to be torn between keeping as many of their cloud options as open as possible and signing lower-cost long-term contracts."
https://venturebeat.com/2021/04/14/sales-management-platform-atrium-raises-13-5m-to-surface-insights-with-ai/,Sales management platform Atrium raises $13.5M to surface insights with AI,"Atrium today revealed it has raised $13.5 million in seed funding to drive the adoption of a sales management platform that leverages AI to surface issues and opportunities of specific interest to sales managers. While most organizations already have a customer relationship manager (CRM) application in place, Atrium is making a case for a software-as-a-service (SaaS) platform that combines data from CRM applications with other applications to enable sales managers to closely track actual levels of customer engagement. For example, if a deal is expected to close in the current quarter but there have been no communications with the customer via email or any other medium, analytics software infused with AI algorithms will issue an alert, company cofounder and chief revenue officer Peter Kazanjy told VentureBeat. Armed with that insight, it then becomes possible for sales managers to more proactively intervene when necessary, Kazanjy added. Sales managers can track a library of key performance indicators (KPIs) using a diagnostic tool that identifies the root causes for sales issues. These might range from consistently poor sales pipeline management to a customer’s tendency to not sign a contract until the very last week of the quarter in the hopes of maximizing discounts and other concessions. The challenge sales managers face today is not so much gaining access to data as it is making sense of it all to identify issues impacting the sales pipeline, Kazanjy said. Much of that data, however, resides in indecipherable dashboards that in some cases conflict with one another, Kasanjy added. “Sales managers today are awash in data,” he said. Atrium claims more than 100 organizations are now using its platform as an alternative to dashboards in other sales applications that lack the context sales managers require. Customers include Salesloft, Chorus, and LaunchDarkly. Atrium also hosts a Modern Sales Pros online community made up of more than 20,000 revenue leaders that work for more than 7,000 companies worldwide. The seed round of funding is being provided by Bonfire Ventures, Charles River Ventures, and Bullpen Capital. Sales management has for much of the past decade been slowly evolving to become a more data-driven science. Organizations of all sizes are being required to predict revenue flows more accurately, which Kazanjy noted requires more visibility into the sales pipeline. The level of forecasting accuracy is now viewed as an indicator of the degree to which a sales manager is on top of events occurring within their territory. Sometimes those events are beyond the control of any sales team. In other instances, however, a salesperson may be deliberately under-forecasting sales opportunities to lower their sales quotas, otherwise known as “sandbagging.” Regardless of their motivation, the sales manager is usually held more accountable to sales forecasts than an individual sales representative would be. Sales managers today are being held to a higher standard than ever at a time when the signal-to-noise ratio within sales pipelines has never been greater. It’s simply too difficult for the average sales manager to make sense of all that input without the aid of an application platform created for that specific purpose, Kazanjy said. Ultimately, turnover in a sales team is bad for business. It takes an average of six months or more to bring a new sales representative up to speed on their accounts. The primary goal for investing in sales software should be to minimize that turnover as much as possible by surfacing the insights a sales manager needs to make a course correction while there’s still enough time."
https://venturebeat.com/2021/04/13/princeton-digital-group-secures-230-million-financing-charts-out-1-billion-expansion-in-china/,"PRINCETON DIGITAL GROUP Secures $230 Million Financing, Charts Out $1 Billion Expansion in China","  SINGAPORE & SHANGHAI–(BUSINESS WIRE)–April 14, 2021– Singapore-based Princeton Digital Group (PDG), Asia’s leading data center provider, today announced it has secured USD 230 million debt refinancing from China Merchants Bank, as part of its USD 1 billion expansion plan in China. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413006080/en/ The refinancing validates PDG’s momentum, with confirmation of the completion of its fully contracted 42MW Shanghai data center campus. PDG also kicked off construction of its 43MW campus in Nanjing and design work of its 60MW campus in Nantong, and is actively evaluating acquisition opportunities in the Beijing, Shenzhen, and Shanghai areas as part of its 300MW expansion plan. PDG continues to build on its position as the largest international partner to domestic cloud companies in China, with ongoing partnership as they expand across Asia. For international companies, PDG is the only global operator in China offering the scale and service expected from these companies. “On the heels of our latest round of equity financing from Ontario Teachers’ Pension Plan and Warburg Pincus, this debt from China Merchants Bank is a testament to our ability to successfully execute on our strategy to build and deliver scale across APAC’s fastest-growing markets. Our execution track record has helped us win business from some of the world’s largest cloud companies,” says Rangu Salgame, Chairman and CEO of PDG. “We are delivering on the capacity requirements our hyperscale customers seek, when and where they need it. It is core to our strategy to be the leading data center operator in APAC.” “China is seeing growth come from both enterprise and hyperscale users of data center colocation. China is home to a number of cloud infrastructure platforms, social media, e-commerce, and content providers that all have multi-MW requirements. This has created a healthy stream of demand that guarantees steady and consistent colocation absorption,” stated Jabez Tan, Head of Research at Structure Research. According to Structure Research estimates, the data center colocation market in China generated USD $13.5 billion in revenue in 2020 and is projected to grow at a 13.6% CAGR in the next five years. Visit www.princetondg.com for more information about PDG.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413006080/en/ Princeton Digital GroupGrace ChenPR@princetondg.com +65 6679 6273"
https://venturebeat.com/2021/04/13/latest-edelman-survey-rates-trust-in-tech-at-a-21-year-low/,Latest Edelman survey rates trust in tech at a 21-year low,"The technology sector plummeted from being the most trusted industry sector in 2020 to 9th place in 2021, according to the 21st annual analysis from communications firm Edelman. Lack of accountability and unwillingness to self-govern is eroding the public’s trust in technology. Trust in technology reached all-time lows in 17 of 27 countries over the past year, Edelman said in its recent 2021 Edelman Trust Barometer: Trust In Technology report. The report is based on a survey of more than 33,000 people from 28 countries, including both general population respondents and what the firm calls “informed public respondents” for a well-rounded picture. Trust and fear have a reciprocal relationship: The faster one rises, the faster the other drops. Traditionally, the technology sector was something of an expert at managing the two, but that is no longer the case. Edelman found that fear of technology is growing at a faster rate than trust in technology. It will take years for the technology industry to bounce back and regain the public trust. Edelman’s survey results show respondents feel both betrayed by, and fearful of, technology. Job loss is the single greatest driver of societal fears, followed by the loss of civil liberties. There is a 6% drop in the number of people who are willing to share their personal information online. Social media, traditional media, and search engines are also at record low levels of trust. While the technology industry is full of entrepreneurs who believe in unleashing creativity and innovation and pursuing moonshot ideas, there are also those who monitor customers and invade privacy. The tendency to use technology as an authoritarian tool to monitor dissent is a concern, which explains China’s 16% drop in trust. The sheer drop is ironic, because China is also a global leader in tech R&D, innovation, and tech manufacturing. Edelman recorded one of the steepest declines in trust in the eight months between May 2020 and January 2021, when the public’s trust in technology dropped from 74% to 67%. People were increasingly concerned about AI and robots, and 53% of the respondents in Edelman’s survey worried the pandemic would accelerate the rate at which their employers would replace human workers with AI and robots. Cyberattackers capitalizing on the pandemic didn’t help matters, as 35% of respondents reported being fearful of attackers and breaches. Edelman’s Trust in Technology study presents a paradox between tech employees and their employers. Employer trust is highest among tech sector employees, with 83% saying they trust their employers, and 62% believing they have the power to make corporations change. Yet the public’s trust in those employers is plummeting. The disconnect comes from the public perception that humans are not controlling technology, but that technology is trying to control them. There is a growing perception that technology — especially social media — is more capable at manipulating people than previously believed. One way for the industry sector to regain some trust is to re-evaluate how they handle customer data and to be transparent about what they do with the information. Businesses as a whole are still trusted in most of the countries surveyed, with 61% of all respondents trusting companies above nonprofit organizations, government, and media. The most effective step businesses can take to increase trust is to guard the quality of information. Additional factors include embracing sustainable practices, implementing a robust COVID-19 health and safety response, driving economic prosperity, and emphasizing long-term thinking over short-term profits. However, just saying they will protect information isn’t enough. Businesses need to take a data-centric security approach to achieve greater resiliency and cybersecurity. Businesses should also address the concerns employees have over job loss and automation. They should be transparent and honest with their employees if robotics and automation are part of the business plan. Investing in re-skilling employees for new jobs is a great way to transform a business digitally. In short, senior management teams should remember that lasting transformation starts with employees."
https://venturebeat.com/2021/04/13/video-conferencing-company-touchcast-uses-ai-to-add-context-to-conversations/,Video conferencing company Touchcast uses AI to add context to conversations,"Video conferencing is increasingly becoming a commodity as technology giants like Microsoft and Google incorporate the feature into their free services. Touchcast is staying a step ahead of the giants by innovating on AI-powered services for premium users. Special effects are important, but the key differentiator lies in creating more context to drive the next wave of communication, Touchcast CEO Edo Segal told VentureBeat. Touchcast is doing so by taking advantage of Nvidia Maxine, a software development kit for creating GPU powered applications. The SDK includes various primitives for things like AI powered background removal, simulating eye contact, and measuring body pose in sports. “The fact that a company like Nvidia, the leader in AI powering hardware, has the foresight to invest in the research and development on the conceptual and software side helps companies like Touchcast accelerate time to market and focus on building on the shoulders of giants,” said Segal. Nvidia Maxine sets a new baseline of capabilities from which to innovate. “It allows us to focus on other areas where there is still no work being done as we chart this frontier,” Segal said. One big goal is to reduce the effort involved in creating quality events. Live presenters can be virtually teleported into mixed reality sets without a green screen. Live semantic segmentation uses AI to separate a person from the background in high quality, making it possible to automatically place people in a mixed reality set. “This literally used to take days or weeks of work and rendering and is now done live,” Segal said. Neural upscaling can clean a basic webcam image and scale it to an ultra-HD 4K screen. This works in a similar way to an artist asked to paint a mural from a small picture by intuiting how they might fill in the missing parts. Another new feature called auto framing can keep a speaker centered in the view even when they move. Words can be automatically transcribed, translated, and dubbed into multiple languages. Maxine allows all of this to occur in a fraction of a second so that the audio appears in sync with the speaker. Another new feature is the ability to break up a video and better organize it with summaries, table of contents, and short-form articles. A talk can be broken down by themes and have machine-generated titles and descriptions for each section. “Humanity has long lost its ability to commit to long-form content, and by creating this AI article view, we allow the viewer to skim the content quickly in the same way you might do with a blog post,” Segal said. Segal is also excited about the potential for semantic vector search to help bring new context to content discovery. “We believe that the next generation of search and discovery will evolve to ambient streams of information that are contextualized to the task you are performing,” he said. He has been working on this problem for decades and wrote about it in 2009. Semantic vector search works more like the human associative memory system rather than traditional Boolean keyword searches. It starts by translating content into concepts into a multi-dimensional space such that closely related concepts are represented closer to each other. Video conferencing is a crowded market, but Segal believes it is still growing because the idea of what constitutes a communications platform is also expanding. Previous advances focused on better compression and noise reduction algorithms, but they didn’t do much to help people make sense of the material being communicated. Segal is excited about features that aren’t easy to see but that help make information more accessible, such as how neural networks can instantly add context and curate what we communicate to make information better and more relevant. These innovations will usher in “the age of inferences” that could increase comprehension, accessibility, and insight, Segal said."
https://venturebeat.com/2021/04/13/deeplite-raises-6-million-series-seed-to-enable-ai-for-everyday-life/,Deeplite Raises $6-million Series Seed to Enable AI for Everyday Life,"MONTREAL–(BUSINESS WIRE)–April 13, 2021– Montreal-based AI startup Deeplite Inc. today announced the closing of a $6-million seed financing round led by the Boston-based venture capital firm PJC with participation from leading AI technology venture firms and industry leaders Innospark Ventures, Differential Ventures, Ajay Shah Executive Chairman, Smart Global Holding, and included follow on investment from Somel Investments, BDC Capital and Desjardins Capital. This financing will accelerate Deeplite’s R&D development, expand the team and accelerate market expansion for its optimization software that delivers faster, smaller and more energy-efficient AI models. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413005850/en/ “We are incredibly impressed with the team, technology and market adoption of Deeplite’s software stack for Edge AI” said Rob May, General Partner at PJC and writer of the world’s most popular newsletter on Artificial Intelligence – InsideAI. “Deploying AI, particularly deep learning, on resource-constrained devices, is a broad challenge in the industry with scarce AI talent and know-how available. Deeplite’s automated software solution will create significant economic benefit as Edge AI continues to grow as a major computing paradigm.” The significant challenge when deploying deep learning models commercially is how large, processor-heavy and power-intensive they can be to operate. Deeplite solves this problem by providing an automated software engine to optimize DNN (Deep Neural Network) models and enable AI for edge computing on any device. Incubated at TandemLaunch in Montreal, Deeplite launched in 2019 when co-founders Dr. Ehsan Saboori, Davis Sawyer and Nick Romano teamed up to bring AI to Everyday Life. Since the initial release of Deeplite’s NeutrinoTM software in mid-2020, there has been tremendous demand from major OEM brands, semiconductor and application companies for Neutrino’s automated optimization engine. AI engineers can use the software within existing MLOps frameworks like PyTorch, ONNX or TensorFlow to create highly compact, energy-efficient AI models that save on cloud costs and allow new applications to run on small, battery-powered edge devices. “AI for Everyday Life is at the heart of what we are building. Deep learning is poised to bring massive benefits by way of automation, to unlock the potential to run AI on billions of microcontrollers (MCUs) in billions of devices at the point of data capture” said Deeplite CEO, Nick Romano. “We are excited to team up with PJC and this blue-chip syndicate as we enable AI to become untethered, decentralized and everywhere.” Deeplite has been named to the CB Insights AI 100 list of Top 100 privately held AI companies in the world and is also collaborating with Turing-Award winner and deep learning pioneer Yoshua Bengio at Mila/UdeM, a renowned AI research institute based in Montreal. “Accelerating time-to-market for accurate computer vision and perception AI models is fundamental to realizing the value of many diverse applications that will have a positive impact on our everyday life,” said Professor Yoshua Bengio, Scientific Director of Mila. “Addressing the challenge of running complex and sizeable deep neural networks on limited compute power is crucial, and we’re excited to support Deeplite’s unique technology strategy and the innovation resulting from this partnership.” This funding will be invested in research, development, sales and marketing to accelerate our product roadmap and go-to-market. Deeplite has key roles to fill and continues to hire in our Montreal and Toronto hubs and recruits globally as an accredited partner of Canada’s Global Talent Stream program. Major milestones ahead include a free community version coming out this spring and the release of Deeplite’s proprietary, ultra-efficient inference engine targeted for commodity CPUs and MCUs later this year. About Deeplite Based in Montreal, Canada, Deeplite is an AI software company dedicated to enabling AI for everyday life. Deeplite uses AI to automatically make other AI models faster, smaller and more energy-efficient creating highly compact, high-performance deep neural networks for deployment on edge devices such as cameras, sensors, drones, phones and vehicles. Deeplite was named to the 2020 CB Insights AI100 list of top 100 privately-held AI companies and has been featured by Gartner, Forbes, Inside AI and ARM AI as a premier Edge AI innovator. To learn more about Deeplite, visit www.deeplite.ai. About PJC PJC is an early-stage venture capital firm investing in innovative, high-growth companies that are building technology to disrupt the status quo. The firm seeks to be the first institutional capital deployed into a company and is focused on being a true partner to entrepreneurs as they drive their companies toward becoming industry leaders in their respective categories. The firm has invested in over 100 companies across North America including notable companies like Expensify, Nest, Blockfi, GetWellNetwork, Nexamp, and Yandex. PJC was founded in 2001 and is based in Boston, MA. For more information, visit www.pjc.vc.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005850/en/ Media:Anastasia Hamel – Marketing Manager, Deepliteanastasia@deeplite.ai"
https://venturebeat.com/2021/04/13/google-makes-business-process-tool-appsheet-automation-generally-available/,Google makes business process tool AppSheet Automation generally available,"Last year, Google launched AppSheet Automation, an “intent-driven” experience in Google Cloud powered by AI that enabled enterprises to connect to a number of data sources to model automated business processes. After several months in early access, Google today announced that AppSheet Automation is generally available with new capabilities, including document processing, a monitoring app, and expanded eventing support. According to Forrester, while automation has been a major force reshaping work since before the pandemic, it’s taking on a new urgency in the context of business risk and resilience. A McKinsey survey found that at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce reported that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than 4 hours saved each week per employee. AppSheet Automation, which arose from Google’s acquisition of AppSheet in January 2020, is an AI-enabled, no-code development platform designed to help automate existing business processes. The service offers an environment for building custom apps and pipelines, delivering governance capabilities and leveraging AI to understand goals and construct process artifacts.  One new feature in AppSheet Automation, Intelligent Document Processing, automatically extracts text from unstructured files like invoices and W-9s to eliminate the need for manual entry. Another, a monitoring app, allows customers to build AppSheet apps that can then monitor their automations. Google also extended AppSheet Automation’s data source eventing, which previously supported Salesforce, to include Google Workspace Sheets and Drive in the general release. Looking ahead, the company says it’s building the ability to embed rich AppSheet views in Gmail to enable users to perform approvals on the go.  “Digital transformation has been an enterprise priority for years, but recent Google Cloud research reinforces that the mandate is more pressing today than ever, with most companies increasing their technology investments over the last year,” Prithpal Bhogill, product manager on AppSheet’s business application platform, wrote in a blog post. “While there are many dependencies shaping the future of work, the challenge is to leverage technology to support shifting work cultures. Automation is the rallying point for this goal.” The launch of AppSheet Automation follows news that Google will collaborate with robotic process automation (RPA) startup Automation Anywhere to accelerate the adoption of RPA with enterprises “on a global scale.” As a part of its agreement with Automation Anywhere, Google plans to integrate the former company’s RPA technologies, including low- and no-code development tools, AI workflow builders, and API management, with Google Cloud services like Apigee, AppSheet, and AI Platform. Automation Anywhere and Google said they’ll also jointly develop solutions geared toward industry-specific use cases, with a focus on financial services, supply chains, health care and life sciences, telecommunications, retail, and the public sector."
https://venturebeat.com/2021/04/13/start-prepping-for-open-finance-if-you-want-to-gain-more-market-sharevb-live/,Start prepping for open finance — if you want to gain more market share(VB Live),"Presented by Envestnet | Yodlee Open banking is only the beginning — the open finance trend is bigger. Join this VB Live event to learn about open finance, what it means for your business, how it is revolutionizing customer experiences in insurance, utilities, telecommunications, and more. Register here for free. Historically, consumers have been at the mercy of screen-scraping, where companies can try and gain access to data, often without their knowledge. Open banking gives back control to the consumer. It’s the sanctioned, secure, reliable, and user-controlled sharing of basic financial data. A user can permission third-party financial service applications, such as accounting apps, tax apps, mortgage applications and so on, to access their data in a reliable and secure manner. The bank is the steward of the consumer’s information, but doesn’t own it. Open finance goes a step further — or the next step, says David Nohe, CEO of FinGoal, which builds analytics and infrastructure for financial brands. Open finance gives the user control of all of their financial information, including insurance data, telecom data, and utilities like gas, electric, even trash and recycling expenses. By giving access to your Geico or Allstate account through an open finance platform, a company like FinGoal gains visibility not just into how much you spend on insurance, but exactly what you get from insurance, he explains. That allows them to shop around, constantly looking to ensure the consumer has the best deal in the market to fit their needs, and isn’t over-paying for coverage or features they don’t use, Nohe says. The same is true in telecom or utilities. You’re paying a certain amount to Verizon every month, but unless you can securely share the details of the plan, you lose the opportunity to leverage an analytics platform that can tell you what kind of deal you could land at another company. “In an open finance world, companies like ours can better serve end users in comparison shopping and ensuring they have the best value, or catching when they’ve been overcharged, or when there was a fee that they might not realize happened,” Nohe says. Credit unions are embracing new ways of delivering information to their members, says Dawn Sirras, SVP fintech partnerships at Constellation Digital Partners, an open development platform for credit unions. Digital banking doesn’t need to be limited to viewing your balance, checking transactions, and making transfers. It can be a much more comprehensive, engaging, and interactive experience. “With open finance, we’re creating a hyper-personalized experience, where the member can look at a 360-view of their financial well-being, performance, and opportunities for improvement,” says Sirras. “By expanding the data set, we give the consumer the opportunity for a much more holistic view, which in turn just leads to better decision-making, and being more informed about their financial position and how they’re performing in the marketplace and in life.” There’s still work to be done, both in terms of regulation and the tech surrounding the initiative, but the groundwork is being laid right now for open finance. Large institutions have seen the writing on the wall for the future of data privacy regulation, particularly with the passing of recent laws in California and the EU confirming that user data belongs to the user. Industry pundits are betting that these laws are likely to be replicated at the federal level within the next handful of years. Smart companies need to be prepared — and be mindful of what happened to the banks in the last decade, Nohe says. Banks that were reluctant to share data, and made it harder for their customers to access their data through platforms like Mint.com, lost customers. So regardless of the size or age of your company, this open finance world is inevitable — data sharing is only going to become more powerful, and the data is only going to become richer, with higher fidelity. That means organizations need to be proactive, and start positioning themselves to take advantage of the opportunities it will bring, Nohe says. Now is the time to think about your road map, your customers today and in the future, and how access to more third-party data will enable you to serve them better, both when they’re on your platform and when they’re in third party platforms. “If you’re thinking about it in that holistic way, where you’re serving your users wherever they are, whether they’re in your interface, your applications, or someone else’s applications, you’re going to win market share and make users happy,” he says. “A lot of the industry will not go that route — they’ll be slow and stubborn to react. But it’s a ton of opportunity for folks that are thinking about the future.” Sirras agrees, urging companies to not be constrained by the past, or current processes. “We all need to be willing to look at new ways to deliver value to our customers or our members, to be more efficient, to be more effective with what we’re doing,” she says. “If that means a period of discomfort as we adjust to change, we need to be okay with that if it brings about a greater end product.” Don’t miss out! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/13/gartner-pc-sales-q1-hit-highest-growth-rate-20-years/,Gartner: PC sales in Q1 hit highest growth rate in 20 years,"(Reuters) — Global shipments of personal computers rose at their fastest pace in two decades in the first quarter as people bought computers to help them work and study remotely during the COVID-19 crisis, according to research firm Gartner. PC shipments, which include both laptops and desktop computers, grew 32% in the quarter to 69.9 million units, Gartner said. China’s Lenovo Group grabbed the lead with a 25.1% market share, followed by HP, Dell, Apple, and Acer, according to the report. “This growth should be viewed in the context of two unique factors: comparisons against a pandemic-constrained market and the current global semiconductor shortage,” said Mikako Kitagawa, research director at Gartner. “Without the shipment chaos in early 2020, this quarter’s growth may have been lower.” The coronavirus-driven surge in demand, along with an unprecedented shortage in semiconductor microchips, has strained the supply chain of personal computers. While the chip shortage was originally concentrated in the auto industry, it has now spread to a range of other consumer electronics, including smartphones, refrigerators, and microwaves. Gartner said demand for Google Chromebooks, which it does not account for in its analysis, tripled in the first quarter, thanks to strong demand from educational institutions in North America. “It is still reasonable to conclude that PC demand could remain strong even after stay-home restrictions ease,” Gartner said."
https://venturebeat.com/2021/04/13/social-yield-farming-platform-don-key-locks-2-million-from-cryptos-top-players/,Social Yield Farming Platform Don-Key Locks $2 Million From Crypto’s Top Players,"TORTOLA, British Virgin Islands–(BUSINESS WIRE)–April 13, 2021– Don-Key has completed a private funding round to bootstrap it’s Defi social yield farming platform. The $2.2 million was raised in the company’s first round from leading blockchain funds, including, Black Edge Capital, AU21. Genesis Block Ventures, Spark Digital, Solidity Ventures, MoonWhale, and Morningstar Ventures. Don-Key aims to reduce the barrier for entry for both yield farmers and liquidity providers, opening the DeFi world for those two distinct groups of people that either don’t master the skills to create strategies or hold a low volume of funds in order to participate within the DEFI world. Outsmarting Defi With Social Farming With the yield farming craze that’s taken the DeFi world by storm over the past 12 months, the number and complexity of yield farms has soared, as have the number of blockchains that now support liquidity mining. Farming is not for the faint-hearted or tech-averse however: it’s a hazardous pursuit that calls for negotiating constantly fluctuating APYs, mitigating impermanent loss, enduring smart contract risk, and frequently entering and exiting positions. On paper, yield farming sounds like a simple way to earn a passive income from your crypto holdings. In practice, executed in a perilous environment where a single mistake can prove costly. That is where Don.Key comes in: By utilizing the proven success of social trading, the platform maximizes the upside by making it easier for humble investors and farmers to generate consistent yield, whatever their ability level. This will be achieved by bringing liquidity providers and yield farmers together on the Don-Key. finance platform, where gamified social trading strategies will incentivize participants to work together and reap the spoils. Don-Key will do for yield farming what social (or “copy”) trading has done for cryptocurrency trading. Gil Shpirman, Co-Founder and CEO of Don-Key said: “We are very excited to see our vision come to life, I think that what is so special about our project is that everyone that is working on it, is also going to be a future user once we launch. That’s not something you see on every project, and I think it says a lot of what we are trying to build here.” “We are excited to be partnering with Don-Key to bring social trading to DeFi. The growth of the DeFI industry has been breathtaking, but the experience is still difficult for many new users. Don-Key addresses this with customizable strategies and ‘copy farming’ make the decision making simpler for the common user. We look forward to developing the product and growing the feature set with them. Said Leslie T, Cofounder and Partner, GBV” Key features will include: About: Don-key.finance started in 2020, at the beginning of the yield farming craze, with a very clear vision of bringing the simplicity of ‘copy trading’ to the complex world of DeFi Yield Farming. An initiative that came from real necessity, Don-key’s founders are DeFi enthusiasts from Israel, Cyprus, Ukraine and India with a true passion to democratize yield farming and helping crypto users around the world stay up to date with the constant evolution of the yield farming space.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005830/en/ Gil Shpirman, CEOGil@don-key.finance"
https://venturebeat.com/2021/04/13/informatica-modernizes-ipaas-platform-using-microservices-and-ai/,Informatica modernizes iPaaS platform using microservices and AI,"At its online Informatica World event today, Informatica announced a cloud platform that employs microservices and an AI engine to combine data management capabilities and enable data and application programming interface (API) integration. The Informatica Intelligent Data Management Cloud (IDMC) is a revamped implementation of an Informatica platform that makes more extensive use of an existing AI engine dubbed CLAIRE. This is used to analyze the metadata generated by each integration, chief product officer Jitesh Ghai said. In effect, IDMC creates a graph that makes it possible to track the relationship between various integrations for optimization and compliance purposes. “It’s a system of record for metadata,” Ghai said. That’s critical because it enables the Informatica integration platform-as-a-service (iPaaS) environment to function as a single source of truth for all integrations on an end-to-end basis, Ghai added. In all, IDMC now provides more than 200 discrete services that have been augmented using the CLAIRE AI engine, he noted. At the same time, Informatica is launching a series of services it makes available on various public clouds and rolling out updates. In addition to making IDMC available on the Microsoft Azure cloud, Informatica is adding an Informatica Cloud Data Governance & Catalog offering for the Amazon Web Services (AWS) cloud. Available in preview, it promises to make it easier to apply data governance policies to the massive amounts of data accumulating in AWS environments, Ghai noted. Finally, Informatica announced that its Cloud Data Integration Elastic (CDI-E) service for processing massive amounts of data at scale is now generally available on Google Cloud, along with a tool for managing APIs and an enhanced version of a visual tool dubbed Cloud Mass Ingestion that makes it easier to ingest data. Collectively, these capabilities are intended to address the need for a more flexible approach to integration at a time when organizations are increasingly launching complex digital business process initiatives spanning multiple applications and data sources, Ghai said. Integrations today routinely involve gigabytes of data that need to be programmatically moved between platforms, Ghai added. That level of automation requires a platform on a microservices-based architecture that can easily scale up and down to meet those requirements, he noted. The CLAIRE AI engine, meanwhile, provides the algorithms required to ensure data quality is maintained across all those integrations, Ghai added. Most legacy iPaaS platforms — like the applications they integrated — are based on monolithic architectures. Today, most new applications are being constructed using a more modular approach that’s based on microservices with their own APIs. The Informatica IDMC makes it simpler for IT teams to construct and deploy microservices that can programmatically invoke a set of integration services to access data strewn across an extended enterprise. Theoretically, a legacy iPaaS environment could service those requests, but not at the level of scale that might be required by thousands of microservices. The shift to microservices-based applications in the enterprise is just beginning, and the bulk of enterprise applications are still based on monolithic applications. But with each new application deployed, the number of microservices running in production environments steadily increases, along with the amount of data being consumed by those microservices. In addition, many monolithic applications will over time be refactored to run as a set of more modular microservices to make applications both easier to upgrade and more resilient. Rather than failing outright, a microservices-based application is designed to degrade gracefully by rerouting requests when a specific service is unavailable. It may be a while before microservices proliferate across the enterprise, but it’s now more a question of if than when. That will inevitably make IT environments more complex, which Informatica is betting will result in a lot more reliance on AI-infused iPaaS services."
https://venturebeat.com/2021/04/13/1password-expands-into-secrets-management-to-help-enterprises-secure-their-infrastructure/,1Password expands into secrets management to help enterprises secure their infrastructure,"Password-management platform 1Password is expanding into the “secrets management” space, helping developer teams across the enterprise safeguard private credentials, such as API tokens, keys, certificates, passwords, and anything used to protect access to companies’ internal applications and infrastructure. Alongside the launch, 1Password has also announced its first acquisition with the purchase of SecretHub, a Dutch startup founded in 2018 that claims to protect “nearly 5 million enterprise secrets” each month. Following the acquisition, SecretHub will be shuttered entirely, with its whole team — including CEO Marc Mackenbach — joining 1Password. Recent data from GitGuardian, a cybersecurity platform that helps companies find sensitive data hidden in public codebases, revealed a 20% rise in secrets inadvertently making their way into GitHub repositories. If this data falls into the wrong hands, it can be used to gain access to private internal systems. By way of example, Uber revealed a major breach back in 2017 that exposed millions of users’ personal data. The root cause was an AWS access key hackers discovered in a personal GitHub repository belonging to an Uber developer. There has been a flurry of activity across the secrets management space of late. Israeli startup Spectral recently exited stealth with $6.2 million in funding to serve developer operations (DevOps) teams with an automated scanner that finds potentially costly security mistakes buried in code. San Francisco-based Doppler, meanwhile, last month raised $6.5 million in a round of funding led by Alphabet’s venture capital arm GV and launched a bunch of new enterprise-focused features. 1Password has built a solid reputation over its 16-year history, thanks to a platform that can store passwords securely and simplify log-in. It allows consumers and businesses to log into all their online services with a single click (rather than having to manually input passwords) and can also be used to store other private digital data, such as credit cards and software licenses. The Toronto-based company raised its first (and only) round of funding back in 2019, securing $200 million to help it push further beyond the consumer sphere and cement itself as an integral security tool for the enterprise. Today, 1Password claims some 80,000 business customers, including enterprise heavyweights such as IBM, Slack, Dropbox, PagerDuty, and GitLab. With its latest “secrets automation” product, the company is striving to make its platform stickier for existing and potential clients searching for an all-in-one platform that protects all their credentials — from employees’ email passwords to core backend business systems. While 1Password’s existing password-management toolset helps people securely access accounts without having to remember dozens of passwords, the “automation” facet of its new product name refers to machine-based system workflows that, for example, enable an application to securely “talk” to a database. “This means being able to roll secrets into your infrastructure directly from within 1Password,” chief product officer Akshay Bhargava told VentureBeat. “We are the first company encompassing human and machine secrets.” Typically, infrastructure secrets can be splayed across countless cloud providers and services, but according to 1Password, it’s not uncommon for companies to cut corners or use a dubious combination of hacks and homegrown tools to manage the security around this issue. According to Bhargava, 1Password was working on a secrets management solution before it acquired SecretHub. In fact, many of 1Password’s customers were already storing their infrastructure secrets in its vaults. “Our customers have raised this workflow as something they’d like 1Password to solve,” Bhargava said. “It’s fair to say our first version is homegrown, and we’ve been focused on solving this problem for a while.” Secrets automation allows admins to define which people and services have access to secrets, as well as what level of access is granted. At launch, it integrates with HashiCorp Vault, Terraform, Kubernetes, and Ansible, with “more on the way.” However, 1Password is also announcing a deeper partnership with GitHub, which will see the duo collaborate to “solve problems for our shared customers and users,” according to Bhargava. “We plan to build a workflow to support customers in delivering secrets and configuration into their CI/CD pipelines on GitHub,” he said. As for costs, all companies will receive three credits for free. The cost then rises to $29 per month for 25 credits, $99 for 100 credits, and $299 for 500 credits. “We prorate based on usage,” Bhargava added. “We will work with companies needing more than 500 credits a month on an individual basis.” In terms of how credit is consumed, companies configure the 1Password vaults they want secrets automation to access and then stipulate the permissions for a development environment with tokens. “If an API client needs read and write access to data stored in a 1Password vault, that access is defined using a token,” Bhargava explained. “One token, accessing one vault, is what defines a credit. If that same API client needs to access two vaults, that then becomes two credits. And similarly, if a single token is created for read access to vault A and another for write access to vault B, that becomes two credits.”"
https://venturebeat.com/2021/04/13/robin-io-brings-pay-as-you-go-pricing-to-red-hat-private-clouds/,Robin.io brings pay-as-you-go pricing to Red Hat private clouds,"Robin.io, a leader in Kubernetes storage and data management, is bringing pay-as-you-go pricing to the Red Hat Marketplace for Robin Cloud Native Storage (CNS). Dynamic pricing models are among the most alluring features of public cloud services but not commonly seen in private clouds. But that is changing as more vendors take advantage of new metering capabilities available in private cloud services like Red Hat OpenShift. Kubernetes storage solutions have traditionally been priced using an annual licensing model, especially for on-premises deployments. The flexibility of paying for hourly consumption promises greater flexibility for businesses deploying ephemeral workloads, such as extract, transform, and load (ETL) processing; AI/ML workloads, such as data preprocessing, feature extraction, and model training; and ad-hoc data analysis on Red Hat OpenShift. “This helps customers with flexible licensing terms that can reduce costs and encourage more experimentation,” Robin.io director of product Ankur Desai told VentureBeat in an email. His team had to develop a metering integration with Red Hat to make this work securely and privately to mitigate concerns about external monitoring. The Red Hat Marketplace makes it easier for enterprises to provision cloud services for hybrid workloads that may span multiple private and public clouds. Robin CNS takes advantage of the “metering definition” as provided by the Red Hat Marketplace operator. There is no information sharing or network connection required with Robin because the Red Hat Marketplace operator collects consumption metrics for Robin CNS and passes the aggregated consumption metrics to the billing service. Robin.io installs natively on any Kubernetes distribution within minutes and creates a block and file storage solution by pooling available storage resources such as HDDs, SSDs, and cloud disks. It also automates complex storage management and data management operations on Kubernetes and provides a simple API to developers. It is also application-aware, in that it understands the scope of a stateful application on Kubernetes and wraps all relevant components — including data, metadata, and config data — into a single entity. All data management operations, such as snapshots, backups, and migration, are performed on the entire application, not just the data. This is important because many microservice applications are designed to be stateless in order to improve scalability and performance. Dynamic pricing could play an important role in the growth of storage capabilities deployed alongside more dynamic applications common with microservice and container architectures."
https://venturebeat.com/2021/04/13/aclima-adds-climate-and-environmental-justice-leaders-to-advisory-board/,Aclima adds climate and environmental justice leaders to advisory board,"Most companies add business advisors to their rosters. But Aclima’s mission is split between business and social good, and today the company announced it’s adding climate and environmental justice leaders to its advisory board. Aclima uses sensors on its own fleet of cars to measure global pollution on a block-by-block level and apply that data to combating climate change. Companies and governments can subscribe to Aclima in order to evaluate the environment and take actions to alleviate pollution. But Aclima has also declared that it’s a Public Benefit Corporation, meaning its charter is to serve the public good through business — maximizing impact, rather than just profit — CEO Davida Herzl told VentureBeat in an interview. “As a technology community, we’ve barely scratched the surface of technology’s potential to really serve to solve big problems that matter and to serve people and the planet,” Herzl said. “If you look back at 2020, it showed us the problems that we need to be solving. It’s about public health, it’s about climate change and the things that are really existential for society. And technology has a really big role to play.” She added, “One of the things that came to a head in 2020 was definitely the issue of racial and social justice. When we think of our technology, we want to be sure that we’re building it and delivering it in a way that actually advances social good for society. We are bringing those voices to the table and bringing them into our process. Our board is a brain trust that ensures that the science and technology that we’re developing is really calibrated to the needs of society.” I talked to the company last summer when California was on fire and smoke was turning our skies orange. Herzl believes the air we breathe is a critical infrastructure. Access to clean air is a human right, yet 90% of us don’t have it, she said. Systemic racial injustices have led to disproportionate environmental burdens for communities of color. And the same emissions that harm our health are changing the climate, she said. These converging crises call on each of us to pool our collective resources, energy, and ingenuity to innovate for environmental justice. Herzl said places like West Oakland are among the most polluted in the Bay Area, and it’s no accident that the area has such a high concentration of communities of color. In the U.S., Black people are 3 times more likely to die from exposure to air pollution, she said. “The data we generate highlights where technologies need to be deployed, not only to reduce emissions, but also to advance these really critical questions of environmental justice,” she said. Herzl wants the company to be a beacon for talent that cares about causes. Aclima hires interns, and Herzl noted that this crew of graduates coming out of college has not only survived a pandemic but is “looking down the barrel of a climate crisis.” And there are a growing number of companies in the “social capital” market that are doing good while making money. Herzl said her team includes a lot of talented engineers and scientists and programmers who built the network and sensors for measuring pollution. But the common thread is that they’re building technology for a good cause. The company also hires drivers for its fleet of cars in the communities that are affected by pollution, and it creates a path for those drivers to rise in the company. Aclima is dedicated to catalyzing bold climate action that protects public health, reduces emissions, and advances equity. The company works in collaboration with environmental advocates across sectors to diagnose air pollution hotspots, target emissions reductions and interventions, and measure progress. “We are bridging very different worlds with our advisory board,” Herzl said. “Our advisers are really excited to see technological innovation being applied to the problems that they have been advocating about for years.” The company’s advisory board reflects those values and includes the following new members: Aclima methodologies have been developed over a decade by leading scientists and validated by research institutions. This newly expanded advisory board will build and strengthen connections with community-engaged researchers. Herzl believes there is a sea change in climate action underway in the U.S., with the requirement for investment in environmental justice across all sectors. These new members of the Aclima Advisory Board, which was originally formed in 2015, will join existing advisors Bill Reilly, Martin Goebel, Luc Vincent, Nick Parker, Greg Niemeyer, and David Sherman."
https://venturebeat.com/2021/04/13/density-launches-occupancy-tracking-software-for-offices-raises-25m/,"Density launches occupancy-tracking software for offices, raises $25M","Density, a startup developing a range of people-counting, AI-powered sensors, today announced it has raised $25 million. Density says the capital will be used to accelerate growth and scale initiatives as it releases an addition to its software suite called Portfolio. In many respects, Density’s products were tailored for a health crisis. Cities around the world have imposed limits on businesses — particularly restaurants — regarding the number of customers they allow in. Moreover, the shift to working from home and pandemic-driven financial headwinds have companies questioning the need for physical office space. Even before the pandemic, U.S. Commercial Real Estate Services estimated unused commercial property in the U.S. to be worth about $1 trillion. In August, Capital Markets reported that direct commercial sales of real estate fell 29% globally to $321 billion in the first six months of 2020 (year-over-year). To this end, Portfolio draws on Density sensor data to show real-time, day-over-day “return to office”  insights. The company says the new benchmarking feature was designed to help real estate teams view office usage information across a collection of properties. Portfolio automatically records weekly occupancy and usage changes, enabling users to set safe maximum capacities based on local requirements. Beyond this, the software surfaces data over time to enable companies to “right-size” their workspaces. Density leverages depth-measuring hardware and an AI backend to perform crowd analytics that overcome the challenges posed by corners, hallways, doorways, and conference rooms. Clients like Pepsi, Delta, Verizon, Uber, Marriot, and ExxonMobil use its stack to figure out which parts of their offices get the most and least use and deliver people-counting metrics to hundreds and even thousands of employees. Density cofounder and CEO Andrew Farah conceived of Density’s technology while he was in graduate school at Syracuse and working at a mobile software development firm. His initial goal — to measure how busy a popular coffee shop was — led him to explore a couple of solutions before settling on the one that formed the foundation for Density’s people-counting sensors. According to Farah, Density’s tracking offers an advantage over other approaches: peace of mind. Unlike a security camera, its sensors can’t determine the gender or ethnicity of the people it tracks, nor perform invasive facial recognition. This approach also allows Density’s sensors to do occupancy detection inside of rooms where cameras can’t go for compliance reasons. Density recently launched Open Area, a sensor that uses AI and radar to track physical workspace usage. More capable than Density’s previous sensors, it detects key points on people’s bodies that the device translates in software to point clouds on a 3D graph. For instance, when positioned above a desk where people are seated, Open Area can show the rough outline of those people as they come and go. The pandemic initially hurt Density’s sales because many customers temporarily shut down. But since the close of its series B funding in June 2018 and its $51 million round in July, the company says its sensors have counted more than 150 million people in dozens of countries across hundreds of millions of square feet. Existing and previous Density backers include Kleiner Perkins, 01 Advisors, Upfront Ventures, Founders Fund, Ludlow Ventures, Launch, Disruptive, and LPC Ventures, with participation from Alex Rodriguez, Julia and Kevin Hartz, Cyan and Scott Banister, and others. This latest round brings the New York-based company’s total raised to over $100 million."
https://venturebeat.com/2021/04/13/sambanova-raises-over-600m-to-mass-produce-ai-chips-for-training-and-inference/,SambaNova raises $676M to mass-produce AI training and inference chips,"SambaNova Systems, a startup developing chips for AI workloads, today announced it has raised $676 million, valuing the company at more than $5 billion post-money. SambaNova says it plans to expand its customer base — particularly in the datacenter market — as it becomes one of the most capitalized AI companies in the world with over $1 billion raised. AI accelerators are a type of specialized hardware designed to speed up AI applications such as neural networks, deep learning, and various forms of machine learning. They focus on low-precision arithmetic or in-memory computing, which can boost the performance of large AI algorithms and lead to state-of-the-art results in natural language processing, computer vision, and other domains. That’s perhaps why they’re forecast to have a growing share of edge computing processing power, making up a projected 70% of it by 2025, according to a recent survey by Statista. SambaNova occupies a cottage industry of startups whose focus is developing infrastructure to handle AI workloads. The Palo Alto, California-based firm, which was founded in 2017 by Oracle and Sun Microsystems veteran Rodrigo Liang and Stanford professors Kunle Olukotun and Chris Ré, provides systems that run AI and data-intensive apps from the datacenter to the edge. Olukotun, who recently received the IEEE Computer Society’s Harry H. Goode Memorial Award, is leader of the Stanford Hydra Chip Multiprocessor research project, which produced a chip design that pairs four specialized processors and their caches with a shared secondary cache. Ré, an associate professor in the Department of Computer Science at Stanford University’s InfoLab, is a MacArthur genius award recipient who’s also affiliated with the Statistical Machine Learning Group, Pervasive Parallelism Lab, and Stanford AI Lab. SambaNova’s AI chips — and its customers, for that matter — remain largely under lock and key. But the company previously revealed it is developing “software-defined” devices inspired by DARPA-funded research in efficient AI processing. Leveraging a combination of algorithmic optimizations and custom board-based hardware, SambaNova claims it’s able to dramatically improve the performance and capability of most AI-imbued apps.  SambaNova’s 40-billion-transistor Cardinal SN10 RDU (Reconfigurable Dataflow Unit), which is built on TSMC’s N7 process, consists of an array of reconfigurable nodes for data, storage, and switching. It’s designed to perform in-the-loop training and allow for model reclassification and optimization on the fly during inference-with-training workloads. Each Cardinal chip has six controllers for memory, enabling 153 GB/s bandwidth, and the eight chips are connected in an all-to-all configuration. This last bit is made possible by a switching network that allows the chips to scale. SambaNova isn’t selling Cardinal on its own, but rather as a solution to be installed in a datacenter. The basic unit of SambaNova’s offering is called the DataScale SN10-8R, featuring an AMD processor paired with eight Cardinal chips and 12 terabytes of DDR4 memory, or 1.5 TB per Cardinal. SambaNova says it will customize its products based on customers’ needs, with a default set of networking and management features that SambaNova can remotely manage. The large memory capacity ostensibly gives the SN10-8R a leg up on rival hardware like Nvidia’s V100. As SambaNova VP of product Marshall Choy told the Next Platform, the Cardinal’s reconfigurable architecture can eliminate the need for things like downsampling high-resolution images to low resolution for training and inference, preserving information in the original image. The result is the ability to train models with arguably higher overall quality while eliminating the need for additional labeling. On the software side of the equation, SambaNova has its own graph optimizer and compiler, letting customers using machine learning frameworks like PyTorch and TensorFlow have their workloads recompiled for Cardinal. The company aims to support natural language, computer vision, and recommender models containing over 100 billion parameters — the parts of the model learned from historical training data — as well as a larger memory footprint allowing for hardware utilization and greater accuracy. SambaNova has competition in a market that’s anticipated to reach $91.18 billion by 2025. Hailo, a startup developing hardware to speed up AI inferencing at the edge, in March 2020 nabbed $60 million in venture capital. California-based Mythic has raised $85.2 million to develop custom in-memory compute architecture. Graphcore, a Bristol, U.K.-based startup creating chips and systems to accelerate AI workloads, has a war chest in the hundreds of millions of dollars. And Baidu’s growing AI chip unit was recently valued at $2 billion after funding.  But SambaNova says the first generation of Cardinal taped out in spring 2019, with the first samples of silicon already in customers’ servers. In fact, SambaNova had been selling to customers for over a year before this point — the only public versions are from the Department of Energy at Lawrence Livermore and Los Alamos. Lawrence Livermore integrated one of SambaNova’s systems with its Corona supercomputing cluster, primarily used for simulations of various physics phenomena. SambaNova is also the beneficiary of a market that’s seeing unprecedented — and sustained — customer demand. Surges in car and electronics purchasing at the start of the pandemic have exacerbated a growing microchip shortage. In response, U.S. President Joe Biden recently committed $180 billion to R&D for advanced computing, as well as specialized semiconductor manufacturing for AI and quantum computing, all of which have become central to the country’s national tech strategy. “We began shipping product during the pandemic and saw an acceleration of business and adoption relative to expectations,” a spokesperson told VentureBeat via email. “COVID-19 also brought a silver lining in that it has generated new use cases for us. Our tech is being used by customers for COVID-19 therapeutic and anti-viral compound research and discovery.” According to Bronis de Supinski, chief technology officer at Lawrence Livermore, SambaNova’s platform is being used to explore a technique called cognitive simulation, where AI is used to accelerate processing of portions of simulations. He claims a roughly 5 times improvement compared with GPUs running the same models. Along with the new SN10-8R product, SambaNova is set to offer two cloud-like service options: The first — SambaNova AI Platform — is a free-to-use developer cloud for research institutions with compute access to the hardware. The second — DataFlow as a Service — is for business customers that want the flexibility of the cloud without paying for the hardware. In both cases, SambaNova will handle management and updates. Softbank led SambaNova’s latest funding round, a series D. The company, which has over 300 employees, previously closed a $250 million series C round led by BlackRock and preceded by a $150 million series B spearheaded by Intel Capital."
https://venturebeat.com/2021/04/13/pegasystems-adds-enterprise-ai-tools-to-simplify-analysis-and-forecasting/,Pegasystems adds enterprise AI tools to simplify analysis and forecasting,"Pegasystems today announced it’s adding the ability to apply AI to business processes midstream to enable companies to determine whether an anticipated outcome will occur as expected. Pegasystems is also adding a feature that lets companies analyze streaming event data on platforms such as the open source Apache Kafka software that is now being widely employed to enable organizations to transfer data in near real time. Pega Process AI combines machine learning algorithms, event processing, business rules, natural language processing, and predictive analytics with low-code tools to analyze processes in real time, Pegasystems CTO Don Schuerman said. That approach makes it possible for organizations to make adjustments to processes to, for example, ensure a service level agreement (SLA) is met. As organizations invest in myriad digital business transformation initiatives, many are discovering the batch-oriented legacy applications that typically process data overnight are not well-suited to driving interactions with customers in near real time, Schuerman noted. As a result, organizations are modernizing applications using platforms such as Kafka that enable them to stream data between applications and platforms. The way legacy applications handle data winds up constraining digital processes that need to process data in real time, he added. “The shift from batch to reactive real-time processes has become table stakes,” he said. Support for event streaming will play a critical role in enabling organizations to achieve that goal. Rather than having to wait to analyze the data at rest on a cloud platform, for example, it is possible to analyze streaming event data in transit, Schuerman noted. As organizations look to infuse AI capabilities into business processes, the tensions that always exist between building a capability versus acquiring it will naturally surface. Pegasystems is making a case for an extensible platform based on AI models it creates and curates within the context of the Pega Platform. That capability makes it simpler for organizations to experiment with AI without having to hire data scientists to construct AI models using various open source toolkits. In contrast, it often takes a data science team several months to construct an AI model that may never make it into a production environment. Schuerman said that rather than simply experimenting with AI capabilities, organizations should generally work backward from a desired business outcome. That approach reduces the chances an organization will wind up investing time and resources into an AI project that never makes it into a production environment. There’s no doubt organizations of all sizes are investing in various forms of AI as part of larger digital business transformation initiatives. The challenge these organizations face is that most data scientists are not especially well-versed in how any given business process should be optimized, which can lead to a lot of trial and error. Providers of platforms such as Pegasystems make it possible for the average business analyst who knows how to work with low-code tools to apply AI to processes they know intimately. It also makes it easier for them to alter those processes, should the AI models need to be updated or start drifting toward a sub-optimal outcome. As AI becomes more democratized, the processes it can be applied to far exceed the number of data science experts that will be available anytime soon. Out of both necessity and fear, organizations are going to enable business users to at least experiment with AI before rivals apply those same capabilities. Hopefully, guardrails will ensure AI models are properly vetted so they do more good than harm."
https://venturebeat.com/2021/04/13/enterprise-security-platform-intrigue-expands-attack-surface-management-with-2m-round/,Enterprise security platform Intrigue expands attack surface management with $2M round,"Security startup Intrigue.io has raised $2 million from LiveOak Venture partners to accelerate product development for its attack surface management (ASM) platform. Intrigue developed an open source approach for discovering and investigating vulnerabilities across mobile, work-from-home, and cloud infrastructures. With even the best-equipped organizations struggling to consistently and automatically identify assets, find exposures, assess security risks, and address problems quickly, attack surface management has become an important area of concern for security teams. The massive adoption of cloud, SaaS, and mobile across a distributed workforce means organizations have an expanding, evolving, and changing attack surface. “We see attack surface management as a continuous process that security teams perform to discover intelligence about assets and exposures, direct that intelligence to the right owner in the business, and enable the business owner to mitigate the risk,” Intrigue founder and CEO Jonathan Cran told VentureBeat in an email. From a technical perspective, attack surface management involves scanning, exploring, and inventorying assets across the enterprise, partner, and third-party infrastructure to map the attack surface and then monitoring the assets to detect any changes to the configuration and exposure to known threats. The final component involves mitigating risks by addressing the vulnerability or fixing the configuration error. Traditional security tools were designed for fixed assets and have trouble tracking software-defined assets that are constantly changing. The Intrigue platform uses various open and customer-configured data sources to find assets across hosts, apps, cloud services, and user accounts. Intrigue relies on a graph database to capture assets and their security properties with more precision than would be possible with a traditional configuration management database. Intrigue Enterprise relies on non-linear mapping technology for asset discovery, workflows for automatic scoping and vulnerability control, and integrations with other network tools. Intrigue Core, an open source asset discovery project that serves as the backbone for the company’s enterprise offering, relies on discoveries by community members. For example, this week one community member introduced a capability to find a device’s internal IP address from F5 load balancers. “This kind of pivot, in combination with the multitude of vulnerability checks driven by threat [intelligence], helps our customers gain an edge on attackers,” Cran said. If the enterprise has a tool that’s good at finding open network ports, Intrigue can incorporate that technology as a data source to enrich the asset inventory. The Intrigue team is also building out integrations into various threat management tools, including CMDB, SOAR, incident management, and ticketing solutions to speed incident resolution in response to the discovery of new security threats. The new investment would help Intrigue expand beyond existing integrations such as BGP routing tables, on-demand DNS lookups, cloud providers, cloud asset repositories, historical DNS, historical Passive DNS, internet-wide scanning, reverse Whois, historical reverse Whois, social media account lookups, GitHub and GitLab repositories, and threat indicator of compromise (IoC) repositories. The latest round of funding will also help support the security and developer communities contributing to Intrigue Core. “You can’t secure what you can’t see. Intrigue goes far beyond current offerings to give enterprises visibility into their entire public-facing footprint so they can both monitor it and secure it,” said LiveOak Venture Partners principal Creighton Hicks."
https://venturebeat.com/2021/04/13/synopsys-84-of-codebases-contain-an-open-source-vulnerability/,Synopsys: 84% of codebases contain an open source vulnerability,"The number of codebases containing at least one open source vulnerability increased by nine percentage points in 2020, according to a new report from Synopsys, the silicon design company behind open source security management platform Black Duck. In the sixth Open Source Security and Risk Analysis (OSSRA) report, Synopsys said it has provided an “in-depth snapshot of open source security, compliance, licensing, and code quality risk in commercial software,” observing that of the 1,546 commercial codebases scanned by Black Duck in 2020, 84% contained at least one open source vulnerability — up from 75% in last year’s report. Most modern software relies to some degree on open source software, as it saves companies the time and resources needed to develop and maintain every component internally. Black Duck, which Synopsys bought in 2017 for $547 million, is one of several software composition analysis (SCA) platforms, with others including Sonatype, which was acquired by Vista Equity Partners in 2019; Snyk, which recently closed a $300 million round of funding; and WhiteSource, which last week raised $75 million. Companies use these platforms to identify every open source component in their stack to surface vulnerabilities and license compliance risks. And it’s these open source “audits” Synopsys and Black Duck primarily use as the basis for their annual OSSRA report. The 1,546 codebases that constituted this year’s report spanned 17 industries, including aerospace, fintech, IoT, and telecommunications, with Synopsys concluding that 98% of codebases contain open source code. This is marginally down from the 99% it reported last year, but incremental deviations are to be expected — the bottom line is that most applications continue to rely on open source components. So why would vulnerabilities be spreading at this rate? Tim Mackey, principal security strategist at the Synopsys Cybersecurity Research Center (CyRC), thinks that while there are some complexities behind the growth of vulnerabilities, for most companies the problem is essentially one of scale. “If you look at the average number of components in an application over the last three years, it’s gone from 298 to 445 and now to 528,” he told VentureBeat. “If someone designed their update and patching processes to manage 300 components per app in 2018, they probably didn’t expect usage to grow that much in two years. Then if you overlay that US-CERT (U.S. cybersecurity and infrastructure agency) reported an average of slightly more than 48 new CVEs (common vulnerabilities and exposures) each day in 2020, keeping up with patching is a huge problem.” At the heart of the problem is the vast array of open source software packages out there. A slew of tools have emerged to help developers and companies make sense of the open source world. Openbase, for example, wants to be the Yelp for open source software packages. OpenLogic’s Stack Builder, meanwhile, helps enterprises choose the right combination of open source software for their needs. And Two Sigma Ventures’ Open Source Index highlights GitHub’s most popular projects right now. But while selecting the right package is important, keeping abreast of updates is equally essential. In short, developers often struggle to keep on top of their open source stack and remember where they got their open source components from when it’s time to download patches. This is an area where companies such as Synopsys are carving their niche. The broad industry consensus is that vulnerabilities are rife within open source code, and bad actors are hell-bent on exploiting them. In its State of Software Security: Open Source Edition report last year, app security company Veracode noted that 70% of applications contained a security flaw in an open source library, while Sonatype recently reported a 430% surge in attacks targeting open source software supply chains. But not all vulnerabilities are created equal, and many offer limited scope for hackers to exploit. In an interview with VentureBeat this week, WhiteSource CEO and cofounder Rami Sass said the company’s research showed that only “15% to 30% of vulnerabilities are effective — the majority of open source vulnerabilities are not called by the proprietary code.” This means it’s important to distinguish between imminently dangerous vulnerabilities and minor flaws. With that in mind, Synopsys’s latest report found that the percentage of codebases containing high-risk open source vulnerabilities grew 11 percentage points to 60% in 2020, with “high-risk” defined as a vulnerability that has been actively exploited, has “documented proof-of-concept exploits,” or has been “classified as a remote code execution vulnerability.” Moreover, several of the top 10 open source vulnerabilities identified in the 2019 report not only reared their heads again in 2020 but showed sizable percentage increases — this, according to Mackey, was the biggest surprise the company saw in its audit. “Normally, we’d expect to see exposure to any given CVE decline over time,” he said. “After all, once a vulnerability is reported, most teams will want to apply the patch.” The top two vulnerabilities were related to jQuery, and both demonstrated double-digit year-on-year growth.  Away from the vulnerability sphere, the latest OSSRA report found that the number of codebases containing open source license conflicts fell marginally year-on-year from 67% to 65%, with nearly three-quarters of these related to a GNU General Public License. Meanwhile, 26% of the codebases used open source with either no license or a customized license. This is important because customized open source licenses often need to be evaluated for potential IP issues or legal uncertainties. Elsewhere, the report showed that 91% of codebases contained open source dependencies with zero development activity in the past two years, up from 88% the previous year. This might not be a problem, but it means the vast majority of codebases, according to Synopsys audits, contain an open source dependency with no recent new features, enhancements, or — more importantly — security fixes. What does this all mean? For one thing, software — open source or otherwise — can become vulnerable if nobody is at the wheel. This is why the Linux Foundation set up the The Core Infrastructure Initiative (CII), with backing from tech heavyweights such as Amazon, Google, Microsoft, Cisco, IBM, and Intel, to support open source projects that are critical to the internet and related devices and systems. But it also means enterprise-focused commercial companies can monetize open source projects with the promise of added features and (enhanced) security. And companies such as Synopsys, WhiteSource, Snyk, and Sonatype can build billion-dollar businesses by helping developer teams keep on top of their open source stack and ensuring major flaws are addressed quickly."
https://venturebeat.com/2021/04/13/waylay-announces-waylay-io-the-first-low-code-developer-friendly-data-automation-and-orchestration-platform/,"Waylay Announces Waylay IO, The First Low-code Developer-friendly Data Automation and Orchestration Platform"," Waylay IO holds the promise of coding at a fast pace, with minimal setup effort and guaranteeing quick deployment  GHENT, Belgium–(BUSINESS WIRE)–April 13, 2021– Waylay, a leading enterprise automation software company at the forefront of digital transformation has today announced Waylay IO, its new product offering for developers. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413005106/en/ Waylay IO is a low-code development platform that reduces the complexity of the application development process. Its automation technology uses small snippets of code, reusable across different use cases, and orchestrated by the powerful rules engine. The automation platform offers necessary tools in one place and helps developers to experiment with data and create insightful new applications and business models quicker than ever before. “The basic building blocks of our low-code platform are small snippets of code that can be reused, just like lego bricks,” said Veselin Pizurica, Founder & CTO of Waylay. “Serverless is the perfect environment for this low-code lego brick approach and Waylay IO guarantees an extremely powerful rules engine to orchestrate these code snippets. Waylay is on the mission to ease the complexity of serverless and liberate developers from getting bogged down into actions that have nothing to do with the problems they ought to solve. The Waylay IO automation concept is so powerful that it is revolutionary. Our first user feedback is unanimously positive and I was impressed to see how fluent and quick developers get up and running and can be more experimental and creative with data without losing focus or valuable time.” Waylay IO is built on top of open-source alternatives, without dependencies on any specific cloud provider. It allows easy integration of API-enabled services and provides excellent debugging and observability capabilities. The platform has built-in state-of-the-art security models similar to Auth0. In short, the Waylay IO platform is a premade automation stack where API gateways, multitenancy, lambdas, databases, and all other services are embedded and pre-installed. Coders only need to code – no hassle, fast, efficient, simple.Waylay provides the Waylay Academy with training and demo material, use cases and templates to kickstart ideas. The support forum and engaged community create the perfect setting to interact with peers and Waylay experts. “Industry analysts (*) point out that there are two impediments hampering the serverless adoption,” said Leonard Donnelly, CEO of Waylay. “The first one is related to architecture complexity: tracing, observability, debugging, deployment, and the second one is related to fear of losing control, fear of vendor lock-in, fear of weak security, unpredictable cost, etc. Waylay IO is designed to solve these problems.” The Waylay IO pricing model is based on consumption, users only pay what they use.During the launch period, new users will receive a $200 coupon at registration: www.waylay.io/io (*) O’Reilly serverless survey: Concerns, what works, and what to expect ABOUT WAYLAY Waylay is a leading enterprise IT-OT digital unification software company delivering low-code orchestration, automation and analytics software solutions. Waylay has a passion for supporting citizen developer communities and ensuring it puts all valuable data to work for developers, data scientists and domain experts. Most importantly, Waylay guarantees its customers a significant reduction in cost and time to market new digital transformation projects to eventually make their enterprises become one.Waylay offers 3 products: Waylay IO for developers and SMEs, Waylay Enterprise, and Waylay Digital Twin for Salesforce. Find out more at www.waylay.io  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005106/en/ PRESS CONTACT Elly SchietseCMO, WaylayElly.schietse@waylay.io Tel +32 479 761825"
https://venturebeat.com/2021/04/13/how-it-leaders-stay-on-the-right-side-of-innovation-with-open-cloud-vb-live/,How IT leaders stay on the right side of innovation with open cloud (VB Live),"Presented by Supermicro With cloud computing accelerating worldwide, companies need to determine what best fits their needs in compute, storage, networking, and the software stack. Join this VB Live event to learn how open clouds reduce latencies, improve innovation, and more. Register here for free. “Thinking about openness is the key to being an IT leader in the cloud computing space,” says Michael McNerney, vice president of marketing and network security at Supermicro. “And that means building your cloud to take advantage of performance and innovation not just today, but in 18 months, 36 months, and on and on.” Cloud infrastructure has become a catch-all for modern datacenter architecture, and the open cloud has been leveraging the innovation in open-source software and industry-standard hardware over the last decade, McNerney says. And that innovation continues at a rapid clip. Moore’s law, that every 18 months hardware will double in performance at the same cost, is not slowing, and is perhaps even accelerating, when you include CPU, memory, storage, and I/O. “The key for IT leaders right now is to make sure they stay on the right side of innovation, and live on that lower cost, better performance curve that the cloud has been driving over time,” he says, “And stay able to leverage rapid improvements to deliver new and more products and services.” But there are a lot of solutions out there, and determining what works for your technology, your industry, and for your company overall means there are a huge number of variables to take into consideration. The key, McNerneyy says, is nailing down your basic business strategy – everything from what your decision points are and how you compete in the market to what’s your value add, and what your competitive landscape will look like in the future “You want to optimize both your software and your hardware for optimal performance and efficiency – the days of the same general-purpose server being used for every application are gone,” he says. “Every workload requires an optimized platform.” For AI, you need high-throughput systems that can manage the high-power consumption of the CPU/GPU solutions. Storage requires a spectrum of offerings from all-flash NVMe boxes for highly responsive applications to scale-out 60/90 bay storage systems for archival. And the same is true in CPUs now. Intel’s newest processor has better instructions per clock and more cores, but the major performance enhancements came from accelerators for specific workloads, like crypto, networking, speed select for CSPs, and more. And from a cost and innovation perspective, the traditional lock-in model that IT departments have embraced in the past is prohibitive. “That’s why we come back to the open cloud,” he says. “When you keep your environment open, you can control costs, leverage multiple vendors, and optimize by taking advantage of the new and best technology without worrying about being locked in to a specific vendor.” The industry has shown again and again that both open standards and open source drives innovation, McNerney says. With open source, technology shared with a community of people who are able to contribute to its development drives quality and innovation. When multiple parties can contribute to a specific implementation without locking you into their implementation you can draw from a rich pool of tools and applications. Open standard interfaces mean that there is a standard layer to build below or build on top of.  That shared standard interface layer is the key to innovation. He points to the evolution of storage: all-flash NVMe drives delivered an order of magnitude better performance than traditional media. But the storage interface didn’t change immediately; NVMe drives supported traditional protocols and applications would get the benefit simply by changing the drive. The open standard interface meant over time the software could be adjusted to maximize the NVMe protocol and deliver even better application performance. Open standards across the features of the cloud allows innovation at every level and allows companies to take advantage of an ecosystem of vendors and solutions for every problem or project. In the end, the most important decision to make is to build openness and Moore’s Law into your cloud strategy. “For openness, make decisions that keep your options open in a way that allows you to control costs, allow substitutions, and drive competitive rivalry in your suppliers,” he says. “For Moores’ Law, ask yourself, what would I do with twice the compute performance or half the costs? Which projects are stalled today that could provide competitive advantage in 18 months when the performance is ready?” To learn more about the benefits of open cloud, and how to ensure you nail the three main components for success – the right hardware, software stack, and network choice – don’t miss this VB Live event! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/13/grid-ai-launches-platform-for-training-ai-models-on-the-cloud-at-scale/,Grid.ai launches platform for training AI models on the cloud at scale,"Grid.ai today announced the general availability of Grid, a new platform that enables researchers and data scientists to train AI models in the cloud. The company says that Grid enables development and training “at scale” without requiring advanced skills in machine learning engineering. Machine learning practitioners can run into challenges when scaling AI workloads because of the infrastructure needed to train and deploy into production. Moreover, this infrastructure can be expensive to maintain. A Synced study estimated that the University of Washington’s Grover fake news detection model cost $25,000 to train in about two weeks and OpenAI reportedly racked up a whopping $12 million to train its GPT-3 language model. At a high level, Grid offers an interface for training models on GPUs, processors, and more. It’s a web app with a command line interface that optimizes datasets to work at the level needed for production and “cutting-edge” research. Grid computes in real time to make it easier to quantify the R&D efforts of AI projects. Additionally, the platform provides access to Jupyter notebooks, a way for data scientists to bundle their answers with the Python code that produced it. To use Grid, users simply clone a project from their GitHub repositories and make minor code changes. Data and artifacts stay on their infrastructure — Grid only orchestrates. And experiment artifacts, like weights and logs, are automatically saved to the cloud so that they can be accessed during or after a training run. Grid will be available starting April 13 in three plans: individual, team, and enterprise. The individual plan is the cost of the cloud processing plus 20%, while the team plan is $1,000 a month in addition to the cloud costs and 15%. The launch of Grid comes after its namesake company, Grid.ai, emerged from stealth in October 2018 with $18.6 million in venture capital backing. Cofounded by William Falcon, the creator of PyTorch Lightning, one of the fastest-growing machine learning frameworks in the world, Grid.ai’s mission is to reduce the distance between deep learning research and its practice in real-life businesses. Grid leverages PyTorch Lightning to decouple the code required to define a full deep learning model from the code required to run it on hardware. Falcon says the main goal is to help companies focus on delivering value instead of worrying about the machines they’re running or clusters. “Think back to when electricity came about. Only a few had access to it until the power grid made it available to everyone,” Falcon told VentureBeat via email. “That’s the goal of Grid.ai, to democratize access by removing the need to be an expert engineer. ML practitioners should be focused on delivering value through models and data, not on becoming expert engineers.”"
https://venturebeat.com/2021/04/13/nvidia-now-expects-q1-sales-to-pass-5-3b-forecast/,Nvidia now expects Q1 sales to pass $5.3B forecast,"(Reuters) — Nvidia’s first-quarter revenue will be above its earlier forecast of $5.3 billion, the chipmaker said on Monday, as it benefits from strong demand for chips that power datacenters and cryptocurrency mining. The company’s shares, which rose on the back of a slew of product announcements during the session, were trading nearly 6% higher. Demand for Nvidia’s gaming graphic chips has soared during the COVID-19 pandemic, but the bigger boost to its sales has come from an aggressive push into artificial intelligence chips, which handle tasks such as speech and image recognition in datacenters. “While our fiscal 2022 first quarter is not yet complete, Q1 total revenue is tracking above the $5.30 billion outlook,” chief financial officer Colette Kress said in a statement. Nvidia’s unit that supports cryptocurrency mining is now expected to report sales of $150 million in the quarter, up from the previous forecast of $50 million, the company said. Nvidia’s upbeat forecast comes as the chip industry continues to reel from a global shortage of components, disrupting the wide-scale manufacturing of products, including vehicles and smartphones. “We expect demand to continue to exceed supply for much of this year,” Kress said. Earlier on Monday, Nvidia said it was planning to make a server processor chip based on technology from Arm, putting it in the most direct competition yet with rival Intel. Reporting by Munsif Vengattil in Bengaluru, editing by Ramakrishnan M. and Anil D’Silva."
https://venturebeat.com/2021/04/13/lexon-breaks-company-record-with-6-wins-at-the-red-dot-design-award-2021/,Lexon Breaks Company Record with 6 Wins at the Red Dot Design Award 2021," The French brand turns 30 and proves its continuous leadership in innovative product design  PARIS–(BUSINESS WIRE)–April 13, 2021– Lexon today announced that it has received 6 Red Dot Awards for its outstanding product design quality in multiple categories, setting a new company record for the number of awards won in a single competition. Sister company MyKronoz also takes home a Red Dot Award for its MyScale, a smart and design body scale with a large and intuitive color display. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210413005520/en/ This announcement comes only a few weeks after Lexon’s best-selling Oblio was honored with two prestigious international prizes – a TIME Best Invention Award and a CES Innovation Award – confirming the French brand’s continued leadership in innovative product design, with a total of more than 200 awards received since its creation in 1991. “2021 marks the 30th anniversary of Lexon, so this exceptional recognition comes at a perfect timing. We couldn’t be happier to celebrate this milestone with such paramount achievement which proves our consistent ability to create objects that combine aesthetics and function, but most importantly, that are part of our daily lives. 2021 is also synonymous with the global crisis, one that inevitably led us to adapt our business to new realities, to rethink our products portfolio and to focus even more on designing solutions that truly meet our changing environments, whether it’s for the home, the work or the new mobility. Today, these distinctions not only celebrate our resilience, they also validate the quality of the experience we strive to bring to our customers in all circumstances,” says Boris Brault, Lexon’s CEO. Since its acquisition by BOW Group in 2018, Lexon has experienced significant online growth and staggering expansion in the US where the brand is today listed among the most prominent retailers including Target, Nordstrom, Neiman Marcus, Best Buy and MoMA Design Store. In the coming weeks, Lexon will pursue its international development by completing the takeover of its long-term Chinese Master License who has built the brand domestically for the past 10 years, opening more than 80 retail stores, developing online sales on JD.com and Taobao, as well as growing profitable B2B activities. The successful integration of Lexon among BOW Group has created highly valuable synergies with sister company MyKronoz, in terms of product design, R&D capabilities, sales channels and digitalization. BOW Group has thereby proven its ability to acquire external companies within its manufacturing, selling and distribution structures to scale its business operations. Today, the Group is looking at new opportunities to further enlarge its portfolio with product offering and services, in order to become a leading 360° platform of lifestyle consumer tech. The Lexon award-winning products include: Oblio: A must-have innovation for today’s world, Oblio is a sleek UV-C sanitizer that prevents the spread of harmful bacteria found on our smartphones. Using its built-in UV-C LED technology located on its front interior, Oblio can deliver a 360° disinfection by simply flipping the phone to expose each surface for a 20-minute sanitizing cycle. Oblio’s efficiency has been proven through lab testing to kill 99.9% of viruses present on the surface of smartphones, including H1N1. Also acting as a 10W wireless charger, Oblio can fully charge a smartphone in 3 hours. Its discrete vase-like shape makes it a beautiful design item that can be proudly displayed in a home or office, encouraging a break from your screen while your phone is being fast-charged and deeply sanitized. Compatible with all mobile phones for sanitizing function, and all Qi-enabled smartphones such as the latest iPhone & Android devices for the wireless charging function. MSRP: 79.90 $/€ Mino T: Made for exploring, Mino T is an outdoor speaker designed with a sleek carabiner to clip to your bag or bike. Boasting an IPX4 water resistance combined with an ultra-robust finish, it is ready to follow you in all your adventures. Carefully engineered with a passive bass system, Mino T allows you to enjoy a more balanced, fuller sound on the go with up to 5h of continuous playtime. Built with a considered design in mind, this speaker is also a stand-alone modern piece that you can proudly display indoors, on its matching stand. MSRP: 49.90 $/€ Mina M: With its compact size, statement-making aluminum base and water-resistant casing, the Mina M is the perfect way to inspire any space, indoor and out. Mina M brings an impressive 9 LED light color range to your bedside, workspace, or patio. The color change and dimming function are all controlled by a press down feature on the top of the light. Mina M is rechargeable using any Qi-enabled wireless charging station. You can also charge Mina M by using a USB-C charging cable. The Mina M has up to 24h of battery life from a single charge, making it the perfect companion all day and night. MSRP: 49.90 $/€ Mino+: This mini portable Bluetooth® speaker fits in the palm of your hand to deliver an impressive 3W sound quality for up to 3 hours! Mino+ offers two charging methods: wirelessly with any Qi charging pad or via its USB-C port. When connected to your phone, you can use Mino+ to take selfies, start and stop recording videos, and answer calls using the bottom button. Thanks to its built-in TWS technology, you can connect two Mino+ together and double the pleasure with an immersive stereo sound. Mino+ comes in 3 finishes (aluminum, chrome and glossy) and 24 colors to bring a stylish touch to your decor. MSRP: 29.90 $/€ These four products have been designed in collaboration with Manuela Simonelli & Andrea Quaglio. Powersound: No connector, no cable: PowerSound is a revolutionary 2-in-1 device that combines a 5000 mAh wireless power bank with a 360° Bluetooth speaker, while charging and recharging itself entirely wirelessly. Capable of charging all Qi-enabled smartphones or earbuds, it can be recharged by setting it on top of any wireless station. Thanks to its 5W built-in Bluetooth® speaker, PowerSound allows you to enjoy an immersive sound for up to 20 hours. Easy to carry and ultra-stylish, PowerSound has a smooth design crafted from durable synthetic leather and fabric. MSRP: 79.90 $/€ C-Pen: The C-Pen offers a unique 2-in-1 writing and storage experience. A ballpoint pen with a 32 GB USB-C memory drive thoughtfully engineered into the pencap. 32 GB of memory means you can transfer and store up to 4000 pictures, 7000 songs, or 40 hours of video. Compatible with computers, tablets, and smartphones with a USB-C port, the C-Pen is a handy way to transfer memories or notes between all your devices. The must have accessory for the modern on-the-go person, available in 6 colors in a glossy or matt finish and working with a universal refill. MSRP:39.90 $/€ These two products have been designed in collaboration with Alain Berteau. About the Red Dot Award: The Red Dot Award – Product Design, whose origins date back to 1955, appraises the best products created every year. In roughly 50 categories, manufacturers and designers can enter their innovations in the competition. According to the motto “In search of good design and innovation”, the jury evaluates the entries and only awards a Red Dot to products that win them over with their high design quality. About Lexon: Since its creation in 1991, Lexon has relentlessly pushed the limits and created a difference in the world of design while remaining true to its commitment to make small objects useful, beautiful, innovative and affordable. Whether in electronics, audio, travel accessories, office or leisure, Lexon has established a special relationship with creativity and partnered with the best designers around the world to create timeless collections of lifestyle products. Following its recent acquisition by BOW Group, a global player in the lifestyle and wearable consumer markets, Lexon is writing a new chapter in its history, experiencing a staggering international growth and digital expansion. Today, with 30 years of existence, more than 200 awards, collaboration with some of the most renowned designers, a retail presence in more than 90 countries across the Globe, Lexon has truly established itself as a worldly-known French design brand.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210413005520/en/ MediaAnnabel Corlaypress@lexon-design.com"
https://venturebeat.com/2021/04/12/cloudera-partners-with-nvidia-expand-gpu-usage-across-ai-applications/,Cloudera partners with Nvidia to expand GPU usage across AI applications,"Cloudera and Nvidia announced a collaboration that will allow organizations to use GPUs in more areas across the AI development lifecycle. Cloudera will integrate its Cloudera Data Platform with Nvidia’s accelerated Apache Spark 3.0 libraries. The integration will make it easier to add machine learning workflows to processes and create architectures without requiring GPU customization. Enterprises will be able to make changes to their data science workflows without having to also update the Nvidia integration manually. GPUs have shown tremendous promise in enhancing the data science side of AI development, enabling enterprises to run some types of workloads on top of GPUs. However, analytics often involve processes that span multiple teams, forcing enterprises to invest in customizing GPU integrations for those use cases. Gartner has predicted that creating new architecture patterns that help operationalize data science and ML pipelines will be one of the major trends in 2021. The partnership will allow enterprises to use GPUs across modern data workflows that span data preparation, data science, and analytics tasks. The typical workflow includes many steps including data ingestion, data curation, data pipeline automation, data science exploration, model development, testing, deployment, model monitoring and retraining, and delivery into the business. Cloudera has been busy in making these processes and the handoffs between them much easier over the last year. The Apache Spark 3.0 libraries are accelerated using Nvidia’s RAPIDS platform, which will dramatically accelerate much of the boring prep work required to bring new machine learning models into production. For example, the US Internal Revenue Service is already seeing a threefold improvement in data science workflows for fraud detection, said Joe Ansaldi, IRS technical branch chief for the Research Applied Analytics & Statistics Division, in a statement. Speeding up data preparation tasks and training models faster will save on infrastructure costs as well. GPU-accelerated Apache Spark 3 runs natively on CDP and can plug into high performance compute tools, Cloudera said. Cloudera was a trailblazer in the development of data lakes built on top of the Hadoop platform. Cloudera merged with Hortonworks, another Hadoop vendor, in 2018 and combined the technologies into a modern architecture called the Cloudera Data Platform (CDP). At the time, many speculated this spelled the end of Hadoop data warehouses, but Cloudera has continued to innovate and extend Hadoop into a more nimble workflow. Cloudera added Applied ML Prototypes (AMPs), a framework for packaging AI and ML models for data scientists, to CDP earlier this year. AMPs allow teams to take the guesswork out of ML projects with prebuilt business application templates for specific use cases, and they often run on Nvidia GPU hardware. Cloudera Data Engineering (CDE) streamlines the data engineering and prep work at the start of a project. This solved common problems data engineers face, such as scheduling and orchestration of complex data, troubleshooting and performance tuning tools for data flows, and improving collaboration with analytic and data science teams. The RAPIDS Accelerator for Apache Spark will be available in CDP Private Cloud this summer. Nvidia and Cloudera will roll out additional accelerated offerings in CDP over time, starting with Accelerated Deep Learning and Machine Learning in CDP Public Cloud in May. “This means that no matter where customers require these GPUs (from on-prem to public cloud, to hybrid cloud and beyond), they’ll be able to leverage best-in-class GPUs out of the box,” said Santiago Giraldo, Cloudera director of product marketing for data engineering and machine learning."
https://venturebeat.com/2021/04/12/why-any-developer-can-and-should-join-the-esports-boom-vb-live/,Why any developer can — and should — join the esports boom (VB Live),"Presented by Xsolla The global esports audience is 495 million strong and growing. And it’s surprisingly easy for developers and publishers to add tournaments, event streaming, and VIP content to their own platform to engage that fanbase. Learn how to tap into the opportunity in this VB Live event. Register here for free. For the past six years, esports global viewership has been booming, with no signs of stopping. At this rate, the worldwide audience is expected to grow to nearly 650 million by 2023, with revenue expected to surpass $1.5 billion. “Esports is one of the best ways for game developers to create a fandom and a following for their game, which in turn produces monetization opportunities,” says Berkley Egenes, vice president of marketing – game commerce at Xsolla. The esports opportunity isn’t just game sales. The audience for competitive, skill-based games is growing, and they’re becoming increasingly mainstream: It’s not just gaming, it’s media, pop culture, and commerce too. And developers benefit not just from the size of their player base, but from the esports athletes who are the force of personality at the center of these games. They act as ambassadors, often streaming content on their own wildly popular channels, and encouraging their fans to follow along with the in-game action, which generates more fans and more players of the game itself. Egenes points to the incredible success of Fortnite, a free-to-play game turned esport. It gained major traction on the streaming channels early on, and then developers embraced the idea of moving into esports . The 2019 Fortnite World Cup had the biggest payout at the time, $30 million. The numbers that Fortnite was seeing just continued to grow and grow, along with many of the top esports franchises like Call of Duty, League of Legends, DOTA2, Overwatch, and more. To tap into the global boom, Egenes recommends partnering with platforms like FACEIT, ESL, and PGL which enable developers to host tournaments and offer competitive gameplay opportunities. These events attract everyone from pros to casual gamers, and generate massive viewership channels for fans to follow along — very similar to the way fans can follow the real-world sports of the NFL or the Premier League, and so on. While a large enterprise game can develop its own platform, if you’re new to esports, platforms like FACEIT help you with everything from hosting and management to marketing the event. FACEIT has 20 million+ players around the world. Developers that work with the platform are getting their competitive gameplay tournaments in front of those millions of fans. And by partnering with an established platform with a secure payment provider, a developer can create a subscription program and greatly expand their payment methods on a global scale, Egenes says. “With the global scale of our payment platform, we’re able to help partners reach more fans and create more player engagement than was possible before,” he says. “Now players can compete in CS:GO, Hearthstone, PUBG, DOTA, Valorant, and other games through matchmaking tournaments, providing skill-based tracking, and anti-cheat detection – benefits that all of these subscribers are looking for.” Players want to participate, entering tournaments in which they can cover themselves in glory and potential prize money, but to do so, they need to hand out personal information. Safety and security is a priority for these players and for the fans tuning in. “Pro esports athletes just want to focus on the game – they want to avoid external perils when they’re going up against other players from across the world,” Egenes says. “If they can do it in a safe, secure environment on a platform like FACEIT, it encourages much higher probability of players getting involved.” But the biggest question is, when is the right time to launch an esports strategy? “It’s sexy and cool right now, but to jump in as your game is initially coming out, you need to do a lot of testing,” he says. “You need to talk to your community and see: one, is it something they’re interested in, and two, is there viability for your game to be an esport on a long-term basis. You don’t want to be a flash in the pan.” The opportunity to get your game on the big screen, across Twitch, and talked about online is something that needs to be considered in any competitive game launch, he adds. And your strategy should be considered from a long-term perspective, even as the boom continues beyond the pandemic. “It’s going to be exciting as we move from 100% online to hybrid tournaments and ultimately back to 100% live and in person again,” he says. “The point when we know we’re back in live esports is when we can put fans back in an arena. I’m optimistic and thinking we’re getting close.” Don’t miss out! Register here for free. Attendees will learn about: Speakers:"
https://venturebeat.com/2021/04/12/some-fda-approved-ai-medical-devices-are-not-adequately-evaluated-stanford-study-says/,"Some FDA-approved AI medical devices are not ‘adequately’ evaluated, Stanford study says","Some AI-powered medical devices approved by the U.S. Food and Drug Administration (FDA) are vulnerable to data shifts and bias against underrepresented patients. That’s according to a Stanford study published in Nature Medicine last week, which found that even as AI becomes embedded in more medical devices — the FDA approved over 65 AI devices last year — the accuracy of these algorithms isn’t necessarily being rigorously studied. Although the academic community has begun developing guidelines for AI clinical trials, there aren’t established practices for evaluating commercial algorithms. In the U.S., the FDA is responsible for approving AI-powered medical devices, and the agency regularly releases information on these devices including performance data. The coauthors of the Stanford research created a database of FDA-approved medical AI devices and analyzed how each was tested before it gained approval. Almost all of the AI-powered devices — 126 out of 130 — approved by the FDA between January 2015 and December 2020 underwent only retrospective studies at their submission, according to the researchers. And none of the 54 approved high-risk devices were evaluated by prospective studies, meaning test data was collected before the devices were approved rather than concurrent with their deployment. The coauthors argue that prospective studies are necessary, particularly for AI medical devices, because in-the-field usage can deviate from the intended use. For example, most computer-aided diagnostic devices are designed to be decision-support tools rather than primary diagnostic tools. A prospective study might reveal that clinicians are misusing a device for diagnosis, leading to outcomes that differ from what would be expected. There’s evidence to suggest that these deviations can lead to errors. Tracking by the Pennsylvania Patient Safety Authority in Harrisburg found that from January 2016 to December 2017, EHR systems were responsible for 775 problems during laboratory testing in the state, with human-computer interactions responsible for 54.7% of events and the remaining 45.3% caused by a computer. Furthermore, a draft U.S. government report issued in 2018 found that clinicians not uncommonly miss alerts — some AI-informed — ranging from minor issues about drug interactions to those that pose considerable risks. The Stanford researchers also found a lack of patient diversity in the tests conducted on FDA-approved devices. Among the 130 devices, 93 didn’t undergo a multisite assessment, while 4 were tested at only one site and 8 devices in only two sites. And the reports for 59 devices didn’t mention the sample size of the studies. Of the 71 device studies that had this information, the median size was 300, and just 17 device studies considered how the algorithm might perform on different patient groups. Partly due to a reticence to release code, datasets, and techniques, much of the data used today to train AI algorithms for diagnosing diseases might perpetuate inequalities, previous studies have shown. A team of U.K. scientists found that almost all eye disease datasets come from patients in North America, Europe, and China, meaning eye disease-diagnosing algorithms are less certain to work well for racial groups from underrepresented countries. In another study, researchers from the University of Toronto, the Vector Institute, and MIT showed that widely used chest X-ray datasets encode racial, gender, and socioeconomic bias. Beyond basic dataset challenges, models lacking sufficient peer review can encounter unforeseen roadblocks when deployed in the real world. Scientists at Harvard found that algorithms trained to recognize and classify CT scans could become biased toward scan formats from certain CT machine manufacturers. Meanwhile, a Google-published whitepaper revealed challenges in implementing an eye disease-predicting system in Thailand hospitals, including issues with scan accuracy. And studies conducted by companies like Babylon Health, a well-funded telemedicine startup that claims to be able to triage a range of diseases from text messages, have been repeatedly called into question. The coauthors of the Stanford study argue that information about the number of sites in an evaluation must be “consistently reported” in order for clinicians, researchers, and patients to make informed judgments about the reliability of a given AI medical device. Multisite evaluations are important for understanding algorithmic bias and reliability, they say, and can help in accounting for variations in equipment, technician standards, image storage formats, demographic makeup, and disease prevalence. “Evaluating the performance of AI devices in multiple clinical sites is important for ensuring that the algorithms perform well across representative populations,” the coauthors wrote. “Encouraging prospective studies with comparison to standard of care reduces the risk of harmful overfitting and more accurately captures true clinical outcomes. Postmarket surveillance of AI devices is also needed for understanding and measurement of unintended outcomes and biases that are not detected in prospective, multicenter trial.”"
https://venturebeat.com/2021/04/12/nvidia-launches-tao-an-enterprise-workflow-for-ai-development/,"Nvidia launches TAO, an enterprise workflow for AI development","During its GTC 2021 virtual keynote, Nvidia introduced a new product designed to help enterprises choose, adapt, and deploy machine learning models. Called TAO and available starting today in early access, it enables transfer learning as well as other machine learning techniques from a single, enterprise-focused pane of glass. Transfer learning’s ability to store knowledge gained while solving a problem and apply it to a related problem has attracted considerable attention in the enterprise. Using it, a data scientist can take an open source model like BERT, for example, which is designed to understand generic language, and refine it at the margins to comprehend the jargon employees use to describe IT issues. TAO integrates Nvidia’s Transfer Learning Toolkit to leverage small datasets, giving models a custom fit without the cost, time, and massive corpora required to build and train models from scratch. TAO also incorporates federated learning, which lets different machines securely collaborate to refine a model for the highest accuracy. Users can share components of models while ensuring datasets remain inside each company’s datacenter. In machine learning, federated learning entails training algorithms across client devices that hold data samples without exchanging those samples. A centralized server might be used to orchestrate rounds of training for the algorithm and act as a reference clock, or the arrangement might be peer-to-peer. Regardless, local algorithms are trained on local data samples and the weights — the learnable parameters of the algorithms — are exchanged between the algorithms at some frequency to generate a global model. TAO also incorporates Nvidia TensorRT, which dials a model’s mathematical coordinates to a balance of the smallest model size with the highest accuracy for the system it’ll run on. Nvidia claims that TensorRT-based apps perform up to 40 times faster than CPU-only platforms during inference. Elements of TAO are already in use in warehouses, in retail, in hospitals, and on the factory floor, Nvidia claims. Users include companies like Accenture, BMW and Siemens Industrial. “AI is the most powerful new technology of our time, but it’s been a force that’s hard to harness for many enterprises — until now. Many companies lack the specialized skills, access to large datasets or accelerated computing that deep learning requires. Others are realizing the benefits of AI and want to spread them quickly across more products and services,” Adel El Hallak, director of product management for NGC at Nvidia, wrote in a blog post. “TAO … can quickly tailor and deploy an application using multiple AI models.” The benefits of AI and machine learning can feel intangible at times, but surveys show this hasn’t deterred enterprises from adopting the technology in droves. Business use of AI grew a whopping 270% from 2015 to 2019, according to Gartner, while Deloitte says 62% of respondents for its corporate October 2018 report deployed some form of AI, up from 53% a year ago. Bolstered by this growth, Grand View Research predicts that the global AI market size will reach $733.7 billion by 2027."
https://venturebeat.com/2021/04/12/nvidia-unveils-rental-model-for-dgx-superpod-mini-supercomputers/,Nvidia unveils rental model for DGX Station A100 mini supercomputers,"The day has come when you can rent your own mini supercomputer. When you’re done with it, you can return it to Nvidia. That’s the plan for the company’s new cloud-native supercomputer, the Nvidia DGX Station A100. Nvidia made the announcement during the keynote speech at its GTC 2021 event. You can use the supercomputer for a short period of time when you need it and then return it after you’re done, said Manuvir Das, head of enterprise computing at Nvidia, in a press briefing. The DGX Station is a multi-tenant supercomputer that can be shared by as many as 28 data scientists. Also announced at GTC is a new Nvidia DGX SuperPod that will be available with Nvidia’s Bluefield-2 DPUs, enabling a cloud-native supercomputer. A DGX SuperPod consists of a bunch of individual DGX Station computers. “You can think of us progressing in two directions, one with constant innovation to raise the bar. But the other is to really democratize AI to put it in the hands of as many companies and scientists as we possibly can,” Das said. “We are also announcing for the first time a rental model. Instead of procuring a DGX Station, customers will be able to rent a station directly from Nvidia at a low monthly price point, and they can use it for as long as they choose and then return it to Nvidia. So that’s an important direction that we are taking with the Station.” This opens the world of AI to more enterprise customers as they investigate areas such as AI, drug discovery, autonomous vehicles, and more. Those DPUs can offload, accelerate, and isolate users’ data — providing customers with secure connections to their AI infrastructure. Nvidia DGX SuperPod will be available with Nvidia’s Bluefield-2 DPUs, enabling a cloud-native supercomputer. It’s a full bare-metal supercomputer that is also sharable. In a keynote speech at GTC 21, Nvidia CEO Jensen Huang said the company will focus on three kinds of chips: DPUs, central processing units (CPUs) like Grace, and graphics processing units (GPUs). It will alternate between Arm-based and x86-based products, he said. “We have to make AI easier to use,” Huang said. This is the first time Nvidia is turning to a rental model for DGX Stations. The idea is to broaden AI adoption for enterprise IT departments that need to support the work of teams in multiple locations or to help academic and research institutions, which often have to grant outside organizations access to their computers. DGX Stations start at $149,000, while the DGX SuperPod starts at $7 million and scales to $60 million. The company also announced NVIDIA Base Command, which enables multiple users and IT teams to securely access, share, and operate their DGX SuperPod infrastructure. Base Command coordinates AI training and operations on DGX SuperPod infrastructure to enable the work of teams of data scientists and developers located around the globe. The DGX SuperPods are AI supercomputers featuring 20 or more Nvidia DGX A100 systems and Nvidia InfiniBand HDR networking. Among the latest to deploy DGX SuperPODs to power new AI solutions and services is Sony, which uses the DGX SuperPod for its corporate research team to infuse AI across the company. Other customers are Naver, Recursion, MTS, and VinAI. Additionally, Nvidia and Schrödinger today separately announced a strategic partnership designed to harness DGX SuperPods to further accelerate drug discovery at a supercomputing scale. Nvidia also introduced a subscription offering for the DGX Station A100. The new subscription program makes it easier for companies at every stage of growth to accelerate AI development outside the datacenter for teams working in corporate offices, research facilities, labs, and home offices. The DGX A100 is a collection of DGXs working together as one cluster of infrastructure to produce a supercomputer, Das said. Each box is capable of 2.5 teraflops of computing power. Cloud-native, multi-tenant Nvidia DGX SuperPods will be available in Q2 through Nvidia’s global partners."
https://venturebeat.com/2021/04/12/mozilla-winds-down-deepspeech-development-announces-grant-program/,"Mozilla winds down DeepSpeech development, announces grant program","In 2017, Mozilla launched DeepSpeech, an initiative incubated within the machine learning team at Mozilla Research focused on open sourcing an automatic speech recognition model. Over the next four years, the DeepSpeech team released newer versions of the model capable of transcribing lectures, phone conversations, television programs, radio shows, and other live streams with “human accuracy.” But in the coming months, Mozilla plans to cease development and maintenance of DeepSpeech as the company transitions into an advisory role, which will include the launch of a grant program to fund a number of initiatives demonstrating applications for DeepSpeech. DeepSpeech isn’t the only open source project of its kind, but it’s among the most mature. Modeled after research papers published by Baidu, the model is an end-to-end trainable, character-level architecture that can transcribe audio in a range of languages. One of Mozilla’s major aims was to achieve a transcription word error rate of lower than 10%, and the newest versions of the pretrained English-language model achieve that aim, averaging around a 7.5% word error rate. It’s Mozilla’s belief that DeepSpeech has reached the point where the next step is to work on building applications. To this end, the company plans to transition the project to “people and organizations” interested in furthering “use-case-based explorations.” Mozilla says it’s streamlined the continuous integration processes for getting DeepSpeech up and running with minimal dependencies. And as the company cleans up the documentation and prepares to stop Mozilla staff upkeep of the codebase, Mozilla says it’ll publish a toolkit to help people, researchers, companies, and any other interested parties use DeepSpeech to build voice-based solutions. Mozilla’s work on DeepSpeech began in late 2017, with the goal of developing a model that gets audio features — speech — as input and outputs characters directly. The team hoped to design a system that could be trained using Google’s TensorFlow framework via supervised learning, where the model learns to infer patterns from datasets of labeled speech. The latest DeepSpeech model contains tens of millions parameters, or the parts of the model that are learned from historical training data. The Mozilla Research team started training it with a single computer running four Titan X Pascal GPUs but eventually migrated it to two servers with 8 Titan XPs each. In the project’s early days, training a high-performing model took about a week. In the years that followed, Mozilla worked to shrink the DeepSpeech model while boosting its performance and remaining below the 10% error rate target. The English-language model shrank from 188MB to 47MB and memory consumption dropped by 22 times. In December 2019, the team managed to get DeepSpeech running “faster than real time” on a single core of a Raspberry Pi 4.  Mozilla initially trained DeepSpeech using freely available datasets like TED-LIUM and LibriSpeech as well as paid corpora like Fisher and Switchboard, but these proved to be insufficient. So the team reached out to public TV and radio stations, language study departments in universities, and others they thought might have labeled speech data to share. Through this effort, they were able to more than double the amount of training data for the English-language DeepSpeech model. Inspired by these data collection efforts, the Mozilla Research team collaborated with Mozilla’s Open Innovation team to launch the Common Voice project, which seeks to collect and validate speech contributions from volunteers. Common Voice consists not only of voice snippets but of voluntarily contributed metadata useful for training speech engines, like speakers’ ages, sex, and accents. It’s also grown to include dataset target segments for specific purposes and use cases, like the digits “zero” through “nine” and the words “yes,” ” no,” ” hey,” and ” Firefox.” Today, Common Voice is one of the largest multi-language public domain voice corpora in the world, with more than 9,000 hours of voice data in 60 different languages including widely spoken languages and less-used ones,  like Welsh and Kinyarwanda. Over 164,000 people have contributed to the dataset to date. To support the project’s growth, Nvidia today announced that it would invest $1.5 million in Common Voice to engage more communities and volunteers and support the hiring of new staff. Common Voice will now operate under the umbrella of the Mozilla Foundation as part of its initiatives focused on making AI more trustworthy. As it winds down the development of DeepSpeech, Mozilla says its forthcoming grant program will prioritize projects that contribute to the core technology while also showcasing its potential to “empower and enrich” areas that may not otherwise have a viable route toward speech-based interaction. More details will be announced in May, when Mozilla publishes a playbook to guide people on how to use DeepSpeech’s codebase as a starting point for voice-powered applications. “We’re seeing mature open source speech engines emerge. However, there is still an important gap in the ecosystem: speech engines — open and closed — don’t work for vast swaths of the world’s languages, accents, and speech patterns,” Mark Surman, executive director of the Mozilla Foundation, told VentureBeat via email. “For billions of internet users, voice-enabled technologies simply aren’t usable. Mozilla has decided to focus its efforts this side of the equation, making voice technology inclusive and accessible. That means investing in voice data sets rather than our own speech engine. We’re doubling down on Common Voice, an open source dataset that focuses on languages and accents not currently represented in the voice tech ecosystem. Common Voice data can be used to feed [open speech] frameworks … and in turn to allow more people in more places to access voice technology. We’re [also] working closely with Nvidia to match up these two sides of the inclusive voice tech equation.”"
https://venturebeat.com/2021/04/12/nvidia-debuts-drive-atlan-system-on-chip-for-autonomous-vehicles/,Nvidia debuts Drive Atlan system-on-chip for autonomous vehicles,"At GTC 2021, Nividia this morning unveiled Drive Atlan, an AI-enabled system-on-a-chip for autonomous vehicles that will target automakers’ 2025 models. Capable of delivering just over 1,000 trillion operations per second (TOPs), Atlan includes Nvidia’s next-generation GPU architecture, ARM CPU cores, and deep learning and computer vision accelerators. An MIT report estimates truly autonomous vehicles might not hit the streets for a decade. Still, some experts predict the pandemic will hasten the adoption of autonomous transportation technologies. Despite needing disinfection, driverless cars can potentially minimize the risk of spreading disease. Allied Market Research predicts that the the global autonomous vehicle market will be valued at $556.67 billion by 2026. Atlan, which integrates an Nvidia BlueField data processing unit for networking, storage, and security functions, is 4 to 33 times as powerful in terms of compute as Nvidia’s other autonomous driving-focused chips, Drive Xavier and Drive Orin. Xavier can reach up to 30 TOPs, while Orin tops out at 254. Nvidia CEO Jensen Huang says Atlan is designed to handle the large number of AI applications that run simultaneously in driverless vehicles.  “Our new Atlan is truly a technical marvel, fusing all of Nvidia’s strengths in AI, auto, robotics, safety and BlueField-secure datacenters to deliver safe, autonomous-driving fleets,” Huang said in a press release. “The transportation industry needs a computing platform that it can rely on for decades. The software investment is too immense to repeat for each car.” Nvidia also announced it would expand its partnership with Volvo to use Orin in upcoming Volvo models. The first car featuring the chip will be the XC90, scheduled to be revealed next year, which will leverage software developed by Volvo’s Zenseact subsidiary, as well as backup systems for steering and braking. A Xavier-powered server will manage core functionalities inside the car, like energy management and driver assistance, while Orin will handle vision and lidar sensor data processing. Nvidia and Volvo began collaborating in 2018, when Volvo agreed to use Xavier for the computers in cars built on its scalable product architecture (SPA2) platform. SPA2 ships with an autonomous driving feature, Highway Pilot, that can be activated in supported locations and conditions. Nvidia also launched its newest Hyperion platform, which includes sensors, high-performance compute, and software for level 4 autonomous vehicle development. The 8th-gen Hyperion employs two Orin chips to process data from 12 exterior cameras, three interior cameras, nine radars, and one lidar sensor. Calibrated for 3D data collection and compatibility with Nvidia’s existing Drive AV and Drive IX software stacks, the platform supports real-time record and capture for driving data processing. The 8th-gen Hyperion will be available later in 2021."
https://venturebeat.com/2021/04/12/nvidia-launches-jarvis-conversational-ai-framework-in-general-availability/,Nvidia launches Jarvis conversational AI framework in general availability,"At its GTC 2021, Nvidia this morning announced the general availability of its Jarvis framework, which provides developers with pretrained AI models and software tools to create interactive conversational experiences. Nvidia says that Jarvis models, which first became available in May 2020 in preview, offer automatic speech recognition, as well as language understanding, real-time language translations, and text-to-speech capabilities for conversational agents. The ubiquity of smartphones and messaging apps — spurred by the pandemic — have contributed to the increased adoption of conversational technologies. Fifty-six percent of companies told Accenture in a survey that conversational bots and other experiences are driving disruption in their industry. And a Twilio study showed 9 out of 10 consumers would like the option to use messaging to contact a business. Leveraging GPU acceleration, Jarvis’ pipeline can be run in under 100 milliseconds and deploy in the cloud, in a datacenter, or at the edge. The framework includes models trained on over 1 billion pages of text and over 60,000 hours of speech that can be adjusted, optimized, fine-tuned with custom data, and tailored to different tasks, industries, and systems.  T-Mobile is among Jarvis’ early users, and Jarvis — which supports five languages including English, Chinese, and Japanese — has racked up more than 45,000 downloads since becoming available early last year. According to Nvidia, the telecom giant is using the framework to help resolve customer service issues in real time. Even before the pandemic, autonomous agents were on the way to becoming the rule rather than the exception, partly because consumers prefer it that way. According to research published last year by Vonage subsidiary NewVoiceMedia, 25% of people prefer to have their queries handled by a chatbot or other self-service alternative. And Salesforce says roughly 69% of consumers choose chatbots for quick communication with brands. Nvidia also announced that it’s partnering with Mozilla Common Voice, an open source collection of voice data for startups, researchers, and developers to train voice-enabled apps, services, and devices. The world’s largest multi-language public domain voice dataset, Common Voice contains over 9,000 total hours of contributed voice data in 60 different languages. Nvidia says it’s using Jarvis to develop pretrained models with the dataset that it will then offer to the community for free. “We launched Common Voice to teach machines how real people speak in their unique languages, accents, and speech patterns,” Mozilla executive director Mark Surman said in a press release. “Nvidia and Mozilla have a common vision of democratizing voice technology — and ensuring that it reflects the rich diversity of people and voices that make up the internet.” Newly revealed features in Jarvis will be released in the second quarter of 2021 as part of Nvidia’s ongoing open beta program. Developers can download the framework today from Nvidia’s NGC catalog."
https://venturebeat.com/2021/04/12/nvidia-announces-morpheus-an-ai-powered-app-framework-for-cybersecurity/,"Nvidia announces Morpheus, an AI-powered app framework for cybersecurity","During its GTC 2021 virtual keynote this morning, Nvidia announced Morpheus, a “cloud-native” app framework aimed at providing cybersecurity partners with AI skills that can be used to detect and mitigate cybersecurity attacks. Using machine learning, Morpheus identifies, captures, and acts on threats and anomalies, including leaks of sensitive data, phishing attempts, and malware. Morpheus is available in preview from today, and developers can apply for early access on Nvidia’s landing page. Reflecting the pace of adoption, the AI in cybersecurity market will reach $38.2 billion in value by 2026, Markets and Markets projects. That’s up from $8.8 billion in 2019, representing a compound annual growth rate of around 23.3%. Just last week, a study from MIT Technology Review Insights and Darktrace found that 96% of execs at large enterprises are considering adopting “defensive AI” against cyberattacks. Morpheus essentially enables compute nodes in networks to serve as cyberdefense sensors — Nvidia says its newly announced BlueField-3 data processing units can be specifically configured for this purpose. With Morpheus, organizations can analyze packets without information replication, leveraging real-time telemetry and policy enforcement, as well as data processing at the edge. Thanks to AI, Morpheus can ostensibly analyze more security data than conventional cybersecurity app frameworks without sacrificing cost or performance.  Developers can create their own Morpheus skills using deep learning models, and Nvidia says “leading” hardware, software, and cybersecurity solutions providers are working to optimize and integrate datacenter security offerings with Morpheus, including Aria Cybersecurity Solutions, Cloudflare, F5, Fortinet, Guardicore Canonical, Red Hat, and VMware. Morpheus is also optimized to run on a number of Nvidia-certified systems from Atos, Dell, Gigabyte, H3C, HPE, Inspur, Lenovo, QCT, and Supermicro. Businesses are increasingly placing their faith in defensive AI like Morpheus to combat the growing number of cyberthreats. Known as an autonomous response, defensive AI can interrupt in-progress attacks without affecting day-to-day business. For example, given a strain of ransomware an enterprise hasn’t encountered in the past, defensive AI can identify the novel and abnormal patterns of behavior and stop the ransomware even if it isn’t associated with publicly known compromise indicators (e.g., blacklisted command-and-control domains or malware file hashes). According to the above-mentioned MIT and Darktrace survey, 44% of executives are assessing AI-enabled security systems and 38% are deploying autonomous response technology. This agrees with findings from Statista. In a 2019 analysis, the firm reported that around 80% of executives in the telecommunications industry believe their organization wouldn’t be able to respond to cyberattacks without AI."
https://venturebeat.com/2021/04/12/nvidia-unveils-grace-arm-based-cpu-for-giant-scale-ai-and-hpc-apps/,Nvidia unveils Grace ARM-based CPU for giant-scale AI and HPC apps,"Nvidia unveiled its Grace processor today. It’s an ARM-based central processing unit (CPU) for giant-scale artificial intelligence and high-performance computing applications. It’s Nvidia’s first datacenter CPU, purpose-built for applications that are operating on a giant scale, Nvidia CEO Jensen Huang said in a keynote speech at Nvidia’s GTC 2021 event. Grace delivers 10 times the performance leap for systems training giant AI models, using energy-efficient ARM cores. And Nvidia said the Swiss Supercomputing Center and the U.S. Department of Energy’s Los Alamos National Laboratory will be the first to use Grace, which is named for Grace Hopper, who pioneered computer programming in the 1950s. The CPU is expected to be available in early 2023. “Grace is a breakthrough CPU. It’s purpose-built for accelerated computing applications of giant scale for AI and HPC,” said Paresh Kharya, senior director of product management and marketing at Nvidia, in a press briefing. Huang said, “It’s the world’s first CPU designed for terabyte scale computing.” The CPU is the result of more than 10,000 engineering years of work. Nvidia said the chip will address the computing requirements for the world’s most advanced applications — including natural language processing, recommender systems, and AI supercomputing — that analyze
enormous datasets requiring both ultra-fast compute performance and massive memory. Grace combines energy-efficient ARM CPU cores with an innovative low-power memory subsystem to deliver high performance with great efficiency. The chip will use a future ARM core dubbed Neoverse. “Leading-edge AI and data science are pushing today’s computer architecture beyond its limits — processing unthinkable amounts of data,” Huang said in his speech. “Using licensed ARM IP, Nvidia has designed Grace as a CPU specifically for giant-scale AI and HPC. Coupled with the GPU and DPU, Grace gives us the third foundational technology for computing and the ability to re-architect the datacenter to advance AI. Nvidia is now a three-chip company.” Grace is a highly specialized processor targeting workloads such as training next-generation NLP models that have more than 1 trillion parameters. When tightly coupled with Nvidia GPUs, a Grace-based system will deliver 10 times faster performance than today’s Nvidia DGX-based systems, which run on x86 CPUs. In a press briefing, someone asked if Nvidia will compete with x86 chips from Intel and AMD. Kharya said, “We are not competing with x86 … we continue to work very well with x86 CPUs.” Grace is designed for AI and HPC applications, but Nvidia isn’t disclosing additional information about where Grace will be used today. Nvidia also declined to disclose the number of transistors in the Grace chip. Nvidia is introducing Grace as the volume of data and size of AI models grow exponentially. Today’s largest AI models include billions of parameters and are doubling every two and a half months. Training them requires a new CPU that can be tightly coupled with a GPU to eliminate system bottlenecks. “The biggest announcement of GTC 21 was Grace, a tightly integrated CPU for over a trillion parameter AI models,” said Patrick Moorhead, an analyst at Moor Insights & Strategies. “It’s hard to address those with classic x86 CPUs and GPUs connected over PCIe. Grace is focused on IO and memory bandwidth, shares main memory with the GPU and shouldn’t be confused with general purpose datacenter CPUs from AMD or Intel.” Underlying Grace’s performance is 4th-gen Nvidia NVLink interconnect technology, which provides 900 gigabyte-per-second connections between Grace and Nvidia graphics processing units (GPUs) to enable 30 times higher aggregate bandwidth compared to today’s leading servers. Grace will also utilize an innovative LPDDR5x memory subsystem that will deliver twice the bandwidth and 10 times better energy efficiency compared with DDR4 memory. In addition, the new architecture provides unified cache coherence with a single memory address space, combining system and HBM GPU memory to simplify programmability. “The Grace platform and its Arm CPU is a big new step for Nvidia,” said Kevin Krewell, an analyst at Tirias Research, in an email. “The new design of one custom CPU attached to the GPU with coherent NVlinks is Nvidia’s new design to scale to ultra-large AI models that now take days to run. The key to Grace is that using the custom Arm CPU, it will be possible to scale to large LPDDR5 DRAM arrays far larger than possible with high-bandwidth memory directly attached to the GPUs.” Grace will power the world’s fastest supercomputer for the Swiss organization. Dubbed Alps, the machine will feature 20 exaflops of AI processing. (This refers to the amount of computing available for AI applications.) That’s about 7 times more computation than is available with the 2.8-exaflop Nvidia Seline supercomputer, the leading AI supercomputer today. HP Enterprise will be building the Alps system. Alps will work on problems in areas ranging from climate and weather to materials sciences, astrophysics, computational fluid dynamics, life sciences, molecular dynamics, quantum chemistry, and particle physics, as well as domains like economics and social sciences, and will come online in 2023. Alps will do quantum chemistry and physics calculations for the Hadron collider, as well as weather models. “This is a very balanced architecture with Grace and a future Nvidia GPU, which we have not announced yet, to enable breakthrough research on a wide range of fields,” Kharya said. Meanwhile, Nvidia also said that it would make its graphics chips available with Amazon Web Services’ Graviton2 ARM-based CPU for datacenters for cloud computing. With Grace, Nvidia will embark on a mult-year pattern of creating graphics processing units, CPUs, and data processing units (CPUs), and it will alternate between Arm and x86 architecture designs, Huang said."
https://venturebeat.com/2021/04/12/nvidia-announces-bluefield-3-dpus-for-ai-and-analytics-workloads/,Nvidia announces BlueField-3 DPUs for AI and analytics workloads,"At GTC 2021, Nvidia this morning took the wraps off of the BlueField-3 data processing unit (DPU), the latest in its lineup of datacenter machines built for AI and analytics workloads. BlueField-3 packs software-defined networking, storage, and cybersecurity acceleration capabilities, offering what Nvidia claims is the equivalent of up to 300 CPU cores of horsepower — or 1.5 TOPs. As of 2019, the adoption rate of big data analytics stood at 52.5% among organizations, with a further 38% intending to use the technology in the future, according to Statista. The advantages are obvious. A 2019 survey by Enterprenuer.com found that enterprises implementing big data analytics have seen a profit increase of 8% to 10%. Nvidia’s BlueField-3 DPUs features 300GbE/NDR interconnects and can deliver up to 10 times the compute of the previous-generation BlueField-2 DPUs, with 22 billion transistors, while isolating apps from the control and management plane. The 16 ARM A78 cores inside can manage 4 times the cryptography performance, and BlueField-3 is the first DPU to support fifth-generation PCIe and time-synchronized datacenter acceleration.  BlueField-3 can additionally act as a monitoring agent for Morpheus, Nvidia’s AI-enabled cloud cybersecurity platform that was also announced today. Moreover, it takes advantage of DOCA, the company’s datacenter-on-a-chip architecture for building software-defined, hardware-accelerated networking, storage, security, and management apps running on BlueField DPUs. BlueField-3 is expected to sample in the first quarter of 2022. It’s fully backward-compatible with BlueField-2, Nvidia says. “Modern hyperscale clouds are driving a fundamental new architecture for data centers,” Nvidia founder and CEO Jensen Huang said in a press release. “A new type of processor, designed to process data center infrastructure software, is needed to offload and accelerate the tremendous compute load of virtualization, networking, storage, security and other cloud-native AI services. The time for BlueField DPU has come.” Nvidia’s datacenter business, which includes its DPU segment, is fast becoming a major revenue driver for the company. In February, it posted record quarterly revenue of $1.9 billion, up 97% from a year ago. Full-year datacenter revenue jumped 124%, to $6.7 billion."
https://venturebeat.com/2021/04/12/nvidia-reveals-omniverse-enterprise-for-simulating-products-and-worlds/,Nvidia reveals Omniverse Enterprise for simulating products and worlds,"Nvidia has announced its Omniverse, a virtual environment the company describes as a “metaverse” for engineers, will be available as an enterprise service later this year. CEO Jensen Huang showed a demo of the Omniverse, where engineers can work on designs in a virtual environment, as part of the keynote talk at Nvidia’s GPU Technology Conference, a virtual event being held online this week. I also moderated a panel on the plumbing for the metaverse with a number of enterprise participants. Huang said that the Omniverse is built on Nvidia’s entire body of work, letting people simulated shared virtual 3D worlds that obey the worlds of physics. “The science fiction metaverse is near,” he said in a keynote speech. “One of the most important parts of Omniverse is that it obeys the laws of physics.” The Omniverse is a virtual tool that allows engineers to collaborate. It was inspired by the science fiction concept of the metaverse, the universe of virtual worlds that are all interconnected, like in novels such as Snow Crash and Ready Player One. The project started years ago as a proprietary Nvidia project called Holodeck, named after the virtual reality simulation in Star Trek. But it morphed into a more ambitious industry-wide effort based on the plumbing made possible by the Universal Scene Description (USD) technology Pixar developed for making its movies. Nvidia has spent years and hundreds of millions of dollars on the project, said Richard Kerris, Nvidia media and entertainment general manager, in a press briefing. Omniverse debuted in beta form in December. More than 17,000 users have tested it since then, and now the company is making the Omniverse available as a subscription service for enterprises. It’s just the kind of thing that engineers need during the pandemic to work on complex projects remotely. BMW Group, Ericsson, Foster + Partners, and  WPP are using Omniverse. It has application support from Bentley Systems, Adobe, Autodesk, Epic Games, ESRI, Graphisoft, Trimble, Robert McNeel & Associates, Blender, Marvelous Designer, Reallusion, and Wrnch. And support comes from the likes of Asus, Boxx Technologies, Cisco, Dell Technologies, HP, Lenovo, and Supermicro. More than 400 enterprises are going to use the new version for enterprises starting this summer. It comes with enterprise support for fully established enterprises, Kerris said. The Omniverse, which was previously available only in early access mode, enables photorealistic 3D simulation and collaboration. It’s a metaverse that obeys the laws of physics, and so it enables companies and individuals to simulate things from the real world that can’t be tested easily in the real world, like self-driving cars, which can be dangerous to pedestrians if they aren’t perfected. Mattias Wikenmalm, technical specialist at Volvo, said on the panel that it’s necessary to simulate not just the car but the context around the car, like a city environment. “The foundation is still the data, and this is the first time we can be data native, where we don’t have to focus on moving data between different systems. In this case, data is a first-class citizen,” Wikenmalm said. “It’s so nice we can just focus on the data and borrow our data for different applications and transform that data. Exchanging data between systems has been complex. If we can get that out of the way, we can start building a proper metaverse.” BMW is using Omniverse to simulate a full car factory before it builds it. And there’s no limit to the testing. If someone wanted to create an entire city, or even build a simulation of the entire United States, for a self-driving car testing ground, it would be possible. It is intended for tens of millions of designers, engineers, architects, and other creators to use at the same time. The designers can work on the same parts of their designs at the same times without overwriting each other, with changes offered as options for others to accept. That makes it ideal for large teams to work together. Susanna Holt, vice president of engineering for Autodesk, said on the panel that being able to understand someone else’s data is important, and it means you don’t have to be locked into a single tool or workflow. “We need the bits to talk to one another, and that’s been so hard until now,” she said. “It is still hard, as you have to import and export data. With USD, it’s the beginning of a new future.” The Omniverse uses Nvidia’s RTX 3D simulation tech to enable engineers to do things like work on a car’s design inside a simulation while virtually walking around it or sitting inside it and interacting with it in real time. Martha Tsigkari, partner at architectural firm Foster + Partners, said on the panel that the architecture and construction industries really need the ability to transfer data easily from one site to the next. “Being able to do that in an easy way without having to think about how we change that information is really important,” Tsigkari said. “In order to run really difficult simulations, or understand how buildings perform, we need to use all kinds of software to do this. Working in these processes right now can be painful, and we need to create all of these bespoke tools to do this. A future where this becomes a seamless process and opens to all kinds of industries is a fantastic opportunity that we need to grasp and go for.” Engineers on remote teams will be able to work alongside architects, 3D animators, and other people working on 3D buildings simultaneously, as if they were jointly editing a Google Doc, Kerris said. He added, “The Omniverse was built for our own needs in development.” Pixar’s Universal Scene Description (USD) is the HTML of 3D, and it’s the foundation for sharing different kinds of images from multiple parties in Omniverse, said Kerris. “We felt that with the entire community starting to move towards this open platform for exchanging 3D information including the objects, scenes, materials and everything, it was the best place for us to start with the foundation for what this platform would become,” Kerris said. Pixar’s USD standard came from over a decade of film production. Guido Quaroni is director of engineering and 3D immersive at Adobe, and before that he was at Pixar, where he was responsible for open sourcing USD. In a panel at GTC, he said the idea emerged at Pixar in 2010 as the company was dealing with multiple libraries that dealt with large scenes in its movies. “Some of the ideas in USD go back 20 years to Toy Story 2, but the idea was to formalize it and write it in a way that we could eventually open source it,” Quaroni said. He worked with Sebastian “Spiff” Grassia, head of the team that built USD at Pixar. “We knew that every studio kind of had something like it,” Quaroni said. “And we wanted to see if we could offer something that became the standard, because for us, the biggest problem was the plugins and integrations with third parties. Why not give it to the world?” The problem that they had was that they needed to be able, at any point in the film pipeline, to extract an asset, to massage it with a third-party tool, and to stick it back into the production process without losing information, said Michael Kass, distinguished engineer at Nvidia and software architect of the Omniverse, in an interview. Grassia said USD is an interchange format for data. “It represents decades of Pixar’s experience in building software that supports collaborative filmmaking,” Grassia said. “It’s for collaborative authoring and viewing for a very large 3D scene. It handles combining, assembling, overriding, and animating the assets that you have created in a non-destructive way. That allows for multiple artists to work on the same scene concurrently.” Before USD, artists had to check out a piece of digital art, work on it, and check it back in. With USD, Nvidia has enabled sharing across all applications and different ways of viewing the art. The changes are transmitted back and forth. A large number of people can view and work on the same thing, Kass said. A feature dubbed Nucleus serves as a traffic cop that communicates what is changing in a 3D scene. Early on, Pixar tried to create tools itself, but it found there were tools like Maya, 3D Studio Max, Unreal Engine, or Blender that were more advanced at doing particular tasks. And rather than have to train those vendors to continuously update their tools, Pixar made USD available as an open standard. The platform also uses Nvidia technology, such as real-time photorealistic rendering, physics, materials, and interactive workflows between industry-leading 3D software products. Pixar built a renderer, a data visualization engine dubbed Hydra. It was designed in a way to hook up other data sources, like a Maya image. So the artists can work with large datasets without having the vendor translate everything into their own native representation. Kass and his colleagues at Nvidia found that USD was a “golden nugget” that let them represent data in a way that could be used for all sorts of different purposes. “We decided to put USD at the center of our virtual worlds, but at Pixar, most of the collaboration was not real time. So we added on top of USD the ability to synchronize with different users,” Kass said. The real test has been making sure that USD can be useful beyond the media and entertainment applications. Omniverse enables collaboration and simulation that could become essential for Nvidia customers working in robotics, automotive, architecture, engineering, construction, and manufacturing. “There really isn’t anything else like it,” Kerris said. “Pixar built the standard, and we saw the potential in it. This is a demand and a need that everybody has. Can you imagine the internet without a standard way of describing a web page? It used to be that way. With 3D, no two applications use the same language today. That needs to change, or else we really can’t build the metaverse.” Nvidia extended USD, which was built for Pixar’s needs, and added what is necessary for the metaverse, Kass said. “We got to stand on top of giants, but we are pushing it forward in a direction they weren’t envisioning when they started,” he added. Nvidia built a tool called Omniverse Create, which accelerates scene composition and allows users in real time to interactively assemble, light, simulate, and render scenes. It also built Omniverse View, which powers seamless collaborative design and visualization of architectural and engineering projects with photorealistic rendering. Nvidia RTX Virtual Workstation software gives collaborators the freedom to run their graphics-intensive 3D applications from anywhere. Omniverse Enterprise is a new platform that includes the Nvidia Omniverse Nucleus server, which manages the database shared among clients, and Nvidia Omniverse Connectors, which are plug-ins to industry-leading design applications. With all of the applications working live, artists don’t have to go through a laborious exporting or importing process. “Omniverse is an important tool for industrial design — especially with human-robot interactions,” said Kevin Krewell, an analyst at Tirias Research, in an email. “Simulation is a big new market for GPU cloud services.” The Omniverse and USD aren’t going to lead to the metaverse overnight. Tsigkari said that getting so many creative industries to work together has been a huge challenge, particularly for architecture firms that have to pull so many different disciplines to get work done from conception to completion. “You need a way to allow for the creative people to quickly pass things directly from engineers to consultants so they can do their analysis and pass it on to the manufacturers,” she said. “In the simplest way, this doesn’t exist.” At the same time, different industries work on different timetables, from long cycles to real time. “For us, this has been really crucial to be able to do this in a seamless way where you don’t have to think about the in-between space,” she said. Holt at Autodesk said she would like to see USD progress forward in dealing with huge datasets, on the level of modeling cities for construction purposes. “It’s not up to that yet,” she said. “Some changes would be needed as we take it into other areas like construction.” Grassia said there are features that allow of “lazy loading,” or different levels of detail becoming visible as a huge dataset loads. Lori Hufford, vice president of applications integration at Bentley Systems, said on a panel her team has had good results so far working on large models. “I’m really excited about the open nature of USD,” she said. “We’ve been very impressed with the scale we have been able to achieve with USD.” The enterprise version will support Windows and Linux machines, and it is coming later this year. What can you do in this engineer’s metaverse? You can simulate the creation of robots through a tool dubbed Isaac. That lets engineers create variations of robots and see how they would work with realistic physics, so they can simulate what a robot would do in the real world by first making the robot in a virtual world. There are also Omniverse Connectors, which are plugins that connect third-party tools to the platform. That allows the Omniverse to be customized for different vertical markets. BMW is using Omniverse to simulate the exact details of a car factory, simulating a complete physical space. The company calls the factory a “digital twin.” The factory has enough detail to include 300 cars in it at a given time, and each car has about 10 gigabytes of data. Thousands of planners, product engineers, facility managers, and lean experts within the global production network are able to collaborate in a single virtual environment to design, plan, engineer, simulate, and optimize extremely complex manufacturing systems before a factory is actually built or a new product is integrated. Milan Nedeljkovic, member of the board of management of BMW AG, said in a statement that the innovations will lead to a planning process that is 30% more efficient than before. Eventually, Omniverse will enable BMW to simulate all 31 of its factories. Volvo is designing cars inside Omniverse before committing to physical designs, while Ericsson is simulating future 5G wireless networks. Industrial Light & Magic has been evaluating Omniverse for a broad range of possible workflows, but particularly for bringing together content created across multiple traditional applications and facilitating simultaneous collaboration across teams that are distributed all over the world. Foster + Partners, the United Kingdom architectural design and engineering firm, is implementing Omniverse to enable seamless collaborative design to visualization capabilities to teams spread across 14 countries. Activision Publishing is exploring Omniverse’s AI-search capabilities for its games to allow artists, game developers and designers to search intuitively through massive databases of untagged 3D assets using text or images. WPP, the world’s largest marketing services organization, is using the Omniverse to reinvent the way advertising content is made by replacing traditional on-location production methods with entirely virtual production. Perry Nightingale, senior vice president at WPP, said on a panel that he is seeing collaboration on an enormous scale with multiple companies working together. “I’m excited how far that could go, with governments doing it for city planning and other sorts of grand scale collaboration around USD,” Nightingale said. Nvidia will use Omniverse to enable Drive Sim 2.0, which lets carmakers test their self-driving cars inside Omniverse. It uses USD as Nvidia transitions from game engines to a true simulation engine for Omniverse, said Danny Shapiro, senior director for automobiles at Nvidia. Nvidia’s own developers will now be able to support new hardware technologies earlier than they could in the past. “We initially built it for our own needs, so that when technologies were being developed in different groups that they could share immediately, rather than have to wait for the development of it into their particular area,” Kerris said. “The same holds true with our developers. It used to be if we brought a technology out, we would then work with our developers, and it would take a period of time for them to support it. However, by building this platform that crosses over these, we have the ability now to bring out new technologies that they can take advantage of day one.” One question is how well Omniverse will be able to deal with latency, or interaction delays across the cloud. That would be important for game developers, who have to create games that operate in real time. Scenes built with Omniverse can be rendered at 30, 60, or 120 frames per second as needed for a real-time application like a game. Kerris said in an earlier chat that most of what you’re looking at doesn’t have to be constantly refreshed on everybody’s screen, making the real-time updating of the Omniverse more efficient. Nvidia’s Nucleus tech is a kind of traffic cop that communicates what is changing in a scene as multiple parties work on it at once. As for viewing the Omniverse, gamers could access it using a high-end PC with a single Nvidia RTX graphics card. Huang said in his speech, “The metaverse is coming. Future worlds will be photorealistic, obey the laws of physics or not, and inhabited by human avatars and AI beings.” He said that games like Fortnite or Minecraft or Roblox are like the early versions of the metaverse. But he said the metaverse is not only a place to play games. It’s a place to simulate the future. “We are building cities because we need to simulate these virtual worlds for our autonomous vehicles,” Kerris said. “We need a world in which we can train them and test them. Our goal is to scale it so so you could drive continuously drive a virtual car continuously from Los Angeles to New York, in real time, using the actual hardware that’s going to be inside the car and give it a virtual reality experience plugged into its sensory inputs, the output of our simulator, and fool it into thinking it’s in the real world. And for that, it has to be an extremely large world. We’re not quite there yet. But that is what we are moving towards.” For game companies, I can foresee game publishers eventually trading around their cities, as one might build a replica of Paris while another might build New York. After all, if everyone works with USD technology, there might not be a need to rebuild every city from scratch for simulations like games. Ivar Dahlberg, technical artist at Embark Studios, a game studio in Stockholm, said it is tantalizing to think about trading cities back and forth between game developers who are working on city-level games. “Traditionally, developers have focused on a world for someone else to experience,” he said. “But now it seems there are lots more opportunities for developers to create something together with the inhabitants of that world. You can share the tools with everybody who is playing. That ties in quite nicely to the idea of a metaverse. USD is definitely a step in that direction.” Tsigkari said, “That is an experience that may not be very far out. It won’t matter if one company builds Paris, London, or New York. It will be more about what you are doing with those assets. What is the experience that you offer to the user with those assets?” As I saw recently in the film A Glitch in the Matrix, it will be easier to believe in the future that we’re all living in a simulation. I expect that Nvidia will be able to fake a moon landing for us next."
https://venturebeat.com/2021/04/12/nlpcloud-io-helps-app-developers-add-language-processing/,NLP Cloud helps app developers add language processing,"NLP tools and services are taking off, but developers often struggle with the hurdle of getting NLP models into production. NLP Cloud is a new AI startup focused on lowering the barriers for developers trying to create apps for sorting support tickets, extracting leads, analyzing social networks, and developing tools for economic intelligence. NLP has been around for decades, but interest has seen a dramatic uptick with the recent introduction of transformers, a new type of neural network. Google researchers demonstrated in 2017 how transformers dramatically improved the speed, performance, and precision of NLP tools. Transformers made possible the much larger models Google’s BERT and OpenAI’s GPT-3. The capabilities are available through innovative open source libraries Hugging Face and spaCy. Developing accurate models and pushing models into production are two different processes. NLP Cloud intends to close this gap by reducing the barriers to production — providing NLP capabilities via an API, rather than a raw AI model that must be pushed into production. Developers only need to worry about integrating the API into their application. “Today, the main challenge remaining in NLP projects is clearly the production side,” NLP Cloud CTO and founder Julien Salinas told VentureBeat in an email. New NLP models make it easier for more types of developers to experiment with weaving language capabilities into their projects. Possible use cases include scanning web pages and other unstructured text and extracting name entities as part of lead generation before conducting sentiment analysis on support tickets and sorting them based on urgency. Content marketers can use the platform to summarize text and generate headlines. Properly deploying and running AI models in production requires strong DevOps, programming, and AI capabilities. Few developers have mastered all three disciplines, especially within smaller companies. The team may have data science knowledge but not the DevOps capabilities, or software engineers who need to deploy NLP without hiring a data science team. The company is focusing on making the best available open source models easier to deploy rather than developing its own models. This allows it to focus on improving the developer experience rather than tweaking the underlying models. Salinas said the company selected Hugging Face and spaCy for their respective strengths. Hugging Face’s transformers are more advanced and accurate than spaCy, Salinas said. Hugging Face is also building a huge open source repository for NLP models, which makes selecting the best model for a given use case more convenient. SpaCy is faster and less resource-intensive than other NLP libraries. The library has been around longer and recently added the capability to natively support transformer-based models. In the future, Salinas plans to add conversational models for chatbots, new summarization models that can handle bigger pieces of text, and text generation models. He also hopes to eventually support more languages but believes non-English models still need more work. Since its launch three months ago, NLP Cloud has been growing rapidly. It currently has around 500 users, 30 of them paid users. While most of the users are startups the company has begun to see some larger customers."
https://venturebeat.com/2021/04/12/consensys-project-virtue-poker-completes-strategic-investment-round-of-5m/,Consensys Project Virtue Poker Completes Strategic Investment Round of $5m,"TA’ XBIEX, Malta–(BUSINESS WIRE)–April 12, 2021– Virtue Poker, a multi-chain, Ethereum-based decentralized poker platform has now completed a strategic investment round of $5 million. Virtue Poker was part of Coinlist’s Seed Winter 2021 Batch and is funded by notable investors, including Pantera Capital, Consensys, DFG Group, Jez San from FunFair. Poker Hall of Famer Phil Ivey is a stakeholder as well as a public spokesperson for the company. “I’ve been working with the Virtue Poker team for nearly 3 years, watching them build a next generation poker platform” said Phil Ivey. “Using a blockchain based system creates a more secure and globally accessible payment system. I’m excited to continue my partnership with the Virtue Poker team and work to bring the platform to poker communities worldwide”. Virtue Poker is the first and only blockchain-based company to be issued a license by the Malta Gaming Authority. The Virtue Poker team and the Malta Gaming Authority worked together over nearly two years in establishing a regulatory framework suitable for blockchain based gambling applications. The company can now legally operate and compete in most global markets worldwide to deliver the promise of fair playing and transparency to mainstream adoption. “After years of consultation, in person meetings, and effort – Virtue Poker can proudly say we are the only licensed blockchain based poker application in the market” said CEO Ryan Gittleson. “Blockchain technology provides modern and secure payment infrastructure that provides global accessibility to consumers, unlike our competitors. By working with regulators to become a licensed online gambling company, Virtue Poker now has legitimacy to crossover and compete for customers from legacy providers to bring blockchain based wagering mainstream.” The funds raised will be used to bootstrap Virtue Poker’s mainnet launch, which is scheduled for May 2021. Virtue Poker will be hosting an exhibition tournament with players like Phil Ivey, Joe Lubin and others as the official launch. One of the oldest projects in the blockchain space and one of the first to have been incubated by Consensys, Virtue Poker was founded shortly after the Ethereum network debuted. Consensys founder Joe Lubin identified online gambling as an industry that was ripe for blockchain disruption, prompting him to take Virtue Poker into his Ethereum-centered family of projects. “I’m excited to see the Virtue Poker team realize its mission in bringing transparency and trust to the online poker industry” said Joe Lubin, Founder of Consensys. “By working with regulators in becoming the first licensed blockchain based platform, Virtue Poker legitimizes the use of this technology in the industry long term going forward.” Through a combination of P2P networking and the verifiable nature of blockchain, Virtue Poker brings an unprecedented degree of trust and transparency to the online poker industry, which is thought to be plagued by shady and dishonest algorithms and costly third-party payment processor middlemen, especially for withdrawals. Virtue Poker uses both Ethereum smart contracts and sidechain infrastructure to provide next-generation security and transparency for players. About Virtue Poker Virtue Poker is a decentralized poker platform that uses the Ethereum blockchain and peer-to-peer networking to provide an online poker site that’s safe, honest and fun. It was founded in 2016 within Consensys, the leading full stack Ethereum software engineering company and incubator founded by Ethereum co-founder Joe Lubin in 2014. Backed by Consensys and stakeholder Phil Ivey, Virtue Poker is on a mission to make blockchain-based betting mainstream. Learn more: https://virtue.poker/  View source version on businesswire.com: https://www.businesswire.com/news/home/20210412005615/en/ Ryan Gittlesonryan@virtue.poker"
https://venturebeat.com/2021/04/12/intel-advances-in-silicon-photonics-can-break-the-i-o-power-wall-with-less-energy-higher-throughput/,"Intel: Advances in silicon photonics can break the I/O “power wall” with less energy, higher throughput","This article is part of the Technology Insight series, made possible with funding from Intel. As we create more content, deploy more sensors at the network’s edge, and replicate more data for AI to contextualize, the demand for compute bandwidth roughly doubles every three years. Keeping up is becoming increasingly difficult as modern computing architectures get closer and closer to the theoretical performance limits of electrical connections linking their processors, storage, and networking components. Silicon photonics technology—a combination of silicon integrated circuits and semiconductor lasers—may help  overcome the bottlenecks imposed by electrical I/O, replacing copper connections with optical ones at the board and package level. According to James Jaussi, senior principal engineer and director of Intel’s PHY research lab, miniaturized silicon photonics components open the door to architectures that are more disaggregated. That could look like pools of compute, memory, and peripheral functionality distributed throughout the system; connected over long distances with optical links, software-defined infrastructure, and high-speed networking. For now, integrated photonics is still the stuff of lab experiments. But a number of breakthroughs introduced during Intel’s recent Labs Day show that the technology is capable of lower power, higher performance, and greater reach than today’s server interconnects. KEY POINTS: Today, silicon photonics technology is used in datacenters for connecting switches that might be miles apart. On one end, transceivers (devices able to transmit and receive) convert electrical signals to light, which is then sent across optical fiber. At the other end, those optical signals are changed back into electrical. What makes the conversion from electrical to optical a worthwhile endeavor? In short, higher bandwidth, coverage over greater distances, and an immunity to electromagnetic interference. But traditional optical transceivers are expensive. Their transmitter and receiver sub-assemblies must be carefully constructed and hermetically sealed for protection, which makes it difficult for manufacturers to keep up with demand. And the myriad of components that go into a transceiver take up significant space. Silicon photonics packs many of the optical and electronic pieces used to build a transceiver into highly integrated chips. These chips are manufactured in advanced fabs by the same machines that produce the latest CPUs, GPUs, and FPGAs. They enjoy the benefits of cutting-edge lithography, automation, and economies of scale, making them much smaller and less expensive than the technology they replace. Intel introduced its own family of 100 Gb/s transceivers based on semiconductor lasers back in 2016, ten years after demonstrating the technology alongside researchers from UC Santa Barbara. It quickly scored wins with performance-sensitive customers like Microsoft’s Azure cloud computing service. Since then, it has shipped more than four million 100 Gb/s modules, according to Labs Day presentations. Intel has its sights set on scaling optical I/O volumes several orders of magnitude higher though—into the billions of devices. That would take optical beyond rack-to-rack communications in the datacenter and down to the board level, right onto the compute engines where electrical I/O currently dominates. Intel calls this research integrated photonics. If electrical I/O works so well between the server boards and processing packages, why look to silicon photonics as a replacement? Unfortunately, electrical interconnects are struggling to keep those resources fed, and every bit of speed-up comes at the cost of disproportionately more power consumption. There’s a wall in sight, and that’s making optical I/O an appealing alternative. Although silicon photonics transceivers offer notable advantages over traditional optical designs, their components are still too large, too expensive, and too power-hungry to displace electrical I/O within servers. The breakthroughs announced at Labs Day 2020 change this. Jaussi says there are six ingredients in the company’s recipe for integrated photonics: light generation, amplification, detection, modulation, CMOS interface circuits, and package integration. Intel already has a hybrid silicon laser in its portfolio, which is used on its silicon photonics transceivers for converting electrical signals into light. So, it’s focusing on the other five building blocks. In a basic transmitter, the laser creates light onto which data is encoded by a modulator. Existing silicon modulators are large, and therefore expensive in the context of integrated photonics. New micro-ring modulators announced during Labs Day shrink this component’s footprint by more than 1000x. Voltage supplied by a circuit above the modulator either traps light in the ring or allows it to travel down its waveguide. A detector at the other end interprets the absence or presence of light as zeroes and ones. The photodiodes in existing silicon photonics optical transceivers rely on materials like Germanium or Indium Phosphide to “see” light in the wavelengths used to move data. Silicon, it was thought, had no light detection capability in that range. Intel showed otherwise by using its all-silicon micro-ring structure as a photodetector operating at 112 Gb/s. “A major advantage of this development is processing and material cost reduction,” says Jaussi. Intel multiplies the bandwidth through each fiber by capturing multiple wavelengths (or colors) of light from one laser. This technology is called wavelength division multiplexing. In his Labs Day demo, Jaussi showed four micro-rings trapping four separate wavelengths from a single optical channel to convey four bits of data. In the early days of silicon photonics research, this would have taken four different lasers, plus a multiplexer. Doing it with one is key to moving data fast enough in  a space-constrained application like on-package I/O, where there isn’t room for lots of laser firing next to each other. The addition of a semiconductor optical amplifier helps optimize integrated photonics systems for power consumption, since an amplifier provides light power more efficiently than the laser. These amplifiers are made from the same materials as the multi-wavelength laser—an important consideration for manufacturing at volume. As part of Intel’s Labs Day demonstration, Haisheng Rong, principal engineer at Intel Labs, showed off a photonic IC with the hybrid silicon laser, micro-ring modulators, an optical amplifier, and micro-ring photodetectors integrated together and manufactured in a high-volume CMOS fab. He was joined by fellow principal engineer Ganesh Balamurugan, who described the electrical IC responsible for driving and controlling Intel’s micro-ring modulators. The two ICs are stacked, one on top of the other, and connected with copper pillars. “This is an example of how we can tightly integrate energy-efficient CMOS circuits with silicon photonics using 3D packaging,” says Balamurugan. “Such cointegration is key to delivering performance and cost-optimized optical transceivers.” By integrating silicon photonics building blocks with compute resources, Intel believes it can break the current trend of larger processors with more I/O pins, which are needed to satisfy growing bandwidth requirements. Silicon photonics makes it possible to achieve lower power consumption, greater throughput between compute elements, and reduced pin counts, all in a smaller footprint. The company is already showing off high-performance Ethernet switch silicon co-packaged with silicon photonics engines, designed to address the power and cost/complexity issues posed by electrical I/O scaling limitations within two switch generations. It’ll be longer before we see integrated photonics inside of servers—Intel acknowledges that the technology isn’t on the product implementation path yet. However, over time, the company hopes to scale its silicon photonics platform up to 1 Tb/s per fiber at 1pJ of energy consumed per bit, reaching distances of up to 1 km. With electrical I/O facing an impending power wall and silicon photonics already a successful component of Intel’s networking catalog, this is a technology you’ll want to keep an eye on. "
https://venturebeat.com/2021/04/12/safeguard-nabs-45m-to-combat-cybersecurity-risks-using-ai/,SafeGuard Cyber nabs $45M to combat cybersecurity risks using AI,"SafeGuard Cyber, a cloud platform designed to protect assets from cybersecurity threats and risk factors, today announced it has raised $45 million in a mix of equity and debt. This brings SafeGuard’s total raised to over $69 million and will be used to expand its business and technology capabilities, the company says. In a 2017 Deloitte survey, only 42% of respondents considered their institutions to be extremely or very effective at managing cybersecurity risk. The pandemic has certainly done nothing to alleviate these concerns. Despite increased IT security investments companies made in 2020 to deal with distributed IT and work-from-home challenges, nearly 80% of senior IT workers and IT security leaders believe their organizations lack sufficient defenses against cyberattacks, according to IDG. SafeGuard Cyber, which was founded in 2014, develops products that identify risks in communication channels such as social media, chat apps, and collaboration platforms — like Slack, LinkedIn, and WhatsApp. SafeGuard Cyber also helps companies take action and claims it can shield high-profile or targeted individuals from account takeovers, spearphishing, malicious content, threats of violence, and misinformation, as well as bad actor connections. Moreover, SafeGuard says it can protect enterprise and employee accounts from inbound threats, including brand impersonation, while providing visibility into potential threat vectors. “Social engineering has traditionally been carried out within emails but has since evolved to incorporate a more targeted, soft-attack approach across social media, collaboration, and mobile chat channels where traditional security defenses simply do not detect and stop such attacks,” cofounder and CEO Jim Zuffoletti told VentureBeat via email. “Attackers also now realize that mass attacks have a low conversion rate and that targeted spearfishing, while more time-intensive, has a much higher success rate. From a cybercriminal’s point of view, social engineering is the perfect means to deliver a broad array of damaging exploits, such as ransomware that encrypts company data, or attacks that are focused on cyberespionage.” According to a 2017 MicroFocus report, up to 463 billion gigabytes of data will be generated every day by 2026. To accommodate the coming influx, organizations are beginning to transform operations models through the adoption of AI to optimize, enhance, and automate threat and risk visibility, detection, and response capabilities, Zuffoletti says. To this end, SafeGuard Cyber leverages an AI-powered engine called Threat Cortex that detects and spotlights risks across different attack surfaces. Threat Cortex searches the dark and deep web to surface attackers and risk events, automatically notifying IT team members when an anomaly crops up. Using SafeGuard Cyber, admins can quarantine unauthorized data from leaving an organization or specific account. It allows them to lock down and revert compromised accounts back to an earlier, uncompromised state. “ThreatCortex allows for transparency in [machine learning] models​, ​offers data analytics​, and manages features and natural language programming building blocks as configurable rules that are used for scoring by the machine learning,” a SafeGuard Cyber spokesperson explained. “Since we support different types of models in the platform that range from out of the box to highly configurable, it is possible to avoid the need to ‘create’ models for each customer.” On the compliance side of the equation, SafeGuard Cyber offers a tool that taps AI to alert employees, customers, and partners if their digital communications are at risk of violating regulations like the Financial Industry Regulatory Authority and Financial Conduct Authority. The platform flags stakeholders about risk assessments in real time, prioritized by machine learning algorithms. It also captures records autonomously with audit trails and archives content and metadata, including legal holds. The global risk management market was valued at $6.25 billion in 2018 and is projected to reach $18.5 billion by 2026, according to Allied Market Research. SafeGuard Cyber has rivals in RiskLens, RiskIQ, Privacera, and Aclaimant, a data-driven safety and risk management platform for the workplace. There’s also LogicGate, which raised $24.75 million in December 2019 to help automate corporate governance, risk, and compliance processes. Despite the competition, Charlottesville, Virginia-based SafeGuard Cyber, which has 78 employees, says its products saw “tremendous” growth in 2020 as enterprises increasingly turned to digital channels during the pandemic. The company counts some of the biggest names in financial services, pharmaceuticals and health care, education, technology, government, sports, media, and entertainment among its customers, Zuffoletti claims. At present, the company manages over 500,000 user and company accounts. For one pharmaceutical client, SafeGuard Cyber’s platform is recording text call notes from customer service interactions. SafeGuard Cyber claims this has enabled the client to gain productivity and analytical insights, satisfying its digital transformation goals. “SafeGuard Cyber achieved 100% supervision coverage of free text call note reporting, powered by language-agonistic machine learning. Initial results indicated that these controls were warranted, as the system flagged over 200 significant company policy violations for investigation within just the first few months of operation,” the spokesperson said. “Today, SafeGuard Cyber is monitoring up to 100,000 call records per day, in multiple languages, with a less than 1% false positive rate. As a result, the company has been able to cut manual review costs by $3 million per month.” NightDragon led SafeGuard Cyber’s strategic growth round announced today, with participation from Cisco Investments and previous investor AllegisCyber."
https://venturebeat.com/2021/04/12/games-and-transmedia-how-we-got-here-and-where-were-going/,Games and transmedia — how we got here and where we’re going,"Presented by Genvid When you create new worlds, particularly worlds which have their own rules and concepts, fans are going to have questions about every aspect of those worlds which they’re not already familiar with. How does a lightsaber work? What the hell is an Ewok, actually? Or, more recently, who is Boba Fett and what do we know about the Mandalorians? Satisfy this curiosity in any format other than the medium of the original and you’re making transmedia. Your existing fans are more engaged and buying more of your products, and you’re adding new fans along the way. Transmedia turns an IP into a brand. Like many successful concepts, transmedia has become so commonplace that we take it for granted. After all, when we look at the way in which a global IP such as Star Wars has used transmedia to add depth, breadth, and reach to its stories, the process seems like such a natural nexus between marketing, merchandising and community engagement that it’s almost impossible to imagine any property achieving that ubiquity without transmedia. Of course, there’s debate on the exact definition — transmedia is a slippery concept and there’s plenty of academic discussion around it. But what I want to cover here is the rough path which transmedia has taken to arrive at one of the most interesting and cutting-edge examples of the concept in use today — what I’ll call ‘concurrent transmedia’: the use of different media formats to tell a story simultaneously, rather than each medium telling stories separately.  This is something we’ve been exploring to great effect in our most recent Massive Interactive Live Event (MILE) Rival Peak: an interactive livestream of a unity-powered CGI “reality show” with a live-action weekly wrap-up show filmed in a TV studio. Outcomes in the interactive stream (such as who is being voted off the show) have huge ramifications for the filmed segments, and we don’t find out what those outcomes are until just a day or so before filming. It’s ambitious and exciting, and it’s working in some very intriguing ways. We think it’s the next step in the evolution of transmedia and storytelling in interactive media. Transmedia licensing has been a standard in the world of digital gaming for decades. Often, these games are simple adaptations of the plot of a film or book, essentially retelling the same story. But more complex examples have seen games extending to more advanced transmedia techniques — telling entirely new “in-universe” stories from the perspectives of characters created specifically for the game itself, adding new themes to the canon of the original. Again, Star Wars, in many ways the ‘gold standard’ of transmedia, has also pushed forward here, with games like The Force Unleashed, Fallen Order, and the KoTOR series exploring different fragments and perspectives of that galaxy far, far away. It makes perfect sense that games, with their uniquely interactive aspect, would thrive best when freed from the chains of rote narrative repetition. Although there’s a clear appeal to playing your favourite hero in a story which you’ve seen played out on the big screen, games shine when they’re allowed to craft experiences which allow agency for the player, rather than aping a story to which we already know the ending. So games can be a good tool in the transmedia playbooks of IP from other sources — but we’re well beyond the days of games always having to follow in the footsteps of other media: some of the world’s biggest transmedia properties started as gaming IP. As gaming has grown to become the world’s most profitable medium, we’ve seen games’ IP blossoming into some truly massive transmedia brands. Pokémon — reckoned to be the highest-grossing media franchise in existence — began as a game before expanding to trading cards, animated series, books, comics, and high-budget Hollywood movies. Whilst Pokémon has made the vast majority ($61B from a total of $92B) of its revenue from straight merchandising, it is the presence of its characters across transmedia which has built it into a brand able to command those merchandising profits.  There are many reasons for Pokémon’s enormous success, and at its core it’s an almost perfect transmedia property. Because of its easily marketed characters, which are almost infinitely re-imaginable, the game’s format lends itself perfectly to the collectible card game and animated series which were the two mainstays of the non-game canon. These two branches of the strategy also neatly demarcated different routes into the fandom — the easy-to-grasp simplicity of the cartoon and the complex, competitive nature of the card game meant that children, and adults, were brought into the IP from opposing ends of the scale of engagement. In addition, both focused their audiences on the Pokémon themselves — which became the products which were the centre of the incredible merchandising empire. Pokémon also expanded beyond these two initial pillars — making guest spot appearances on Nintendo’s cross-franchise showcase Smash Bros. as well as shifting into the quasi-ironic movie market with Detective Pikachu, to say nothing of the massive success of the augmented reality Pokémon Go! mobile game from Niantic. By capturing the minds of a generation of children, and staying with them as they grew into adults, Pokémon has established itself as a global mega-brand which has accrued revenue three times larger than the entirety of the Marvel cinematic universe. As well as being the pre-eminent example of game-led transmedia, one non-official Pokémon transmedia experiment was also a great source of inspiration for our MILE concept: Twitch Plays Pokémon. It was a simple but brilliant idea — a game of Pokémon Red Vs. Blue controlled by the inputs of the viewers of the Twitch channel it was being broadcast on. When it premiered in 2014, TPP became an overnight success, with thousands of participants crowding the channel to deliver often contradictory commands. As chaotic as it was, TPP was an incredible success — over a million people interacted with the stream in the 16 days it took to finish the game. Five years later, Twitch’s head of creator development Marcus Graham hailed it as a pivotal moment in Twitch’s history, acknowledging that it changed the service forever. “TPP not only inspired an entire generation of Pokémon fans, but it directly inspired Twitch. TPP proved that the medium of Twitch was (and still is) ripe for innovation, and that there are new and exciting ways to create interactive content that has never been done before.” There is a direct line from Twitch Plays Pokémon to the development of our concurrent transmedia project Rival Peak. The idea of a stream with a single origin, using only a small amount of computing power at source, but magnified to reach a huge audience through the power of the cloud, was the founding principle behind the technology which enables us to create our own Massive Interactive Live Events. MILEs are only possible with the sort of mass-audience streaming embodied by Twitch, combined with the interactive capabilities of the majority of devices used to view it.  This new transmedia format, combining an existing text (Red Vs. Blue) and an existing distribution technique (Twitch), is not only an excellent example of the evolutionary nature of transmedia, it’s also a brilliant way of bringing together a massive audience in a collectively experienced interactive event with extremely low barriers to entry. Our technology empowers creators to build their own MILEs, with all of the reach and jump-in appeal of TPP, but on any video streaming platform, with any source material, broadcasting to any internet and video capable device. Rival Peak is the biggest example of this concept so far, but how does it come back around to transmedia. The core Rival Peak experience is built in Unity and runs on Facebook Games, broadcast as a stream but able to be interacted with in meaningful ways by participants thanks to Genvid’s unique technology. In addition, we have a weekly “wrap-up show,” filmed in a studio and hosted by Wil Wheaton, which both summarizes the events of the week in-game, and pushes the meta-narrative for the game forward as a whole. On top of that, Rival Peak has an elimination system which is decided by the activity of the participants, so the direction of the wrap-up show and the game itself changes drastically due to factors outside of the control of the developers and writers. The audience is absolutely directing the outcome of both livestream and studio portions of the whole — an interactive, concurrent transmedia project. The results have been incredible. This is a brand new IP, in a totally new format, with an untested transmedia element, subject to potentially massive narrative shifts depending on the actions of participants. In total, we have had 100 million minutes of participants watching and interacting with the livestream. We’ve also had an average of over 10 million viewers for each episode of Rival Speak. These are numbers both game makers and TV studios would be justifiably proud of. Not only has this transmedia approach allowed us to expand our audience beyond those who would be attracted to just one of the consumption methods available, it’s also provided us with the ability to summarize events for new participants, who can go back and watch the existing episodes of Rival Speak — these episodes are also accessible now the season has ended, providing legacy content.  The fact that we were able to film these segments, with a high-profile star and high-production values, in response to audience-decided factors with major ramifications for plotlines, during a pandemic which has all but suspended the production of many filmed experiences, has been incredible. Add in the facts that Rival Peak was built from the ground up in around six months, that we’ve been able to broadcast the whole experience to Facebook’s audience of over 2B people worldwide, translated into eight languages, and Rival Peak looks like a very special project indeed. But as proud as we are, we actually think it’s just the first step in a whole new format of interactive entertainment. MILEs are uniquely positioned to take advantage of this new style of concurrent transmedia project. Because they are infinitely scalable and platform- and engine-agnostic, they can be broadcast on any video stream-capable platform and ported to a new or additional platform with almost no extra work at all — meaning they can live on the same platform as any additional livestreamed or broadcast transmedia video content. This keeps your audience all in one place, allowing for maximum interactivity and retention, and gives you the perfect opportunity to advertise any other transmedia implementations to your existing audience on platform. Plus, this makes them perfect for the ambitious concurrent transmedia projects outlined above. Because they’re available on any device which can handle streamed video, MILEs also have almost no barriers to entry, meaning adoption rates are high — offering an easy in for your brand. Finally, our running costs are low and directly linked to the number of viewers, meaning that it’s very hard to run up unsustainable overheads. Currently, Genvid is the only company in the world to offer the software which makes this possible (our free-to-download SDK is available via our website). We also have a full suite of services and support options available, with a dedicated team of engineers on hand to guide you through every stage of the project from co-development to deployment. Get in touch with us today and find out how we can help you to run your first MILE. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/12/data-access-management-startup-docontrol-emerges-from-stealth-with-13-35m/,Data access management startup DoControl emerges from stealth with $13.35M,"Today marks the launch of DoControl, a platform for automated data access controls designed to improve cloud security and cost efficiency. Emerging from stealth with $13.35 million in funding, including a $10 million series A, DoControl is introducing products that provide monitoring, orchestration, and remediation across software-as-a-service (SaaS) apps like Google Drive and Box. Enterprise SaaS adoption has never been higher. Companies use 16 SaaS apps on average, driving the global industry to an estimated $157 billion. But coinciding with this climb is a decline in app usage transparency. A recent survey of IT leaders conducted by Numerify found that 45% don’t have a complete picture of key apps and business health services, with 57% saying they lacked an overview of IT performance across projects and employees. DoControl provides self-service tools for app monitoring, creating connections through a secure flow that allows DoControl access to the metadata and change logs of each system. The platform automatically creates an inventory of users, external collaborators, assets, third-party domains, and more, enabling visibility and analytics to be used for security investigations, third-party vendor offboarding, compliance evidence, and incident response. DoControl was founded by Adam Gavish, Omri Weinberg, and Liel Ran. Gavis formerly led teams at Google and Amazon and is also an ex-software engineer at data management platform eXelate, which was acquired by Neilsen in 2015 for $200 million, and cloud app analysis startup Skyfence, which Forcepoint bought in 2017. Through no-code workflows, DoControl allows for consistent enforcement across apps, some of which can’t be done natively within each app. Slack and Microsoft Teams chatbots proactively engage with users on behalf of security and IT teams, helping to identify and mitigate outdated or irrelevant information in order to mitigate data breach risk. The pandemic spurred some enterprises to fast-track their adoption of remote work technologies, leveraging SaaS apps to enable the transition. With an uptick in the adoption of SaaS, the focus on managing and securing these apps became increasingly critical. A recent AppOmni study found that two-thirds of IT leaders have less time to oversee SaaS apps, with 68% reporting they rely solely on manual efforts to detect data exposures. New York-based DoControl has 14 employees and competes to a degree with Productiv, which aggregates real-time engagement data and insights for apps and management. Like DoControl, Productiv’s cloud-based dashboard integrates with single sign-on tools to track login activity and extract data from various systems. But in the short time prior to its unveiling, DoControl claims that its over 40 early customers have used the platform to share millions of files, folders, repositories, and other assets with over 10,000 external companies. “In the next five years, digital transformation, remote work, and freelancing will not only accelerate SaaS adoption but also expose more organizations to third-party entities and generate a huge amount of unmanageable data access. As such, SaaS security will become a mandatory, standard checklist in most security programs,” Gavish told VentureBeat via email. “To win this emerging category, SaaS security vendors will have to provide an autonomous solution that not only detects issues but also remediates them automatically on behalf of the customer. This is exactly how DoControl has differentiated itself within this category, by providing fully automated policy enforcement and [a] self-service remediation path for end users that works on behalf of the security team to solve cross-functional and externally facing SaaS data access challenges.” RTP Global led DoControl’s series A, with participation from StageOne Ventures, Cardumen Capital, and CrowdStrike."
https://venturebeat.com/2021/04/12/microsoft-shopping-for-speech-tech-talks-to-buy-nuance-for-16b/,"Microsoft shopping for speech tech, in talks to buy Nuance for $16B [Confirmed]","(Reuters) — Microsoft is in advanced talks to buy artificial intelligence and speech technology company Nuance Communications for about $16 billion, according to a source familiar with the matter. The price being discussed could value Nuance at about $56 a share, the source said, adding that an agreement could be announced as soon as Monday. Bloomberg News, which first reported the potential deal between Nuance and Microsoft, said talks are ongoing and the sale could still fall apart. Burlington, Massachusetts-based Nuance, whose voice recognition technology helped launch Apple assistant Siri, makes software for sectors ranging from health care to the automotive industry. A deal with Nuance would be Microsoft’s second-biggest after its $26.2 billion acquisition of LinkedIn in 2016. Microsoft and Nuance did not immediately respond to Reuters’ request for comment. Update 5:38 a.m. PT: Microsoft confirmed it will buy Nuance Communications in a $19.7 billion deal as the tech giant looks to bolster its suite of enterprise applications with the artificial intelligence firm’s advanced speech technology. Microsoft’s offer price of $56 per share is at a premium of 22.86% over Nuance’s last close at $45.58."
https://venturebeat.com/2021/04/11/what-is-a-log-management-database/,What is a log management database?,"When Socrates reportedly said the “unexamined life is not worth living,” the Greek philosopher didn’t imagine the modern internet with its seemingly unlimited ability to absorb data. Every mouse click, page view, and event seems destined to end up in a log file somewhere. The sheer volume makes juggling all of this information a challenge, which is where a log management database really shines. Collecting information is one thing; analyzing it is much harder. But many business models depend on finding patterns and making sense of the clickstream to gain an edge and justify their margins. The log database must gather the data and compute important statistics. Modern systems are usually tightly coupled with presentation software that distills the data into a visual infographic. Log management databases are special cases of time-series databases. The information arrives in a steady stream of ordered events, and the log files record them. While many web applications are generally focused on web events, like page views or mouse clicks, there’s no reason the databases need to be limited to just this domain. Any sequence of events can be analyzed, such as events from assembly lines, industrial plants, and manufacturing. For instance, a set of log files may track an assembly line, tracking an item as it reaches various stages in the pipeline. The result may be as simple as noting when a stage finished, or it could include extra data about the customization that happened at that stage, like the paint color or the size. If the line is running smoothly, many of the events will be routine and forgettable. But if something goes wrong, the logs can help diagnose which stage was failing. If products need to be thrown away or examined for fault, the logs can narrow that work. Specialized log processing tools began appearing decades ago, and many were focused on simply creating reports that aggregate data to offer a statistical overview. They counted events per day, week, or month and then generated statistics about averages, maxima, and minima. The newer tools offer the ability to quickly search and report on individual fields, like the IP address or account name. They can pinpoint particular words or phrases in fields and search for numerical values. Log data is often said to be “high cardinality,” which means the fields can hold many different values. Indeed, the value in any timestamp is constantly changing. Log databases use algorithms to build indices for locating particular values and optimize these indices for a wide variety of values. Good log databases can manage archives to keep some data while eliminating other data. They can also enforce a retention policy designed by the compliance offices to answer all legal questions and then destroy data to save money when it’s no longer needed. Some log analysis systems may retain statistical summaries or aggregated metrics for older data. The traditional database companies have generally not been focused on delivering a tool for log storage because traditional relational databases have not been a good match for the kind of high cardinality data that’s written much more often than it’s searched. The cost of creating the index that’s the core offering of a relational database is often not worth it for large collections of logs, as there just are’t enough JOINs in the future. Time-series and log databases tend to avoid using regular relational databases to store raw information, but they can store some of the statistical summaries generated along the way. IBM’s QRadar, for instance, is a product designed to help identify suspicious behavior in the log files. The database inside is focused on searching for statistical anomalies. The User Behavior Analytics (UBA) creates behavior models and watches for departures. Oracle is offering a service called Oracle Cloud Infrastructure Logging Analytics that can absorb log files from multiple cloud sources, index them, and apply some machine learning algorithms. It will find issues ranging from poor performance to security breaches. When the log files are analyzed, the data can also be classified according to compliance rules and stored for the future if necessary. Microsoft’s Monitor will also collect log files and telemetry from throughout the Azure cloud, and the company offers a wide range of analytics. An SQL API is one example of a service tuned to the needs of database administrators watching log files of Microsoft’s SQL Server. Several log databases are built upon Lucene, a popular open source project for building full-text search engines. While it was originally built to search for particular words or phrases in large blocks of text, it can also break up values into different fields, allowing it to work much like a database. Elastic is one company offering a tool that starts multiple versions of Lucene on different engines so it will scale automatically as the load increases. The company bundles it together with two other open source projects, LogStash and Kibana, to create what it calls the “ELK stack.” LogStash ingests the data from raw log files into the Elastic database, while Kibana analyzes the results. Amazon’s log analytics feature is also built upon the open source Elasticsearch, Kibana, and LogStash tools and specializes in deploying and supporting the tools on AWS cloud machines. AWS and Elastic recently parted ways, so differences may appear in future versions. Loggly and  LogDNA are two other tools built on top of Lucene. They integrate with most log file formats and track usage over time to identify performance issues and potential security flaws. Not all companies rely on Lucene, in part because the tool includes many features for full-text searching, which is not as important for log processing, and these features add overhead. Sumo Logic, another performance tracking company, ingests logs with its own version of SQL for querying the database. Splunk built its own database to store log information. Customers who work directly with the applications designed to automate monitoring tasks — like looking for overburdened servers or unusual access patterns that might indicate a breach — generally don’t use the database. Splunk’s database is designed to curate the indexes and slowly archive them as time passes. EraDB offers another database with a different core but the same API as Elastic. It promises faster ingestion and analysis because its engine was purpose-built for high cardinality log files without any of the overhead that might be useful for text searching. Log databases are ideal for endless streams of events filled with different values. But not all data sources are filled with high cardinality fields. Those with frequently repeating values may find some reduction in storage by a more traditional tabular structure that can save space. The log systems built upon text search engines like Lucene may also offer extra features that are not necessary for many applications. In a hypothetical assembly line, for instance, there’s little need to search for arbitrary strings or words. Supporting the ability for arbitrary text search requires more elaborate indexes that take time to compute and disk space to store. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/11/automation-is-expanding-how-worried-should-we-be-about-jobs/,Automation is expanding. How worried should we be about jobs?,"A few days ago I was having network problems with the WiFi in my home office — my connection was very slow and video conferences were freezing. After fussing with the mesh network extenders with no result, I called the cable provider. Normally, this involves having to navigate several automated voice response menus before a call center representative comes on the line to help. I explain the problem and they then run a remote diagnostic and usually suggest a cable modem reset. Often, that solves the problem. But in my latest attempt to get back online, the menus had changed and speaking to a representative was no longer an option. Just, “press 1 to reset the modem.” And just like that, it worked. Something is lost in this process, but something is gained. The human was removed, and the problem was resolved — probably in less time. Explaining the problem to a human had often been a source of frustration, due to language challenges or possibly my poor descriptive abilities. In fact, some of my worst customer service experiences have been with this cable company. But with this change it was hard to miss the advance in automation and I had to acknowledge the role of AI as a key enabler. This story is, in fact,  an example of what is now often called intelligent automation, the combination of artificial intelligence and automation that synthesizes vast amounts of information to automate entire processes or workflows I also had to wonder what happened to the call center representative. Did they go on to one of those positions we hear about that entail higher strategy tasks? Or perhaps they received a layoff notice. I tried not to think of their personal circumstances, about whether they could readily find other work or would face real hardships. Then again, maybe this will free this worker to pursue a more interesting opportunity. This dual nature of automation — the increase in efficiency and productivity along with the potential human impacts — is the stuff of anxious dreams. Because we hear the same two stories — many jobs will disappear, while new professions will emerge to replace them. The anxiety lives in the gap between, wondering and worrying about what this new reality will bring. Even if these new professions do not materialize, not to worry because there will be so much wealth generated by AI and automation that every adult will receive a monthly stipend, much as Alaska residents receive from oil royalties. At least that is the point of view of OpenAI Co-founder and CEO Sam Altman, as expressed in a recent blog, where he writes that we are witnessing a “recursive loop of innovation” that is both accelerating and unstoppable. Altman goes on to argue that the AI revolution will generate enough wealth for everyone to have what they need, spinning off dividends of $13,500 a year. It could be that his view is inspired and filled with a generosity of spirit, or it could be disingenuous. The Universal Basic Income Altman envisions would be great as a bonus but a poor Faustian bargain if in the process many join the ranks of the long-term unemployed. Several other people have pointed to the flaws in his proposal. For example, Matt Prewitt, president of non-profit RadicalxChange, commented: “The [Altman] piece sells a vision of the future that lets our future overlords off way too easy, and would likely create a sort of peasant class encompassing most of society.” The prospect of a permanent underclass brought about by AI and automation is increasingly portrayed in fiction looking out on the next 20 to 50 years. In The Resisters, a novel by Gish Jen, unemployed people are deemed “Surplus,” meaning there is no work for them. Instead, they are issued a Universal Basic Income at levels just above subsistence. In the new novel Klara and the Sun from Nobel prize winning author Kazuo Ishiguro, large swaths of the population have been “substituted” by automation. The novel describes how a growing income disparity between those with jobs and those without leads to a fracturing of society with increasing tribalism and fascist ideology. Burn-In, a novel from P. W. Singer and August Cole, describes growing automation that has taken millions of jobs and left many people fearful that the future is leaving them behind. In their extensively documented novel, referencing technology that already exists or is far along in development, AI has advanced so far that once-safe fields such as law or finance have been taken over by algorithms, leading to political backlash, with large numbers of people becoming radicalized in extreme virtual communities. Taken together, these portrayals of the not-too-distant future are a long way from Altman’s utopia. It remains to be seen where automation will lead us. Perhaps what will determine the true tipping point is how our institutions respond to this new reality as it accelerates and evolves. Altman warns that if in response to these changes “public policy doesn’t adapt accordingly, most people will end up worse off than they are today.” Not everyone is concerned about AI and automation. On the one hand, it is broadly granted that the COVID-19 pandemic accelerated automation and reduced employment, what the World Economic Forum describes as a “double-disruption” scenario for workers leading to growing inequality. On the other hand, some argue there will be changes in the types of available work — and some people will be displaced (like my call center representative) — but overall employment will not be greatly impacted. Afterall, as these arguments often go, this is what has happened in prior technology revolutions. According to Richard Cooper, the Maurits C. Boas Professor of International Economics at Harvard University, “new technology often destroys existing jobs, but it also creates many new possibilities through several different channels.” Cooper says those new opportunities can take decades to emerge, though, which doesn’t sync with the pace of post-COVID job losses. Others argue that dystopian predictions about automation are fraught with exaggerated timelines and that the feared robot apocalypse is still far away. Most likely, the full impact of automation will not likely be seen until some years into the future. That is the conclusion of a PwC study from a couple of years ago that described several waves of automation. During the first wave, they expect relatively low displacement, “perhaps only around 3% by the early 2020s.” This could explain why the debate about the impact still seems more theoretical than pressing, with far more substantial impacts over the next 10 to 15 years. During the first and second waves, women could be at greater risk of automation due to their higher representation in clerical and other administrative functions, but later automation will put more men at risk.​ It’s worth underscoring that PwC did this analysis pre-COVID and so its conclusions don’t account for the rapid uptake of automation over the past year and how this could further accelerate the waves of automation going forward.  Source: PwC estimates by gender based on OECD PIAAC data (median values for 29 countries) Nevertheless, we can already see and feel it, and this is permeating throughout society. It is not only those doing routine work who are at risk, but increasingly those in white collar professions. A recent PwC survey of employees worldwide revealed that “60% are worried that automation is putting many jobs at risk; 48% believe ‘traditional employment won’t be around in the future,’ and 39% think it is likely that their job will be obsolete within five years.” The longer-term impact of AI and automation on work is not really in doubt. Many positions will be disrupted and people replaced, even as other employment opportunities may be created. The net effect is likely to be positive for the economy. This could be good for economists and corporate shareholders. Though whether it is positive for a large percentage of the population or produces a sizable permanent underclass is very much to be determined. Automation will not likely bring about either utopia or dystopia. Instead, it will lead to both, with different groups experiencing these very different realities. Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
https://venturebeat.com/2021/04/10/black-women-ai-and-historical-patterns-of-abuse/,"Black women, AI, and overcoming historical patterns of abuse","After a 2019 research paper demonstrated that commercially available facial analysis tools fail to work for women with dark skin, AWS executives went on the attack. Instead of offering up more equitable performance results or allowing the federal government to assess their algorithm like other companies with facial recognition tech have done, AWS executives attempted to discredit study coauthors Joy Buolamwini and Deb Raji in multiple blog posts. More than 70 respected AI researchers rebuked this attack, defended the study, and called on Amazon to stop selling the technology to police, a position the company temporarily adopted last year after the death of George Floyd. But according to the Abuse and Misogynoir Playbook, published earlier this year by a trio of MIT researchers, Amazon’s attempt to smear two Black women AI researchers and discredit their work follows a set of tactics that have been used against Black women for centuries. Moya Bailey coined the term “misogynoir” in 2010 as a portmanteau of “misogyny” and “noir.” Playbook coauthors Katlyn Turner, Danielle Wood, and Catherine D’Ignazio say these tactics were also used to disparage former Ethical AI team co-lead Timnit Gebru after Google fired her in late 2020 and stress that it’s a pattern engineers and data scientists need to recognize. The Abuse and Misogynoir Playbook is part of the State of AI report from the Montreal AI Ethics Institute and was compiled by MIT professors in response to Google’s treatment of Gebru, a story VentureBeat has covered in depth. The coauthors hope that recognition of the phenomena will prove a first step in ensuring these tactics are no longer used against Black women. Last May, VentureBeat wrote about a fight for the soul of machine learning, highlighting ties between white supremacy and companies like Banjo and Clearview AI, as well as calls for reform from many in the industry, including prominent Black women. MIT assistant professor Danielle Wood, whose work focuses on justice and space research, told VentureBeat it’s important to recognize that the tactics outlined in the Abuse and Misogynoir Playbook can be used in almost any arena. She noted that while some cling to a belief in the impartiality of data-driven results, the AI field is in no way exempt from this problem. “This is a process, a series of related things, and the process has to be described step by step or else people won’t get the point,” Wood said. “I can be part of a system that’s actually practicing misogynoir, and I’m a Black woman. Because it’s a habit that is so prolific, it’s something I might participate in without even thinking about it. All of us can.” The playbook outlines the intersectional and unique abuse aimed at Black women in five steps: Step 1: A Black woman scholar makes a contribution that speaks truth to power or upsets the status quo. Step 2: Disbelief in her contribution from people who say the results can’t be true and either think a Black woman couldn’t have done the research or find another way to call her contribution into question. Step 3: Dismissal, discrediting, and gaslighting ensues. AI chief Jeff Dean’s public attempt to discredit Gebru alongside colleagues is a textbook example. Similarly, after current and former Dropbox employees alleged gender discrimination at the company, Dropbox CEO Drew Houston attempted to discredit the report’s findings, according to documents obtained by VentureBeat. Gaslighting is a term taken from the 1944 movie Gaslight, in which a character goes to extreme lengths to make a woman deny her senses, ignore the truth, and feel like she’s going crazy. It’s not uncommon at this stage for people to consider the targeted Black woman’s contribution an attempt to weaponize pity or sympathy. Another instance that sparked gaslighting allegations involved algorithmic bias, Facebook chief AI scientist Yann LeCun, and Gebru. Step 4: Erasure. Over time, counter-narratives, deplatforming, and exclusion are used to prevent that person from carrying out their work as part of attempts to erase their contributions. Step 5: Revisionism seeks to paper over the contributions of Black women and can lead to whitewashed versions of events and slow progress toward justice. There’s been a steady stream of stories about gender and racial bias in AI in recent years, a point highlighted by news headlines this week. The Wall Street Journal reported Friday that researchers found Facebook’s algorithm shows different job ads to men and women and is discriminatory under U.S. law, while Vice reported on research that found facial recognition used by Proctorio remote proctoring software does not work well for people with dark skin over half of the time. This follows VentureBeat’s coverage of racial bias in ExamSoft’s facial recognition-based remote proctoring software, which was used in state bar exams in 2020. Investigations by The Markup this week found advertising bans hidden behind an algorithm for a number of terms on YouTube, including “Black in tech,” “antiracism,” and “Black excellence,” but it’s still possible to advertise to white supremacists on the video platform. Google’s treatment of Gebru illustrates each step of the playbook. Her status quo-disrupting contribution, Turner told VentureBeat, was an AI research paper about the dangers of using large language models that perpetuate racism or stereotypes and carry an environmental impact that may unduly burden marginalized communities. Other perceived disruptions, Turner said, included Gebru building one of the most diverse teams within Google Research and sending a critical email to the Google Brain Women and Allies internal listserv that was leaked to Platformer. Shortly after she was fired, Gebru said she was asked to retract the paper or remove the names of Google employees. That was step two from the Misogynoir Playbook. In academia, Turner said, retraction is taken very seriously. It’s generally reserved for scientific falsehood and can end careers, so asking Gebru to remove her name from a valid piece of research was unreasonable and part of efforts to make Gebru herself seem unreasonable. Evidence of step three, disbelief or discredit, can be found in an email AI chief Jeff Dean sent that calls into question the validity of the paper’s findings. Days later, CEO Sundar Pichai sent a memo to Google employees in which he said the firing of Gebru had prompted the company to explore improvements to its employee de-escalation policy. In an interview with VentureBeat, Gebru characterized that memo as “dehumanizing” and an attempt to fit her into an “angry Black woman” trope. Despite Dean’s critique, a point that seems lost amid allegations of abuse, racism, and corporate efforts to interfere with academic publication is that the team of researchers behind the stochastic parrots research paper in question was exceptionally well-qualified to deliver critical analysis of large language models. A version of the paper VentureBeat obtained lists Google research scientists Ben Hutchinson, Mark Diaz, and Vinodkumar Prabhakaran as coauthors, as well as then-Ethical AI team co-leads Gebru and Margaret Mitchell. While Mitchell is well known for her work in AI ethics, she is most heavily cited for research involving language models. Diaz, Hutchinson, and Prabhakaran have backgrounds in assessing language or NLP for ageism, discrimination against people with disabilities, and racism, respectively. Linguist Emily Bender, a lead coauthor of the paper alongside Gebru, received an award from organizers of a major NLP conference in mid-2020 for work critical of large language models, which VentureBeat also reported. Gebru is coauthor of the Gender Shades research paper that found commercially available facial analysis models perform particularly poorly for women with dark skin. That project, spearheaded by Buolamwini in 2018 and continued with Raji in a subsequent paper published in early 2019, has helped shape legislative policy in the U.S and is also a central part of Coded Bias, a documentary now streaming on Netflix. And Gebru has been a major supporter of AI documentation standards like datasheets for datasets and model cards, an approach Google has adopted. Finally, Turner said, steps four and five of the playbook, erasure and revisionism, can be seen in the departmental reorganization and diversity policy changes Google made in February. As a result of those changes, Google VP Marian Croak was appointed to head up 10 of the Google teams that consider how technology impacts people. She reports directly to AI chief Jeff Dean. On Tuesday, Google research manager Samy Bengio resigned from his role at the company, according to news first reported by Bloomberg. Prior to the restructuring, Bengio was the direct report manager for the Ethical AI team. VentureBeat obtained a copy of a letter Ethical AI team members sent to Google leadership in the weeks following Gebru’s dismissal that specifically requested Bengio remain the direct report for the team and that the company not implement any reorganization. A person familiar with ethics and policy matters at Google told VentureBeat that reorganization had been discussed previously, but this source described an environment of fear after Gebru’s dismissal that prevented people from speaking out. Before being named to her new position, Croak appeared alongside the AI chief in a meeting with Black Google employees in the days following Gebru’s dismissal. Google declined to make Croak available for comment, but the company released a video in which she called for more “diplomatic” conversations about definitions of fairness or safety. Turner pointed out that the reorganization fits neatly into the playbook. “I think that revisionism and erasure is important. It serves a function of allowing both people and the news cycle to believe that the narrative arc has happened, like there was some bad thing that was taken care of — ‘Don’t worry about this anymore.’ [It’s] like, ‘Here’s this new thing,’ and that’s really effective,” Turner said. The playbook’s coauthors said it was constructed following conversations with Gebru. Earlier in the year, Gebru spoke at MIT at Turner and Wood’s invitation as part of an antiracism tech design research seminar series. When the news broke that Gebru had been fired, D’Ignazio described feelings of anger, shock, and outrage. Wood said she experienced a sense of grieving and loss. She also felt frustrated by the fact that Gebru was targeted despite having attempted to address harm through channels that are considered legitimate. “It’s a really discouraging feeling of being stuck,” Wood said. “If you follow the rules, you’re supposed to see the outcome, so I think part of the reality here is just thinking, ‘Well, if Black women try to follow all the rules and the result is we’re still not able to communicate our urgent concerns, what other options do we have?'” Wood said she and Turner found connections between historical figures and Gebru in their work in the Space Enabled Lab at MIT examining complex sociotechnical systems through the lens of critical race studies and queer Black feminist groups like the Combahee River Collective. In addition to instances of misogynoir and abuse at Amazon and Google, coauthors say the playbook represents a historical pattern that has been used to exclude Black women authors and scholars dating back to the 1700s. These include Phillis Wheatley, the first published African American poet, journalist Ida B. Wells, and author Zora Neale Hurston. Generally, the coauthors found that the playbook tactics visit great acts of violence on Black women that can be distinguished from the harms encountered by other groups that challenge the status quo. The coauthors said women outside of tech who have been targeted by the same playbook include New York Times journalist and 1619 Project creator Nikole Hannah-Jones and politicians like Stacey Abrams and Rep. Ayanna Pressley (D-MA). The researchers also said they took a historical view to demonstrate that the ideas behind the Abuse and Misogynoir Playbook are centuries old. Failure to confront forces of racism and sexism at work, Turner said, can lead to the same problems in new and different tech scenarios. She went on to say that it’s important to understand that historical forces of oppression, categorization, and hierarchy are still with us and warned that “we will never actually get to an ethical AI if we don’t understand that.” The AI field claims to excel at pattern recognition, so the industry should be able to identify tactics from the playbook, D’Ignazio said. “I feel like that’s one of the most enormous ignorances, the places where technical fields do not go, and yet history is what would inform all of our ethical decisions today,” she said. “History helps us see structural, macro patterns in the world. In that sense, I see it as deeply related to computation and data science because it helps us scale up our vision and see how things today, like Dr. Gebru’s case, are connected to these patterns and cycles that we still haven’t been able to break out of today.” The coauthors recognize that power plays a major role in determining what kind of behavior is considered ethical. This corresponds to the idea of privilege hazard, a term coined in the book Data Feminism, which D’Ignazio coauthored last year, to articulate people in privileged positions failing to fully comprehend the experience of those with less power. A long-term view seems to run counter to the traditional Silicon Valley dogma surrounding scale and growth, a point emphasized by Google Ethical AI team research scientist and sociologist Dr. Alex Hanna weeks before Gebru was fired. A paper Hanna coauthored with independent researcher Tina Park in October 2020 called scale thinking incompatible with addressing social inequality. The Abuse and Misogynoir Playbook is the latest AI work to turn to history for inspiration. Your Computer Is On Fire, a collection of essays from MIT Press, and Kate Crawford’s Atlas of AI, released in March and April, respectively, examine the toll datacenter infrastructure and AI take on the environment and civil rights and reinforce colonial habits about the extraction of value from people and natural resources. Both books also investigate patterns and trends found in the history of computing. Race After Technology author Ruha Benjamin, who coined the term “new Jim Code,” argues that an understanding of historical and social context is also necessary to safeguard engineers from being party to human rights abuses, like the IBM workers who assisted Nazis during World War II. The coauthors end by calling for the creation of a new playbook and pose a challenge to the makers of artificial intelligence. “We call on the AI ethics community to take responsibility for rooting out white supremacy and sexism in our community, as well as to eradicate their downstream effects in data products. Without this baseline in place, all other calls for AI ethics ring hollow and smack of DEI-tokenism. This work begins by recognizing and interrupting the tactics outlined in the playbook — along with the institutional apparatus — that works to disbelieve, dismiss, gaslight, discredit, silence, and erase the leadership of Black women.” The second half of a panel discussion about the playbook in late March focused on hope and ways to build something better, because, as the coauthors say, it’s not enough to host events with the term “diversity” or “equity” in them. Once abusive patterns are recognized, old processes that led to mistreatment on the basis of gender or race must be replaced with new, liberatory practices. The coauthors note that making technology with liberation in mind is part of the work D’Ignazio does as director of the Data + Feminism Lab at MIT, and what Turner and Wood do with the Space Enabled research group at MIT Media Lab. That group looks for ways to design complex systems that support justice and the United Nations Sustainable Development Goals. “Our assumption is we have to show prototypes of liberatory ways of working so that people can understand those are real and then try to adopt those in place of the current processes that are in place,” Wood said. “We hope that our research labs are actually mini prototypes of the future in which we try to behave in a way that’s anticolonial and feminist and queer and colored and has lots of views from people from different backgrounds.” D’Ignazio said change in tech — and specifically for the hyped, well-funded, and trendy field of AI — will require people considering a number of factors, including who they take money from and choose to work with. AI ethics researcher Luke Stark turned down $60,000 in funding from Google last month, and Rediet Abebe, who cofounded Black in AI with Gebru, has also pledged to reject funding from Google. In other work at the intersection of AI and gender, the Alan Turing Institute’s Women in Data Science and AI project released a report last month that documents problems women in AI face in the United Kingdom. The report finds that women only hold about 1 in 5 jobs in data science and AI in the U.K. and calls for government officials to better track and verify the growth of women in those fields. “Our research findings reveal extensive disparities in skills, status, pay, seniority, industry, job attrition, and education background, which call for effective policy responses if society is to reap the benefits of technological advances,” the report reads. Members of Congress interested in algorithmic regulation are considering more stringent employee demographic data collection, among other legislative initiatives. Google and Facebook do not currently share diversity data specific to employees working within artificial intelligence. The Abuse and Misogynoir Playbook is also the latest AI research from people of African descent to advocate taking a historical perspective and adopting anticolonial and antiracist practices. In an open letter shortly after the death of George Floyd last year, a group of more than 150 Black machine learning and computing professionals outlined a set of actions to bring an end to the systemic racism that has led Black people to leave jobs in the computing field. A few weeks later, researchers from Google’s DeepMind called for reform of the AI industry based on anticolonial practices. More recently, a team of African AI researchers and data scientists have recommended implementing anticolonial data sharing practices as the datacenter industry in Africa continues growing at a rapid pace."
https://venturebeat.com/2021/04/09/microsoft-open-sources-tool-to-use-ai-in-simulated-attacks/,Microsoft open-sources tool to use AI in simulated attacks,"As part of Microsoft’s research into ways to use machine learning and AI to improve security defenses, the company has released an open source attack toolkit to let researchers create simulated network environments and see how they fare against attacks. Microsoft 365 Defender Research released CyberBattleSim, which creates a network simulation and models how threat actors can move laterally through the network looking for weak points. When building the attack simulation, enterprise defenders and researchers create various nodes on the network and indicate which services are running, which vulnerabilities are present, and what type of security controls are in place. Automated agents, representing threat actors, are deployed in the attack simulation to randomly execute actions as they try to take over the nodes. “The simulated attacker’s goal is to take ownership of some portion of the network by exploiting these planted vulnerabilities. While the simulated attacker moves through the network, a defender agent watches the network activity to detect the presence of the attacker and contain the attack,” the Microsoft 365 Defender Research Team wrote in a post discussing the project. Microsoft has been exploring how machine learning algorithms such as reinforcement learning can be used to improve information security. Reinforcement learning is a type of machine learning in which autonomous agents learn how to make decisions based on what happens while interacting with the environment. The agent’s goal is to optimize the reward, and agents gradually make better decisions (to get a bigger reward) through repeated attempts. The most common example is playing a video game. The agent (player) gets better at playing the game after repeated tries by remembering the actions that worked in previous rounds. In a security scenario, there are two types of autonomous agents: the attackers trying to steal information out of the network and defenders trying to block the attack or mitigate its effects. The agents’ actions are the commands that attackers can execute on the computers and the steps defenders can perform in the network. Using the language of reinforcement learning, the attacking agent’s goal is to maximize the reward of a successful attack by discovering and taking over more systems on the network and finding more things to steal. The agent has to execute a series of actions to gradually explore the networks but do so without setting off any of the security defenses that may be in place. Much like the human mind, AI learns better by playing games, so Microsoft turned CyberBattleSim into a game. Capture the flag competitions and phishing simulations help strengthen security by creating scenarios in which defenders can learn from attacker methods. By using reinforcement learning to get the reward of “winning” a game, the CyberBattleSim agents can make better decisions on how they interact with the simulated network. The CyberBattleSim focuses on threat modeling how an attacker can move laterally through the network after the initial breach. In the attack simulation, each node represents a machine with an operating system, software applications, specific properties (security controls), and set of vulnerabilities. The toolkit uses the Open AI Gym interface to train automated agents using reinforcement learning algorithms. The open source Python source code is available on GitHub. Erratic behavior should quickly trigger alarms, and security tools would respond and evict the malicious actor. But if the actor has learned how to compromise systems more quickly by shortening the number of steps it needs to succeed, that gives defenders insight into the places that need security controls and helps with detecting the activity sooner. The CyberBattleSim is part of Microsoft’s broader research into using machine learning and AI to automate many of the tasks security defenders are currently handling manually. In a recent Microsoft study, almost three-quarters of organizations said their IT teams spent too much time on tasks that should be automated. Autonomous systems and reinforcement learning “can be harnessed to build resilient real-world threat detection technologies and robust cyber-defense strategies,” Microsoft wrote. “With CyberBattleSim, we are just scratching the surface of what we believe is a huge potential for applying reinforcement learning to security,” the company added."
https://venturebeat.com/2021/04/09/ai-weekly-continual-learning-offers-a-path-toward-more-humanlike-ai/,AI Weekly: Continual learning offers a path toward more humanlike AI,"State-of-the-art AI systems are remarkably capable, but they suffer from a key limitation: statisticity. Algorithms are trained once on a dataset and rarely again, making them incapable of learning new information without retraining. This is as opposed to the human brain, which learns constantly, using knowledge gained over time and building on it as it encounters new information. While there’s been progress toward bridging the gap, solving the problem of “continual learning” remains a grand challenge in AI. This challenge motivated a team of AI and neuroscience researchers to found ContinualAI, a nonprofit organization and open community of continual and lifelong learning enthusiasts. ContinualAI recently announced Avalanche, a library of tools compiled over the course of a year from over 40 contributors to make continual learning research easier and more reproducible. The group also hosts conference-style presentations, sponsors workshops and AI competitions, and maintains a repository of tutorial, code, and guides. As Vincenzo Lomonaco, cofounding president and assistant professor at the University of Pisa, explains, ContinualAI is one of the largest organizations on a topic its members consider fundamental for the future of AI. “Even before the COVID-19 pandemic began, ContinualAI was funded with the idea of pushing the boundaries of science through distributed, open collaboration,” he told VentureBeat via email. “We provide a comprehensive platform to produce, discuss and share original research in AI. And we do this completely for free, for anyone.” Even highly sophisticated deep learning algorithms can experience catastrophic learning or catastrophic interference, a phenomenon where deep networks fail to recall what they’ve learned from a training dataset. The result is that the networks have to be constantly reminded of the knowledge they’ve gained or risk becoming “stuck” with their most recent “memories.” OpenAI research scientist Jeff Clune, who helped to cofound Uber AI Labs in 2017, has called catastrophic forgetting the “Achilles’ heel” of machine learning and believes that solving it is the fastest path to artificial general intelligence (AGI). Last February, Clune coauthored a paper detailing ANML, an algorithm that managed to learn 600 sequential tasks with minimal catastrophic forgetting by “meta-learning” solutions to problems instead of manually engineering solutions. Separately, Alphabet’s DeepMind has published research suggesting that catastrophic forgetting isn’t an insurmountable challenge for neural networks. And Facebook is advancing a number of techniques and benchmarks for continual learning, including a model that it claims is effective in preventing the forgetting of task-specific skills. But while the past several years have seen a resurgence of research into the issue, catastrophic forgetting largely remains unsolved, according to Keiland Cooper, a cofounding member of ContinualAI and a neuroscience research associate at the University of California, Irvine. “The potential of continual learning exceeds catastrophic forgetting and begins to touch on more interesting questions of implementing other cognitive learning properties in AI,” Cooper told VentureBeat. “Transfer learning is one example, where when humans or animals learn something previously, sometimes this learning can be applied to a new context or aid learning in other domains … Even more alluring is that continual learning is an attempt to push AI from narrow, savant-like systems to broader, more general ones.” Even if continual learning doesn’t yield the sort of AGI depicted science fiction, Cooper notes that there are immediate advantages to it across a range of domains. Cutting-edge models are being trained on increasingly larger datasets in search of better performance, but this training comes at a cost — whether waiting weeks for training to finish or the impact of the electricity usage on the environment. “Say you run a certain AI organization that built a natural language model that was trained over weeks on 45 terabytes of data for a few million dollars,” Cooper explained. “If you want to teach that model something new, well, you’d very likely have to start from scratch or risk overwriting what it had already learned, unless you added continual learning additions to the model. Moreover, at some point, the cost to store that data will be exceedingly high for an organization, or even impossible. Beyond this, there are many cases where you can only see the data once and so retraining isn’t even an option.” While the blueprint for a continual learning AI system remains elusive, ContinualAI aims to connect researchers and stakeholders interested in the area and support and provide a platform for projects and research. It’s grown to over 1,000 members in the three years since its founding. “For me personally, while there has been a renewed interest in continual learning in AI research, the neuroscience of how humans and animals can accomplish these feats is still largely unknown,” Cooper said. “I’d love to see more of an interaction with AI researchers, cognitive scientists, and neuroscientists to communicate and build upon each of their fields ides towards a common goal of understanding one of the most vital aspects of learning and intelligence. I think an organization like ContinualAI is best positioned to do just that, which allows for the sharing of ideas without the boundaries of the academic or industry walls, siloed fields, or distant geolocation.” Beyond the mission of dissemination information about continual learning, Lomonaco believes that ContinualAI has the potential to become a reference points for a more inclusive and collaborative way of doing research in AI. “Elite university and private company labs still work mostly behind close doors, [but] we truly believe in inclusion and diversity rather than selective elitiarity. We favor transparency and open-source rather than protective IP licenses. We make sure anyone has access to the learning resources she needs to achieve her potential.” For AI coverage, send news tips to Kyle Wiggers — and be sure to subscribe to the AI Weekly newsletter and bookmark our AI channel, The Machine. Thanks for reading, Kyle Wiggers AI Staff Writer"
https://venturebeat.com/2021/04/09/ibm-releases-qiskit-modules-that-use-quantum-computers-to-improve-machine-learning/,IBM releases Qiskit modules that use quantum computers to improve machine learning,"IBM is releasing Qiskit Machine Learning, a set of new application modules that’s part of its open source quantum software. The new feature is the latest expansion of the company’s broader effort to get more developers to begin experimenting with quantum computers. According to a blog post by the Qiskit Applications Team, the machine learning modules promise to help optimize machine learning by using quantum computers for some parts of the process. “Quantum computation offers another potential avenue to increase the power of machine learning models, and the corresponding literature is growing at an incredible pace,” the team wrote. “Quantum machine learning (QML) proposes new types of models that leverage quantum computers’ unique capabilities to, for example, work in exponentially higher-dimensional feature spaces to improve the accuracy of models.” Rather than replacing current computer architectures, IBM is betting that quantum computers will gain traction in the coming years by taking on very specific tasks that are offloaded from a classic computing system to a quantum platform. AI and machine learning are among the areas where IBM has said it’s hopeful that quantum can make an impact. To make quantum more accessible, last year IBM introduced an open source quantum programming framework called Qiskit. The company has said it has the potential to speed up some applications by 100 times. In the case of machine learning, the hope is that a system that offloads tasks to a quantum system could accelerate the training time. However, challenges remain, such as how to get large data sets in and out of the quantum machine without adding time that would cancel out any gains by the quantum calculations. Developers who use Qiskit to improve their algorithms will have access to test them on IBM’s cloud-based quantum computing platform."
https://venturebeat.com/2021/04/09/researchers-detail-systemic-issues-and-risk-to-society-in-language-models/,Researchers detail systemic issues and risk to society in language models,"Researchers at Google’s DeepMind have discovered major flaws in the output of large language models like GPT-3 and warn these could have serious consequences for society, like enabling deception and reinforcing bias. Notably, coauthors of a paper on the study say harms can be proliferated by large language models without malicious intent on the creators’ part. In other words, these harms can be spread accidentally, due to incorrect specifications around what an agent should be learning from or the model training process. “We believe that language agents carry a high risk of harm, as discrimination is easily perpetuated through language. In particular, they may influence society in a way that produces value lock-in, making it harder to challenge problematic existing norms,” the paper reads. “We currently don’t have many approaches for fixing these forms of misspecification and the resulting behavioral issues.” The paper supposes that language agents could also enable “incitement to violence” and other forms of societal harm, particularly by people with political motives. The agents could also be used to spread dangerous information, like how to make weapons or avoid paying taxes. In a prime example from work published last fall, GPT-3 tells a person to commit suicide. The DeepMind paper is the most recent study to raise concerns about the consequences of deploying large language models made with datasets scraped from the web. The best known paper on this subject is titled “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?” and was published last month at the Fairness, Accountability, and Transparency conference by authors that include former Google Ethical AI team co-leads Margaret Mitchell and Timnit Gebru. This paper asserts language models that seem to be growing in size perpetuate stereotypes and carry environmental costs most likely to be born by marginalized groups. While Google fired both of its researchers who chose to keep their names on the paper and required other Google research scientists to remove their names from a paper that reached a similar conclusion, the DeepMind research cites the stochastic parrots paper among related works. Earlier this year, a paper from OpenAI and Stanford University researchers detailed a meeting between experts from fields like computer science, political science, and philosophy. The group concluded that companies like Google and OpenAI, which control the largest known language models in the world, have only a matter of months to set standards around the ethical use of the technology before it’s too late. The DeepMind paper joins a series of works that highlight NLP shortcomings. In late March, nearly 30 businesses and universities from around the world found major issues in an audit of five popular multilingual datasets used for machine translation. A paper written about that audit found that in a significant fraction of the major dataset portions evaluated, less than 50% of the sentences were of acceptable quality, according to more than 50 volunteers from the NLP community. Businesses and organizations listed as coauthors of that paper include Google and Intel Labs and come from China, Europe, the United States, and multiple nations in Africa. Coauthors include the Sorbonne University (France), the University of Waterloo (Canada), and the University of Zambia. Major open source advocates also participated, like EleutherAI, which is working to replicate GPT-3; Hugging Face; and the Masakhane project to produce machine translation for African languages. Consistent issues with mislabeled data arose during the audit, and volunteers found that a scan of 100 sentences in many languages could reveal serious quality issues, even to people who aren’t proficient in the language. “We rated samples of 205 languages and found that 87 of them had under 50% usable data,” the paper reads. “As the scale of ML research grows, it becomes increasingly difficult to validate automatically collected and curated datasets.” The paper also finds that building NLP models with datasets automatically drawn from the internet holds promise, especially in resolving issues encountered by low-resource languages, but there’s very little research today about data collected automatically for low-resource languages. The authors suggest a number of solutions, like the kind of documentation recommended in Google’s stochastic parrots paper or standard forms of review, like the datasheets and model cards Gebru prescribed or the dataset nutrition label framework. In other news, researchers from Amazon, ChipBrain, and MIT found that test sets of the 10 most frequently cited datasets used by AI researchers have an average label error rate of 3.4%, impacting benchmark results. This week, organizers of NeurIPS, the world’s largest machine learning conference, announced plans to create a new track devoted to benchmarks and datasets. A blog post announcing the news begins with the simple declaration that “There are no good models without good data.” Last month, the 2021 AI Index, an annual report that attempts to define trends in academia, business, policy, and system performance, found that AI is industrializing rapidly. But it named a lack of benchmarks and testing methods as major impediments to progress for the artificial intelligence community."
https://venturebeat.com/2021/04/09/consilient-ucsf-health-intel-deploy-confidential-computing-to-safeguard-data-in-use/,"Consilient, UCSF Health, Intel deploy confidential computing  to safeguard data in use","Sponsored by Intel Confidential computing and privacy-preserving analytics are relatively new terms in the tech world. Yet they’ve continued to gain wider interest as a way to solve the complex challenges of protecting and securing data in use and algorithm code bases on secure public clouds, at the edge, and in data centers — especially for large compute requirements around artificial intelligence. This emerging approach provides a secure, hardware-based platform (confidential computing) for multiple parties to collaborate, allowing an algorithm to compute on sensitive data (via privacy preserving analytics) without exposing either to the other party. Together, the technologies support secure enclaves, virtual machines, databases, and more. Interest has been strong, especially in health care, finance, and government. Two early-adopter examples show the possibilities: In health care, the University of California San Francisco (UCSF) is leveraging its BeeKeeperAI Zero Trust solution to accelerate the validation of clinical-grade AI algorithms that can safely and consistently perform at the point of care. This markedly reduces time and cost, while addressing data security concerns using the latest confidential computing solutions. Consilient’s next-generation, AI-based system fights money laundering and the financing of terrorism, increasing fraud detection by more than 75%. Both cases illustrate how confidential computing can enable collaboration while preserving privacy and regulatory compliance. Unique challenges face developers of health care AI aimed at improving clinical outcomes and workflow efficiencies. UCSF’s Center for Digital Health Innovation (CDHI) experienced these first-hand with partner GE Healthcare in their co-development of the industry’s first regulatory-cleared, on-device health care AI solution. To ensure equitable care for all patients, health care AI applications are required by regulators to be ethnically, clinically, and geographically agnostic. Validating AI models that perform consistently across multiple variables, including race, workflows, and treatment settings requires timely access to highly diverse, non-biased data. UCSF’s BeeKeeperAI Zero Trust platform helps accelerate health care AI by applying privacy-preserving computing to protected, harmonized clinical and health data from multiple institutions. In its proof of concept, BeeKeeperAI leveraged Intel Software Guard Extensions (SGX) and Fortanix’s confidential computing platform running in Microsoft Azure CCE cloud to enable several key protections: Taken together, these security measures help health care organizations protect their data while accelerating the research and development of clinical AI to improve patient care and operational efficiencies. BeeKeeperAI accelerates the development of health care AI by supporting single-source contracting and standardization —  across multiple academic medical institutions — of arms-length harmonization and transformation of data, which remains in the secure control of the data owner at all times. BeeKeeperAI technology is being employed to create a diagnostic screening tool for diabetic retinopathy, the primary cause of blindness in diabetic patients. AiVision’s Olyatis solution uses artificial intelligence algorithms to read images of the eye fundus (the part opposite the pupil). Here’s how it will work: During Phase One collaboration, the BeeKeeperAI confidential computing enclave, powered by Fortanix, will be deployed into the UCSF HIPAA-compliant Azure environment. There, secured by Intel’s SGX technology, UCSF will place an encrypted fundus photo data set into a secured container. AiVision will place the encrypted algorithm into a secure container. Upon mutual agreement, the Olyatis model will move into UCSF’s secure container, where it will be run on the encrypted photo data set.The resulting output will be a validation report documenting the sensitivity and specificity of the algorithm’s performance on the encrypted data. Throughout this process, neither data or algorithm will be visible to one another. Every year, financial institutions and governments around the globe spend billions on anti-money laundering and countering the financing of terrorism (efforts known in the industry as AML/CFT). Unfortunately, experts say the bad actors are winning. The UN estimates that trillions of dollars are laundered every year, roughly 2-5% of worldwide GDP. One study found compliance costs are a hundred times greater than recovered criminal funds. “The current global AML/CFT system is an outdated model that requires a new 21st-century design,” says Juan Zarate, co-founder and chairman of Consilient, a fintech company launched in October. The first-ever Assistant Secretary of the U.S. Treasury for terrorist financing and financial crimes, and global co-managing partner and chief strategy officer at K2 Integrity, Zarate says Consilient is reinventing today’s costly and ineffectual system with “revolutionary” next-generation architecture, governance, and analytics. Consilient’s Dozer technology uses transfer-learning, so that models can be trained across multiple sets of training data, allowing financial institutions to collaborate without putting private data at risk. This secure computing uses Intel SGX technology, which uses a hardware-based Trusted Execution Environment (TEE) to help isolate and protect specific application code and data in memory. The company says its “leapfrog” approach lets financial institutions, authorities, and other regulated actors discover and manage evolving and complicated illicit risks more proactively, effectively, and efficiently. Consilient helps overcome one of the biggest challenges hamstringing financial crime fighters: sharing information among siloed institutions. Today’s reactive system relies on individual institutions to detect and report suspicious activity on their own, explains Zarate. Policy and operational concerns — especially data privacy and security — inhibit information sharing, dynamic feedback, or collective learning, both within and outside the institution. The resulting blind spots make it difficult to identify illicit activity in “island” systems and economies. This paved the way for a behavioral-based, ML-driven governance model that allows its algorithm to access and interrogate data sets in different institutions, databases, and even jurisdictions — without ever moving the data. To make this work, Consilient employs federated learning, a model for distributed machine learning across large and/or diverse datasets. The algorithm, not data, is exchanged between servers at different institutions, enabling it to detect illicit activity more accurately within their networks. Secure, private processing and analysis take place within a protected memory enclave created by Intel SGX built into Xeon processors. This “black box” approach eliminates the need to create complex webs of trust, where data or code could still be exposed to an untrusted party. It facilitates cross-industry machine learning, while still helping to maintain the privacy of individual data and the confidentiality of proprietary algorithms. Consilient can analyze, identify, and find “normal” and “abnormal” patterns in datasets that human eyes and most current technologies cannot see. Traditional rule-based screening and AML/CFT systems typically have a false-positive rate exceeding 95%. Self-learning Dozer technology has proven to reduce the false-positive rate to 12%. With Consilient, government and financial institutions can detect enormous amounts of illicit activity quickly, with far greater accuracy. The company says this capability will help organizations save costs, redeploy personnel, and more effectively prioritize efforts to counter illicit finance. Industry has taken notice; several banks are evaluating the Consilient solution. “Identifying and disrupting the financial networks of criminal enterprises is a top priority for our member banks,” says Greg Baer, president and CEO of the Banking Policy Institute. “This promising technology presents new opportunities to more effectively identify illicit financing at the source.” Go deeper: Confidential Computing with Intel Software Guard Extensions (video)Federated Learning through Revolutionary Technology (white paper)Why Intel believes confidential computing will boost AI and machine learning (article)  Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/09/tableau-adds-support-to-einstein-discovery-for-user-control-over-ai-models/,Tableau adds support to Einstein Discovery for user control over AI models,"Salesforce’s Tableau arm is making a case for employing AI to drive a new era of business science using analytics applications infused with machine learning algorithms the average end user can easily employ. Tableau has added support for Einstein Discovery to the 2021.1 update of its namesake analytics application. Originally developed by Salesforce, the Einstein Discovery module uses machine learning algorithms to surface patterns in data. Including that capability within Tableau will make it possible for business users to employ data science techniques to analyze data without having to write code and without any intervention on the part of a data scientist team, Tableau CTO Andrew Beers said. Prior to being acquired by Salesforce in 2019, Tableau had begun to embed machine learning algorithms within its application, along with natural language processing (NLP) capabilities. The merger with Salesforce enables Tableau to leverage Salesforce’s ongoing Einstein research and development efforts. As analytics applications continue to incorporate capabilities such as Einstein Discovery, organizations will need to determine when the AI model that needs to be constructed is simple enough for end users to create using a graphical tool versus adding yet another project for a data scientist team to complete. “That line today is kind of blurry,” Beers said. Data scientists are, of course, in short supply. And most data science teams have a backlog of projects they are not likely to complete anytime soon. Most are fortunate to be able to successfully complete more than a few projects in a year. However, the backlog of projects a data science team is being asked to complete might decline as more AI capabilities are added to analytics applications. The bulk of AI models end users might ask a data scientist to build would never be built in the first place, as there simply isn’t enough time. The only way to address those requirements is to democratize AI within the context of an application such as Tableau. Those efforts will lead to a set of best practices for business science that will be distinct from the more complex AI projects a typical data science team will take on, said Beers. It’s not quite clear what level of expertise will be required to enable end users to build their own AI models. Einstein Discovery comes with several built-in capabilities for surfacing bias and promoting AI model transparency. But most organizations would be well-advised to review AI models created by end users before making any business decisions that cannot be reversed. In the meantime, organizations should expect AI models to become a lot easier to create using natural language processing (NLP) engines that are increasingly providing support for various speech recognition engines. Business users will be able to invoke those engines to create AI models that would, for example, make it much simpler to engage in what-if analysis involving multiple scenarios. That kind of capability is more valued than ever in the wake of a pandemic that proved conclusively that all business assumptions are subject to rapid change. In fact, as the pace of change continues to accelerate in the age of digital business, it’s doubtful the average user will be able to keep pace without some AI help. That doesn’t mean AI models will replace the need for business analysts, but it does mean the days when data science required a small army of specialists are coming to an end as the AI playing field becomes increasingly level."
https://venturebeat.com/2021/04/09/the-deanbeat-welcoming-xbox-head-phil-spencer-and-diverse-speakers-to-gamesbeat-summit/,The DeanBeat: Microsoft’s Phil Spencer joins our most diverse GamesBeat Summit event,"Black Lives Matter. Border trouble. Anti-Asian hate. Trans athlete sports bans. Sexual harassment. Suicides and the pandemic. These racial, cultural, and social issues are making headlines, and they have spilled into the game industry. It is an important time to take stock of where we are as an industry when it comes to dealing with racism, sexism, and other kinds of toxicity. Just how diverse and inclusive are we? Diversity, inclusion, and mental health challenges are going to be big topics for discussion at our GamesBeat Summit 2021: Growing the Next Generation event coming on April 28 and April 29. Our latest slate of speakers includes Phil Spencer, the executive vice president of gaming at Microsoft, and a panel of Microsoft leaders including Agnes Kim, a senior strategic partner manager for content partnerships; Esteban Lora, the director of external technology and suppliers; and Cierra McDonald, a principal program manager. They’re going to talk on a panel entitled “Team Xbox on Gaming for Everyone.” And they will discuss Microsoft’s journey to bring the joy and community of gaming to everyone on the planet. They will explore the challenges of ensuring that gaming is inclusive for all players and that the game industry welcomes all creators. I’m delighted that important leaders like Spencer care deeply about these issues and that they’ve chosen our conference as a destination to highlight them. But I’m also proud that we’ll have speakers who are off the beaten path and bring new perspectives to our GamesBeat community. About 37 of our 79 speakers (so far) come from diverse backgrounds. That’s about 47% of our speakers who are women, Black, LatinX, or Asian American. We can of course do better, but I’m proud of that ratio, as it’s not easy to accomplish in an industry where women and minorities aren’t well represented. This is our most diverse conference ever. Diversity is a long game, and I’ve waited a long time to hold an event like this — ever since the Los Angeles riots of 1992. Since we last talked about the event, we’ve added speakers including Alina Soltys of Quantum Tech Partners, Katie Madding of Adjust, Katie Jansen of AppLovin, Victor Lazarte of Wildlife, Jason Docton of Rise Above the Disorder, Mark Chandler of TIGS, Ian Fitzpatrick of New Balance, Glen Schofield of Striking Distance Studios, N’Gai Croal of Hit Detection, Stanley Pierre-Louis of the Entertainment Software Association, Kahlief Adams of Spawn On Me, ZombaeKillz, Hemal Thaker of Goldman Sachs, Gabrielle Heyman of Zynga, Brad Hart of Perforce, Christian Kelly and Seth Shuller of Accenture, Ryan Mullins of Aglet, Laura Higgins of Roblox, and Anthony Crouts of Tencent. We’re going to talk about why representation matters at companies (Activision Blizzard gave us a whole panel of women leaders to talk about this), and we’re going to dive into how it makes a difference in the kind of games that get created. We’ve also tapped mental health experts and leaders to talk about issues like burnout, crunch, suicide, and working in the pandemic. Halley Gross, the co-writer of The Last of Us Part II, will talk about why Naughty Dog’s masterpiece, which won 215 Game of the Year Awards in 2020, was so infused with diversity from the non-player characters to strong female leads. I believe it’s important to celebrate the lessons of important games like this one, and to give people a chance to talk about issues that reflect the fact that we just went through one of the most difficult years in modern history. We’re going to celebrate the industry and have a little fun too, now that optimism is coming back. Phil Spencer is the executive vice president of gaming at Microsoft. In this role, Spencer is accountable for leading Microsoft’s gaming business across all devices and services. With his team and game development partners, Spencer continues to push the boundaries of creativity, technical innovation and fun across gaming genres, audiences and devices. Spencer is both a passionate gamer and seasoned gaming executive serving more than 15 years in the gaming industry leading global business, creative and engineering teams. Spencer has held various roles across Microsoft including the head of Xbox; corporate vice president, Microsoft Studios; and GM, Microsoft Game Studios EMEA. In these roles, Spencer led the Xbox organization with the launches of Xbox One S and Xbox One X as well as Microsoft Publishing and the acquisition of Minecraft, and he influenced blockbuster game franchises including Halo, Gears of War, and Forza Motorsport. He also led the expansion of cross-platform gaming with Xbox Live, which now counts more than 50 million monthly active users. Before beginning his career as an intern with Microsoft in 1988, Spencer earned his bachelor’s degree from the University of Washington. He currently serves on the board of Entertainment Software Association and of The Paley Center for Media. Here’s more info about Spencer’s crew: Agnes Kim is leading gaming content partnerships in South Korea, China, and Southeast Asia. Her goal is to engage developers in these markets and bring more content into Xbox’s ecosystem to ensure that our gamers can experience all the amazing content from the world. She was an avid gamer from very young, spending a lot of time in PC Cafes in South Korea and always dreaming of working in a cross section of Entertainment and Technology. She worked at Sony Pictures Entertainment prior to joining Xbox focusing on partnerships and strategic alliances. Before that, she worked at Deloitte consulting concentrating most of her time in M&A projects. Esteban Lora manages the External Technology and Suppliers team at Xbox Game Studios, which is responsible for business relationships with gaming middleware companies, external development studios and music supervision talent supporting all Microsoft franchises and first party Studios including Forza, Halo and Gears of War. He studied computer engineering and was influenced by his interest in 3D graphics and passion for video games, then complemented his technical background by specializing on business management and strategic partnerships in the software industry, where he has been active for more than 20 years working for blue chip companies worldwide. Originally from the south side of Chicago, Cierra McDonald is a principal program manager at Xbox. She has enjoyed 12 years working on Team Xbox and a total of 17 years at Microsoft. Cierra founded the Blacks at Xbox employee community in 2015, through which she organized Xbox’s annual Blacks in Gaming networking event at the Game Developers Conference (GDC) and started the Jerry Lawson Grant for Career Development. In 2019, the International Game Developers Association (IGDA) Foundation presented Cierra with their inaugural Jerry A. Lawson Award for Achievement in Game Development in recognition of her accomplishments as an engineer, advocate, and community leader within the gaming industry. Cierra is passionate about encouraging the youth to embrace gaming as both a hobby and a career where they can express themselves creatively and can develop proficiency across many disciplines, ranging from computer science and research to music and storytelling. She volunteers with youth-oriented nonprofit organizations such as Gameheads and IGNITE and is an advisory board member for the University of Washington Certificate in Game Design. The question is what will keep the growth going. We’ll explore new parts of the business including blockchain and nonfungible tokens, the post-IDFA world, augmented and virtual reality, esports, the metaverse, and the explosion of opportunities that come from having an unprecedented amount of money coming into the industry through investments, public offerings, and acquisitions. Not long ago, I announced the lineup includes Laura Miele of Electronic Arts, Robert Antokol of Playtika, Aaron Loeb of Scopely, Jen Oneal of Blizzard Entertainment, Geoff Keighley of The Game Awards, Lydia Bottegoni of Blizzard, Eunice Lee of Activision, Nour Polloni of Beenox, Ronnie Nelis of Lion Castle, Sushama Chakraverty of Prodigy Education, Wanda Meloni of M2 Insights, Keza MacDonald of the Guardian, Kelli Dunlap of American University, Noah Falstein of The Inspiracy, Brennan Spiegel of Cedars-Sinai, Susanna Pollack of Games for Change, Paul Doyle of Epic Games, Vladimir Mastilović of Epic Games, Caroline Stokes of Forward, and Eve Crevoshay of Take This, Chris Hewish of Xsolla, and Matt Casamassina of Rogue Games. Our other announced speakers so far include Bobby Kotick of Activision Blizzard, Brenda Romero of Romero Games, Jens Hilgers of Bitkraft Ventures, Chris DeWolfe of Jam City, Keisha Howard of Sugar Gamers, Daniel Melville (who has a bionic arm based on Konami’s Metal Gear series), chief gaming architect Frank Azor of Advanced Micro Devices, Iron Galaxy leaders Adam Boyes and Chelsea Blasko, Eric Goldberg of Playable Worlds, Andrew Sheppard of Transcend Funds, Simon Zhu of NetEase, Mike Vorhaus of Vorhaus Advisors, Michael Metzger of Drake Star, Ed Fries of 1Up Ventures, Raffael “Doctor B” Boccamazzo of Take This, Rob Lowe of Lego Ventures, Itamar Benedy of Anzu, Mike Minotti of GamesBeat, Steve Peterson of Storyphorce, Tim Guhl of Singtel, Lisa Cosmas Hanson of Niko Partners, Karsten Lund from Lightbrick Studios, and Liontree’s Nick Tuosto. We want to continue our reputation as the most intimate gaming event where business meets passion. Our event will include fireside chats, panels, and small-group roundtables. We’ll provide Q&A sessions for VIP attendees, and a way for attendees to network with each other and make new connections. We have a wide range of partners including the International Game Developers Association and Women in Gaming International. And our sponsors include Lego Ventures, Anzu, Xsolla, Jam City, Adjust, Accenture, Rogue Games, Accenture, Epic Games, Scopely, Lego, Singtel, the Entertainment Software Association, Wildlife, Perforce, Outfit7, and more. And one more thing: I’m also doing a regular Game Industry event on Clubhouse about the week’s news in gaming on Fridays at 2:30 p.m. Pacific time."
https://venturebeat.com/2021/04/09/verizon-study-companies-relaxed-mobile-security-policies-get-employees-online/,Verizon finds companies relaxed mobile security policies to get employees online,"Security was not a priority for many organizations as they rushed to get employees up and running a year ago, Verizon said, adding that this was especially true for mobile security. According to the fourth annual Verizon Mobile Security Index, 45% of organizations sacrificed mobile security for expediency over the past year. Interestingly, the number of organizations experiencing a compromise or breach dropped to 23% over the past year, a slight decrease from 27% in 2018. The Verizon Mobile Security Index is based on interviews with 865 business professionals located in the U.S., U.K., and Australia who are responsible for buying and managing mobile and IoT devices for their companies. With VPN and Wi-Fi connectivity inconsistent and unreliable for many remote employees during the pandemic, mobile devices and cloud applications quickly became the go-to platforms for getting work done. IT teams are still under pressure to grant greater access privileges to less secure mobile devices, often operating on networks the company doesn’t own. IT supports a wider variety of remote workers than ever before, from commuters to road warriors in sales and service, putting more time pressures on them. Add all these factors together, and it’s easy to understand why mobile devices are the most vulnerable across the threat landscape today. Approximately three-quarters of IT teams (76%) are being asked to relax security policies for mobile devices so employees can meet deadlines and achieve business goals. Verizon’s index reflects the conflicts IT teams face between protecting mobile assets and helping employees do their jobs. IT teams recognize that mobile devices pose significant risks to the organization: 40% named mobile devices as the biggest security risk and 50% said risks posed by mobile devices are growing faster than other risks. As part of its Mobile Security Index, Verizon tracks how many companies have four basic protections: changing all default/vendor-supplied passwords; encrypting sensitive data when sent across open, public networks; restricting access to data on a need-to-know basis; and regularly testing security systems and processes. Even though these four items are considered security fundamentals, the Index found just 9% of organizations had all four in place this year, while the average from previous years was 12%. Almost half (49%) said they regularly tested security systems and processes, but just 39% said they regularly changed passwords or restricted data on a need-to-know basis. Even more worrying, 15% did not have any of these four protections in place. The past year has been especially challenging for organizations relying on legacy trusted and untrusted domains to protect the rapidly growing number of mobile devices that needed to be mapped into domains. And this situation isn’t changing anytime soon. Organizations consequently need to treat identity as the new security perimeter and consider a data-centric security model to scale more effectively. The pandemic caught many organizations without enough laptops and tablet devices to outfit their workforce. There was also an acute laptop shortage, with lead times of 16 weeks or more for many models as manufacturers shut down and supply chains faced disruptions. The pressure points over the past year exposed some of the glaring security weaknesses organizations currently have. Too few are getting basic protections right, but the Verizon Mobile Security Index offers ways to improve. Over the past year, 36% of organizations opened access to corporate resources and systems for employees using their own devices, according to Verizon. The Mobile Security Index tracks both BYOD (bring your own device) and BYOPC (bring your own PC), and it shows many organizations began considering BYOD and BYOPC as viable strategies this year. Over 25% of organizations reported already allowing BYOD, but fewer than 25% of organizations supported BYOPC. A key element of BYOD and BYOPC strategies, or any kind of virtual work, is providing secure access to company applications, databases, and internal systems. If not done correctly, there is the risk of large-scale data breaches. The Cybersecurity Framework from the National Institute of Standards and Technology (NIST) makes a strong case for zero trust frameworks for data-centric security in organizations relying on BYOD and BYOPC. Implementing a zero trust framework requires mobile device management (MDM) and unified endpoint management (UEM) to secure endpoints at scale. Having a UEM platform supporting BYOD and BYOPC devices helps ensure every endpoint can be self-diagnosed and self-remediating. Leading providers of MDM and UEM solutions include Ivanti, Hexnode, ManageEngine, and Sophos. Identities are the new security perimeter and why zero trust in a mobile-first, cloud-first IT environment is a clear path forward."
https://venturebeat.com/2021/04/09/accenture-ai-expert-on-how-first-principles-prevent-problems/,Accenture AI expert on how first principles prevent problems,"As more organizations begin employing AI in production environments, it’s clear not everyone has completely thought through how AI will fundamentally change their business. Most of the focus today tends to be on AI to reduce operational costs in the wake of the economic downturn brought on by the COVID-19 pandemic. VentureBeat talked with Fernando Lucini, global data science and machine learning engineering lead for Accenture, about why organizations shouldn’t focus on initial success. Lucini stressed how important it is for organizations adopting AI to keep first principles uppermost in mind. This interview has been edited for clarity and brevity. VentureBeat: Prior to the COVID-19 pandemic, most organizations were struggling when it came to AI. Now we’re seeing more AI than ever. How has the pandemic impacted those projects? Fernando Lucini: It’s been a confluence of events. CEOs are starting to ask “Where has all the money gone?” People started to ask some really deep questions about those investments. We’re thinking more about the value of AI. From a human perspective, companies that were affected needed to get smart because they were squeezed a bit because of COVID. VentureBeat: Is there a danger organizations are now moving too quickly without really understanding AI? Lucini: We all get very excited about AI, but it needs to run with the right kind of controls and ethics. Three years from now, you’re going to be in a land where there’s a model that connects to a model that connects to a model. It will all be intertwined in a complex way. I think there’s a ways to go. VentureBeat: Will different models conflict with one another? Lucini: There are no models interacting yet, but synthetic data is quite exciting. We have customers who literally can’t get ahold of their own data because it’s so protected, so there’s going to be in the modeling world the concept of synthetic data that is a true synthesis. It’s not a copy anymore. It reflects the original pattern but never has any of the original data. I think there’s going to be a lot of synthetic data out in the world. That’s when you’ll see a model created by a bank interacting with a model from an insurance company. As we move along and we get into more complex models, the winners are going to be those that actually have a great handle on things. They understand how things are happening, why they’re happening, and have strong controls and strong governance around how they do things. VentureBeat: Right now it takes a fair amount of time to train an AI model. Will that process become faster? Lucini: I always joke that if you put five software engineers in a room and you give them five hours, no code will be written but they will know how to compile everything and what standards to use. If you put five data scientists in the next room for the same five hours, you’ll get five models based on five different mechanisms that are badly coded but very brilliant. We need to bring those two things together if you want to get the kind of speed of innovation we need. If you just have a few patterns, it’s very clear that you can go from data to model to production in an industrialized way. Where people fall down at the moment is because there have been loads of pilots in the last six months, but none of them can go to production. VentureBeat: Machine learning operations (MLOps) has emerged as an IT discipline for implementing AI. Does this need to be folded into traditional IT operations? Lucini: In time. Data science and ML engineering are in the same group at Accenture. These folks need to have quite a deep understanding of the mechanisms to make these things. They need to have knowledge that is a little bit more specific to the model. I suspect there’ll be specialization for a while. I don’t think that’s going to go away anytime soon. VentureBeat: There’s a lot of talk about the democratization of AI these days using AutoML frameworks. Is that really possible to achieve? Lucini: It’s inevitable that some of these platforms are doing more and more AutoML. I was speaking to a professor at Stanford a couple of weeks ago, and he was telling me that 90% of the people that go to his course on neural nets are not computer science students. The average education of people understanding statistical mathematics is going up. You also need industry expertise. Having somebody who understands how to use a model but doesn’t understand the problem at hand quite as deeply doesn’t work. My view is you’re going to have more AutoML that people can use, but we’re also going to need more guardrails to make sure that whatever it is they’re using is within the scope of safety. Education takes them to a point where they do understand whether they created a monster or not. We’re going to have to add more of these industry people that know more of the science. There are already generalists and citizen data scientists. I joke with CIOs and CEOs that these people can also be dangerous amateurs. Then you have this debate about how people don’t really understand how cars work and they still drive them. We still test people so they can drive cars. There’s a good reason for that, so let’s do the same. It’s important to have enough of an education. VentureBeat: What’s your best advice to organizations then? Lucini. Think about the first principles. If you think about AI as being important to you, then you should think about what is your business strategy for AI? Not how AI is part of your business strategy. Educate yourself sufficiently so you can apply principles to understand how AI might actually make a difference to what you’re doing. The truth is AI has a hidden cost of learning how to do it at scale. “Think 10 times” is the first principle of education."
https://venturebeat.com/2021/04/09/u-s-blacklists-7-more-chinese-supercomputing-entities/,U.S. blacklists 7 more Chinese supercomputing entities,"(Reuters) — The U.S. Commerce Department said Thursday it was adding seven Chinese supercomputing entities to a U.S. economic blacklist for assisting Chinese military efforts. The Commerce Department said the seven were “involved with building supercomputers used by China’s military actors, its destabilizing military modernization efforts, and/or weapons of mass destruction programs.” The department is adding Tianjin Phytium Information Technology, Shanghai High-Performance Integrated Circuit Design Center, Sunway Microelectronics, the National Supercomputing Center Jinan, the National Supercomputing Center Shenzhen, the National Supercomputing Center Wuxi, and the National Supercomputing Center Zhengzhou to its blacklist. China’s foreign ministry spokesperson Zhao Lijian said Beijing will take “necessary measures” to protect its companies’ rights and interests. “U.S. containment and suppression cannot hold back the march of China’s scientific and technological development,” he said at a daily news conference in Beijing on Friday. Companies or others named on the U.S. Entity List are required to apply for licenses from the Commerce Department and face tough scrutiny when they seek permission to receive items from U.S. suppliers. “Supercomputing capabilities are vital for the development of many — perhaps almost all — modern weapons and national security systems, such as nuclear weapons and hypersonic weapons,” Commerce Secretary Gina Raimondo said in a statement. The new rules take effect immediately but do not apply to goods from U.S. suppliers already en route. During the administration of former U.S. President Donald Trump, the U.S. added dozens of Chinese companies to its economic blacklist, including the country’s top smartphone maker Huawei; top chipmaker SMIC, and the largest drone manufacturer, SZ DJI."
https://venturebeat.com/2021/04/09/study-finds-that-facebook-ad-targeting-algorithms-exhibit-gender-skew/,"Facebook failed to fix ad-targeting gender discrimination, study finds","Two years ago, researchers at the University of Southern California published a study showing that Facebook’s algorithms could deliver job and housing ads to audiences skewed by race and gender. The methodology they used didn’t account for differences in the job qualifications of the targeted audiences. But in a new paper, the coauthors of the original research claim to have found evidence of a skew by gender for job ads on Facebook even when controlling for qualifications. “Our results show Facebook needs to re-evaluate how their algorithms that optimize for user relevance or their business goals in a non-transparent way may result in discriminatory job ad delivery,” Aleksandra Korolova, assistant professor of computer science at the University of Southern California and a lead author on the study, told VentureBeat via email. “Our study also shows that, from an external auditing point of view, Facebook has not made visible progress in improving the fairness of its ad delivery algorithms despite prior studies and a civil rights audit that raised concerns about the role its algorithms may play.” In response, a Facebook spokesperson told VentureBeat via email: “Our system takes into account many signals to try and serve people ads they will be most interested in, but we understand the concerns raised in the report. We’ve taken meaningful steps to address issues of discrimination in ads and have teams working on ads fairness today. We’re continuing to work closely with the civil rights community, regulators, and academics on these important matters.” Many previous studies have established that Facebook’s ad practices are at best problematic. This came to a head in March 2019, when the U.S. Department of Housing and Urban Development filed suit against Facebook for allegedly “discriminating against people based upon who they are and where they live,” in violation of the Fair Housing Act. When questioned about the allegations during a Capital Hill hearing in October 2019, CEO Mark Zuckerberg said that “people shouldn’t be discriminated against on any of our services,” pointing to newly implemented restrictions on age, ZIP code, and gender ad targeting. Facebook claims its written policies ban discrimination and that it uses automated controls — introduced as part of the 2019 settlement — to limit when and how advertisers target ads based on age, gender, and other attributes. Platforms like Facebook leverage algorithms to deliver ads to a subset of a targeted audience. Every time a user visits the company’s website or app, Facebook runs an auction among advertisers who are targeting that user. In addition to the advertiser’s chosen parameters, such as a bid or budget, the auction takes into account an ad relevance score, which is based on the ad’s predicted engagement level and value to the user. To determine what skew might be present in these algorithms, the researchers developed an auditing methodology for benchmarking the delivery of job ads, an area where U.S. law prohibits discrimination based on certain attributes. Title VII of the U.S. Civil Rights Act of 1964 allows organizations who advertise job opportunities to only show preference based on bona fide occupational qualifications, which are the requirements necessary to carry out a job function. In the course of experiments with a nearly $5,000 ad campaign budget, the researchers ran ads on Facebook with gender-neutral text and images across three categories: Since their methodology compared two ads for each category, the researchers selected two jobs at companies for which they had evidence of gender distribution differences. They also ran the ads on LinkedIn to compare the initial findings with algorithms on a different platform. According to the researchers, the results show evidence of a statistically significant gender skew on Facebook compared with no gender skew on LinkedIn. Across three campaign runs on Facebook, the Domino’s ad delivered to a higher fraction of men than the Instacart ad — despite the fact that 98% of delivery drivers for Domino’s are male and over 50% of Instacart drivers are female. Moreover, a higher fraction of women on Facebook saw software engineering ads that the researchers created featuring Netflix (where 35% of employees in tech-related positions are female) versus Nvidia (where 19% of all employees are female). LinkedIn had no such skew. “Facebook’s job ad delivery is skewed by gender, even when the advertiser is targeting a gender-balanced audience,” Korolova and coauthors wrote in the paper. “Our findings suggests that Facebook’s algorithms may be responsible for unlawful discriminatory outcomes.” The researchers recommend several steps that might address this skew, including more targeting and delivery statistics, replacing ad-hoc privacy techniques with rigorous approaches, and reducing the cost of auditing. They emphasize that privacy-preserving techniques such as differentially private data publishing, which aims to output aggregate information without disclosing any person’s record, might be able to strike a balance between auditability and privacy. “We recommend ad platforms use approaches with rigorous privacy guarantees, and whose impact on statistical validity can be precisely analyzed, such as differentially private algorithms, where possible,” the researchers wrote. “Overall, making auditing ad delivery systems more feasible to a broader range of interested parties can help ensure that the systems that shape job opportunities people see operate in a fair manner that does not violate anti-discrimination laws. The platforms may not currently have the incentives to make the changes proposed and, in some cases, may actively block transparency efforts initiated by researchers and journalists; thus, they may need to be mandated by law.”"
https://venturebeat.com/2021/04/08/amazon-launches-ml-powered-maintenance-tool-lookout-for-equipment-in-general-availability/,Amazon launches ML-powered maintenance tool Lookout for Equipment in general availability,"Amazon today announced the general availability of Lookout for Equipment, a service that uses machine learning to help customers perform maintenance on equipment in their facilities. Launched in preview last year during Amazon Web Services (AWS) re:Invent 2020, Lookout for Equipment ingests sensor data from a customer’s industrial equipment and then trains a model to predict early warning signs of machine failure or suboptimal performance. Predictive maintenance technologies have been used for decades in jet engines and gas turbines, and companies like GE Digital’s Predix and Petasense offer Wi-Fi-enabled, cloud- and AI-driven sensors. According to a recent report by analysts at Markets and Markets, predictive factory maintenance could be worth $12.3 billion by 2025. Startups like Augury are vying for a slice of the segment, beyond Amazon. With Lookout for Equipment, industrial customers can build a predictive maintenance solution for a single facility or multiple facilities. To get started, companies upload their sensor data — like pressure, flow rate, RPMs, temperature, and power — to Amazon Simple Storage Service (S3) and provide the relevant S3 bucket location to Lookout for Equipment. The service will automatically sift through the data, look for patterns, and build a model that’s tailored to the customer’s operating environment. Lookout for Equipment will then use the model to analyze incoming sensor data and identify early warning signs of machine failure or malfunction. For each alert, Lookout for Equipment will specify which sensors are indicating an issue and measure the magnitude of its impact on the detected event. For example, if Lookout for Equipment spotted an problem on a pump with 50 sensors, the service could show which five sensors indicate an issue on a specific motor and relate that issue to the motor power current and temperature. “Many industrial and manufacturing companies have heavily invested in physical sensors and other technology with the aim of improving the maintenance of their equipment. But even with this gear in place, companies are not in a position to deploy machine learning models on top of the reams of data due to a lack of resources and the scarcity of data scientists,” VP of machine learning at AWS Swami Sivasubramanian said in a press release. “Today, we’re excited to announce the general availability of Amazon Lookout for Equipment, a new service that enables customers to benefit from custom machine learning models that are built for their specific environment to quickly and easily identify abnormal machine behavior — so that they can take action to avoid the impact and expense of equipment downtime.” Lookout for Equipment is available via the AWS console as well through supporting partners in the AWS Partner Network. It launches today in US East (N. Virginia), EU (Ireland), and Asia Pacific (Seoul) server regions, with availability in additional regions in the coming months. The launch of Lookout for Equipment follows the general availability of Lookout for Metrics, a fully managed service that uses machine learning to monitor key factors impacting the health of enterprises. Both products are complemented by Amazon Monitron, an end-to-end equipment monitoring system to enable predictive maintenance with sensors, a gateway, an AWS cloud instance, and a mobile app."
https://venturebeat.com/2021/04/08/mlops-startup-comet-raises-13m-to-launch-model-monitoring-products/,MLOps startup Comet raises $13M to launch model monitoring products,"MLOps startup Comet today announced that it raised $13 million in a series A funding round led by Scale Venture Partners. The capital, which Comet plans to put toward product, sales, marketing, and engineering growth, comes as the company acquires U.K.-based Stakion to bolster the launch of Comet Model Production Monitoring (MPM), a product that enables organizations to track and monitor AI model quality. MLOps, or machine learning operations, encompasses the ways organizations build and deploy models. In the wake of the COVID-19 pandemic, enterprises have accelerated their investments in AI as part of an effort to drive digital business transformations. MLOps platforms could generate annual revenues in excess of $4 billion by 2025, according to Deloitte. That’s not surprising in light of a McKinsey report suggesting that AI, if successfully implemented, could drive about 20% of a company’s earnings. Comet provides self-hosted and cloud-based MLOps solution that allows data scientists and engineers to track, compare, and optimize experiments and models. The ostensible aim is to deliver insights and data to build better, more accurate AI models while improving productivity, collaboration, and explainability across teams.  Comet supports code panels, an ecosystem of plugins, extensions, and visualizations built by the community and industry teams. It also offers tools like the aforementioned MPM, which is designed to provide visibility into model performance throughout a model’s lifecycle, from creation to production. Comet has a rival in Weights and Biases, a provider of a platform for enabling collaboration and governance across teams building machine learning models. Among others, Domino Data Lab, a startup developing a platform focused on enterprises with large data science teams, is vying for a slice of the growing MLOps segment. But Comet appears to have carved out a chunk of the expanding market. It claims to serve “thousands” of users and “multiple” Fortune 100 companies, with 500% revenue growth over the past year. And this latest funding round, which saw the participation of  Trilogy Equity Partners and Two Sigma Ventures, brings the New York-based company’s total raised to over $19 million."
https://venturebeat.com/2021/04/08/klevu-raises-12m-for-ai-that-personalizes-ecommerce-search/,Klevu raises $12M for AI that personalizes ecommerce search,"Klevu, a company developing AI products that personalize ecommerce search and discovery experiences, today announced it has closed a $12 million series A round. This marks the launch of Klevu’s Discovery Suite, an end-to-end solution that captures online shoppers’ intent to boost conversions, average order value, and loyalty. And it will help fund Klevu’s Global Semantic Lab initiative, which aims to build “next-generation” technologies at the intersection of machine learning and natural language processing for ecommerce. While year-over-year consumer spending in the U.S. dipped last month, the pandemic has supercharged ecommerce. According to data from IBM’s U.S. Retail Index, business closures and shelter-in-place orders accelerated the shift to digital shopping by five years, with online shopping projected to grow nearly 20% in 2020. Based on the survey data from BMC and Mercatus, ecommerce grocery orders alone totaled $5.9 billion, up 3.6% from $5.7 billion in August. Cofounded in Finland in 2013 by Nilay Oza, Niraj Aswani, and Jyrki Kontio, Klevu aims to connect people to products they wish to buy. Through AI and natural language processing-powered technologies, the company’s products help merchants deliver relevant experiences powered by behavior.  Klevu says its search product, which plugs into ecommerce platforms like Shopify, continuously learns from shoppers’ interactions to optimize results. It automatically handles typos while identifying and understanding stop words (e.g., “iPhone” in the phrase “iPhone with case”) and adding contextually relevant synonyms to product catalogs. Klevu also enables merchants to fine-tune search results using rules across certain categories and keywords. Klevu’s complementary category merchandising product reacts to trends in real-time data, enhancing the results that customers see on product category pages. Featuring filtering and custom layouts, Klevu claims it improves website usability by delivering a consistent experience across mobile and desktop. “Text mining is all about analyzing a piece of text and identifying hidden meaningful information from within it. At Klevu, several AI techniques are used to make this happen in the ecommerce domain,” a spokesperson told VentureBeat via email. “Our objective is to enrich the catalog with the information, originally missing in the catalog, that the shoppers may search for … In our training data, we have millions of products from 45 different industries and in 30 different languages. The product data is systematically organized with various product attributes and images associated with each product. This data is then further enhanced, automatically, with an average of 10 million shopping signals collected daily directly from the high-intent shoppers to understand how they search, locate products, and use them.” Conversion remains a challenge in the ecommerce space, despite the pandemic-bolstered growth. As of Q1 2019, just 2.72% of ecommerce website visits converted into purchases, according to Statista. In a recent report, KPMG stressed the need to promote relevant products right at the time of checkout, which the firm said can have an outsized impact on buying decisions.  Klevu customer Puma says it saw an over 50% increase in search-led conversions on one of its websites after adopting the platform. Besides Puma, Klevu says more than 3,000 brands, including Yamaha, Callaway, and Jack Daniel’s, have made over 5 billion APIs to its product. “Amazon is well known for product discovery, and Google is known for its content,” Oza told VentureBeat via email. “Klevu marries these two together to provide a seamless shopping journey, ensuring the consumer is served the most relevant products and stays highly engaged. With this investment, we are well-positioned toward our mission of democratizing discovery in online retail. We will further invest into strengthening our leadership in machine learning- and natural language processing-led innovations for online retail that bring data-driven business success for our customers.” Alfvén & Didrikson led Klevu’s latest funding round, with participation from existing investors EVLI Growth Partners, Jerry Pruttz Holding, and Jonas Dromberg. Klevu has over 90 employees across offices in the U.K., U.S., India, Australia, and Sweden, in addition to Finland. And the company has raised over $17.5 million to date."
https://venturebeat.com/2021/04/08/onetrust-raises-210m-to-expand-its-enterprise-compliance-solutions/,OneTrust raises $210M to expand its enterprise compliance solutions,"OneTrust, a privacy, marketing, security, and data governance firm based in Atlanta, Georgia, today announced it has raised $210 million in a series C extension led by SoftBank’s Vision Fund 2, with participation from Franklin Templeton. OneTrust says the round adds a strategic geographical position in Japan — SoftBank is Tokyo-based — as market demands accelerate in the Asia Pacific region and across the globe. According to a Thales report, about 64% of respondents around the world feel that adhering to compliance requirements is a “very” or “extremely” effective way to keep data secure. But compliance is expensive. In a 2017 PricewaterhouseCoopers survey of execs at U.S., U.K., and Japanese tech companies, 88% said their company planned to spend over $1 million preparing for the EU’s General Data Protection Regulation (GDPR) in the run-up to its full May 2018 implementation. A smaller percentage of respondents (40%) said they expected to spend $10 million or more. Kabir Barday, a former developer at BlackRock and former director of product management at Dell-owned VMWare, anticipated the nearly $51.5 billion compliance management market’s growth in 2016 when he founded OneTrust with co-chair Alan Dabbiere, a cofounder of Manhattan Associates and AirWatch. Barday was an early employee at AirWatch, which raised $200 million in 2013 before VMware acquired it for $1.5 billion. OneTrust went on to raise $210 million in a series B round last February at a whopping $2.7 billion valuation — a valuation the firm more than doubled to $5.1 billion in December 2020. This latest cash infusion comes after roughly a year during which OneTrust grew its customer base to more than 8,000 organizations across 100 countries. According to Barday, nearly half the Fortune 500 companies now use the company’s product suite, including brands like Aetna, Oracle, 21st Century Fox, and Salesforce. OneTrust offers a privacy management program that helps companies comply with the GDPR, the California Consumer Privacy Act (CCPA), and hundreds of other global privacy laws by using research portals and automation tools. The company streamlines the intake and fulfillment of consumer and subject rights requests and allows customers to benchmark against peers, map and inventory records of processing, and generate custom reports as data flows through their organization. OneTrust’s tools enable companies to drive opt-in demand while demonstrating full compliance. Businesses can deploy interfaces and experiences from marketing and sales activities that collect consent and preferences and sync them across channels while automating the fulfillment of consumer rights requests and the maintenance of historical consent records from a single portal. On the third-party risk side of the equation, OneTrust assesses IT and non-IT vendors, direct suppliers, services, legal organizations, franchisees and retailers, agents, and contractors with risk mitigation workflows and ongoing monitoring. It prepopulates security and privacy data on thousands of global vendors in total, each with information at the service and product level, and it lets managers create automated rules to trigger reassessment or receive alerts when enforcement actions occur. It’s safe to say that compliance management is a red-hot sector. In 2019, San Francisco-based TrustArc raised a $70 million round of funding to help companies implement privacy and compliance programs; Privitar nabbed $40 million to better enable businesses to engineer privacy protection into their data projects; and InCountry exited stealth with $7 million in seed funding to help multinationals comply with local data residency regulations. Back in 2018, BigID nabbed $30 million to expand its data privacy management platform for enterprises. And at the end of 2019, LogicGate, which provides a platform that automates processes and compliance tracking, raised $24.75 million to invest in content, frameworks, data partnerships, and integration. To stay ahead of the competition, in 2020 OneTrust launched Athena, an AI and robotic automation engine built into the OneTrust platform. After acquiring Seattle-based Integris, OneTrust also rolled out new data governance and guidance, ethics, and privacy products; DataDiscovery, a data discovery and classification solution; and free tools to automate GDPR and CCPA compliance programs. OneTrust Environmental, Social & Governance is the company’s newest software and allows clients to manage initiatives in those areas, like carbon output or diversity, equity, and inclusion programs. More recently, OneTrust acquired Convercent, a Denver, Colorado-based firm developing “whistleblowing” software that lets employees report problems they see within their company — anonymously or otherwise. The purchase came weeks after OneTrust bought Docuvision for its ability to use AI to redact specific information, and it marks the company’s sixth acquisition overall. OneTrust, which is co-headquartered in London, with additional offices in Bangalore, Melbourne, Seattle, San Francisco, New York, São Paulo, Munich, Paris, Hong Kong, and Bangkok, has over 1,500 employees globally. It recently expanded to France with a dedicated team of local privacy and marketing experts and a datacenter, shortly after announcing new operations in Brazil with hosting options and support for Brazilian Portuguese and “dozens” of other languages. OneTrust has raised $920 million in funding to date from investors Insight Partners, Coatue, and TCV, in addition to Softbank and Templeton. The extension brings the company’s series C round to $510 million."
https://venturebeat.com/2021/04/08/computer-vision-development-platform-crowdai-raises-6-25m/,Computer vision development platform CrowdAI raises $6.25M,"CrowdAI, a computer vision development platform, today announced that it closed a $6.25 million series A financing round led by Threshold Ventures. The fundraising coincides with the launch of the startup’s new solution that allows customers to create AI that analyzes images and videos. The AI skills gap remains a significant impediment to adoption in most enterprises, a 2020 O’Reilly survey found. Slightly more than one-sixth of respondents cited difficulty in hiring experts as a barrier to AI deployment in their organizations. In a separate report, this by Deloitte, 23% of “seasoned” AI adopters characterized the gap between their AI needs and current abilities as “extreme.” Devaki Raj founded CrowdAI in an effort to eliminate adoption blockers specifically in the area of computer vision. Prior to starting CrowdAI,  Raj studied statistics and machine learning at Oxford, where she earned a master’s degree before working on Maps and Android at Google. She left Google 5 years ago to launch CrowdAI alongside Nicolas Borensztein and Pablo Garcia. Borensztein previously founded and sold ad platform Ember to Adaptive Media, while Garcia worked at Google on the AdWords API team building out data pipelines. CrowdAI helps develop computer vision models for clients primarily in the property insurance, finance, and technology markets. The first iteration of the platform focused on cutting down time-to-value for developers and data scientists familiar with AI integrations and workflows, but CrowdAI’s newest release — which is available in free and enterprise versions — requires no coding. “Our business model isn’t about selling already-trained models — quite the opposite. We’re focused on providing a platform for subject matter experts who aren’t data scientists to build their own models that solve their specific problem,” Devaki Raj told VentureBeat via email. “For example, natural gas utilities are under enormous public pressure to identify and mitigate leaks and microleaks from thousands of miles of pipeline and related infrastructure. These leaks are easy to see on thermal video, but reviewing all that footage in real-time just isn’t scalable. Using computer vision, a machine learning model could review terabytes of imagery each day looking for methane leaks so humans don’t have to.”  Controversially, CrowdAI has had contracts with the U.S. military, including one with the U.S. Air Force to turn public satellite data into combat insights as part of a prototype. (CrowdAI’s history with the Air Force began in 2018, when it participated in the inaugural cohort of the Air Force Research Lab’s Hyperspace Challenge.) The company has proposed highlighting patterns, movements, and changes in maps used by airmen that might otherwise go unnoticed, as well as compiling, analyzing, and mapping out regions in which the U.S. military operates. In pitches in front of academics, contractors, investors, and Air Force acquisition officers, CrowdAI has demonstrated its technology identifying flooding in the aftermath of Hurricane Harvey, the extent of damage after wildfires, and buildings after bombings in Aleppo. In one presentation, Raj showed the platform mapping all major roads in Syria within six hours. In a previous interview with Wired, Raj declined to say which applications of CrowdAI’s platform she considered off limits. But a CrowdAI spokesperson clarified that the company doesn’t currently have an Air Force contract for combat insights. “Our goal is to equip the next generation of employees to become educated in how computer vision can empower them,” Raj said. “It’s not about teaching computer science or AI per se, but arming them with those skills so they can recognize where in their workflow AI can be their partner.” It’s proven to be a winning strategy. CrowdAI says it saw a 200% increase in its customer base over the past year. “In some ways, the pandemic has accelerated the need for AI across the board. Companies seem acutely aware of the fragility of the supply chain, and are looking for tools and tech that can help make it more efficient and predictable,” Raj continued. “When it comes to computer vision, our goal is to empower — not replace — humans at the companies and organizations we work with. We help them create AI and machine learning to meet their own specific needs, which frees these people up to focus on higher-order tasks, such as decision making and planning. This means the same workforce can now work smarter and faster because they don’t have to spend as much time on analyzing and managing visual data.” CrowdAI has raised over $10 million to date. Other investors that participated in its series A included Susa Ventures, Ron Conway’s SV Angel, Jerry Yang at AME Cloud, and Y Combinator."
https://venturebeat.com/2021/04/08/facebook-dataset-combats-ai-bias-by-having-people-self-identify-age-and-gender/,Facebook dataset combats AI bias by having people self-identify age and gender,"Facebook today open-sourced a dataset designed to surface age, gender, and skin tone biases in computer vision and audio machine learning models. The company claims that the corpus — Casual Conversations — is the first of its kind featuring paid people who explicitly provided their age and gender as opposed to labeling this information by third parties or estimating it using models. Biases can make their way into the data used to train AI systems, amplifying stereotypes and leading to harmful consequences. Research has shown that state-of-the-art image-classifying AI models trained on ImageNet, a popular dataset containing photos scraped from the internet, automatically learn humanlike biases about race, gender, weight, and more. Countless studies have demonstrated that facial recognition is susceptible to bias. It’s even been shown that prejudices can creep into the AI tools used to create art, potentially contributing to false perceptions about social, cultural, and political aspects of the past and hindering awareness about important historical events. Casual Conversations, which contains over 4,100 videos of 3,000 participants, some from the Deepfake Detection Challenge, aims to combat this bias by including labels of “apparent” skin tone. Facebook says that the tones are estimated using the Fitzpatrick scale, a classification schema for skin color developed in 1975 by American dermatologist Thomas B. Fitzpatrick. The Fitzpatrick scale is a way to ballpark the response of types of skin to ultraviolet light, from Type I (pale skin that always burns and never tans) to Type VI (deeply pigmented skin that never burns).  Facebook says that it recruited trained annotators for Casual Conversations to determine which skin type each participant had. The annotators also labeled videos with ambient lighting conditions, which helped to measure how models treat people with different skin tones under low-light conditions. A Facebook spokesperson told VentureBeat via email that a U.S. vendor was hired to select annotators for the project from “a range of backgrounds, ethnicity, and genders.” The participants — who hailed from Atlanta, Houston, Miami, New Orleans, and Richmond — were paid. “As a field, industry and academic experts alike are still in the early days of understanding fairness and bias when it comes to AI … The AI research community can use Casual Conversations as one important stepping stone toward normalizing subgroup measurement and fairness research,” Facebook wrote in a blog post. “With Casual Conversations, we hope to spur further research in this important, emerging field.” In support of Facebook’s point, there’s a body of evidence that computer vision models in particular are susceptible to harmful, pervasive prejudice. A paper last fall by University of Colorado, Boulder researchers demonstrated that AI from Amazon, Clarifai, Microsoft, and others maintained accuracy rates above 95% for cisgender men and women but misidentified trans men as women 38% of the time. Independent benchmarks of major vendors’ systems by the Gender Shades project and the National Institute of Standards and Technology (NIST) have demonstrated that facial recognition technology exhibits racial and gender bias and have suggested that current facial recognition programs can be wildly inaccurate, misclassifying people upwards of 96% of the time. Beyond facial recognition, features like Zoom’s virtual backgrounds and Twitter’s automatic photo-cropping tool have historically disfavored people with darker skin. Back in 2015, a software engineer pointed out that the image recognition algorithms in Google Photos were labeling his Black friends as “gorillas.” And nonprofit AlgorithmWatch showed that Google’s Cloud Vision API at once time automatically labeled a thermometer held by a dark-skinned person as a “gun” while labeling a thermometer held by a light-skinned person as an “electronic device.” Experts attribute many of these errors to flaws in the datasets used to train the models. One recent MIT-led audit of popular machine learning datasets found an average of 3.4% annotation errors, including one where a picture of a Chihuahua was labeled “feather boa.” An earlier version of ImageNet, a dataset used to train AI systems around the world, was found to contain photos of naked children, porn actresses, college parties, and more — all scraped from the web without those individuals’ consent. Another computer vision corpus, 80 Million Tiny Images, was found to have a range of racist, sexist, and otherwise offensive annotations, such as nearly 2,000 images labeled with the N-word, and labels like “rape suspect” and “child molester.”  But Casual Conversations is far from a perfect benchmark. Facebook says it didn’t collect information about where the participants are originally from. And in asking their gender, the company only provided the choices “male,” “female,” and “other” — leaving out genders like those who identify as nonbinary. The spokesperson also clarified that Casual Conversations is available to Facebook teams only as of today and that employees won’t be required — but will be encouraged — to use it for evaluation purposes. Exposés about Facebook’s approaches to fairness haven’t done much to engender trust within the AI community. A New York University study published in July 2020 estimated that Facebook’s machine learning systems make about 300,000 content moderation mistakes per day, and problematic posts continue to slip through Facebook’s filters. In one Facebook group that was created last November and rapidly grew to nearly 400,000 people, members calling for a nationwide recount of the 2020 U.S. presidential election swapped unfounded accusations about alleged election fraud and state vote counts every few seconds. For Facebook’s part, the company says that while it considers Casual Conversations a “good, bold” first step, it’ll continue pushing toward developing techniques that capture greater diversity over the next year or so. “In the next year or so, we hope to explore pathways to expand this data set to be even more inclusive with representations that include more geographical locations, activities, and a wider range of gender identities and ages, the spokesperson said. “It’s too soon to comment on future stakeholder participation, but we’re certainly open to speaking with stakeholders in the tech industry, academia, researchers, and others.”"
https://venturebeat.com/2021/04/08/gitpod-nabs-13m-for-cloud-based-open-source-software-development-platform/,Gitpod nabs $13M for cloud-based open source software development platform,"Gitpod, a cloud-based open source development environment, today revealed it has raised $13 million in a round of funding. The German startup also introduced a handful of new features, including native support for Microsoft’s Visual Studio Code editor. The raise comes amid a boom in activity in the browser-based coding sphere, as developers move away from local development environments to the collaboration-friendly cloud — particularly important in a world that has rapidly transitioned to remote work. Moreover, local development can cause problems when it comes to testing performance and security because not everyone has the same technological setup as the developer. Moving to the cloud helps circumvent many of these problems. “Developers are automating the world, yet they waste a lot of precious energy manually setting up and maintaining development environments,” Gitpod CEO Sven Efftinge told VentureBeat. “Millions of developers are slowed down on a daily basis with tedious tasks to get into a productive state while also facing annoying ‘works-on-my-machine’ problems. Our purpose is to remove all friction from the developer experience. This makes everyone always ready to code and software engineering more collaborative, joyful, and secure.” Gitpod broadly adheres to a similar ethos as continuous integration (CI), a popular software engineering practice that involves automatically merging code changes from multiple developers working on the same project. CI is all about ensuring that developers are committing smaller changes more frequently and shipping new code and fixes more quickly. From Gitpod’s perspective, the ethos basically means that it “listens” to changes within a git repository and prebuilds the source code whenever someone pushes a change to it — these prebuilds, according to Efftinge, are “key to preparing dev environments that are truly ready to code.” “We invented prebuilds so application code, configuration, and infrastructure can all be stored as machine-executable code in your git repositories and applied to dev environments automatically and continuously,” he said. “We are preparing your whole dev environment even before you start. Only then, you are always ready to code with a single click.” This also highlights a licensing limitation of Gitpod’s open-core open source model, as its free self-hosted offering only includes limited prebuild times. Gitpod works with all the main git platforms, including GitHub, GitLab, and Bitbucket, allowing developers to spin up a server-side (i.e. not local) development environment from any repository in just a few seconds. This includes the IDE (integrated development environment) and all the related tools and dependencies needed to run the project, including compilers, interpreters, runtimes, build tools, databases, and application servers. In short, Gitpod enables developers to start coding immediately, bypassing the local setup and maintenance process entirely. Cloud-based coding environments aren’t exactly new, with the likes of Codenvy — which was acquired by Red Hat four years ago — built on the Eclipse Che open source cloud IDE. More recently, we’ve seen a slew of cloud-based developer tools, including GitHub’s Codespaces, which launched in early access last year and is similar to Gitpod in many ways. Then there’s CodeSandbox, which raised $12.7 million in October to help developers create a web app development sandbox in the browser; Replit, a browser-based IDE built for cross-platform collaborative coding that raised $20 million in February; and CoScreen, which exited stealth last month with $4.6 million in funding to bring multi-user screen sharing and editing to remote engineering teams. Not all of these are exactly the same proposition as Gitpod, but they demonstrate that development environments are shifting away from “local.” Gitpod’s decision last August to release its platform under an open source AGPL license was a big move for the company, one that afforded developers more freedom to deploy Gitpod however they want, whether through a SaaS subscription managed and hosted by Gitpod or self-hosted on Kubernetes, Amazon’s AWS, or Google Cloud Platform. And a native integration with GitLab announced late last year will only serve to deepen its appeal. But Gitpod’s big pitch is that it’s not purely focused on IDE — it’s about automating developer environments in the cloud. The company is currently piloting a feature that allows users to benefit from Gitpod while working with third-party IDEs such as GoLand or IntelliJ and connect to Gitpod containers from their local environment. “We built Gitpod in a way that its architecture is scalable and it can work with other IDEs as well,” Efftinge said. “The feature is currently still in beta, but [it’s] important to understand our future direction.” It’s worth noting that Gitpod’s target market is developers, so embracing open source makes a great deal of sense. Developers, after all, play a big part in driving companies’ software buying decisions. “Making it open source builds trust and allows users to become contributors as well, or at least take part in the development process,” Efftinge said. “From a business perspective, buying power in companies shifts toward the individual engineer — for Gitpod to be successful, we have to win the hearts and minds of developers.” Founded out of Germany in 2019, Gitpod had previously raised $3 million in funding. Its latest $13 million cash injection was spearheaded by General Catalyst, with participation from Speedinvest, Crane Venture Partners, and Vertex Ventures. Gitpod claims some 350,000 users, including developers from major businesses such as Google, Amazon, Facebook, Uber, Intel, and GitLab, though Gitpod didn’t confirm whether the companies are paying customers or not. “What we can say is that all of those companies have projects where they use Gitpod to streamline their development workflows for either their own developers or for external contributors,” Efftinge said. Alongside its funding, Gitpod also announced today that it now supports Docker and sudo privileges (a Linux program to give temporary root privileges to specific users), which means developers can now run Docker in their workspaces. And Microsoft’s Visual Studio Code will now also work in Gitpod natively. “You get exactly the same editing experience that you would get if you have VS Code installed locally,” Efftinge said."
https://venturebeat.com/2021/04/08/immutable-x/,Immutable X debuts as marketplace platform for NFT games,"Immutable X has launched today as a platform for games with nonfungible tokens (NFTs). That means that those games will have uniquely identifiable digital items that players can earn or buy or sell, allowing the players to own the items permanently. And Immutable X has created a marketplace for players in games such as Gods Unchained to buy and sell the items they have collected. Immutable X, still in its alpha stage, can be a blueprint for other blockchain games, creator marketplaces, traders, and digital economies. The platform’s mainnet will be open to Gods Unchained players, so they can use a marketplace for buying and selling. Immutable X is the brainchild of Immutable, an Australian game team that runs the NFT trading card game Gods Unchained. Gods Unchained is an important NFT game, as it is built by a 40-person development team headed by Chris Clay, the former director of Magic the Gathering: Arena. Gods Unchained is a “play to earn” game, where players can earn collectibles over time, said Immutable founder Robbie Ferguson in a recent interview with GamesBeat. And they can make money by trading those collectibles, including the unique NFTs which can be proven by the blockchain, the secure digital ledger technology, to not be copies. Ferguson said that the move will help accelerate the mass adoption of NFTs in games. In the past few months, NFTs have exploded in other applications such as art, sports collectibles, and music. NBA Top Shot (a digital take on collectible basketball cards) is one example. Published by Animoca Brands and built by Dapper Labs, NBA Top Shot has surpassed $100 million in sales, five months after going public to a worldwide audience. And an NFT digital collage by the artist Beeple sold at Christie’s for $69.3 million. Investors are pouring money into NFTs, and some of those investors are game fans. “With today’s Immutable X Alpha release, we’ve made this vision reality. Any NFT can now be traded, earned, shared, gamed, and collected completely gas-free (gas fees are associated with creating the NFT) on Ethereum,” said Ferguson. In 2020, Immutable found the solution on the crypto frontier with StarkWare, which tapped the benefit of using the Ethereum cryptocurrency and its security without incurring huge fees. Immutable X is built on top of Starkware’s “Layer 2 scaling” technology. The bottom line is that users don’t have to trust in Immutable lasting permanently in order to keep owning their NFTs. They can just trust in Ethereum. Immutable X’s mainnet is now available as the first Layer 2 solution for NFTs on Ethereum, the company said. “At the end of the day, this security is the whole point,” Ferguson said. “Otherwise, you might as well just make a new centralized database. “What our company has become focused on is to scale these games and these applications in a way that is best for users, and ultimately, still decentralized, while still being super easy for mainstream applications to use. That’s why we decided to build Immutable X. We spent a very long time searching for a scaling solution. We’ve got to make the proposition of NFT ownership available to everyone.” Other solutions to Ethereum are creating alternative, faster cryptocurrencies with different methods of reaching a consensus. But these alternatives aren’t as popular as Ethereum. Another solution is to create a side chain, with a different kind of processing for transactions. But Ferguson said those solutions can fail because their security isn’t still as strong as Ethereum’s. If the security fails, then so does the authenticity of the NFT, and that would be disastrous, Ferguson said. “Ethereum is the leading public blockchain in terms of network effects,” Ferguson said. “We decided to go with a true Layer 2 solution. We use Ethereum for everything we do. We’re just compressing the data on it by zero-knowledge proofs [a verification technique], which allows us to reach really high levels of scale.” Last week, Immutable added some Japanese blockchain games to help build momentum for Immutable X, Ferguson said. Gumi, the publisher of Brave Frontier, invested in Double Jump.Tokyo, and MCH spun out of Double Jump.Tokyo. Gumi is the largest shareholder of Double Jump.Tokyo. MCH+ is the name of the partnership between Double Jump.Tokyo and MCH. Together, Double Jump.Tokyo and MCH+ are taking four games to Immutable X: My Crypto Heroes, My Crypto Saga, Crypto Spells, and Brave Frontier Heroes. The transition will be live as Immutable X comes online soon, and once it is done, gamers will be able to truly own the one-of-a-kind items that they buy in games. My Crypto Heroes has lifetime sales of 26,000 Ethereum, or $41 million. NFTs use blockchain technology, the secure and transparent digital ledger behind cryptocurrencies such as Bitcoin and Ethereum, to uniquely identify digital objects. Blockchain can verify the authenticity of something by spreading the verification out among a bunch of computers, a validation process known as reaching consensus, so that you can verify the authenticity and rarity of a particular NFT. But in the past, developers and creators creating NFT games faced a lot of friction to unlock the network effects of a scaled marketplace and economic potential of secondary NFT markets. Those parties faced high “gas fees” per mint or trade. Ferguson said the integration will allow Immutable X users to save on gas fees (the fees for doing transactions on the blockchain) and enjoy more attractive game mechanics. Under the partnership, Immutable will be powering the minting and trading of all games under Double Jump.Tokyo and MCH+ through its scaling protocol for NFTs on Ethereum, Immutable X. Ethereum itself enables NFTs, but it is slow because it handles only about 15 transactions per second and the transaction fees (known as gas fees) can be enormous compared to the price of the digital item. To deal with those problems, Immutable created Immutable X, a Layer 2 scaling solution that sits on top of Ethereum but can handle 9,000 transactions per second and zero gas fees. It also has instant trade confirmation and is designed to make the world of NFTs less opaque, Ferguson said. Ferguson said Immutable X is secure as a scaling solution because it relies on the Ethereum security system itself to guarantee transactions. That’s important because someone could attack the consensus system of a blockchain to try to wrest control of an item. Attacking Ethereum is a lot harder than attacking other solutions such as side chains, Ferguson said. Ethereum has the strongest community around self-custody and decentralization. But it does have its downsides. Ethereum transactions use a lot of energy for verification, and people have pointed out that could prove costly for the environment over time. Ferguson acknowledged that is a drawback, but he also said Ethereum and Bitcoin have a lot of mainstream support and are the strongest available cryptocurrencies. They could thus draw a mass market. Over time, Ferguson believes Immutable X will have a solution for energy usage. Immutable X is ensuring any NFT activity on the protocol will be completely carbon neutral by purchasing carbon credits to offset gas consumed on Ethereum. Ferguson said other projects on Immutable X include Double Jump.Tokyo Inc / MCH+ (My Crypto Heroes, Crypto Saga, Crypto Spells, Brave Frontier Heroes, Chojo), Mintable (NFT marketplace), SuperfarmDAO (decentralized finance NFT farm), Epics GG (collectibles), Illuvium (auto battler role-playing game), Guild of Guardians (mobile RPG), and Crypto Assault (strategy MMO). Rivals include Enjin and Dapper Labs’ Flow. Immutable was started three years ago as Fuel Games, and it saw success with its second game, Gods Unchained, which is a kind of digital Hearthstone. The company now has 90 employees, and it raised $15 million about 18 months ago. Gods Unchained has tens of thousands of players playing every week — with more than 2.5 million matches played — and it has grown tenfold over the past quarter, Ferguson said. That has been enough to make Immutable profitable. But Gods Unchained hasn’t been able to scale up so far, and that’s why Immutable built Immutable X, Ferguson said. As to how Immutable can get millions of players, Ferguson said that the Japanese games have large numbers of players — as many as a million — by comparison. That’s why their move onto Immutable X is so important. “The games are going to get there,” Ferguson said. “The fact that the revenues are here means that this is the correct value proposition for game developers. With the NFT craze with art, that is going to be tremendously good.”"
https://venturebeat.com/2021/04/08/securiti-releases-ai-powered-data-privacy-and-security-platform-to-provide-unified-controls/,Securiti releases AI-powered data privacy and security platform to provide unified controls,"The proliferation of network configurations has added new layers of complexity to digital security. As enterprises embrace approaches like multi-cloud deployments and APIs, it can be more difficult than ever to maintain a broad view of the most sensitive data and detect attacks. But for Securiti CEO Rehan Jalil, the evolution in network architecture represents a big opportunity for companies to embrace a more unified approach to data management and security. To address this emerging challenge, Securiti today announced the release of its new privacy and control platform. The goal is to unify data security, privacy, governance, and compliance across all types of networks. This is particularly critical as more sensitive data moves online, creating even more enticing targets for hackers. “Data brings two big O’s,” Jalil said. “One big O is the opportunity, and we all know about it. But the other big O is the obligations. And those obligations are tied to making sure that you can keep it secure.” The company also announced that Cisco had invested an undisclosed amount as part of a security partnership. In a blog post, Cisco Investments director Prasad Parthasarathi wrote that Securiti had made big strides toward overcoming the fragmented approach to security. The deal is also part of Cisco’s growing investment in security. “The issues of security, privacy, governance, and compliance for sensitive assets and data have been addressed in separate silos,” he wrote. “Legacy architectures have proven to be ill-equipped to handle all these needs, particularly at today’s hyperscale environments with data across hundreds of different types of data systems.” Rather than taking a piecemeal approach to different aspects of data security, the company set out two years ago to build a broad platform, Jalil said. He described the company’s approach as creating a “cyber mesh of a perimeter of security around the data wherever it exists.” With the expanding number of environments, data is becoming increasingly distributed, making it a bigger challenge to manage security, privacy, and compliance. This has given rise to a market of solutions known as Data SPaC. “Often these things are done in silos,” Jalil said. “But our company provides all three capabilities.” The key to enabling that is the company’s sensitive data intelligence (SDI) technology, which helps automate the creation of the security perimeter while building a framework around how data is being used internally. Using artificial intelligence, the SDI discovers, classifies, tags, and catalogs sensitive data that can be found scattered across all cloud environments and on-premises. Once the company has a unified view of that data, Securiti’s tools can help do everything from monitoring to protection and remediation. In addition to catching Cisco’s eye, Securiti has landed on CB Insights’ list of the 100 most innovative AI companies. The company has also raised $81 million in venture capital to date."
https://venturebeat.com/2021/04/08/the-b2b-ecommerce-boom-will-continue-beyond-the-pandemic/,The B2B ecommerce boom will continue beyond the pandemic,"Presented by Amazon Business The last few years have seen a rapid rise in B2B ecommerce adoption across industries. Buyers are shifting purchasing to digital channels to streamline operations and gain access to millions of sellers, while sellers seek new customers and greater efficiency. While buyers’ and sellers’ digital transformations during the pandemic spurred eprocurement adoption, the rise in digitization preceded COVID-19 — and will outlast it. B2B ecommerce has empowered businesses by boosting efficiency and providing a more diverse supplier base, and it will continue to do so as more and more businesses shift online. Today’s purchasing leaders play a critical role in their organizations’ success. They’re being asked to essentially reinvent procurement to free up more time and resources to go directly to support their organizations’ core goals and missions. The disruptions of 2020 shone a light on the importance of efficient, streamlined procurement processes that can be adapted to meet unexpected challenges. In this context, it’s become clear that digital purchasing offers a level of agility, resiliency, and efficiency that simply isn’t possible with traditional manual processes. For example, prior to the pandemic, Exxon Mobil consolidated thousands of transactions into a new procure-to-pay system that allowed employees to purchase supplies from a B2B online store. When the pandemic hit and new supply needs surfaced, the fact that Exxon Mobil had already automated routine purchases enabled their teams to get what they needed quickly and maintain operations globally, while keeping costs in check. Shifting to eprocurement yields efficiency gains for buyers of all sizes, especially when online stores integrate with their internal accounting systems. For large enterprise and government buyers, purchasing through a multi-seller online store that integrates with an enterprise resource planning (ERP) solution enables real-time expense management and can help reduce the overall cost of operations. Small buyers can see efficiency gains from similar integrations that automatically import and categorize purchases from online stores into bookkeeping software, eliminating the need for tedious manual reconciliation. Once organizations understand the efficiencies possible with eprocurement, few if any return to old offline processes. While buyers of all sizes are going digital, the shift has been particularly notable for large enterprise and government buyers, which represent some of the fastest-growing segments in B2B ecommerce today. Digital purchasing is attractive to these large buyers in part because it helps them meet goals around diversifying their supplier bases. For government buyers in particular, it can be important to purchase from local sellers or sellers that possess certain nationally recognized certifications, such as those for small, woman-owned, minority-owned and LGBT-owned businesses. However, sellers with these qualifications sometimes struggle to connect with large buyers through traditional means. For example, a 2020 study by Censeo Consulting Group found that 93% of small businesses experienced “significant barriers” to reaching government buyers. Buyers can use a multi-seller online store with in-depth seller profiles to easily find sellers with desired characteristics, whether that’s a diversity certification or location in a local zip code. Reporting tools help buyers track their spending with sellers in different categories. By enabling buyers to manage seller relationships at scale, features like these also help large enterprises and government entities work with a wide array of small businesses more easily. We’ve already seen these trends accelerate growth for many small, diverse and local sellers. For example, certified Black- and veteran-owned small business Aldevra raised its sales on Amazon Business 315% in part by leveraging its diversity certifications online. Less than five years after joining an online store, the medical and food service equipment supplier is now a contractor for multiple state and local governments and government agencies including the U.S. Department of Defense. The shift to eprocurement benefits both large buyers and small, diverse sellers — and that’s likely to fuel continued growth for a long time. At Amazon Business, we’ve witnessed the accelerating growth of B2B ecommerce firsthand. Within a year of our 2015 launch, we reached 1 million customers and $1 billion in sales. Continuing that momentum, today we are serving more than 5 million customers and have reached $25 billion in worldwide annualized sales. The rise in B2B e-commerce adoption is not just a pandemic trend. Buyers and sellers alike are embracing digitization because it serves their long-term goals for savings, efficiency and supplier diversification. In the future, online stores will continue to innovate and evolve, unlocking new benefits for users — and powering continued growth in 2021 and beyond. Alexandre Gagnon is VP of Amazon Business Worldwide. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/08/tines-which-helps-enterprise-security-teams-automate-repetitive-workflows-raises-26m/,"Tines, which helps enterprise security teams automate repetitive workflows, raises $26M","As businesses seek ways to address the heavily reported technical skills gap, automation is playing an increasingly big role in their efforts. This is perhaps no more evident than it is in the cybersecurity fray, which has a longstanding skills shortage that is only widening, and which has been embracing AI and other automated tools. It’s against that backdrop that Tines is setting out to grab a slice of the $153 billion cybersecurity market, with a rules-based no-code platform that helps enterprise security teams automate repetitive workflows. To help, the company today announced a fresh $26 million investment led by Addition, with participation form Accel and Blossom Capital. Tines can perhaps be best summarized as something like IFTTT for cybersecurity teams, insofar as it enables personnel to pre-build what the company calls “automation stories” that trigger specific steps whenever a particular event occurs — for example, this could be to carry out a log and threat intelligence search to figure out whether a security alert requires further action. For context, most enterprise-level companies employ security professionals that are dedicated to spotting and responding to cyberattacks. They will typically use various automated tools to help them, though these tools can create a lot of noise and false alarms. And this is where Tines shines, as it helps teams automate many of the manual steps that follow after an alert is triggered, freeing security personnel to focus on more mission-critical work. A drag-and-drop interface also opens things up to non-coding team members such as security analysts. Tines is built upon a visual storyboard where users can access seven agents, or “action types,” to construct their automations. They can be combined in any number of ways to create complex automation flows — for example, the webhook agent and email agent can be configured so that the relevant on-call personnel is automatically notified whenever a change is made to a GitHub repository. “When we launched Tines, our core product principal was that every single enterprise process could be broken down and automated using just seven types of action,” Tines cofounder Eoin Hinchy told VentureBeat. “With Tines, you take the process you want to automate, break it into steps, and decide which of these seven action types is best suited to perform that step. This makes the platform extraordinarily easy to learn — there are only seven things you need to know — and once you know them, you can automate anything.” Tines also serves up a bunch of prebuilt sample agent configurations to support some of the more common requirements — for example, to automatically send a Slack message whenever a specific type of security alert is triggered. Tines sports dozens of integrations out of the box, including CrowdStrike, Snyk, PagerDuty, Gmail, Box, Datadog, Okta, Facebook, Freshdesk, and more. However, Tines can in fact be integrated with any tool that has an API, something Hinchy says separates it from other players in the security orchestration, automation, and response (SOAR) space — its users are not restricted to the integrations it has created. The Dublin-based startup has amassed an impressive roster of clients since its founding just three years ago, including Box, Databricks, Sophos, OpenTable, Canva, and soon-to-be Okta-owned Auth0. At the time of its series A raise back in 2019, Hinchy said that while security would remain its core focus, some of its customers were already using the platform to support other workflows across IT, developer operations (DevOps), and even HR, raising the possibility that Tines could broaden its horizons in the future. Some 16 months on, Tines for now remains focused on the security realm. But with a fresh $26 million in the bank, the company is well-financed to expand its product “beyond the scope of security automation” over the next year or so. According to Hinchy, roughly one-quarter of its customers are using Tines outside of security, in places where “there’s a mission critical process that’s complex and time consuming,” he said. “Tines is flexible enough to support any workflow.” While Hinchy added that they were in “no hurry” to officially open the floodgates to the broader enterprise automation market, it will meander down that road. “We will stay focused on security for the next year while being opportunistic and selectively working with teams and companies in other parts of the enterprise,” he said. A typical example outside the security sphere might involve removing user-generated content that breaches company policy, updating help desk tickets, and employee onboarding. It’s worth noting that some of these use cases are supported by existing tools, but if a company is already paying to use Tines to orchestrate their security, then they may as well use it to automate other workflows across their business. Or to liven things up in a virtual office environment, they could also use Tines to randomly send dad-jokes to their colleagues as this user did — replete with SMS alerts.  In terms of pricing, Tines offers a basic free “community” edition, while its main hosted cloud product starts at $2,999 per month with support for 10 users and 250 actions. Elsewhere, the Tines platform can also be deployed on any public cloud or on-premises. “Tines can run anywhere,” Hinchy said. “Although the most common deployment mechanism for Tines is our hosted cloud, we have customers that run in Google Cloud, Azure, DigitalOcean, AWS, bare metal, private datacenters, and even FedRAMP-compliant facilities.” Tines said that it more than tripled its revenue last year. While it’s not clear how much of that can be directly attributed to the pandemic, Hinchy thinks that the increase in demand he’s seen for Tines is down to the fact that companies are trying to be more efficient. “They want to use their resources more intelligently and reduce margin for human error,” he said. “This has never been more true than in the last 12 months.” Moreover, as remote work has emerged as the “new normal” for millions of workers, this has perhaps created a more competitive landscape for technical talent. Jobs that are more rewarding and which involve fewer tedious tasks are likely to be more appealing, which is where Tines can help. “The sudden rise in remote, flexible work means that top talent can work for any company from almost anywhere,” Hinchy said. “As a result, we’ve had conversations with an increasing number of CISOs and CIOs that want to empower staff with the tools they need to automate their manual workloads end-to-end, allowing them to refocus on higher-impact, more rewarding, engaging projects. Ultimately reducing risk of burnout while also increasing job satisfaction.”"
https://venturebeat.com/2021/04/08/tasktop-nabs-100m-to-turn-devops-metrics-into-visualizations-at-scale/,Tasktop nabs $100M to turn DevOps metrics into visualizations at scale,"Value stream management (VSM) platform Tasktop today announced that it raised $100 million, bringing its total raised to over $129 million. The company says it plans to use the funding to accelerate growth while expanding the size of its customer base. As traditional businesses pour billions into digital transformation initiatives, they often struggle with the complexity of the teams, tools, and metrics at the core of those investments. Technical leaders deeply understand the software development process and business leaders know the investment strategies, but the two aren’t always aligned. In a study, Geneca found that 75% of executives surveyed admitted that their projects were either “always” or “usually” doomed right from the start. Vancouver, Canada-based Tasktop, which was founded in 2007, offers a VSM platform designed to reduce time to market and increase the velocity of software development. Sitting above the software development toolchain, Tasktop integrates with software development tools like Jira Software, ServiceNow, Azure DevOps, and more to allow organizations to see potential blockers.  VSM was born out of the frustration that most enterprises aren’t adequately adaptive. According to Forrester, only 16% say that they can release software more than once a month. For Tasktop’s part, the company asserts that VSM allows organizations to break down silos as well as identify and remove bottlenecks, eliminate waste, and accelerate delivery. Tasktop’s platform overlays the value stream to provide abstractions, visualizations, and diagnostics that measure all types of software delivery work. Connectors let customers send work between different dev tools, eliminating duplicate data entry and automating traceability. And Tasktop’s testing regimen runs 500,000 tests daily over 300 tool versions to ensure they work properly, handling tooling and API changes to minimize outages and delays. Coinciding with the new funding, Tasktop this morning launched a dashboard within its Tasktop Viz product — VSM Portfolio Insights — that rolls up analytics generated at the individual product value stream level to the executive plane. The dashboard presents consolidated insights into the performance, quality, value, and impact of delivery, including:  Since the birth of VSM, the market category has grown exponentially compared with the longer-tail development of agile and DevOps. The expansion speaks for itself with players like ServiceNow, IBM, Digital.ai, and of course Tasktop joining the fray. Last December, Tasktop announced record year-over-year growth with a 30% uptick in both revenue and customers. The company now claims to serve leading brands, including over half of the Fortune 100. Sumeru Equity Partners led Tasktop’s latest funding round, a strategic investment. Management and existing investors also participated."
https://venturebeat.com/2021/04/08/automated-corporate-spend-management-platform-ramp-nets-115m/,Automated corporate spend management platform Ramp nets $115M,"Corporate credit cards are pretty much a dime a dozen, as long-established players like American Express vie with newer VC-backed players such as Brex, Divvy, and Moss. To stand out against established players like Amex, it has become increasingly important to go beyond offering a simple piece of plastic and differentiate under the hood. Against this backdrop, New York-based Ramp launched in early 2020 as a comprehensive corporate credit card and automated spend-management platform that bypasses convoluted or duplicitous reward programs with the promise of saving companies money. To further this goal, Ramp today confirmed rumors that it has raised $115 million in fresh funding from notable names including Stripe and Goldman Sachs at a valuation of $1.6 billion. “Ramp’s unique differentiator is our savings-focused approach to spend management,” cofounder and CEO Eric Glyman told VentureBeat. “It is a well-known fact that complex rewards programs offered by incumbent corporate cards actually encourage businesses to spend more — ultimately hurting their bottom line. So we designed a corporate card aligned with the interests of our customers, which is time and money savings.” Ramp is in many ways a classic corporate card: Businesses distribute them to employees who use them for job-related purchases, and then everything is charged directly back to the company. Businesses can get an unlimited 1.5% cash back on their purchases, while Glyman says various automated smarts can save finance teams “hundreds” of hours each month. And it’s here, on the spend-management software side, that Ramp wants to make its mark. Admins and managers can set individual spend limits on a per-employee level, stipulate spending rules based on company policies and more. Using machine learning techniques, Ramp can identify duplicate spending — for example, if different teams are paying for the same software licences when a single licence should cover the whole organization. It can also flag “troublesome spend patterns,” surface partner rewards that might have gone unused, and even negotiate better rates with vendors, according to Glyman. More broadly, Ramp leverages ML to power its savings insights feature, which combines transaction data with customer feedback to “regularly deliver relevant and tailored savings insights for each customer,” according to Glyman. Companies can also automate many of the laborious processes involved in managing their accounts, such as chasing missing receipts for expenses — they set a policy and Ramp automatically messages the employee to ask them to submit a photo of their receipt. Ramp then uses optical character recognition (OCR) to verify and match the receipts to transactions automatically. The pandemic has transformed entire industries, and the corporate expense sphere is no different. Travel booking and management company TripActions essentially had to pivot last year to a broader spend management platform incorporating travel and expenses, and a few months back it raised $155 million at a $5 billion valuation. Ramp is a corporate credit card, but it also enables businesses to manage all of their corporate spending, powered by direct integrations with accounting software such as Quickbooks, NetSuite, Xero, and Sage Intacct. “The biggest problem Ramp solves is financial software consolidation,” Glyman said. “As organizations grow, the tools they use to manage payroll, out-of-pocket expenses, [and] AP/AR creates burdensome manual processes for finance teams and a fractured view of companywide spend for executives. Ramp combines corporate cards, expense management software, reimbursements, vendor management, and reporting and analytics — all in one free package.” The company said transaction volume on Ramp has grown fourfold over the past six months and is now at a run rate of $1 billion annually. But perhaps more interestingly, it said one-third of its customers, which include billion-dollar businesses such as Ro, Better, ClickUp, and Applied Intuition, switched from American Express, and more than 90% ditched existing spend management tools such as Expensify and Concur for Ramp’s offering. Like many others in this space, Ramp offers the card for free, and there are no transaction fees or interest. Ramp gets a cut from banks’ interchange fees, and it also plans to eventually make money from its own software platform as part of a SaaS subscription. “Ramp is currently free of charge — we are currently exploring seat-based pricing for our software offering,” Glyman said. “[There is] no set date on when we will launch new pricing.” Ramp has now raised a total of $320 million in debt and equity financing since it was founded in 2019. Its latest cash injection was spearheaded by D1 Capital Partners and Stripe, with participation from Goldman Sachs, Founders Fund, Coatue Management, Thrive Capital, Redpoint Ventures, Box Group, Neo, and Contrary Capital. Glyman said the company already has plans for allocating the funds. “We’re laser-focused on three areas of product development this year — more sophisticated workflow capabilities for mid-market and enterprise customers, payment consolidation for a wider swath of business expenses, and strategic partner integrations that will deliver customers a seamless experience across their IT, finance, and HR software stack,” he said."
https://venturebeat.com/2021/04/08/trisalus-life-sciences-and-md-anderson-announce-strategic-research-collaboration-to-evaluate-treatment-of-solid-tumors/,TriSalus Life Sciences and MD Anderson Announce Strategic Research Collaboration to Evaluate Treatment of Solid Tumors,"  HOUSTON & DENVER & CHICAGO–(BUSINESS WIRE)–April 8, 2021– The University of Texas MD Anderson Cancer Center and TriSalus Life Sciences®, an emerging immuno-oncology company committed to transforming outcomes for patients with liver and pancreatic tumors, today announced a strategic research collaboration to evaluate the treatment of tumors of the pancreas and liver by integrating interventional delivery of SD-101, an investigational toll-like receptor 9 (TLR9) agonist, in combination with checkpoint inhibition immunotherapy. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210408005178/en/ Under the agreement, MD Anderson and TriSalus will collaborate on studies evaluating the administration of investigational SD-101 intravascularly via TriSalus’ Food and Drug Administration (FDA) cleared, proprietary Pressure-Enabled Drug Delivery™ (PEDD™) technology across a range of liver and pancreatic solid tumors. The initial study will focus on liver metastases from uveal melanoma, followed by studies focused on metastatic disease from pancreatic ductal adenocarcinoma and colorectal cancer. Programs for hepatocellular carcinoma and locally advanced pancreatic ductal adenocarcinoma also are under development. TriSalus will provide funding and technology for the studies. “We’re pleased to collaborate with MD Anderson in pursuit of our collective goal to improve outcomes for patients with tumors of the liver and pancreas. The goal of these studies is to augment the potential of existing therapies through novel drug delivery technology and to investigate the strategic modulation of immune microenvironments with investigational candidate SD-101,” said Steven Katz, M.D., Chief Medical Officer, TriSalus Life Sciences. “Collaborations such as this are an integral part of our development strategy to evaluate treatments to help overcome the challenges inherent to solid tumors and enable a broader population of cancer patients to benefit from immunotherapy.” SD-101 has been evaluated in phase 2 studies in advanced cutaneous melanoma and head and neck cancer.1,2 Early data suggests SD-101 augmented responsiveness to checkpoint inhibitors through stimulation of innate immune cells, along with favorable programming of the T cell population.3 The planned studies under the research collaboration are intended to deliver SD-101 deep into the vasculature of solid tumors using PEDD™, a technique not previously possible using standard delivery approaches.4 Solid tumors continue to represent one of the single biggest hurdles to successful cancer treatment.5 High levels of pressure inside solid tumors prevent the delivery of oncology therapeutics, with less than 1% of therapy penetrating solid tumors in some circumstances.6,7 TriSalus developed PEDD to deliver immuno-oncology therapeutics directly into the vasculature of solid tumors. “Tumors in the pancreas and liver are notoriously difficult to treat effectively, and these patients need new therapeutic options. Our collaboration with TriSalus provides a unique opportunity to evaluate immunotherapy in combination with a novel delivery approach,” said Sapna Patel, M.D., associate professor of Melanoma Medical Oncology at MD Anderson. “We look forward to our work together to advance new treatments aimed at improving clinical outcomes and the lives of our patients.” About TriSalus TriSalus Life Sciences is a revenue generating, emerging immuno-oncology company dedicated to developing immunotherapy treatments for liver and pancreatic tumors using novel delivery technologies to improve patient outcomes. TriSalus intends to pursue multiple solid tumor indications with investigational SD-101 and acquire other immuno-oncology agents to combine with its proprietary Pressure-Enabled Drug Delivery technology for the administration of therapeutics intravascularly into visceral organ solid tumors. In combination with checkpoint inhibitors, TriSalus’ focus is to reprogram the dominant immunosuppressive cell population in liver and pancreatic tumors. This innovative approach in development has the potential to leverage multiple mechanisms that can work together with the goal to overcome inherent immune suppression within the solid tumor microenvironment. For more information, please visit www.trisaluslifesci.com. About MD Anderson The University of Texas MD Anderson Cancer Center in Houston ranks as one of the world’s most respected centers focused on cancer patient care, research, education and prevention. The institution’s sole mission is to end cancer for patients and their families around the world. MD Anderson is one of only 51 comprehensive cancer centers designated by the National Cancer Institute (NCI). MD Anderson is ranked No.1 for cancer care in U.S. News & World Report’s “Best Hospitals” survey. It has ranked as one of the nation’s top two hospitals for cancer care since the survey began in 1990 and has ranked first 16 times in the last 19 years. MD Anderson receives a cancer center support grant from the NCI of the National Institutes of Health (P30 CA016672).  View source version on businesswire.com: https://www.businesswire.com/news/home/20210408005178/en/ TriSalus Media and Investors Lisa DeScenzaVice President, Integrated CommunicationsLaVoieHealthScience(978) 395-5970ldescenza@lavoiehealthscience.com MD Anderson Media Clayton R. Boldt, Ph.D.(713) 792-9518crboldt@mdanderson.org"
https://venturebeat.com/2021/04/08/gupshups-chatbot-authoring-and-multichannel-messaging-platform-gets-100m-boost/,Gupshup raises $100M to expand its chatbot platform,"Gupshup, a messaging-focused customer engagement platform, today announced that it raised $100 million in funding from Tiger Global Management, valuing the startup at $1.4 billion. Gupshup says the proceeds will be put toward its go-to-market, product, and customer acquisition efforts as the company anticipates the close of a second, “significant” additional round. The ubiquity of smartphones and messaging apps — as well as the pandemic — have contributed to the increased adoption of conversational technologies. Fifty-six percent of companies told Accenture in a survey that conversational bots and other experiences are driving disruption in their industry. And a Twilio study showed that 9 out of 10 consumers would like the option to use messaging to contact a business. Gupshup, which was founded in 2014, offers a chatbot authoring service that allows developers to create, deploy, and publish chatbots across over 30 channels. The company claims to send more than 6 billion marketing, sales, and support messages between over 100,000 businesses and 300 million customers via Facebook Messenger, Telegram, Skype, Slack, and more per month.  India is one of Gushup’s largest markets. According to cofounder and CEO Beerud Seth, in recent years, business messaging has become more widespread there as text messages have begun to include hyperlinks and leverage newer IP-based messaging channels like WhatsApp, rich communications services, and Gupshup’s own Gushup IP (GIP). Gupshup pitches GIP as a “smarter,” “more conversational” messaging service that’s compatible with a range of mobile devices. It comes in two flavors, GIP Native (which embeds in native messaging apps) and GIP Widget (a device-agnostic chat widget), and features end-to-end encrypted messages with buttons and rich media like ecommerce flows, mini games, and cards. Gupshup also developed an on-device AI model for message classification and visualization. The model reads incoming messages and divides them into categories and sub-categories, after which it visualizes elements like amount and account number. Server-side AI models for natural language understanding field user queries sent to Gupshup’s chatbots. Gushup occupies a chatbot market that’s anticipated to be worth $142 billion by 2024, according to Insider Intelligence, up from $2 billion in 2019. Gartner predicts that over 50% of enterprises will spend more per annum on chatbot creation than mobile app development by this year. And Juniper Research expects that 75% to 90% of customer queries will be handled by chatbots within the next year.  Even before the pandemic, autonomous agents were on the way to becoming the rule rather than the exception, partly because consumers prefer it that way. According to research published last year by Vonage subsidiary NewVoiceMedia, 25% of people prefer to have their queries handled by a chatbot or other self-service alternative. And Salesforce says roughly 69% of consumers choose chatbots for quick communication with brands. Perhaps reflecting the growing chatbot demand, Gushup had an annual run rate of around $150 million as of 2020. It’s expecting over 50% growth in for next few years. “The growth in business use of messaging and conversational experiences, transforming virtually every customer touchpoint, is an exciting secular trend,” Tiger Global Management partner John Curtius said in a statement. “Gupshup is uniquely positioned to win in this market with a differentiated product, a clear and sustainable moat, and an experienced team with a proven track record. In addition to its market leadership, Gupshup’s unique combination of scale, growth, and profitability attracted us.” This latest tranche brings San Francisco, California-based Gushup’s total raised to over $150 million. Propr to this in March 2010, the company, which has around 250 employees, closed a $12 million series D led by Globespan Capital Partners."
https://venturebeat.com/2021/04/08/canadian-banks-shrink-workforce-to-cover-big-tech-investments/,Canadian banks shrink workforce to cover big tech investments,"(Reuters) — Canada’s top banks are shedding workers for the second straight year, moving toward leaner operations to satisfy investors demanding returns on tens of billions of dollars that lenders have poured into new technologies. Five of Canada’s six biggest banks cut their workforces 4.4% from a year earlier to a combined total of 291,409 full-time equivalent employees as of January 31. That is down 5.2% from a peak in the third quarter of 2019. Despite growing optimism about a robust economic recovery, loan growth outside of mortgages has been stagnant due to the relatively slow pace of COVID-19 vaccinations in Canada and renewed lockdowns in some major cities. “It’s very difficult to grow” revenues, said Todd Johnson, chief investment officer at BCV Asset Management, which owns shares of all the big banks. Banks are likely to continue investing in technology at levels similar to the past few years, which will be “welcomed by investors as long as earnings and dividends continue to grow, and especially if tech investment displaces some labor costs,” he said. The pullback in headcounts follows combined quarterly year-on-year growth of 4% to 5% in 2018 and 2019 across the six big banks. The cuts have reduced efficiency ratios, or non-interest expenses as a proportion of revenues, by about 2 percentage points from a year ago at most banks, disclosures show. The phenomenon isn’t unique to Canada. U.S. and European banks last year joined Bank of Montreal and Canadian Imperial Bank of Commerce in announcing or resuming layoffs, with the former expected to shrink headcounts by an average of 5-10%. While job cuts at banks in other countries have included technology roles, Canadian lenders are still growing in this area because their digital shift has lagged. Toronto-Dominion Bank has been expanding its technology teams while redeploying employees from temporarily closed branches to other areas, CEO Bharat Masrani said in an interview. TD’s workforce has shrunk by about 0.7% from its peak in the fourth quarter of 2019, following quarterly growth of 4-6% over the prior year. “You should view this as the bank constantly adapting to evolving expectations,” Masrani said. TD declined to comment on its technology spending plans. Bank of Nova Scotia (Scotiabank), which has been divesting some international operations, and BMO, which has been working on improving efficiencies, have had the biggest year-on-year headcount reductions, 9.5% and 5.3%, respectively. Royal Bank of Canada, the country’s biggest lender, has been the only one to grow its workforce, by 1.9% from a year earlier, as it expands its wealth management divisions in the U.S. and Canada. A spokesperson said RBC continues to hire “selectively.” In February, CIBC executives said the bank had saved CA$800 million ($633.91 million) over the past five years by streamlining operations. It reinvested the funds in high-growth areas and accelerated technology spending. The other banks declined to comment. Much of the technology investment so far has gone into automating manual processes, such as enabling online mortgage applications and e-signing documents. Future investments will likely focus on beefing up cybersecurity, upgrading systems, and data and analytics, said Robert Colangelo, senior vice president for credit ratings at DBRS Morningstar. Headcounts are unlikely to “grind lower for years and years,” but they are expected to lag revenue growth, said Goodreid Investment Counsel portfolio manager Brian Madden, who estimates that lenders have invested a combined CA$10 billion annually in technology in the last few years. With labor the biggest part of non-interest expenses and the pandemic’s “unexpected turbo boost” to customer adoption of online banking, “most of the return on investment in tech spend is going to have to come from efficiency gains/headcount reductions,” he said."
https://venturebeat.com/2021/04/08/truelayer-raises-70m-to-build-the-worlds-most-valuable-open-banking-network/,TrueLayer Raises $70m to Build the World’s Most Valuable Open Banking Network,"  LONDON–(BUSINESS WIRE)–April 8, 2021– TrueLayer, Europe’s leading open banking platform, today announced it has secured a $70m Series D investment round led by new investor Addition. The latest raise reflects the growing demand for its open banking-based services and marks another significant milestone for TrueLayer on its mission to open up finance, building an open banking network that brings together payments, financial data, and identity to redefine how people spend, save, and transact online. Existing investors, including Anthemis Group, Connect Ventures, Mouro Capital, Northzone, and Temasek, also participated, with a significant increase to the company’s valuation. Additional investors in the round include Visionaries Club, Surojit Chatterjee (CPO Coinbase), Zack Kanter (CEO Stedi), Daniel Graf (ex-Uber, Google, Twitter) and David Avgi (ex-CEO SafeCharge, CEO UniPaaS). It brings the total investment to date in TrueLayer to $142m. The new funding will be used to fuel global expansion and accelerate the development of premium open banking-based services that will continue to drive innovation and revenue growth for clients. It will also be used to expand TrueLayer’s engineering, product and commercial teams to meet the increasing global demand for its open banking platform. TrueLayer’s API-first platform accounts for more than half of all open banking traffic in the UK, Ireland and Spain, processing billions of pounds in payments. It powers services for some of Europe’s fastest-growing brands, including Revolut, Trading 212 and Payoneer. TrueLayer has a market-leading payment conversion rate that is 22% higher than other providers, according to OpenBanking UK and other bank sources, and up to 40% higher than cards. Merchants offering TrueLayer as a payment method in their checkout have found that on average, 1 in 3 consumers chose to pay via TrueLayer. As a result, open banking is displacing other payment methods, such as cards, as the default payment option online. Over the past 12 months, TrueLayer has expanded its services across 12 European markets, growing payment volumes by 600x, and adding hundreds of new customers across digital banking, eCommerce, trading and investment, wealth management, crypto and iGaming. It has continued to innovate, for example, with the recent launch of PayDirect, combining instant pay-in capabilities with instant pay-outs, to deliver a higher converting, lower fraud method for online payments. “When Luca and I started TrueLayer in 2016, we imagined open banking becoming a new digital channel for solving cost and complexities around payments, digital identity, credit data and much more. We wanted to open up this newly built infrastructure to many businesses and consumers. It is such a joy to see our vision coming alive and open banking based payments quickly becoming the new normal,” commented Francesco Simoneschi, CEO and Co-Founder at TrueLayer. TrueLayer is rapidly expanding as demand for its open banking platform increases, largely driven by consumer demand for digital financial services that work better for them, and give them more control over their financial lives. “We have achieved this milestone thanks to the hard work of our stellar team. Bringing radical new products into the hands of consumers and businesses is incredibly exciting,” explained Luca Martinetti, Co-Founder and CTO at TrueLayer. “This new financial network we are building on top of open architectures has massive long term implications for the whole fintech ecosystem and we won’t compromise our vision in any way.” “That is why it is so important to select investors that can help you to plan for the next 15 years, not the next 15 months,” added Simoneschi. “The Addition team thinks very long term and it has been such a pleasure working together. They complement the incredibly strong group of experienced backers who align with our vision of how financial services are evolving.” Lee Fixel, Founder of Addition, commented: “TrueLayer is ideally positioned to benefit from the trends shaping the future of financial services as more and more companies embed digitally native payments into their platforms. We look forward to supporting the TrueLayer team as they scale their offering and drive continued innovation.” –ends– About TrueLayer TrueLayer provides global financial connectivity through open APIs. Our open banking platform empowers engineers, innovators and enterprises in every industry to create smarter financial services. Founded in 2016, TrueLayer is connected to major banks globally, backed by leading investors including Addition, Tencent, Temasek, Northzone, Anthemis Group, Mouro Capital and Connect Ventures, and trusted by some of the biggest names in fintech including Revolut, Trading 212 and Payoneer.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210408005064/en/ Geoff WhitehousePR and Communications Leadgeoff.whitehouse@truelayer.com +44 (0)7766 555077"
https://venturebeat.com/2021/04/08/survey-finds-96-of-execs-are-adopting-offensive-ai-against-cyberattacks/,Survey finds 96% of execs are considering adopting ‘defensive AI’ against cyberattacks,"“Offensive AI” will enable cybercriminals to direct attacks against enterprises while flying under the radar of conventional, rules-based detection tools. That’s according to a new survey published by MIT Technology Review Insights and Darktrace, which found that more than half of business leaders believe security strategies based on human-led responses are failing. The MIT and Darktrace report surveyed more than 300 C-level executives, directors, and managers worldwide to understand how they perceive the cyberthreats they’re up against. A high percentage of respondents (55%) said traditional security solutions can’t anticipate new AI-driven attacks, while 96% said they’re adopting “defensive AI” to remedy this. Here, “defensive AI” refers to self-learning algorithms that understand normal user, device, and system patterns in an organization and detect unusual activity without relying on historical data. Sixty-eight percent of the executives surveyed expressed concern about attacks employing AI for impersonation and phishing, while a smaller majority said they’re worried about more effective ransomware (57%), misinformation and the undermining of data integrity (56%), and the disruption of remote workers by targeting home networks (53%). Of the respondents, 43% underlined the damaging potential of deepfakes, or media that takes a person in an existing image, audio recording, or video and replaces them with someone else’s likeness using AI. As the report’s coauthors write, when offensive AI is thrown into the mix, “fake email” could become nearly indistinguishable from trusted contact messages. And with employees working remotely during the pandemic — without the security protocols of the office — organizations have seen successful phishing attempts skyrocket. Google registered over 2 million phishing websites since the start of 2020, when the pandemic began — a 19.91% increase compared with 2019. Businesses are increasingly placing their faith in defensive AI to combat the growing cyberthreats. Known as an autonomous response, defensive AI can interrupt in-progress attacks without affecting day-to-day business. For example, given a strain of ransomware an enterprise hasn’t encountered in the past, defensive AI can identify the novel and abnormal patterns of behavior and stop the ransomware even if it isn’t associated with publicly known compromise indicators (e.g., blacklisted command-and-control domains or malware file hashes). According to the survey, 44% of executives are assessing AI-enabled security systems and 38% are deploying autonomous response technology. This agrees with findings from Statista. In a 2019 analysis, the firm reported that around 80% of executives in the telecommunications industry believe their organization wouldn’t be able to respond to cyberattacks without AI. Reflecting the pace of adoption, the AI in cybersecurity market will reach $38.2 billion in value by 2026, Markets and Markets projects. That’s up from $8.8 billion in 2019, representing a compound annual growth rate of around 23.3%. “With the onset of AI-powered attacks, organizations need to reform their strategies quickly, be prepared to defend their digital assets with AI, and regain the advantage over this new wave of sophisticated attacks,” the report’s coauthors wrote. “By automating the process of threat detection, investigation, and response, AI augments human IT security teams by stopping threats as soon as they emerge, so people have the time to focus on more strategic tasks at hand.”"
https://venturebeat.com/2021/04/07/study-suggests-that-ai-model-selection-might-introduce-bias/,Study suggests that AI model selection might introduce bias,"The past several years have established that AI and machine learning are not a panacea when it comes to fair outcomes. Applying algorithmic solutions to social problems can magnify biases against marginalized peoples, and undersampling populations always results in worse predictive accuracy. But bias in AI doesn’t arise from datasets alone. Problem formulation, or the way researchers fit tasks to AI techniques, can also contribute. So can other human-led steps throughout the AI deployment pipeline. To this end, a new study coauthored by researchers at Cornell and Brown University investigates the problems around model selection — the process by which engineers choose machine learning models to deploy after training and validation. The team found that model selection presents another opportunity to introduce bias because the metrics used to distinguish between models are subject to interpretation and judgement. In machine learning, a model is typically trained on a dataset and evaluated for a metric (e.g., accuracy) on a test dataset. To improve performance, the learning process can be repeated. Retraining until a satisfactory model is produced is what’s known as a “researcher degree of freedom.” While researchers may report average performance across a small number of models, they often publish results using a specific set of variables that can obscure a model’s true performance. This presents a challenge because other model properties can change during training. Seemingly minute differences in accuracy between groups can multiply out to large groups, impacting fairness with regard to specific demographics. The coauthors underline a case study in which test subjects were asked to choose a “fair” skin cancer detection model based on metrics they identified. Overwhelmingly, the subjects selected a model with the highest accuracy even though it exhibited the largest gender disparity. This is problematic on its face, the researchers say, because the accuracy metric doesn’t provide a breakdown of false positives (missing a cancer diagnosis) and false negatives (mistakenly diagnosing cancer when it’s not actually present). Including these metrics could have biased the subjects to make different choices concerning which model was “best.” “The overarching point is that contextual information is highly important for model selection, particularly with regard to which metrics we choose to inform the selection decision,” the coauthors of the study wrote. “Moreover, sub-population performance variability, where the sub-populations are split on protected attributes, can be a crucial part of that context, which in turn has implications for fairness.” Beyond model selection and problem formulation, research is beginning to shed light on the various ways humans might contribute to bias in models. For example, researchers at MIT found just over 2,900 errors arising from labeling mistakes in ImageNet, an image database used to train countless computer vision algorithms. A separate Columbia study concluded that biased algorithmic predictions are mostly caused by imbalanced data but that the demographics of engineers also play a role, with models created by less diverse teams generally faring worse. In future work, the Cornell and Brown University team say they intend to see if they can ameliorate the issue of performance variability through “AutoML” methods, which divests the model selection process from human choice. But the research suggests new approaches might be needed to mitigate every human-originated source of bias."
https://venturebeat.com/2021/04/07/enterprise-software-devices-to-drive-2021-it-spending-to-4-1t/,"Enterprise software, devices to drive 2021 IT spending to $4.1T","Global information technology spending will grow 8.4% to $4.1 trillion in 2021, driven in part by enterprises accelerating their digital transformation plans, Gartner said in its latest IT spending forecast. IT spending will be driven by digital business plans that will be refined and completed in 2021, Gartner said. More digital initiatives are originating from business departments outside of IT, making IT a full participant in business value delivery, said John-David Lovelock, a distinguished research vice-president at Gartner. As a result, Gartner expects to see the source of funding to be charged as a cost of revenue or cost of goods sold (COGS). The source of funding changes “from an overhead expense that is maintained, monitored and sometimes cut, to the thing that drives revenue,” Lovelock said in the press release. Every IT spending category is expected to have positive growth through 2022, Gartner said. Device demand will show the most growth, at 14%, followed by enterprise software, at 10.8%. Every category will show solid growth in 2021, as “organizations focus on providing a more comfortable, innovative and productive environment for their workforce,” the company asserted.
For example, organizations are focusing on areas such as social software and collaboration platforms and human capital management (HCM) software to improve employee experience and well-being, Gartner said. Organizations will still focus on optimizing costs and other cost-savings efforts, but the IT spending focus in 2021 will be on revenue growth because there’s more economic certainty, according to the report. “Last year, IT spending took the form of a ‘knee jerk’ reaction to enable a remote workforce in a matter of weeks. As hybrid work takes hold, CIOs will focus on spending that enables innovation, not just task completion,” Lovelock said. IT spending took a hit in 2020, but Gartner estimated that IT spending in nearly every industry sector will recover and surpass 2019 levels within the next few years. Some sectors and regions will recover sooner than others, which will result in a “K-shape economic recovery,” Gartner said in its release. From an industry perspective, banks and financial services IT spending will reach 2019 levels as early as 2021. Retail and manufacturing IT spending will recover at a slower pace, and will not recover to pre-pandemic levels until about 2023. In an interview with VentureBeat, Lovelock was able to go in a bit more detail, noting that most sectors would return to 2019 levels of spending at some point in 2021. Government IT spending was back to 2019 levels exceptionally early, hitting that mark in mid-2020, Lovelock told VentureBeat. Transportation, however, is not expected to really recover until closer to 2025. There are also regional differences. China has already recovered, while Gartner estimates that North America and Western Europe will see IT spending recover in late 2021. Middle East/North Africa also has a quicker recovery path, Lovelock told VentureBeat. There were some variations in Asia, with “mature” Asia/Pacific markets expected to see their spending reach 2019 levels in early 2021, compared to “emerging” Asia/Pacific markets looking closer to 2022, Lovelock said. Latin America IT spending will grow much slowly, with Gartner predicting recovery around 2024. Regional recovery will also likely be influenced by what kind of industry is the most dominant in that region. Countries that may be more manufacturing-heavy will lag behind countries that have a mix of industry sectors, Lovelock said."
https://venturebeat.com/2021/04/07/why-company-culture-is-the-foundation-of-your-companys-growth-vb-live/,Why company culture is the foundation of your company’s growth (VB Live),"Presented by TriNet How do you create a company culture that empowers your employees to grow and gives you a competitive edge? Learn how to build a positive environment, align your culture with your org’s values and goals, and more in the first of this three-part webinar series, The Employee Experience: From Recruitment to Retention. Register here for free. As tragic as the pandemic has been in so many ways, this last year has provided a unique opportunity because companies simply haven’t had a choice – they’ve had to innovate. “There’s this window of opportunity where muscles have been stretched, particularly in the human capital space,” says Kristine Gunn, executive director of talent and organizational management at TriNet. “Companies have had to do things in six-week sprints that would have taken six years to get leadership buy-in.” Up until now, a common theme in organizational strategy many organizations have honed in on is efficiency and operational excellence to the detriment of innovation, Gunn says – not just R&D, but investing in reinventing the company. But this past year has blown the roof off disruption, and the need to keep reinventing and growing will continue to push companies forward, to not only survive the next disruption, but thrive. As part of that, some major employee trends have reared their heads in the past year. By way of example, a large tech CEO has said that digital adoption has moved five years in a period of just months, and digitization is putting tremendous pressure on organizations to adapt. Flexible work environments have become a huge topic of conversation in the human capital community, as employees realize the tremendous work-life benefits and companies learn that employees can work from home and stay productive, even in a fully remote hybrid environment. As part of that, people-centricity has taken center stage, the focus is shifting to the importance of employee well-being, and creating safety in organizations, especially for historically marginalized populations. The public focus on the fight for equity and equality for all has brought home the need to keep moving forward toward diversity and inclusion. These issues were important before, but creating inspiring organizations where people thrive is more important than ever, Gunn explains. “Research in talent and organization management has shown that those companies that have thrived through this trying past year have been those with strong missions, strong purpose – they’ve shown up in their communities,” she says. “Now more than ever, people are looking for organizations that have stepped up and risen to the challenge, with a very mission-driven, purpose-driven culture that is agile and can respond quickly to changes.” At its basic level, a mission-driven culture is one where people show up every day with purpose, and in which leaders have created spaces where their people can thrive. “It’s the environment, the behaviors, leadership,” Gunn says. “It’s how companies approach challenges. It’s how companies make decisions. It’s the unspoken rules about what people believe it takes to thrive – or what they believe it takes to get fired. It’s the conversations by the water cooler. It’s the seen and the unseen.” Culture is the secret sauce that drives employee engagement, innovation, business growth, and continuous reinvention. When the pandemic ends, there will be many more disruptions to come. It’s the companies that build resilience that will succeed and grow. Right now, the conversation has moved toward adapting more agile practices, like cross-functional teaming, and collaborative, empathetic types of leadership, where employees are empowered and equipped with the tools and resources to have authorship over projects, where they’re working together instead of in silos. It’s also about attracting people who have the ability to change, who are not afraid to take risks, and more importantly, who are not punished for taking risks, because there’s positive association to experimentation and failing forward. “When companies look at creating more agile organizations, they’re definitely looking at how to create entrepreneurism, authorship,” says Gunn. “How do they unleash people? How do they create a peer-to-peer environment instead of such a strong hierarchical environment?” In today’s landscape, leaders have to be agile enough to meet the current business challenges, and at the same time create stability and security for their members. It comes from the top down, and requires taking a stand, being intentional, knowing yourself, and knowing your company. It means telling a unique story and being authentic and thoughtful about what you stand for, while staying conscious of the fact that one size does not fit all. Companies have to find their own unique way – what works for Google or a daycare center, may actually derail your org. It also requires being highly in touch with your customer, and your customer promise and building the culture you need to deliver on that promise “Make sure that you have a value system that backs that up,” Gunn says. “Make sure that leaders are making decisions that align with that, particularly under pressure, when it’s easy to sway from your beliefs. Having that profound mission, knowing what’s important to your customers, and living that out – and being non-tolerant of folks that do not align with that culture and that vision that you’re looking to create.” As part of that, diversity and inclusion needs to be embedded into the company’s value system, and in the way decisions about talent are made – from fair and unbiased recruitment in order to attract the best candidates to how employees are onboarded, how they’re promoted, and how long they’re kept. Diversity isn’t just the right thing to do – it directly impacts your company’s attractiveness to talent, and your bottom line. More than two-thirds of active and passive job seekers said that a diverse workforce is an important factor when evaluating companies and job offers – and on average, the most diverse enterprises are the most innovative, achieving 19% higher innovation revenues and 9% higher EBIT margins. “You have to be super mindful of diversity and inclusion as the enabler of an organization’s culture,” says Gunn. “It’s deeply embedded in the values and how work gets done. Organizations have to focus on it. It doesn’t happen by accident. It takes conscious and very intentional effort to make sure that you’re educating people, embedding these practices, and reinventing and evolving. Leaders need to do everything in their power to create safe, happy, and healthy workplaces where everyone can bring their whole unique identity to work and feel valued and respected for their differences. It is more important than ever before.” Don’t miss out! Register here for free. Attendees will learn: Speakers: More to come!"
https://venturebeat.com/2021/04/07/experian-consumers-prefer-invisible-security-to-passwords/,Experian: Consumers prefer ‘invisible security’ to passwords,"Could the era of passwords be drawing to a close? Decades of fumbling around to remember the right password or constantly resetting or having to jump through multiple authentication hoops have made them dirty words for many consumers. All those headaches, and there’s still a good chance your personal information will wind up somewhere for sale on the internet. Perhaps it’s not surprising then, according to a new survey released by Experian, that consumers are embracing new methods of security that are physical and behavior-based. Indeed, according to the company’s 2021 Global Identity and Fraud Report, consumers did not rank passwords among the top 3 most secure ways to protect identity. Instead, the top 3 are so-called “invisible” methods: The Experian survey included 9,000 consumers and more than 2,700 businesses spread across 10 countries. The push to end passwords is getting greater attention from the security industry, enterprises, and venture capitalists. In December 2020, Beyond Identity raised $75 million for its solution that uses digital certificates to replace passwords. And just a few weeks ago, Identiq raised $47 million for a cryptographic network that can be used to confirm identity. The Experian study comes on the heels of a remarkable shift as COVID-19 drove a surge in online activity, from distance learning to remote work to ecommerce. Experian tracked a 20% increase in online consumer transactions over the past year. While that digital convenience became essential to helping businesses and consumers adapt to the pandemic, it also raised security concerns, with 55% of people surveyed ranking security as “the most important aspect of their online experience,” the report says. Of course, it’s a positive sign that consumers are taking security more seriously. The study found that 34% of consumers worry about privacy, up from 29% before the pandemic. Likewise, 33% worry about identity theft, up from 28% one year ago. And 49% have bigger concerns about fraud, compared to just 37% last year. These responses highlight a key challenge as businesses expand their digital footprint: how to securely authenticate real customers without making the process too burdensome, but still weeding out fraud? The answer would appear to be those invisible security strategies. In the survey, 48% of consumers under the age of 40 said they felt safer using biometric security now than before COVID-19, though that number drops to 37% for those over 40. “Consumers want to be recognized digitally without extra steps to identify themselves, and they don’t want to remember yet another password,” said Eric Haller, Experian EVP and general manager of Identity, Fraud and DataLabs, in a statement. “They are open to more practical solutions in today’s digital era.”"
https://venturebeat.com/2021/04/07/snorkel-ais-app-development-platform-lures-35m/,Snorkel AI’s app development platform lures $35M,"Snorkel AI, a startup developing data labeling tools aimed at enterprises, today announced that it raised $35 million in a series B round led by Lightspeed Venture Partners. The funding marks the launch of the company’s Application Studio, a visual builder with templated solutions for common AI use cases based on best practices from academic institutions. According to a 2020 Cognilytica report, 80% of AI development time is spent on manually gathering, organizing, and labeling the data that’s used to train machine learning models. Hand labeling is notoriously expensive and slow, with limited leeway for development teams to build, iterate, adapt, or audit apps. In a recent survey conducted by startup CloudFlower, data scientists said that they spend 60% of the time just organizing and cleaning data compared with 4% on refining algorithms. Snorkel AI hopes to address this with tools that let customers create and manage training data, train models, and analyze and iterate AI systems. Founded by a team spun out of the Stanford AI Lab, Snorkel AI claims to offer the first AI app development platform, Snorkel Flow, that labels and manages machine learning training data programmatically.  Application Studio will expand the Snorkel AI platform’s capabilities in a number of ways, the company says, by introducing prebuilt solution templates based on industry-specific use cases. Customers can leverage templates for contract intelligence, news analytics, and customer interaction routing as well as common AI tasks such as text and document classification, named entity recognition, and information extraction. Application Studio also provides packaged app-specific preprocessors, programmatic labeling templates, and high-performance open source models that can be trained with private data, in addition to collaborative workflows that decompose apps into modular parts. Beyond this, Application Studio offers a feature that versions the entire development pipeline from datasets to user contributions. With a few lines of code, apps can be adapted to new data or goals. And they keep training data labeling and orchestration in-house, mitigating data breach and data bias risks. Application Studio is in preview and will be generally available later this year within Snorkel Flow, Snorkel AI says. Palo Alto, California-based Snorkel AI’s latest fundraising round brings the startup’s total raised to date to $50 million, which 40-employee Snorkel AI says will be used to scale its engineering team and acquire new customers. Previous investors Greylock, GV, In-Q-Tel, and Nepenthe Capital, along with new investor Walden and funds and accounts managed by BlackRock, also participated in the series B."
https://venturebeat.com/2021/04/07/captivateiq-raises-46-million-to-automate-sales-commission-programs/,CaptivateIQ raises $46 million to automate sales commission programs,"Incentive compensation platform CaptivateIQ today announced that it raised $45 million in series B financing led by Accel. The company says it plans to use the funding to expand its reach, as well as to develop AI technologies that benefit sales planning and other parts of the business connected to sales. Management processes around commission programs can be both rigid and expensive. In fact, sales compensation represents the single largest investment for most business-to-business companies. U.S.-based enterprises alone spend over $800 billion on it each year, in aggregate — three times more than what they spend on advertising. CaptivateIQ, which launched in 2017 as part of Y Combinator’s 2018 winter cohort, claims to automate commission workflows using AI. The 90-employee startup was founded by Conway Teng, Hubert Wong, and Mark Schopmeyer, who has a fintech background in investment banking and private equity. Teng was a corporate finance analyst at McKinsey who later helped launch loyalty rewards startup Fivestars and worked on finance at Gusto. “Several years ago, when I was a finance lead at Brightroll, commissions was dumped on me and I was told I needed to build a new Excel model to pay out our team of 30 reps within 14 days,” Schopmeyer told VentureBeat via email. ” As a former banker, it seemed easy, but I spent the next 10 days working until 2 a.m. cleaning up the data, building the financial model, and sending out statements to the entire sales team … I later caught up with Teng and learned he was experiencing a similar pain. One thing led to another and we quit our jobs and brought on our college friend, Wong, and started building a new commissions solution that we wanted to use.” CaptivateIQ’s no-code software collates data from disparate sources like invoices and billing systems to power real-time calculations. Customers can use it to build, preview, and launch commission plans and share them in customized reports. Accel partner Ben Fletcher believes that CaptivateIQ’s value proposition lies in its digital, autonomous approach to tabulation. Companies typically manage commission programs using spreadsheets or costly legacy solutions. These are prone to data drift — a 2017 survey by Xactly found that four in five companies reported inaccuracies in their sales commission payouts. “CaptivateIQ is more than just commission software. Similar to UiPath, Ada, and Celonis, CaptivateIQ’s powerful and easy-to-use no-code platform is automating an important aspect of the sales and finance workflow,” Fletcher said in a press release. “When several of our other portfolio companies kept raving about how much time and headache it was saving their growing sales teams and businesses, we knew something special was happening ” There’s truth to this. While CaptivateIQ isn’t yet profitable, in 2020, the company says that revenue grew 6 times as companies like Affirm, Gong, Intercom, TripActions, and hundreds of other companies used its product. CaptivateIQ claims to have processed over $2 billion in commissions for tens of thousands of employees as of this year. “A big part of our growth is that we can help any company that offers a performanced-based compensation plan, so we don’t have any restrictions with the types of businesses we work with. We typically see conversations start with teams that have a minimum of 25 sales people, though we easily serve enterprises and public companies as well,” Schopmeyer said. “In terms of users, which we define as the number of payees, basically anybody who receives a payout is in our system, so that’s also grown significantly. December 2020 ended up 4 times from the prior year.” Ninety-employee CaptivateIQ says that a portion of the proceeds from this latest funding round will enable it to build AI models that help people understand how taking certain actions will impact future earnings. The idea is that, by being able to model and predict outcomes, companies will be able to drive behavior that benefits their bottom line while helping employees achieve their financial goals. “To start, we’re introducing more powerful data transformations, a richer set of formulas, and off-the-shelf templates,” Conway said. “Another goal of ours is to automate and streamline the end-to-end commissions process. We’re expanding our data integrations to support all major data systems and introducing new dashboarding capabilities that will let teams track performance and unlock business insights. We’re also enhancing existing collaboration workflows around approvals, inquiries, and contracts to enable every stakeholder in the commissions process to stay connected.” The series B in San Francisco, California-based CaptivateIQ announced today brings the company’s total raised to $63 million. Beyond Accel, Amity, S28 Capital, Sequoia, and Y Combinator participated."
https://venturebeat.com/2021/04/07/ataccama-expands-data-management-automation-tools-to-meet-pandemic-driven-needs/,Ataccama expands data management automation tools to meet pandemic-driven needs,"During its digital Ataccama Innovate 2021 event, Ataccama today announced general availability of the next major upgrade to its data management platform, which relies on an expert system and machine learning algorithms to automate processes. The company also released the results of a global survey of more than 1,000 executives and business users — from midsize to large enterprises. The survey finds 79% of executives and 75% of line-of-business users are contending with data quality issues at a time when more than three-quarters of respondents (78%) are collecting and processing more data now than they were prior to the pandemic. Conducted by ResearchScape, the survey also finds over half of users (55%) require additional help to transform data to fit their purpose, with 44% having to wait a day or more to get help from a technical user or IT team. The top three impacts of poor data quality identified by respondents are slower times to data insight (46%), negative impacts to business performance and decision-making (42%), and negative impacts on strategic initiatives (40%). But close to half of survey respondents (43%) admit they have implemented data governance strategies at either a low maturity level or not at all. Specifically, survey respondents are looking to implement processes to better organize data (59%), consolidate documented data (55%), and ensure data is only used for purposes that are compliant with regulations (40%). The survey results suggest executives are now a lot more conscious of the need to consistently manage data as they invest in AI-enabled digital business transformation initiatives that require access to massive amounts of data, Ataccama CEO Michal Klaus said. In fact, the top priority technologies survey respondents identified for the next 12 months are AI or machine learning (43%), data governance (41%), data security (41%), and data quality management (37%). The Ataccama ONE Gen2 platform is an expert system that has been optimized to manage large volumes of disparate data, Klaus said. It enables organizations to create data profiles, track data quality monitoring, set up data previews to better understand the relationship between different datasets, and enforce the policies that determine which end users are allowed to access what data. Earlier this year, Ataccama acquired Tellstory to add data visualization capabilities to the data catalog that has been revamped as part of the latest release of its platform. Tasks and processes that are being enhanced using AI capabilities include data classification, rules suggestion, relationship and data lineage discovery, anomaly detection, pattern matching, data quality monitoring, and integration with master data management (MDM) tools Ataccama provides. As organizations invest more in data lakes and data warehouses, they are discovering the limitation of their existing data management processes. In many cases, the return on investment in those data lakes never materializes because there’s no way to navigate all the data being dumped into them, Klaus notes, adding “They become data swamps.” Ultimately, the goal is to provide a data management platform that makes it simpler for IT teams to enable end users to self-service their own data needs at scale in a way that complies with privacy regulations and other considerations, Klaus said. Most IT organizations are not especially proficient when it comes to data management, and data is typically managed within the context of the application employed to create it. This means applications wind up creating data that is often conflicting because, for example, the way a customer is described varies from one application to the next. Applying advanced analytics to data depends on IT teams addressing data quality issues that stem from the way different application silos have rendered data. It not uncommon for much of that data to also be incomplete or simply wrong. Regardless of the tools employed to clean up that mess, data management will need to become a lot more automated than it is today to address the true scale of the challenges ahead."
https://venturebeat.com/2021/04/07/email-is-the-answer-to-the-death-of-cookies-for-digital-publishers/,Email is the answer to the death of cookies for digital publishers,"Presented by Jeeng For years, digital publishers have relied on third-party cookies to help them learn about audience behavior in a sort of ad hoc, outsourced data strategy. By relying on ad targeting through cookies, they could also infer a bit about their site visitors’ preferences and behavior. But publishers didn’t actually own that data; they only borrowed it from third-party ad servers. And, thanks to privacy concerns — most notably the fact that site visitors had no idea they were being tracked — cookies have now come under fire and will soon be obsolete as the web’s mostly widely-used browsers have moved to eliminate cookies to protect users’ privacy. This leaves publishers in a lurch. Consumers increasingly expect personalized content and experiences, across every experience — from TV to shopping to news and entertainment. And, without it, they’ll take their business elsewhere. Obviously, this is something no publisher can afford right now, especially as Google and Facebook gobble up a progressively larger share of the advertising pie. Publishers need all the well-targeted eyeballs they can possibly get onto their site to drive revenue. Facing tremendous pressure from the death of cookies and a tight budget squeeze, publishers need a new solution — a way to connect with their audiences directly, to cut out the middleman and deliver the personalized content and experiences audiences expect. Enter: email. Converting site visitors to logged-in subscribers through an email address opens a whole new world of opportunity in 1:1 engagement. From personalized content delivery over multiple channels to precise advertising that drives revenue, email is the answer to the death of cookies for digital publishers. Here’s why: Cultivating a direct relationship with known subscribers is vital for publishers to survive in the post-cookie era. In an ironic twist, as cookies die, email — the channel many called dead a decade ago — is the answer to publishers’ needs. Email is not dead, and in fact is breathing new life into the trusted relationships publishers need with their audiences. As digital publishing adapts to a cookie-less world, the winners will be those who can leverage email-based tracking, targeting, and delivery into a 1:1, personalized, automated multichannel experience for their subscribers. To learn more about how digital publishers can leverage email and multi-channel messaging to engage subscribers and drive more revenue, visit www.jeeng.com. Jeff Kupietzky is CEO of Jeeng, formerly PowerInbox. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/07/synthesis-ai-emerges-from-stealth-with-4-5m-to-create-synthetic-face-datasets/,Synthesis AI emerges from stealth with $4.5M to create synthetic face datasets,"Synthesis AI, a synthetic data company, today emerged from stealth with the announcement that it closed a $4.5 million funding round. The startup says that the capital will allow it to expand its R&D team and develop new synthetic data technologies. Self-driving vehicle companies alone spend billions of dollars per year collecting and labeling data, according to estimates. Third-party contractors enlist hundreds of thousands of human data labelers to draw and trace the annotations machine learning models need to learn. (A properly labeled dataset provides a ground truth that the models use to check their predictions for accuracy and continue refining their algorithms.) Curating these datasets to include the right distribution and frequency of samples becomes exponentially more difficult as performance requirements increase. And the pandemic has underscored how vulnerable these practices are, as contractors have been increasingly forced to work from home, prompting some companies to turn to synthetic data as an alternative. Synthesis AI’s platform leverages generative machine learning models, image rendering and composition, and other techniques to create and label images of objects, scenes, people, and environments. Customers can modify things like geometries, textures, lighting, image modalities, and camera locations to produce varied data for training computer vision models. Synthesis AI offers datasets containing 10,000 to 200,000 scenes for common use cases including head poses and facial expressions, eye gazes, and near infrared images. But what the company uniquely provides is an API that generates millions of images of realistic faces captured from different angles in a range of environments. Using the API, customers can submit a job in the cloud to synthesize as much as terabytes of data. Synthesis AI says its API covers tens of thousands of identities spanning genders, age groups, ethnicities, and skin tones. It procedurally generates modifications to faces to reflect changes in expressions and emotions, as well as motions like head turns and features such as head and facial hair. Built-in styles adorn subjects with accessories like glasses, sunglasses, hats and other headwear, headphones, and face masks. Other controls enable adjustments in camera optics, lighting, and post-processing. Synthesis AI makes the claim that its data is unbiased and “perfectly labeled,” but the jury’s out on the representativeness of synthetic data. In a study last January, researchers at Arizona State University showed that when an AI system trained on a dataset of images of engineering professors was tasked with creating faces, 93% were male and 99% white. The system appeared to have amplified the dataset’s existing biases — 80% of the professors were male and 76% were white. On the other hand, startups like Hazy and Mostly AI say that they’ve developed methods for controlling the biases of data in ways that actually reduce harm. A recent study published by a group of Ph.D. candidates at Stanford claims the same — the coauthors say their technique allows them to weight certain features as more important in order to generate a diverse set of images for computer vision training.   Despite competition from startups like Datagen and Parallel Domain, Synthesis AI says that “major” technology and handset manufacturers are already using its API to generate model training and test datasets. Among the early adopters is Affectiva, a company that builds AI it claims can understand emotions by analyzing facial expressions and speech. “One of our teleconferencing customers leveraged synthetic data to create more robust facial segmentation models. By creating a very diverse set of data with more than 1,000 individuals with a wide variety of facial features, hairstyles, accessories, cameras, lighting, and environments, they were able to significantly improve the performance of their models,” founder and CEO Yashar Behzadi told VentureBeat via email. “[Another one] of our customers is building a car driver and occupant sensing systems. They leveraged synthetic data of thousands of individuals in the car cabin across various situations and environments to determine the optimal camera placement and overall configuration to ensure the best performance.” In the future, 11-employee Synthesis AI plans to launch additional APIs to address different computer vision challenges. “It is inevitable that simulation and synthetic data will be used to develop computer vision AI,” Behzadi continued. “To reach widespread adoption, we need to continue to build out 3D models to represent more of the real world and create scalable cloud-based systems to make the simulation platform available on-demand across a broad set of use cases.” Existing investors Bee Partners, PJC, iRobot Ventures, Swift Ventures, Boom Capital, Kubera VC, and Leta Capital contributed to San Francisco, California-based Synthesis AI’s seed round announced today."
https://venturebeat.com/2021/04/07/trifacta-expands-data-preparation-tools-with-databricks-integration/,Trifacta expands data preparation tools with Databricks integration,"Trifacta today announced it has integrated its data preparation tools with a data warehouse platform based on the open source Apache Spark framework provided by Databricks. This is in addition to repositories based on an open source data built tool (DBT) that is maintained by Fishtown Analytics. In both cases, Trifacta is extending the reach of tools it provides for managing data pipelines to platforms that are widely employed in the cloud to process and analyze data, Trifacta CEO Adam Wilson said. Trifacta traces its lineage back to a research project that involved professors from Stanford University and the University of California at Berkley and resulted in a visual tool that enables data analysts without programming skills to load data. In effect, Trifacta automated extract, transform, and load (ETL) processes that had previously required an IT specialist to perform. There is no shortage of visual tools that let end users without programming skills migrate data. But Trifacta has extended its offerings to a platform that enables organizations to manage the data pipeline process on an end-to-end basis as part of its effort to meld data operations (DataOps) with machine learning operations (MLOps). The goal is to enable data analysts to self-service their own data requirements without requiring any intervention on the part of an IT team, Wilson noted. Google and IBM already resell the Trifacta data preparation platform, and the company has established alliances with both Amazon Web Services (AWS) and Microsoft. Those relationships enable organizations to employ Trifacta as a central hub for moving data in and out of cloud platforms. The alliance with Databricks and the support for DBT further extend those capabilities at a time when organizations have begun to more routinely employ multiple cloud frameworks to process and analyze data, Wilson said. In general, data engineering has evolved into a distinct IT discipline because of the massive amount of data that needs to be moved and transformed. While visual tools make it possible for data analysts to self-service their own data requirements, organizations are now also looking to programmatically move data to clouds as part of a larger workflow. Many individuals that have ETL programming expertise, often referred to as data engineers, are now in even higher demand than data analysts, Wilson said. Once considered the IT equivalent of a janitorial task that revolved mainly around backup and recovery tasks, data engineering is now the discipline around which all large-scale data science projects revolve, Wilson noted. In fact, IT professionals with ETL skills have reinvented themselves to become data engineers, Wilson added. “In the last 12 months, data engineering has become the hottest job in all of IT,” Wilson said. It remains to be seen just how automated data engineering processes can become in the months and years ahead. Not only is there more data to be processed and analyzed than ever, the types of data that need to be processed have never been more varied. Going forward, a larger percentage of data will be processed and analyzed on edge computing platforms, where it is created and consumed. But the aggregated results of all that data processing will still need to be shared with multiple data warehouse platforms residing in the cloud and in on-premises IT environments. Regardless of where data is processed, the sheer volume of data moving across the extended enterprise will continue to increase exponentially. The issue now is figuring out how to automate the movement of that data in a way that scales much more easily."
https://venturebeat.com/2021/04/07/whitesource-raises-75m-to-move-beyond-open-source-security-and-compliance-management/,WhiteSource raises $75M to move beyond open source security and compliance management,"WhiteSource, a platform that companies such as Microsoft, IBM, and Comcast use to secure their open source software components, has raised $75 million in a series D round of funding. Founded in 2011, WhiteSource automatically identifies every open source component in a company’s technology stack, then identifies and prioritizes vulnerabilities, issuing real-time alerts on genuine risks it detects. “In order to mitigate open source risks, it’s essential to remediate open source vulnerabilities as soon as they are discovered,” WhiteSource CEO and cofounder Rami Sass told VentureBeat. “However, in most cases it’s impractical to fix all vulnerabilities, and some require major development work. WhiteSource research shows that only 15% to 30% of vulnerabilities are effective — the majority of open source vulnerabilities are not called by the proprietary code.” There is a strong case to support the widely uttered mantra that open source has eaten the world. All the major tech companies not only use open source software, but contribute back to the communities and even open-source their own internal tools. Indeed, most modern software relies on at least some open source components, as it saves the companies that build it time and resources in having to develop and maintain everything themselves. A recent IBM-commissioned study called The Value of Open Source in the Cloud Era noted that most of the respondents (developers and managers) used open source software in some aspect of their operations, while in its recent State of Enterprise Open Source report, Red Hat found that 90% now use enterprise open source in their organizations — up from 89% last year. “We certainly have noticed the trend as well,” Sass said. “Over the past three years, we have seen the numbers of our enterprise customers triple and seen our revenue grow by 800%, underscoring the enormous demand by organizations developing software to effectively manage their use of open source components. In our view, the current pace of enterprise software development, using modern application architecture like microservices and containers, is only sustainable through a high-level of reliance on open source.” Although there are counter arguments that posit the exact opposite, open source software has often been plagued by the notion that it is less secure than its proprietary counterpart. Equifax, for example, blamed its mega 2017 security breach on the open source server framework Apache Struts. In its recent State of Software Security: Open Source Edition report, app security company Veracode found that “open source libraries are ubiquitous and risky,” with 70% of applications containing a security flaw in an open source library, while WhiteSource rival Sonatype recently reported a 430% surge in cyberattacks aimed at “infiltrating open source software supply chains.” Elsewhere, a joint report produced by WhiteSource and the Ponemon Institute found that “more than 70% of enterprise application portfolios have become more vulnerable to attack” in the past year. “There are a number of reasons for the increase of vulnerability in enterprise applications,” Sass explained. “A misalignment between risk-levels and the level of annual spending across different protection layers. The gap is most evident in the application layer, where the percentage of allocated budget is significantly lower compared with the perceived level of risk.” Sass also cited a “lack of a formal approach” to securing the software development life cycle, as well as limited collaboration between development and security teams as other reasons why enterprise applications may have become more vulnerable. Moreover, matters have been compounded by faster software release cycles, with developers expected to ship more code and faster, leading companies to play a delicate balancing act between security and speed. Developers can integrate WhiteSource with many of the popular development environments, including IDEs, so they can see immediately whether any open source component has security vulnerabilities before they make a pull request (i.e., before the component enters a live code base). WhiteSource offers four core plans which offer incrementally more features: free; Essentials, at $2,400 per year; Teams, at $10,000 per year; and Enterprise, which starts at $28,000 per year. The platform includes a dashboard that serves up an overview of an organization’s open source dependencies and license risks, among other data points. Users can dig down into specific vulnerabilities to see where they exist and how they can go about remediating them (such as upgrading to a new version). Although open source is generally free for developers to use, it often has some restrictions in terms of how third-parties are allowed to use it — as such, WhiteSource also helps companies adhere to any licensing policies that are in place. Other notable players in the space, a sector that is commonly referred to as software composition analysis (SCA), include Black Duck, which Synopsys bought for $547 million in 2017; the aforementioned Sonatype, which was acquired by Vista Equity Partners in 2019; and Snyk, which just last month closed a $300 million round of funding at a whopping $4.7 billion valuation. So, what would a world look like without such automated tools? Well, the onus would likely fall on security teams to manually review and approve all the open source components in their tech stack, which is a lengthy, never-ending process of checking and testing. “Sometimes, information security teams may enforce open source security standards and block components from use, without consideration for the implication on development teams,” Sass said. “Other times, developers would use their own tools to detect and avoid open source vulnerabilities, and manage the findings using spreadsheets, with limited visibility to other teams or external auditors.” Prior to now, WhiteSource had raised around $46 million, the chunk of which arrived through its series C round in 2018. With its latest $75 million cash injection — which attracted existing investors including Microsoft’s M12, 83North, and Susquehanna Growth Equity, in addition to Pitango Growth, which led the round — WhiteSource is gearing up to broaden its reach beyond the SCA sphere and into the wider application security testing (AST) space. “This will go beyond detection to offer prioritization and auto-remediation of open source vulnerabilities to cover all threats and all application attack vectors,” Sass said. “Our vision is not limited to open source code, and we will announce more exciting developments in the near future.”"
https://venturebeat.com/2021/04/07/malomo-raises-5m-to-drive-repeat-purchases-through-shipment-tracking/,Malomo raises $5M to drive repeat purchases through shipment tracking,"Shipment tracking platform Malomo today announced it has raised $5 million in seed funding. The company says it will use the capital to make strategic hires, develop new big data tools, and broaden international carrier supports while expanding its customer base. Malomo says the funds will also enable it to integrate its product with marketing automation platform Klaviyo to give merchants greater control over their post-purchase experience. While year-over-year consumer spending in the U.S. dipped last month, the pandemic has on the whole supercharged ecommerce. According to data from IBM’s U.S. Retail Index, business closures and shelter-in-place orders accelerated the shift to digital shopping by five years, with online shopping projected to have grown nearly 20% in 2020. Based on the survey data from BMC and Mercatus, ecommerce grocery orders alone totaled $5.9 billion, up 3.6% from $5.7 billion in August. Indianapolis, Indiana-based Malomo, which was founded in 2018, uses shipment tracking technology to drive repeat sales. Using its solution, retailers can share shipping updates with buyers via webpages, app notifications, and emails. When customers check their order tracking, they’re treated to products, ads, and other content Malomo serves to spur additional purchases. Driving repeat purchases remains a challenge for retailers in the exploding ecommerce space. It’s estimated that 73% of first-time purchasers won’t make a second purchase from the same online seller. But according to Malomo CEO Yaw Aning, delivery experiences can have a major impact on sentiment. Ninety-six percent of respondents to a MyCustomer survey said a positive shipment experience would encourage them to shop with a retailer again.  “The number one question for the billions of people that buy things online is ‘Where is my order?'” Aning said in a press release. “While Amazon spends billions of dollars on [its] post-purchase experience to ease customer anxiety around that question, independent retailers who sell their products directly through their own website to consumers don’t have the same resources to compete. We level the playing field by helping retailers own the customer experience from the buy button all the way to the doorstep and beyond. We help them activate post-purchase as a channel to drive growth, retention, and trust with their fans.” Malomo says on average its clients see support tickets cut in half and a 2% to 3% increase in repeat purchase rate. Customer Grace Eleyae says its click rate climbed 18.7% from its tracking pages, and repeat purchases through its emails and pages were up 8 times. “When COVID forced small businesses to rely entirely on online sales, it became challenging for merchants to maintain meaningful customer relationships that build trust,” said Base10 Partners’ Chris Zeoli, a Malomo investor. “Malomo’s transparent and proactive post-purchase shipment tracking platform absolutely delights customers throughout the purchase and delivery experience and is a no-brainer return on investment for merchants looking to acquire and retain customers.” Beyond Base10, participants in Malomo’s funding round included existing investors Harlem Capital, High Alpha, Hyde Park Venture Partners, Paul Singh, and new strategic investors The Vaan Group, CX Collective, Curtis Cheng, Jeremy Cai, and Alexandre Perrier. The company’s total raised now stands at over $8 million."
https://venturebeat.com/2021/04/07/ourcrowd-50-index-fund-is-now-available-on-the-bny-mellon-pershing-alternative-investment-network/,OurCrowd 50 Index Fund is Now Available on the BNY Mellon Pershing Alternative Investment Network," Advisors on the Pershing platform to gain access to privately held companies at lower minimums than via traditional VC firms  NEW YORK & JERUSALEM–(BUSINESS WIRE)–April 7, 2021– OurCrowd, Israel’s most active venture investor, announced today that its flagship portfolio index fund OurCrowd 50 (“OC50”) has been added to BNY Mellon Pershing’s (“Pershing”) Alternative Investment Network. OC50 is a hyper-diversified investment vehicle enabling investors to participate in the next 50 OurCrowd opportunities in which OurCrowd invests at least $1M. The addition of OurCrowd 50 to the Pershing platform is part of OurCrowd’s initiative enabling Registered Investment Advisors (“RIAs”) and other financial intermediaries in the United States to offer OurCrowd’s venture capital opportunities to their clients for a minimum as low as $50,000 per fund. Ilana Odess, OurCrowd Partner and Managing Director of North America leading the RIA initiative said: “We are very pleased that Pershing has onboarded our flagship OC50 Fund. Pershing is one of the leading clearing and custodial platforms and we are glad that RIAs who clear through Pershing will be able to offer the OC50 Fund to their clients.” “Private companies are offering their shares to the public through an IPO much later in their lifecycle than they did in the late 1990s through early 2000s. RIAs who offer OurCrowd’s venture capital opportunities enable their clients to access privately-held companies at much lower minimums than traditional venture capital firms,” Odess said. OC50 was created as a hyper-diversified investment vehicle to break open the gates and offer more investors access to highly vetted startups across a variety of sectors, stages, and geographies, enabling them to participate in the next 50 OurCrowd investment opportunities across a spectrum of today’s most dynamic growth industries, including healthcare technology, transportation, agriculture, communication, enterprise, robotics, and AI. Through the first three series of the OC50 Fund, investors have benefited from OurCrowd companies entering the public equity markets – such as food company Beyond Meat and insurance provider Lemonade – or others being acquired by companies like Vimeo, H&R Block, Uber, and NetApp. The original OC50 Series I Fund launched in 2017. Six companies in the fund have completed exits, generating on average 2.6 times gross return on invested capital as of the end of Q4 2020.* Financial advisors interested in learning more about this opportunity can contact RIA@OurCrowd.com. END About OurCrowd: OurCrowd is a global venture investment platform that empowers institutions and individuals to invest and engage in emerging companies. With $1.5 billion of committed funding, and investments in more than 240 portfolio companies and 25 venture funds, OurCrowd offers access to its membership of 80,000 individual accredited and institutional investors, family offices, and venture capital partners from over 195 countries to invest alongside, at the same terms. Rated by PitchBook as the most active venture investor in Israel, OurCrowd portfolio companies have been acquired by some of the most prestigious brands in the world, including Microsoft, Uber, Canon, Oracle, Nike, and Intel. To register visit www.ourcrowd.com. *Note: Nothing contained in and accompanying this communication shall be construed as an offer to sell, a solicitation of an offer to buy, or a recommendation to purchase any security by OurCrowd, its portfolio companies or any third party. Information regarding OurCrowd’s limited partnerships and/or portfolio companies and the investment opportunities on OurCrowd’s website is intended and available for accredited investors only (criteria at http://www.ourcrowd.com/). OurCrowd urges potential investors to consult with licensed legal professionals and investment advisors for any legal, tax, insurance, or investment advice. Please be aware that investments in early-stage companies or in venture capital funds contains a high level of risk and you should consider this prior to making any investment decisions. Past performance is not indicative of future results.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210407005063/en/ PRESS CONTACT: OurCrowd: Leah Stern, Partner, Global Communications / Europe: +44 747 0196826 / E: leah@ourcrowd.com"
https://venturebeat.com/2021/04/07/sverica-capital-management-announces-strategic-investment-in-winwire-technologies/,Sverica Capital Management Announces Strategic Investment in WinWire Technologies,"SAN FRANCISCO–(BUSINESS WIRE)–April 7, 2021– Sverica Capital Management LP (“Sverica”), a private equity investment firm, announced today that it has made a strategic investment in WinWire Technologies, Inc. (“WinWire” or the “Company”), a leading data-driven digital engineering company specializing in Microsoft Azure and other Microsoft cloud platforms. Based in Santa Clara, California, WinWire was founded in 2007 with the mission of “stitching the digital fabric” to help organizations gain competitive advantage through innovative software solutions. As a Microsoft Managed Partner and AI Inner Circle Partner with numerous Gold partner accreditations, WinWire has demonstrated extensive familiarity with Microsoft cloud platforms and positioned itself as a technology leader with enterprise clients. With expertise and solution offerings across a range of cloud platforms, data analytics, artificial intelligence, and other technology domains and a deeply embedded “People First” culture, WinWire serves as a trusted, long-term partner to enterprises seeking to modernize their businesses and accelerate their digital transformations. “WinWire is excited to partner with Sverica as this strategic investment allows us to accelerate our growth, expand our global presence and focus on building domain competencies and solution accelerators in the areas of Healthcare, Retail, Hi-Tech and Manufacturing to deliver world-class customer experiences. WinWire thanks its customers and partners for their commitment and partnership and its employees who, especially during these unprecedented times, continue to make WinWire a Great Place to Work®,” said Ashu Goel, CEO of WinWire. “Ashu has done a remarkable job building WinWire into one of the leading providers of Azure-centric services in the Microsoft ecosystem today,” said Frank Young, Managing Partner at Sverica. “Beyond WinWire’s impressive growth and clear, customer-focused strategy, we particularly admire the manner in which he has humbly built the business through a strong, authentic, people-centric culture. We are thrilled to partner with him and look forward to working together to ensure WinWire accelerates its high growth trajectory in the years to come.” Ryan Harstad, Partner at Sverica, added: “As part of Sverica’s dedicated focus on cloud services, our team actively sought to invest with a proven leader in Microsoft Azure services. With deep technical expertise and significant momentum, WinWire is an exciting company that we enthusiastically welcome to the Sverica family. The ‘People First’ foundation upon which Ashu has built WinWire resonates closely with Sverica’s partnership-driven approach, and we’re eager to get started in collaborating with him and his great team to drive the company’s continued growth.” About WinWire WinWire Technologies is a data-driven digital engineering company that supports enterprises across Healthcare, Retail, Hi-Tech and Manufacturing and several other industry domains in navigating their digital transformation journey. WinWire enables its customers to drive business growth and gain competitive advantage by aligning business value and digital technologies, calling this process “Stitching the Digital Fabric.” WinWire has extensive expertise across a range of digital technologies and delivers large enterprise solutions leveraging cloud, AI, machine learning, mixed/augmented reality, Internet of Things (IoT), mobility, security, and UI/UX to help clients harness business value. For more information, please visit www.winwire.com. About Sverica Capital Management Sverica Capital Management is a leading growth-oriented private equity firm that has raised over $1.1 billion across five funds. The firm acquires, invests in and actively builds companies that are, or could become, leaders in their industries. Since inception, Sverica has followed a “business builder” approach to investing and takes an active supporting role in its portfolio companies. Sverica devotes significant internal time and resources to help its management teams develop and execute growth strategies and proactively looks for levers to pull to accelerate growth by reinvesting back into those companies. Sverica firmly believes in building businesses collaboratively that can endure for the long term by starting with a strong foundation and bringing the right people and playbook to drive reinvestment and ultimately strong returns for our investors. For more information, please visit www.sverica.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210407005268/en/ Media Inquiries: Sverica Capital Management LPNathalie Allen415-249-4906nathalie@sverica.com"
https://venturebeat.com/2021/04/07/5-jobs-that-are-honestly-as-awesome-as-they-sound/,5 jobs that are honestly as awesome as they sound,"Looking for a new job can be really exciting, but also tricky. It doesn’t happen overnight (well, most of the time anyway). People usually say that looking for a job is like a job in itself — and we want to make that search of yours a little bit easier. That’s why we have a brilliant job board on our site. It’s a one-stop shop if you’re looking for an exciting new role in the U.S. (and beyond, if you fancy packing your bags!) Take a look here at a handful of what’s on offer. Klavyio is looking for a product designer who will lead proposed and design solutions that influence direction and outcomes. They will evaluate UX for one or more domains and apply designs to create new opportunities, while determining UX direction with respect to competition. The successful candidate will have a Bachelor’s degree in arts, science, new media, design, psychology, or any other related field, or foreign academic equivalent, and at least one year of experience in the job offered or in digital product design using software such as Figma, Sketch, Adobe CC, Framer, and/or Invision. As a Software Engineer, Server, you will be working to improve the onboarding experience for Zwift customers. Onboarding new customers is a non-trivial problem as new customers learn what Zwift is, what they need to Zwift, and help in walking through the steps to get set up and become an active customer. You will collaborate in cross-functional teams including teammates from engineering, product, design, operations, and support disciplines. They are looking for someone comfortable in this highly collaborative environment, working with ambiguity, and continuously looking to proactively make improvements, to build a world-class experience for their customers. As a Computer Engineer at Micron Technology, Inc., you will be working in a fast-paced, collaborative, production support role evaluating, developing, enhancing, and debugging both in-house and commercial EDA tools and flows for the design, physical layout, and verification of CMOS integrated circuits. The successful candidate will innovate to develop tools, flows, and methodologies to increase the productivity and reliability of memory designs. They will own production level flows across a worldwide company, while providing training, documentation, and support to end-users on new tools and methods. Inpixon is looking for a Pre-Sales Engineer to design, architect, and detail customized solution demonstrations and proofs-of-value (POV) for their Aware product prospects. This is an exciting opportunity to bridge your technical experience with your client-facing ability, dive into the details for product offerings, and play a critical role in the success of the Inpixon sales efforts and resulting growth. You will work with sales, development, and product to understand the needs of the prospect and rapidly convert those requirements into interactive, customized demonstrations, providing the full Inpixon experience and the extent of technical expertise and capabilities. The Crypsis Group is seeking a Network Traffic Analyst to join their growing team. The Network Traffic Analyst will work in tandem with the Threat Intelligence Analyst and Mobile Application Analyst teams to investigate suspected malicious mobile applications. Suspected malicious mobile applications will be tested in a sandboxed environment with proxies capturing all network traffic. The Network Traffic Analyst will work to identify anomalous activities within captured logs, as well as supplemental information discovered via Threat Intelligence."
https://venturebeat.com/2021/04/07/blue-dot-raises-32m-for-ai-that-helps-companies-comply-with-tax-codes/,Blue Dot raises $32M for AI that helps companies comply with tax codes,"Tax compliance platform Blue Dot (previously VatBox) today announced it has raised $32 million, bringing its total raised to over $96 million. The firm says it will put the funds toward product R&D and expanding the size of its globlal workforce. The tax compliance burden for enterprises can be significant. In 2019, half of companies responding to an EY Americas survey indicated that their biggest compliance challenge would be staying current on legislative and regulatory developments. Another 23% said modeling the tax impacts of business and legislative change was their next biggest barrier to overcome. Blue Dot offers software designed to reduce companies’ tax liabilities by addressing a few of these challenges. Leveraging a knowledge base, as well as AI and machine learning algorithms, it looks for anomalies in evidence, supplier data, and reports, consolidating data sources and processes into a single workflow and flagging anything that might be amiss. Blue Dot was founded in 2013 by Isaac Saft and Noam Guzman, who sought to streamline the recovery of value-added taxes (VAT), or taxes levied on products at each stage of production, distribution, and sale. Along with over 100 employees, the two cofounders built a business-to-business software-as-a-service solution that relies on the cloud and automation to chase higher returns.  Blue Dot’s algorithms monitor transactional data across entities and jurisdictions, identifying unclaimed tax returns, avoidable costs, and more. The software extracts, matches, and analyzes invoices, employing an access point for global and local management of corporate tax policies. Using Blue Dot, companies can adjust the strictness of their tax returns based on rulings with local authorities while maintaining regulatory compliance. For example, Blue Dot can tell customers how much VAT has been spent across their business, as well as the likely amount of VAT that can be reclaimed — by country, date, and expense type. Blue Dot’s optimization features show things like the difference between potential and actual VAT returns, in addition to information on lost VAT refunds. “Our ecosystem combines hundreds of extractors and algorithms, using semantic networks, statistical relationships, anomaly detection, and many other models,” a company spokesperson told VentureBeat via email. “Blue Dot harnesses the power of AI and machine learning while leveraging external data sources and historical data to create an end-to-end story of each employee-driven transaction. [The company’s] architecture ensures the data’s integrity via a continuously updated knowledge base of tax rules and a configuration wizard that allows each client to apply their own policies to optimize the end-to-end tax compliance process.” The global tax management market is expected to grow from $15.5 billion in 2019 to $27.0 billion by 2024. Factors driving the climb include an increasing volume of transactions due to digitalization, the complex nature of existing tax systems, and the increased vigilance of tax administrators, according to Markets and Markets. Blue Dot is based in Amstelveen, Netherlands and claims more than 3,000 multinational clients, including brand leaders and Fortune 500 companies such as Dell, BCD Travel, and Michelin."
https://venturebeat.com/2021/04/07/eu-may-probe-facebooks-kustomer-acquistion-over-antitrust-concerns/,EU may probe Facebook’s Kustomer acquisition over antitrust concerns,"(Reuters) — Facebook’s acquisition of customer service startup Kustomer may be subjected to European Union antitrust scrutiny after Austria asked EU enforcers to take over the task, the European Commission said on Tuesday. The move comes as the EU competition regulator girds up to vet more tech, pharma, and biotech startup deals, sending a warning to tech giants criticized by some for so-called killer acquisitions, where startups buy nascent rivals with the goal of shutting them down. The world’s largest social network announced the deal in November. It could help the company scale up its instant messaging app WhatsApp, which has seen usage jump during the COVID-19 pandemic. Facebook sought approval for the deal from the Austrian competition agency on March 31. “We can confirm that we have received a request for referral from Austria,” a Commission spokesperson said. Other national watchdogs have 15 working days to inform the EU competition enforcer whether they too want it to review the deal. “Following the expiry of the deadline for other Member States to join the referral, the Commission will have 10 working days to decide whether to accept or reject the referral,” the spokesperson said. Facebook said the deal would bring more innovation to businesses and consumers in a dynamic and competitive space. “We look forward to demonstrating to regulators that Facebook and Kustomer would offer more choices and better services through this pro-competitive deal,” a Facebook spokesperson said."
https://venturebeat.com/2021/04/06/intel-touts-latest-xeon-processor-for-scaling-5g-networks/,Intel touts latest Xeon processor for scaling 5G networks,"Intel launched its latest datacenter platform in the form of the 3rd Gen Intel Xeon Scalable processors. The Santa Clara, California-based chipmaker said that the processors deliver a 46% performance increase on datacenter workloads. The server chips with integrated AI will power cloud-native datacenters and applications such as 5G networks, cryptography, drug discovery, and confidential computing. For 5G, the new chips deliver on average 62% more performance on network and 5G workloads. In an online briefing, Intel executive vice president Navin Shenoy said Intel has added advanced security with Intel Software Guard Extension and Intel Crypto Acceleration. Intel has shipped more than 200,000 chips for revenue in the first quarter, and it boasts more than 250 design wins for the chips with 50 partners, 15 telecom equipment and communications firms, and 20 high-performance computing labs. AT&T said it is seeing 1.9 times higher throughput and 33% more memory capacity with the combination of the Intel Xeon Scalable processors and Intel Optane Persistent Memory, so the network can serve the same number of subscribers at higher resolution or a greater number of subscribers at the same resolution. Verizon and Vodafone also said they’re using the new Xeons. With the chips, Intel said communication service providers can increase 5G user plane function performance by up to 42%. The chip uses Intel’s 10-nanometer manufacturing process (equivalent to the 7-nanometer process of rivals based on nomenclature), and it delivers up to 40 cores per processor and up to 2.65 times higher average performance gain compared to five-year-old systems. Intel CEO Pat Gelsinger said in an online briefing that over the past year companies have been forced to undertake a warp-speed cloudification of infrastructure to serve remote workforces, and he said the new processors have flexible architecture for advanced security and built-in AI to handle processing from the edge to the cloud. “Technology is like magic,” he said. “It has the power to improve the lives of every person on the planet. It’s a new day at Intel. We are no longer just the CPU company.” He said Intel combines software, silicon, and manufacturing to differentiate itself from rivals. The company will operate internal factories, strategically use foundry services to make Intel chips with the help of outside contract manufacturers, and offer its own foundry services to others. “With a backdrop of fierce competition, Intel is leading with its strengths with its 3rd Gen Xeon processors,” said Patrick Moorhead, an analyst at Moor Insights & Strategy, in a message to VentureBeat. “The company is offering a platform approach to provide its partners solutions incorporating CPUs, storage, memory, FPGAs, and networking ASICs. This is in addition to its ability to leverage resources for co-marketing and co-development. I also believe the company is differentiated with its on-chip ML inference and cryptographic capabilities versus its closest competitors.” The latest hardware and software optimizations deliver 74% faster AI performance compared with the prior generation and provide up to 1.5 times higher performance across a broad mix of 20 popular AI workloads versus AMD Epyc 7763 and up to 1.3 times higher performance on a broad mix of 20 popular AI workloads versus Nvidia A100 GPU, Intel said. Shenoy said its security-focused SGX protects sensitive code and data with the smallest potential attack surface within the system. It is now available on two-socket Xeon Scalable processors with enclaves that can isolate and process up to a terabyte of code and data to support the demands of mainstream workloads. And Shenoy said Intel Crypto Acceleration delivers performance across a variety of important cryptographic algorithms. Businesses that run encryption-intensive workloads, such as online retailers who process millions of customer transactions per day, can leverage this protection without impacting user response times or overall system performance. Intel said that more than 800 of the world’s cloud service providers run on Intel Xeon Scalable processors, and all of the largest cloud service providers are planning to offer cloud services in 2021 powered by the newest chips. HP Enterprise said it has launched new computers across eight different models with the new Xeons, and it uses AMD’s latest Epyc processors as well."
https://venturebeat.com/2021/04/06/google-launches-lyra-codec-in-beta-to-reduce-voice-call-bandwidth-usage/,Google launches Lyra codec in beta to reduce voice call bandwidth usage,"Google today open-sourced Lyra in beta, an audio codec that uses machine learning to produce high-quality voice calls. The code and demo, which are available on GitHub, compress raw audio down to 3 kilobits per second for “quality that compares favorably to other codecs,” Google says. While mobile connectivity has steadily increased over the past decade, the explosive growth of on-device compute power has outstripped access to reliable, fast internet. Even in areas with reliable connections, the emergence of work-from-anywhere and telecommuting have stretched data limits. For example, early in the pandemic, nearly 90 out of the top 200 U.S. cities saw internet speeds decline as bandwidth became strained, according to BroadbandNow. It’s Google’s assertion that Lyra can make a difference in these scenarios. Lyra’s architecture is separated into two pieces, an encoder and decoder. When someone talks into their phone, the encoder captures distinctive attributes, called features, from their speech. Lyra extracts these features in 40-millisecond chunks and then compresses and sends them over the network. It’s the decoder’s job to convert the features back into an audio waveform that can be played out over the listener’s phone. According to Google, Lyra’s architecture is similar to traditional audio codecs, which form the backbone of internet communication. But while these traditional codecs are based on digital signal processing techniques, the key advantage for Lyra comes from the ability of its decoder to reconstruct a high-quality signal. Google believes there are a number of applications Lyra might be uniquely suited to, from archiving large amounts of speech and saving battery to alleviating network congestion in emergency situations. “We are excited to see the creativity the open source community is known for applied to Lyra in order to come up with even more unique and impactful applications,” Google Chrome engineers Andrew Storus and Michael Chinen wrote in a blog post. “We [want] to enable developers and get feedback as soon as possible.” The Lyra code is written in C++ using the Bazel build framework. The core API provides an interface for encoding and decoding at the file and packet levels, and the complete signal processing toolchain is provided, which includes filters as well as transforms. Google’s example code integrates with the Android NDK to show how Lyra can work with Java-based Android apps, and Google has also provided the weights and vector quantizers necessary to run Lyra. “This release provides the tools needed for developers to encode and decode audio with Lyra, optimized for the 64-bit ARM android platform, with development on Linux,” Storus and Chinen continued. “We hope to expand this codebase and develop improvements and support for additional platforms in tandem with the community.”"
https://venturebeat.com/2021/04/06/segment-founder-on-future-of-customer-data-management-and-acquisition-by-twilio/,Segment founder on future of customer data management and acquisition by Twilio,"A little more than eight years ago, Segment almost died an early death when it seemed to be out of money and ideas. Last November, the company capped its improbable rebound and rise to stardom when it was acquired by Twilio in late 2020 for $3.2 billion. The deal to put two customer data companies together seemed like a good fit on the surface. Both focus on developers and delivering solutions by leveraging APIs. But Segment founder Peter Reinhardt told VentureBeat recently that there is much more to the deal, and he remains even more bullish about that potential almost four months later. Particularly with the pandemic reshaping daily life, it’s become more important than ever for companies to have insight into customers and to be able to deliver an engaging digital experience. “Ultimately what we’re both trying to do is improve customer engagement and help companies engage better with their customer,” Reinhardt said. “We’re like the two halves of the coin that handle the two halves of that problem.  One actually delivering communications and the other the data to figure out what to send in the first place.” When Reinhardt last spoke with VentureBeat in late 2019, the company was experiencing meteoric growth. After a couple of early ideas misfired and almost put them out of business, the Segment founders posted a JavaScript library they had created, dubbed simply “analytics.js”, to GitHub that allowed developers to funnel all their data through any analytics service of their choice and then monitor the results to get better insight into customer behavior. That proved to be a hit, and Segment quickly began developing its own commercial services on top of that on the way to raising $300 million in venture capital for a $1.5 billion valuation. Those services include an API that enhances the customer data centralization and expands the range of services where the information could be sent for further analysis or personalization. At the start of 2020, Reinhardt said Segment getting 500 billion API calls per month. By the end of the year, that had grown to 1 trillion per month. Meanwhile, Twilio CEO Jeff Lawson had started to make overtures. Twilio had built a big business around messaging APIs that help brands connect with consumers and manage interactions. “He had been coming to me about once a quarter and making a pitch for how Segment and Twilio would be better together,” Reinhardt said. “And at first, I kind of just gently and politely dismissed it out of hand. But he’s very persistent.” Over time, as Reinhardt listened to Lawson’s pitch, he also began to notice a pattern among Segment customers. Reinhardt would be discussing how they were going to use Segment data and customers would bring up the desire to have more integration with messaging. “Some of my conversations with [Jeff Lawson] started to stick in my head, like, ‘Oh, yeah, I see they’re using the data to drive the communication channels,” Reinhardt said. “And then mid-last year, Jeff and I had yet another quarterly check-in and I was like, ‘You know what? I see it. And I actually think it is a much, much bigger opportunity.'” After the intense entrepreneurial journey, Reinhardt has undergone some personal and professional adjustments following the decision to sell. “I’ve been through the whole emotional roller coaster that every founder goes through in this kind of situation,” he said. “Relief, sadness, letting go of, excitement, fear. Like, everything all bundled into one two-week period. But now we’re super excited to be on the other side of that and have sort of a fresh beginning.” The business is now dubbed Twilio Segment. Going forward, the goal is to develop a customer engagement tool that offers developers a more flexible approach to managing customer data than the customer management software tools that have been dominant. That tool is now available as a beta. The idea is to blend the Segment and Twilio experience into a seamless customer data management system. “I’m sending my customer an email and sending them an SMS and I’m doing a voice call with them,” Reinhardt said. “Whatever the mode of interaction, I’m doing that to engage them. The question is: ‘How do I do that better?’ If you’re going to do that better, it means sending a more personalized message that’s better targeted and that’s more interesting to them. You do that with data.” For that reason, Reinhardt argues that Twilio’s communications tools and Segment’s data management tools “sit extremely adjacent to each other in the higher-level problem of customer engagement.” To take advantage of these underlying capabilities, Reinhardt said the new product will become a feedback loop where data continually flows to improve communications and engagement, which in turn generates new data that can be used to shape the next engagements. “The first thing is actually getting feedback out of the communications,” he said. “And that is already in beta. It takes data out of Twilio and sends it downstream for usage in Segment but also more broadly joining the concept of building blocks.” The plan is to unveil more details around this evolution at Twilio’s Signal conference in late October. Meanwhile, Reinhardt has felt a greater sense of urgency to expand these tools over the past year, both before and after the Twilio deal. As the pandemic put the digital experience front and center for companies, it became a huge area of investment. But it’s also one where many are struggling to stand out from competitors while meeting the expectations — and increasingly higher standards — of customers. That has become even more challenging in a world where companies can’t rely on face-to-face meetings or other forms of traditional interaction. The answer is to get more sophisticated about how that data is being used. “In a world where you need to compete primarily in a digital world, the question is, ‘How do you meaningfully differentiate?'” Reinhardt said. “It’s going to be something in that customer experience. And if it’s going to be different, then that means you’ll have to build something. You have to have an engineering team that’s building something and that’s pulling together some aspects of data about the customer experience that allows marketers to send more creative messaging than competitors. Fundamentally, at its root, that relies on developers. That’s why Twilio and Segment are so focused on being tools for marketers but are ultimately built for developers.”"
https://venturebeat.com/2021/04/06/ex-akamai-cso-guide-security-startups-on-strategy-as-yl-ventures-partner/,Ex-Akamai CSO will guide security startups on strategy as new YL Ventures partner,"Andy Ellis, the former CSO of Akamai Technologies, has joined YL Ventures as an operating partner. Ellis will draw upon his experiences as a security decision-maker to advise startups on a broad range of services, including product development, go-to-market strategies, and customer pipeline management. YL Ventures funds Israeli cybersecurity companies from “seed to lead,” but the support goes beyond just funding, Ellis said in an interview with VentureBeat. YL Ventures provides strategic and operational guidance to the companies in its portfolio “in the time that YL’s going to be part of their journey,” Ellis added. The firm also publishes the CSO Circuit, a newsletter that tells startup founders what CSOs need and what the market is currently looking for to help shape product roadmaps and sales strategies. The support could be as straightforward as access to a marketing or press team before the company is big enough to have its own. It is also about time and practical advice, with YL acting as an advisor on anything the founders and their teams need help with. Ellis makes himself available “anywhere in the pipeline that I can be useful,” such as joining the company on a customer call, providing feedback on product design, advising on the product roadmap, helping develop the marketing presentation, and advising on how to recruit and develop talent.  Ellis spent 20 years at Akamai and grew the security business to more than $1 billion in annual revenue. As the CSO, he dealt with the challenges enterprise security leaders face in developing a security program, as well as deploying and integrating multiple platforms and technologies. One thing he regularly encountered was the question of how to protect as many people as possible — security at scale. “I can bring some of the lessons about solutions that didn’t always work because they were great on paper and in the pilot [but not across the entire organization],” Ellis said. He also has the vendor perspective, thanks to “secondhand experiences across thousands of CSOs” while working with Akamai customers. He knows enterprise leaders have budget constraints and integration challenges and can advise security companies on how to address those specific needs. On the selling side, he understands how different dynamics play out in different marketplaces and knows the difference between selling to financial services, retail, and manufacturing. One of the challenges early startups face is shifting their focus from investors to customers. The first slide deck a startup creates typically leads with how much it has already raised and is designed to sell the company to VCs. You almost have to throw that entire deck out and start over with a presentation that considers the target market and what the customer cares about, Ellis said. A cloud-native business, for example, will want to know how the technology will solve the problem it is having. “How much money you raised is a signal that says, ‘Maybe you have a great idea, but the idea [technology] is what you want to talk about and should always be what you’re talking about in selling the business,'” Ellis said. YL Ventures consults with CSOs — there are over 90 CSOs on the advisory board, as well as a less formal network of a thousand security leaders — to “get market feedback before making an investment,” Ellis said. These conversations help YL Ventures stay up to date with the challenges organizations are facing and understand any needs and gaps in the industry. This collaboration helps YL Ventures decide which areas to focus on and which security companies to add to their investment portfolio. Ellis joined that advisory board about four years ago. “Sometimes the input was ‘Wow, brilliant people, but this technology is never going to sell in the market,'” Ellis said. “Other times it was, ‘Oh my goodness, I want to do this one.'” YL Ventures currently manages over $300 million and is investing from its fourth $135 million fund. Its portfolio is currently focused on the following areas: application security and securing code; security controls for software-as-a-service applications; extended detection and response (XDR) capabilities; next-generation cloud security solutions; and data security. YL Ventures is betting on these technologies as the areas enterprise leaders are most concerned about. Over the last 15 years, organizations have built out extensive security platforms to protect the applications and data they have, but those security platforms are left behind when organizations move their applications or data to the cloud, Ellis said. Most public cloud platforms already have “fantastic” security tools, but the challenge is finding them and knowing how to use them, Ellis said. So he will be looking at ways to make it easier to deploy security services in the cloud. This kind of thinking applies to SaaS applications as well, since security teams have to make sure the employees are using them safely and that information remains secure. “It really does come down to ‘How do we make security easy? How can we give security teams scale?'” Ellis said. Another area where Ellis sees a lot of CSO interest is in application development, in helping developers build secure code. “That said, any idea is on the table,” Ellis said. “If a company comes with a brilliant idea and it’s not in those three spaces, I’m totally excited about that too.” While YL Ventures didn’t provide aggregate totals on how much it has invested in each of these areas, it offered some insights into recent funding decisions. The sheer number of attacks against applications and the ever-widening attack surface is driving organizations to allocate more resources to application security and secure software development. One emphasis is on security solutions to “shift left” to implement security earlier in the software development lifecycle. The firm led the seed round in application security startup Enso Security, vulnerability management company Vulcan Cyber, and source code protection startup Cycode. The growing attack landscape means organizations are also increasingly looking for new ways to detect threats and respond quickly. XDR is a new approach that collects and automatically correlates data across multiple security layers so threats can be detected faster and security analysts can improve investigation and response times. YL Ventures led a seed round in autonomous threat intelligence startup Hunters and participated in follow-on rounds. The shift to the cloud, the growing remote workforce, and rapid adoption of digital transformation initiatives mean enterprise leaders are willing to spend to protect cloud platforms and software-as-a-service applications. This is especially true as more employees are connecting remotely to corporate assets and accessing sensitive data from private devices and networks. YL Ventures led the seed round in Orca Security, a security startup that reached a unicorn valuation in less than two years. Data security and governance is another big area for YL Ventures, especially as enterprises try to manage the massive amount of data being generated. Organizations have to figure out how to comply with a growing slate of regulations, such as the European Union’s General Data Protection Regulation and the California Consumer Privacy Act. As more states follow California’s example, organizations have to ensure their governance processes and data management practices keep up with each regulation’s requirements. YL Ventures led a seed round in data protection and management startup Satori in 2019. Other areas YL Ventures invested in recently include authorization (build.security), medical device security (Medigate), and embedded security for connected systems (Karamba). VCs often straddle the fine line between investing in companies that address the needs of the market and trying to anticipate future problems and find startups that are beginning to tackle those issues. There is an educational opportunity for companies to prepare for the moment when organizations realize they have a particular problem. Ellis gave the example of how organizations started buying technologies to deal with distributed denial-of-service (DDoS) attacks. Akamai was already working on the technology in 2004, before most organizations were even thinking about it. When Operation Payback — a series of coordinated DDoS attacks against financial services and other major organizations — hit in 2010, Akamai was ready. Vulcan Security, a company in the YL Ventures portfolio, is another example. When the company was raising its seed round, very few organizations were thinking about vulnerability remediation orchestration. Just a handful of years later, it is something organizations are actively looking at. “My favorite part of this business is helping,” Ellis said. “How do you make the internet safer by finding things that scale as solutions that the market needs?”"
https://venturebeat.com/2021/04/06/how-intel-and-burger-king-built-an-order-recommendation-system-that-preserves-customer-privacy/,How Intel and Burger King built an order recommendation system that preserves customer privacy,"The pandemic has placed enormous strains on the restaurant and fast food industries. Within a month of the health crisis, 3% of restaurants had closed for good and another 11% anticipated doing so within the following month, according to a National Restaurant Association study. While fine dining and casual dining establishments suffered the bulk of the impact, the fast food industry wasn’t immune. A Datassential survey found that sales among fast food operators declined 42% during the first few weeks of the pandemic. As more customers began relying on take-out and drive-thru options instead of indoor dining, fast food retailers like Burger King turned to AI and machine learning for solutions. In collaboration with Intel, Burger King developed an AI system that uses touchscreen menu boards to recommend items to customers as they’re about to order. It can predict whether a customer will order a hot or cold drink or a light or large meal, potentially saving time and leading to a better customer experience. Burger King and Intel say the solution has already been piloted in over 1,000 Burger King locations. Burger King isn’t the first fast food chain to experiment with AI in customer service. McDonald’s has been using AI in its drive-thrus since acquiring tech company Dynamic Yield in 2019. Dunkin’ Donuts is testing drive-thrus that can recognize a loyalty member as soon as they pull up. Some Sonic drive-ins recently got AI-powered menu kiosks. And Chick-fil-A is using AI to spot signs of foodborne illness from social media posts. As Luyang Wang, director of advanced analytics and machine learning at Burger King, explained to VentureBeat via email, fast food recommendation has its own set of unique challenges. There’s no easy way to identify customers and retrieve their profiles because all of the recommendations happen offline. Moreover, context features like location, time, and weather conditions have to be preprocessed before they can be loaded into a model. To solve these challenges, TxT was built with what’s called a “double” Transformer architecture that learns real-time order sequence data, as well as features like location, weather, and order behavior. TxT leverages all data points available in a restaurant without having to identify customers prior to the order-taking process. For example, if a customer puts a milkshake as the first item in their basket, that will influence what TxT suggests — based on what’s been sold in the past, what’s selling today, and what is sold at that location. TxT was developed within Analytics Zoo, Intel’s open source platform for big data analytics workloads running in datacenters. Intel and Burger King collaborated to create an end-to-end recommendation pipeline, which includes distributed Apache Spark data processing and Apache MXNet training on an Intel Xeon cluster. The TxT model was deployed using Intel’s RayOnSpark library, which allows enterprises to directly run programs on existing clusters. According to Wang, TxT has already led to surprising sales insights. For one, Burger King customers will order milkshakes in any weather — even when it’s cold out. And people are much more willing to add a dessert when they have a high-calorie basket versus a low-calorie basket. “At Burger King, we are always looking to improve our guests’ experience,” Wang said. “This AI recommender system — Transformer Cross Transformer (TxT) — allows Burger King to better learn customer habits and, essentially, better communicate with guests.”"
https://venturebeat.com/2021/04/06/tvscientific-launches-connected-tv-performance-advertising-platform/,tvScientific Launches Connected TV Performance Advertising Platform," The New Platform combines the Power of TV Advertising with the Control and Measurability of Digital Platforms  PASADENA, Calif.–(BUSINESS WIRE)–April 6, 2021– tvScientific announced the launch of its Connected TV (CTV) buying, measurement, and attribution platform, built for businesses of all sizes that value performance media. CTV represents the fastest growing segment of a $72 billion TV advertising market, which is dominated by roughly 300 national advertisers. tvScientific aims to eliminate the high barriers of entry around TV advertising by making it accessible and measurable for all businesses. The tvScientific platform was designed for performance marketers, media agencies and businesses that want to take advantage of the rapidly growing and engaged CTV audience. By combining the powerful viewing experience of TV advertising with the capabilities of SmartTVs, tvScientific can help businesses buy and measure advertising performance. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210406005332/en/ An Idealab company, tvScientific co-founders include Bill Gross, founder of GoTo.com and father of the paid search business model, and Jason Fairchild, an early executive at GoTo.com and cofounder of OpenX. The company raised $1.5 million in seed funding from IdealabX and MathCapital in addition to well-known leaders of the advertising tech community including Tim Cadogan (GoFundMe, OpenX, Yahoo, and Overture), Tom Chavez (super{set}, Krux), John Gentry (OpenX and Overture), and Kent Wakeford (Integral Ad Science and Kabam) among others. “There were two reasons GoTo.com and the paid search model were so revolutionary. First, paid search campaigns were measurable so advertisers could see the clicks and sales that search ads generated. Second, any business could set up an account and start driving clicks in 10 minutes. The combination of these two simple dynamics attracted millions of businesses to search advertising. I believe tvScientific will do the same thing for TV advertising, where we already have 200 million CTV viewers in the US, but with only a few hundred participating advertisers,” said Bill Gross, Co-Founder of tvScientific, and Founder and Chairman of Idealab. “Until now, Connected TV advertising has been complex, inefficient, and not measurable in the way that digital is, preventing most businesses from advertising on the most influential screen in the house,” said Jason Fairchild, Co-Founder and CEO of tvScientific. “Our new platform allows businesses to buy and measure Connected TV media on one simple platform, and evaluate it the way they do with search and social.” tvScientific is the only CTV platform to offer a self-serve solution that combines fully optimized buying with comprehensive measurement and attribution. The platform has direct access to over 90% of premium CTV publishers, bypassing unnecessary technology layers, which allows businesses to buy premium CTV inventory at efficient rates. The tvScientific platform also features advanced audience targeting, measurement, attribution solutions, and optimization: tvScientific connects buyers to hundreds of premium CTV inventory sellers and data providers across a wide variety of platforms, CTV OEMs and tech infrastructure providers, including SpotX, Magnite, OpenX, Samsung, running on CTV devices like Samsung, Vizio, Roku, and more. This unique end-to-end platform empowers businesses to quickly create an account and launch measurable CTV advertising campaigns in minutes. tvScientific is available now, visit tvscientific.com for more information. About tvScientific tvScientific is a TV advertising technology company that finally brings the power of digital advertising to television. tvScientific’s industry-first buying and attribution platform is the only CTV platform to offer a self-serve solution that combines fully optimized media buying with comprehensive measurement and attribution. An Idealab company, tvScientific co-founders include Bill Gross, founder of GoTo.com and father of the paid search business model, Jason Fairchild, an early executive at GoTo.com and co-founder of OpenX, in addition to David Koye, a veteran advertising and media executive from Cox Media and Kent Wakeford, co-founder of Integral Ad Science and COO of Kabam.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210406005332/en/ Media Contact: Kristen Morquechopress@tvscientific.com"
https://venturebeat.com/2021/04/06/ai-observability-startup-aporia-nabs-5m-to-provide-guardrails-for-ai/,AI observability startup Aporia nabs $5M to provide guardrails for AI,"Machine learning observability startup Aporia today emerged from stealth with $5 million in venture capital funding. The company says the proceeds will support the unveiling of its platform for AI models, which enables companies to monitor AI running in cloud environments. Businesses are investing upwards of $50 billion annually on AI adoption, a recent report from the MIT Sloan School of Management and Boston Consulting Group found. But a lack of ability to detect issues in models as they enter production could be undermining investments. “AI needs guardrails,” Aporia CEO Liran Hason told VentureBeat via email. “Companies need to have confidence in their machine learning models, and the only way to get there is by robust monitoring to ensure they’re doing what they’re supposed to do.” Hason, a veteran of the Israel Defense Forces’ elite 81 intelligence unit, was one of the first employees at cloud security company Adallom, which Microsoft acquired for $320 million in 2015. At Adallom, he led the machine learning production architecture, which served as many as millions of users.  “I founded the company in late 2019 after leading machine learning production architecture at Adallom and then working at Vertex Ventures VC where I was involved in dozens of startup investments,” Hason told VentureBeat via email. “It seemed natural to use my development best practices, hard-learned lessons with data science challenges, and my gravitation towards startups to start a company that would apply best practices from production engineering and adjust them to machine learning, in the hope of transforming doubt into trust and build what analysts often call ‘responsible AI.'” Aporia lets data scientists create, maintain, or modify monitors for models and set alerts that trigger notifications via email, Slack, and other channels.  The Aporia platform can be installed with a few lines of code and set to monitor billions of daily model predictions asynchronously. Alongside its public cloud offering, Aporia provides a managed on-premises solution for enterprises with data privacy and security requirements. Machine learning models can work perfectly in the experimentation phase but start to drift in production over time due to changes in their datasets, Hason explained. Something as routine as a company expanding into a new market can affect the performance of a model. Customers and businesses typically suffer the consequences — predictions based on the wrong data are flawed, resulting in unintended outcomes and in turn lost revenue.  “Companies are struggling to keep watch of their AI in the ways that matter for their specific machine learning model and use case,” Hason added. “Aporia’s platform contains three pillars: (1) visibility, allowing data scientists to explore production data easily, (2) monitoring, the beating heart of the system, where users can implement any monitoring logic they’d like and adapt it to their use case and investigation, and (3) toolbox, for root cause analysis. Aporia aims to be the place where organizations manage the reliability of their models, and ensure responsible usage, whether in regard to performance or bias and fairness matters.” Sixteen-employee Aporia has rivals in data reliability startup Monte Carlo and WhyLabs, a startup developing a solution for model monitoring and troubleshooting. There’s also Domino Data Lab, a company that claims to prevent AI models from mistakenly exhibiting bias or degrading. But according to Hason, Aporia’s differentiator is its experienced team. Already, the company’s platform is being used by roughly a dozen users across over 11 “multi-billion-dollar” companies. One organization is tapping Aporia to monitor a model that predicts whether an applicant will be able to repay a loan without defaulting.  “We had one case where the credit history data provider had changed the schema of the data without notifying anyone, resulting in a significant drift in model’s behavior, leading it to approve or deny loans unjustifiably,” Hason explained. “Without a proper monitoring system in place, it would only have been discovered a few months later once loans were starting to be defaulted on and there was major revenue loss. However, with Aporia, they got an alert about that drift on the very same day the problem had started, which allowed them to react quickly and avoid potential deficiencies.” When asked about Aporia’s fundraising, Rona Segev, managing partner at investor TLV Partners, said, “Monitoring production workloads is a well-established software engineering practice, and it’s past time for machine learning to be monitored at the same level. Aporia’s team has strong production-engineering experience, which makes their solution stand out as simple, secure, and robust.” Vertex Ventures and TLV Partners led Tel Aviv, Israel-based Aporia’s seed round."
https://venturebeat.com/2021/04/06/catch-nvidia-gtc-21-live-right-here/,Catch Nvidia GTC 21 live right here,"This article is part of the VB Lab / Nvidia GTC insight series. Nvidia GTC, the conference for AI innovators, developers, technologists, startups, and creatives, is offering over 1,600 sessions online this year. Running from April 12 – 16, 2021, the opening keynote will be livestreamed free right here. The keynote starts at 8:30 am PDT on April 12 with Nvidia CEO Jensen Huang – bookmark this page and tune in to watch live. Be sure to also register to attend any of the 1,600 free sessions offered during the conference featuring the world’s brightest researchers and thought leaders from top companies covering breakthroughs in AI, data center, accelerated computing, autonomous vehicles, health care, intelligent networking, game development, and more.  VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/06/onestream-raises-200m-to-bringing-intelligent-financial-planning-to-the-modern-enterprise/,OneStream raises $200M to bring intelligent financial planning to the enterprise,"OneStream, a corporate performance management (CPM) company that unifies customers’ financial planning processes and reporting through a single platform, has raised $200 million in a series B round of funding at a whopping $6 billion valuation. CPM encapsulates all of the processes, systems, and methodologies an enterprise might use to track its business performance and usually involves gleaning data from myriad sources, such as enterprise resource planning (ERP), customer relationship management (CRM), and human capital management (HCM). Let’s say a business that collects global sales transaction data through its ERP tool wants to transform that information into something more meaningful and useful on a day-to-day basis, rather than waiting for monthly or quarterly reviews. OneStream essentially uses the data to generate “performance signals” that are delivered each day to help execs make “mid-stream” decisions. “You can think of OneStream as the ‘financial brain’ for modern business,” CFO Bill Koefoed told VentureBeat. “OneStream helps organizations turn the data they are collecting into actionable, financially intelligent information. This is critical because traditionally it might take 30 days or more to pull insights from financial reporting. Companies have realized — especially in the midst of COVID-19 — that they need to access critical data insights on company performance much faster than that. We make that intelligence available daily, and that’s game-changing.” OneStream typically integrates data from general ledger/ERP systems for financial data, though it also links to systems spanning HCM, CRM, and data warehouses, among other sources. Founded out of Rochester, Michigan in 2010, OneStream occupies a space that includes legacy players such as Oracle Hyperion, SAP, and IBM, as well as products focused on more specific business problems, such as those from Anaplan and Blackline. OneStream said it notched up 85% growth in annual recurring revenue (ARR) in 2020 and increased its customer base — which includes UPS, CapitalOne, and Costco — by 40% to more than 650 enterprises globally. It added that the majority of these customers were replacing legacy CPM applications. “OneStream is replacing multiple cumbersome and antiquated legacy systems with a single, unified platform to help organizations better and more efficiently support their financial operations, such as financial close, consolidation, planning, reporting, and analysis,” Koefoed said, adding that CPM constitutes “just a small part” of its competitors’ businesses, while newer cloud-based solutions only address a subset of CPM requirements. “OneStream goes beyond this, addressing all aspects of corporate finance, local finance, and diverse operational business units in a single application — streamlining financial processes and planning while also maintaining compliance and regulatory reporting.” OneStream had bootstrapped its way to profitability ahead of a $500 million-plus investment from KKR in 2019 that valued the company at more than $1 billion. A new $6 billion valuation puts OneStream alongside Anaplan and Blackline, which currently hold similar valuations on the public markets. Indeed, with a fresh $200 million in the bank from backers such as D1 Capital Partners, Tiger Global, and Investment Group of Santa Barbara (IGSB), Koefoed confirmed that OneStream is now contemplating the transition from being a private to a public company, though he stopped short of specifying a time frame."
https://venturebeat.com/2021/04/06/servicenow-partners-with-qualtrics-on-feedback-loop-using-sentiment-data-from-surveys/,ServiceNow partners with Qualtrics on feedback loop using sentiment data from surveys,"ServiceNow and Qualtrics today announced they are working together to integrate sentiment data captured using surveys within workflows running on the Now platform. The feature will become available sometime in the second half of this year. The two providers of software-as-a-service (SaaS) platforms will integrate their offerings for managing both IT and customer service. The goal is to create a feedback loop within those workflows that will enable organizations to better optimize workflows over time, said Michael Ramsey, vice president of product management for customer workflow products at ServiceNow. In some cases, that feedback loop will be created by manually reviewing survey data, while in others it might be automatically integrated with workflows that could be dynamically adjusted based on the feedback captured by a Qualtrics survey, Ramsey noted. “It’s about making feedback actionable,” Ramsay said. “We’re enabling a modern digital experience.” ServiceNow already has a similar relationship with SurveyMonkey to capture survey data. But Qualtrics is evolving into an experience management platform that makes it possible to, for example, trigger escalations within workflows based on feedback gathered, Qualtrics chief product officer Jay Choi said. As organizations embrace digital business transformation more aggressively in the wake of the COVID-19 pandemic, many of them have no way of knowing whether customers and end users are viewing these changes in a positive light, Choi noted. SaaS platforms such as Qualtrics provide a simple way to capture that feedback at a time when many organizations are relying on online processes to engage customers. “There’s an experience gap,” Choi said. In the last year, many organizations have accelerated digital business transformation initiatives out of necessity. But in the rush to roll those processes out, many of the organizations didn’t have the time to properly vet them. As the pandemic eventually subsides, most organizations will need to revisit digital processes that were hastily created to simply stay engaged with customers. At the same time, internal IT teams created self-service portals for employees working from home to help combat the COVID-19 pandemic. Most of those internal IT teams, however, don’t have any direct feedback from end users on the value of those efforts. Following the rollout of COVID-19 vaccinations, employees will be split between home and the office, depending on their personal preference and company mandates. Internal IT teams will once again need to adjust the IT services they provide to those end users — hopefully, with more input this time. Organizations have been launching customer surveys for decades. Platforms like Qualtrics and SurveyMonkey simply make it easier for certain types of businesses to launch them without having to contract a dedicated customer research team. Of course, the feedback any organization collects is only as good as the questions asked, so savvy organizations will still need to ask questions in a way that doesn’t bias survey results. In the meantime, a lot of organizations are about to discover just how well-received their digital business transformation efforts actually are. A digital business process might be more cost-effective for organizations to deliver, but it’s not uncommon for end customers to resent the level of data entry that has been shifted their way to enable that process. In fact, as organizations are increasingly judged by the online experience they provide as much as by the products and services they sell, many are about to discover to what degree online experiences negatively impact customer loyalty. But the first step toward determining what customers think is capturing their sentiment at the point where those digital services are being consumed."
https://venturebeat.com/2021/04/06/sendbird-secures-100m-to-help-businesses-add-chat-voice-and-video-calling-to-their-apps/,"Sendbird secures $100M to help businesses add chat, voice, and video calling to their apps","Sendbird, a platform that makes it easier for businesses to add messaging, voice, and video chat functionality to their apps, has raised $100 million in a series C round of funding at a $1.05 billion valuation. The raise comes while the so-called API economy is thriving as businesses across the spectrum have been forced to embrace digital transformation, be that through extending online customer service channels or expanding into video-based telehealth. The API economy was on an upward trajectory long before the global pandemic took hold, though, driven in part by a gradual shift from monolithic on-premises software to the cloud and microservices-based applications. Smaller, function-based components are easier to develop and maintain, with individual teams or developers taking responsibility for a single service — and APIs are integral to joining them all together. Moreover, consumers and end-users increasingly expect to be able to engage with companies directly through their mobile apps. But a company that offers an app-based food delivery service, for example, doesn’t really want to consume resources building their own communications infrastructure to enable customers to chat with their driver — it’s much easier if they can leverage platforms that were custom-built for that purpose. This is where Sendbird comes into play. “Even before Covid, there has been a shift to more and more of the tasks we accomplish in our lives occurring within mobile apps — online purchases, entertainment, food delivery, and lots of others,” Sendbird cofounder and CEO John S. Kim told VentureBeat. “Brands are increasingly choosing in-app chat over SMS as the way of connecting with users and connecting users with each other within the mobile and sometimes Web experience. These interactions facilitate purchases, provide support, and build loyalty. This is what’s been driving Sendbird’s growth for the last five years and continues to do so as the shift from offline to online continues.” There has been a flurry of activity across the API sphere of late: MessageBird acquired Pusher to expand its real-time communication APIs; Idera, meanwhile, acquired Apilayer, a startup that provides cloud-based APIs to big names such as Amazon, Apple, and Facebook; and RapidAPI acquired Paw to help developers build, test, and manage APIs. And in the funding sphere, companies including MessageBird, Postman, and Kong have all raised large sums of money at multi-billion dollar valuations over the past year. Founded out of Korea in 2013, Sendbird had largely focused on offering chat and messaging services to developers, but last March it expanded to offer real-time voice- and video-calling too. Although businesses can already choose from a wide array of free existing tools to connect with clients or customers, they don’t provide sufficient control over the experience, which is why many prefer to create custom solutions themselves in-house. “Smaller companies typically rely on free services like Zoom or WhatsApp to connect with their customers,” Kim said. “But brands who want to control the branding and user experience, get the benefits of the data and analytics, and integrate conversations into a core workflow — such as connecting a seller with a buyer who has questions — those businesses are going to invest in a great mobile experience and that experience is going to need chat, voice, and video interactions as a core piece.” Sendbird’s typical customer is a mobile-first digital company rather than traditional enterprise clients such as banks or insurance companies. For example, it counts several mobile wallets as customers, such as Indian super app Paytm. That said, Sendbird does have traditional enterprise clients too, including Korea Telecom and ServiceNow. “We do have traditional industries that did not start cloud first or mobile first as our customers, but going after those companies proactively is not a focus for us,” Kim said. Prior to now, Y Combinator alum Sendbird had raised around $121 million in funding, the bulk of which arrived via its series B round of funding which closed in 2019. The company’s latest cash injection was spearheaded by Steadfast Capital Ventures, with participation from Emergence Capital, Softbank Vision Fund 2, World Innovation Lab, Iconiq Growth, Tiger Global Management, and Meritech Capital, and it said that it plans to use the funds to “aggressively accelerate its R&D efforts” and hire across its key hubs in San Mateo (California), New York, London, Munich, Singapore, Bengaluru, and Seoul."
https://venturebeat.com/2021/04/06/upsolver-raises-25m-to-enable-no-code-data-analytics-in-the-cloud/,Upsolver enables no-code data analytics in the cloud with $25M funding,"Upsolver, a no-code startup that enables analytics on cloud data lakes, this morning announced that it raised $25 million in financing led by Scale Venture Partners. The company, which also today launched a free community edition of its product, says the funds will be used to hire engineers and scale its go-to-market efforts. Enterprises are rapidly adopting the cloud — 68% of CIOs ranked migrating to the cloud as their top IT spending driver in 2020, according to a Deloitte survey. But building an analytics-ready cloud data lake can be complex and expensive. A recently published Statista report found that around 83% of cloud practitioners considered security, managing cloud spend, governance, and a lack of resources to be significant barriers to entry. Upsolver develops software designed to prime data lakes for analytics, CEO Ori Rafael told VentureBeat via email. Founded in 2014 by Rafael and Yoni Eini, the company’s platform offers a visual structured query language (SQL) interface and automations for data optimization, tuning, and orchestration. “We wanted to store data affordably in the cloud without analytics vendor lock-in,” Rafael said. “Unfortunately, what used to take three hours using SQL turned into 30 days of hand-coding and complex Spark configuration. We created Upsolver to bridge this gap between raw cloud data and analytics-ready data.”  Upsolver’s product lets companies perform analytics using a range of query engines and data systems including PrestoDB, Trino, Athena, Snowflake, Redshift, Synapse Analytics, Splunk, and Elastic. Ultimately, the goal is to replace all a customer’s code-heavy approaches with Upsolver’s compute layer, which sits between the customer’s cloud storage and their preferred tools, engines, and apps. “Upsolver is a critical component for successfully implementing a cloud data lake for analytics, which is a popular approach due to the affordability that data lakes provide. We complement cloud data warehouses, search engines, and other purpose-built data stores as well,” Rafael said. “Upsolver is often used to output prepared data to those platforms making data available to standalone query engines. Data engineers use Upsolver’s visual user interface to build any data transformations and preparation tasks. This generates SQL that the data engineer can also directly edit to fine-tune the processing.” Rafael says that Upsolver will soon add the ability to replicate databases into data lakes while keeping them up to date. The platform recently launched on Azure and is set to become available in a community edition delivering “free-forever” capabilities to those with smaller workloads, providing a proving ground for companies who might want to work with Upsolver on larger use cases. Scale partner Ariel Tseitlin asserts that Upsolver benefits from having a foot in two fast-growing markets: big data analytics and data lakes. The global data lake market size was valued at $7.6 billion in 2019, according to Grand View Research. And in 2017, Forbes reported that 53% of companies had adopted big data analytics, with a large portion opting to run workloads in the cloud. Thirty-employee Upsolver says that revenue tripled 2020 as brands including Cox Automotive, Wix, and AppsFlyer joined its customer base. “Monolithic analytics platforms are a thing of the past. Today’s organizations require a variety of analytics tools to fully capitalize on their data,” Tseitlin said in a press release. “Data lakes originally promised this variety and openness but also required a large, ongoing investment in engineering. Upsolver eliminates this trade-off.” Existing investors Vertex Ventures US, Wing Venture Capital, and JVP also participated in Upsolver’s series B round. It comes on the heels of a $13 million series A and brings the company’s total raised to date to $42 million."
https://venturebeat.com/2021/04/06/threat-intelligence-platform-threatquotient-secures-22-5m/,Threat intelligence platform ThreatQuotient secures $22.5M,"ThreatQuotient, a security operations platform provider, today announced that it raised $22.5 million, a mix of debt and $13 million in equity. The company says it’ll use the proceeds to expand its workforce and the availability of its products globally. More than three quarters of IT security leaders anticipate a major breach involving a critical infrastructure organization in the near future, according to a Black Hat USA survey. It’s estimated that 31% of organizations have experienced cyberattacks on operational technology infrastructure. Perhaps unsurprisingly, Gartner found that organizations planned to invest $17.48 billion in infrastructure protection in 2020. ThreatQuotient was founded in 2013 by Ryan Trost and Wayne Chiang as a part of an effort to build a centrally managed source of cybersecurity solutions. The company’s product, ThreatQ, is designed to serve as a threat intelligence platform with an integrated, self-tuning library that features a workbench allowing for threat detection and response. ThreatQ can score and prioritize cyber threats based on customers’ parameters as well as facilitate aggregation, operationalization across systems and teams. It offers configuration and integrations in addition to workload and enrichment, coordinating inefficiencies that span security operations. Using ThreatQ, team leaders can direct actions, assign tasks, and see the results in real time. They can also import and aggregate data sources and export intelligence to third-party tools via integrations with feeds and security systems. “ThreatQ [uses] analytics and machine learning to determine relevance and priority for specific companies. This is the basis for our dynamic scoring and self-tuning capabilities. As more data and context is captured in the threat library, the platform will reprioritize the data to ensure priority is known and appropriate actions are taken,” CEO and president John Czupak explained to VentureBeat via email. “ThreatQuotient also offers [a]  cybersecurity situation room designed for collaborative threat analysis, shared, accelerated understanding, and coordinated response. Built on top of the ThreatQ platform, [it] allows for the capturing, learning and sharing of knowledge.” ThreatQ can be deployed solely on-premises or in cloud-based environments, plus software-only distributions for virtual machines. ThreatQuotient also offers a family of appliances to meet various performance requirements. In 2020, 115-employee ThreatQuotient says it secured a “record” number of new customers in 12 countries, bringing its customer base to over 100 companies. The startup also claims to have achieved a “record” number of transactions following the launch of a fully managed version of its platform. “As a result of strong performance in 2020, we welcomed an opportunity to secure additional funding and add new investors to our syndicate. ThreatQuotient is meeting a critical need for security operations solutions, and we have significant expansion plans to continue this momentum,” Czupak continued. “Fortunately, we are in a position where the products and services we provide are critical to our customer’s security operations. In some cases, opportunities accelerated because security became higher on the list of priorities for organizations during the pandemic.” New Enterprise Associates, Adams Street Partners, Escalate Capital, Blu Ventures, Cisco Investments, and Gaingels participated in the latest funding round. It brings the company’s total raised to over $60 million."
https://venturebeat.com/2021/04/05/google-triumphs-after-epic-java-api-copyright-battle-with-oracle/,Google triumphs after epic Java API copyright battle with Oracle,"(Reuters) — The U.S. Supreme Court handed Alphabet’s Google a major victory on Monday, ruling that its use of Oracle’s software code to build the Android operating system that runs most of the world’s smartphones did not violate federal copyright law. In a 6-2 decision, the justices overturned a lower court’s ruling that Google’s inclusion of Oracle’s software code in Android did not constitute fair use under U.S. copyright law. Justice Stephen Breyer, writing for the majority, said that allowing Oracle to enforce a copyright on its code would harm the public by making it a “lock limiting the future creativity of new programs. Oracle alone would hold the key.” Oracle and Google, two California-based technology giants with combined annual revenues of more than $175 billion, have been feuding since Oracle sued for copyright infringement in 2010 in San Francisco federal court. Google had appealed a 2018 ruling by the U.S. Court of Appeals for the Federal Circuit in Washington reviving the suit. The ruling spares Google a potentially massive damages verdict. Oracle had been seeking more than $8 billion, but renewed estimates went as high as $20 billion to $30 billion, according to two people with knowledge of the situation. “The decision gives legal certainty to the next generation of developers whose new products and services will benefit consumers,” said Kent Walker, Google’s senior vice president of global affairs. Oracle’s lawsuit accused Google of plagiarizing its Java software by copying 11,330 lines of computer code, as well as the way it is organized, to create Android and reap billions of dollars in revenue. Android, for which developers have created millions of applications, now powers more than 70% of the world’s mobile devices. Google has said it did not copy a computer program but rather used elements of Java’s software code needed to operate a computer program or platform. Federal copyright law does not protect mere “methods of operation.” The companies also disputed whether Google made fair use of Oracle’s software code, making it permissible under the 1976 Copyright Act. Dorian Daley, Oracle’s executive vice president and general counsel, said that with the ruling “the Google platform just got bigger and market power greater” and “the barriers to entry higher and the ability to compete lower.” “They stole Java and spent a decade litigating as only a monopolist can. This behavior is exactly why regulatory authorities around the world and in the United States are examining Google’s business practices,” Daley said. Technology industry trade groups cheered the ruling, saying an Oracle victory in the case would have inhibited competition by making it harder to use programming elements to ensure computer interoperability. “The high court’s decision that fair use extends to the functional principles of computer code means companies can offer competing, interoperable products,” said Matt Schruers, president of the Computer & Communications Industry Association. Shares in Oracle rose nearly 4%, and Alphabet gained 4.4% in mid-afternoon trading. In Monday’s ruling, Breyer wrote, “Google’s copying was transformative,” adding that the company repurposed Oracle’s code in a way that helps developers create programs. The ruling sidestepped the question of whether Oracle’s code was entitled to copyright protection in the first place. In a dissenting opinion, Justice Clarence Thomas, joined by Justice Samuel Alito, said the court should have found that Oracle’s work deserved a copyright and Google’s use was “anything but fair.” Noting that Apple and Microsoft did not resort to copying like Google did to create mobile operating systems, Thomas said the ruling will harm competition. If “companies may now freely copy libraries of declaring code whenever it is more convenient than writing their own, others will likely hesitate to spend the resources Oracle did to create intuitive, well-organized libraries that attract programmers and could compete with Android,” Thomas wrote. Google twice lost at the Federal Circuit, in 2014 and 2018. A jury cleared Google in 2016. The Federal Circuit overturned that decision in 2018, finding that Google’s incorporation of elements of Oracle’s “application programming interfaces” was not permitted under the fair use doctrine, rejecting Google’s argument that by adapting them to a mobile platform it transformed them into something new. Justice Amy Coney Barrett did not participate in the ruling. She had not yet joined the court when arguments were held on October 7."
https://venturebeat.com/2021/04/05/m-files-acquires-hubshare-to-gain-access-to-content-sharing-portal/,M-Files acquires Hubshare to gain access to content-sharing portal,"M-Files today announced it has acquired Hubshare as part of an effort to make it easier to share files and data stored in its enterprise content management (ECM) platform. Terms of the deal were not disclosed. Hubshare presents end users with a portal through which they can access data and files without requiring IT teams to move everything into one central repository, said M-Files CEO Antti Nivala. M-Files plans to continue to offer Hubshare as a standalone platform while simultaneously working to tighten integration between Hubshare and the ECM platform provided by M-Files, Nivala added. In general, most Hubshare users have been people working for different organizations who needed an easier way to collaborate and share files, documents, and other classes of data, noted Nivala. As the number of digital business transformation initiatives involving multiple organizations continues to expand, the need to seamlessly share information will only increase in the months ahead, Nivala said. In effect, Hubshare is evolving into the front-end portal through which customers can centrally access multiple backend platforms for storing data via a portal that keeps track of which users are authorized to access specific files and documents, Nivala said. M-Files earlier this year raised an additional $80 million in funding. Ultimately, the company’s goal is to apply AI to metadata independently of the ECM platform it was captured from. Armed with AI capabilities, it should become simpler to optimize workflows spanning multiple ECM platforms. Despite the rise of massive data lakes in the cloud, Nivala said there will never be a single source of truth in any enterprise. Data will continue to be created and managed within the context of a wide range of application silos that will each continue to have their own set of master data. “There are going to be a number of master locations for data,” he said. The challenge IT organizations face is finding a way to affordably make all that data accessible to end users inside and outside of an organization in a way that doesn’t compromise security and, just as importantly, can be audited from a compliance perspective, Nivala added. Most IT organizations don’t have a great track record when it comes to centralizing data management. However, there are now more users than ever that need to directly access data that was created outside of an application environment they have express permission to use. As such, the need for a portal that makes it easier to navigate data residing in multiple master data repositories is rising. Data is now being stored independently of the application first employed to create it. In fact, many organizations will soon find themselves judged based on how easy they are to work with while, paradoxically, being required to make sure that content stays secure. As critical as security might be, productivity is still the most critical of all metrics applied to any workflow. Of course, competition among providers of ECM platforms is already fierce. These platforms are racing to embed AI capabilities that will make it simpler to access and integrate a wide range of content. It’s not clear to what degree those capabilities might dissuade organizations from launching massive data warehouse projects as it becomes simpler for both end users and third-party applications to access content wherever it happens to reside. Regardless of approach, however, the one thing that is apparent is that legacy approaches to managing content will no longer suffice in an era where the value of data rises in direct proportion to how often and widely it’s employed."
https://venturebeat.com/2021/04/05/government-audit-of-ai-with-ties-to-white-supremacy-finds-no-ai/,Government audit of AI with ties to white supremacy finds no AI,"In April 2020, news broke that Banjo CEO Damien Patton, once the subject of profiles by business journalists, was previously convicted of crimes committed with a white supremacist group. According to OneZero’s analysis of grand jury testimony and hate crime prosecution documents, Patton pled guilty to involvement in a 1990 shooting attack on a synagogue in Tennessee. Amid growing public awareness about algorithmic bias, the state of Utah halted a $20.7 million contract with Banjo, and the Utah attorney general’s office opened an investigation into matters of privacy, algorithmic bias, and discrimination. But in a surprise twist, an audit and report released last week found no bias in the algorithm because there was no algorithm to assess in the first place. “Banjo expressly represented to the Commission that Banjo does not use techniques that meet the industry definition of artificial Intelligence. Banjo indicated they had an agreement to gather data from Twitter, but there was no evidence of any Twitter data incorporated into Live Time,” reads a letter Utah State Auditor John Dougall released last week. The incident, which VentureBeat previously referred to as part of a “fight for the soul of machine learning,” demonstrates why government officials must evaluate claims made by companies vying for contracts and how failure to do so can cost taxpayers millions of dollars. As the incident underlines, companies selling surveillance software can make false claims about their technologies’ capabilities or turn out to be charlatans or white supremacists — constituting a public nuisance or worse. The audit result also suggests a lack of scrutiny can undermine public trust in AI and the governments that deploy them. Dougall carried out the audit with help from the Commission on Protecting Privacy and Preventing Discrimination, a group his office formed weeks after news of the company’s white supremacist associations and Utah state contract. Banjo had previously claimed that its Live Time technology could detect active shooter incidents, child abduction cases, and traffic accidents from video footage or social media activity. In the wake of the controversy, Banjo appointed a new CEO and rebranded under the name safeXai. “The touted example of the system assisting in ‘solving’ a simulated child abduction was not validated by the AGO and was simply accepted based on Banjo’s representation. In other words, it would appear that the result could have been that of a skilled operator as Live Time lacked the advertised AI technology,” Dougall states in a seven-page letter sharing audit results. According to Vice, which previously reported that Banjo used a secret company and fake apps to scrape data from social media, Banjo and Patton had gained support from politicians like U.S. Senator Mike Lee (R-UT) and Utah State Attorney General Sean Reyes. In a letter accompanying the audit, Reyes commended the results of the investigation and said the finding of no discrimination was consistent with the conclusion the state attorney general’s office reached because there simply wasn’t any AI to evaluate. “The subsequent negative information that came out about Mr. Patton was contained in records that were sealed and/or would not have been available in a robust criminal background check,” Reyes said in a letter accompanying the audit findings. “Based on our first-hand experience and close observation, we are convinced the horrible mistakes of the founder’s youth never carried over in any malevolent way to Banjo, his other initiatives, attitudes, or character.” Alongside those conclusions are a series of recommendations for Utah state agencies and employees involved in awarding such contracts. Recommendations for anyone considering AI contracts include questions they should be asking third-party vendors and the need to conduct an in-depth review of vendors’ claims and the algorithms themselves. “The government entity must have a plan to oversee the vendor and vendor’s solution to ensure the protection of privacy and the prevention of discrimination, especially as new features/capabilities are included,” reads one of the listed recommendations. Among other recommendations are the creation of a vulnerability reporting process and evaluation procedures, but no specifics were provided. While some cities have put surveillance technology review processes in place, local and state adoption of private vendors’ surveillance technology is currently happening in a lot of places with little scrutiny. This lack of oversight could also become an issue for the federal government. The Government by Algorithm report Stanford University and New York University jointly published last year found that roughly half of algorithms used by federal government agencies come from third-party vendors. The federal government is currently funding an initiative to create tech for public safety, like the kind Banjo claimed to have developed. The National Institute of Standards and Technology (NIST) routinely assesses the quality of facial recognition systems and has helped assess the role the federal government should play in creating industry standards. Last year, it introduced ASAPS, a competition in which the government is encouraging AI startups and researchers to create systems that can tell if an injured person needs an ambulance, whether the sight of smoke and flames requires a firefighter response, and whether police should be alerted in an altercation. These determinations would be based on a dataset incorporating data ranging from social media posts to 911 calls and camera footage. Such technology could save lives, but it could also lead to higher rates of contact with police, which can also cost lives. It could even fuel repressive surveillance states like the kind used in Xinjiang to identify and control Muslim minority groups like the Uyghurs. Best practices for government procurement officers seeking contracts with third parties selling AI were introduced in 2018 by U.K. government officials, the World Economic Forum (WEF), and companies like Salesforce. Hailed as one of the first such guidelines in the world, the document recommends defining public benefit and risk and encourages open practices as a way to earn public trust. “Without clear guidance on how to ensure accountability, transparency, and explainability, governments may fail in their responsibility to meet public expectations of both expert and democratic oversight of algorithmic decision-making and may inadvertently create new risks or harms,” the British-led report reads. The U.K. released official procurement guidelines in June 2020, but weeks later a grading algorithm scandal sparked widespread protests. People concerned about the potential for things to go wrong have called on policymakers to implement additional legal safeguards. Last month, a group of current and former Google employees urged Congress to adopt strengthened whistleblower protections in order to give tech workers a way to speak out when AI poses a public harm. A week before that, the National Security Commission on Artificial Intelligence called on Congress to give federal government employees who work for agencies critical to national security a way to report misuse or inappropriate deployment of AI. That group also recommends tens of billions of dollars in investment to democratize AI and create an accredited university to train AI talent for government agencies. In other developments at the intersection of algorithms and accountability, the documentary Coded Bias, which calls AI part of the battle for civil rights in the 21st century and examines government use of surveillance technology, started streaming on Netflix today. Last year, the cities of Amsterdam and Helsinki created public algorithm registries so citizens know which government agency is responsible for deploying an algorithm and have a mechanism for accountability or reform if necessary. And as part of a 2019 symposium about common law in the age of AI, NYU professor of critical law Jason Schultz and AI Now Institute cofounder Kate Crawford called for businesses that work with government agencies to be treated as state actors and considered liable for harm the way government employees and agencies are."
https://venturebeat.com/2021/04/05/bank-software-provider-alkami-chases-2b-ipo/,Bank software provider Alkami chases $2B IPO,"(Reuters) — Alkami Technology is aiming for a valuation of up to $2.08 billion in a U.S. initial public offering (IPO), the banking software provider said in its prospectus on Monday. The company said it was looking to raise as much as $150 million by selling 6 million shares at a price range of $22 to $25 per share. Reuters reported on the Plano, Texas-based firm’s IPO plans in February. Backed by investors including General Atlantic and D1 Capital, Alkami supplies cloud-based platforms that banks and credit unions can use to set up digital offerings for their retail and business customers. The company’s listing plans come as the COVID-19 pandemic has shown that financial services companies need secure and effective digital platforms. Smaller banks typically do not have the resources to invest in developing such systems internally and instead turn to third-party providers like Alkami. Alkami’s website shows its services are used by more than 160 financial institutions. The company earlier said its shares would be listed on the Nasdaq under the symbol “ALKT.” Goldman Sachs, JP Morgan, and Barclays are the lead underwriters for the offering."
https://venturebeat.com/2021/04/04/what-is-a-streaming-database/,What is a streaming database?,"The internet of things is everywhere, and the data is piling up. The new tiny, embeddable computers are energizing managers and engineers with the possibility of using all of this data to control everything from industrial plants to personal homes. The database administrators, though, aren’t as thrilled because they are expected to gather, store, and analyze this often unceasing firehose of bits. Some programmers and DBAs are developing pipelines that can accept, analyze, and store the important bits. These so-called streaming databases are tools designed to handle both the unstoppable incoming flow as well as the endless queries from tools that want to make decisions based upon the data. Streaming databases are close cousins to other new classes of tools like time-series databases or log databases. All are designed to track a series of events and enable queries that can search and produce statistical profiles of blocks of time. The streaming databases can respond to queries for data and also statistics about the data, generate reports from these queries, and populate all of the dashboards that track what’s happening to allow the users to make smart decisions about the telemetry. The tools are essentially pipelines that start out analyzing the incoming data flow and end up storing aggregated data in a database that’s easily queried. Some think of the streaming database as the entire system, and some imagine that the system is created by attaching the pipeline to a more traditional database. In both cases, the entire system is ready to answer questions. Some good examples of important use cases include: The data inside is often split, at least philosophically, into two tiers. The raw input, often called “streams,” are immutable, append-only sequences of events. They’re meant to be a historical record of what happened and when. The second tier is built from watching the streams and constructing summaries, often statistical, about the events. They might, for instance, count the number of times that an event happened each day over the last month or find the average value over each week in a year. The analysis is usually stored in tables that are often similar in structure and behavior to traditional relational databases. Indeed, it’s not uncommon for developers to connect a traditional database for these results. Some streaming databases are designed to dramatically reduce the size of the data to save storage costs. They can, say, replace a value collected every second with an average computed over a day. Storing only the average can make long-term tracking economically feasible. Streaming opens up some of the insides of a traditional database. Standard databases also track a stream of events, but they’re usually limited to changes in data records. The sequence of INSERTs, UPDATEs, and DELETEs are normally stored in a hidden journal or ledger inside. In most cases, the developers don’t have direct access to these streams. They’re only offered access to the tables that show the current values. Streaming databases open up this flow and makes it simpler for developers to adjust how the new data is integrated. Developers can adjust how the streams from new data are turned into tabular summaries, ensuring that the right values are computed and saved while the unneeded information is ignored. The opportunity to tune this stage of the data pipeline allows streaming databases to handle markedly larger datasets. The traditional databases are finding a role in streaming applications, but usually as a destination that lies downstream. The data flows through another tool that analyzes it and generates more concise values for more permanent storage in a traditional database. The legacy software and reporting tools can work easily with it. For instance, Oracle Streams can be deployed either as a service or as on-premises installation. It will gather and transform data from a variety of sources and then deposit it with other services that can include their own databases. The message format is designed to be compatible with Apache Kafka, an open standard, allowing it to be integrated with other Kafka applications. IBM’s product, also called Streams, emphasizes the analytical power of the pipeline integrated with some of the machine learning products. It is also compatible with Kafka and can deposit the results in a number of destinations, including IBM’s own data warehouses. Microsoft’s Stream Analytics also emphasizes the analytics that can occur along the path from the event’s first appearance to its eventual destination, which can be any of Azure’s storage solutions including the SQL databases. The processing, which can be written in an SQL-like language and incorporate other common languages like JavaScript, also may train machine learning models via Azure’s ML Service. The SQL dialect includes temporal constraints used to transform the incoming data, which is usually tracking the time and date. The Azure Stream Analytics service is also tightly integrated with Microsoft’s AI services to use machine learning and video analytics to deconstruct the data stream. It offers an SQL-like syntax that can be extended with code written in JavaScript or C#. New companies are tackling the challenge by either building entirely integrated tools or simply creating a stream-handling layer that works with existing databases. Those that integrate with established infrastructure can leverage all of the other compatible tools, while the entirely new versions have the advantage of building everything from scratch. Many of the tools that integrate with existing databases are built on Apache’s Kafka, an open source message handing framework that’s often used to link together multiple software packages. Kafka itself handles the chores of buffering and delivering the messages containing the events. This buffering, incidentally, requires storing the stream of events, making Kafka a kind of very basic database that eventually delivers the data to another. Equalum, for instance, offers a tool for transforming a data stream en route to a data warehouse or data lake using more traditional databases. It’s built upon an open source foundation of Apache’s Kafka and Spark and offers a simplified, visual coding framework that allows the data pathway to be defined as a flowchart. Developers who enjoy working in SQL will appreciate ksqlDB, a tool for ingesting and storing data that uses a version of SQL to specify major tasks. “Use a familiar, lightweight syntax to pack a powerful punch,” the sales literature promises. “Capture, process, and serve queries using only SQL. No other languages or services are required.” The tool is tightly integrated with Kafka to ensure it is simpler to install in existing applications that use it. Amazon calls its major offering Kinesis and offers special, preconfigured pathways for working with video feeds. It’s integrated with some of AWS’ AI tools like the Rekognition for video analysis and SageMaker for basic machine learning. Others are starting to build open source projects like Debezium that can transform data from event streams managed by Kafka or other pipelines. In many ways, streaming databases are just supersets of the traditional model. If you think of the standard INSERTs and DELETEs as events, then any of the standard applications can be handled by the streaming cousins. Much of the overhead, though, may be wasted if the application does not require constantly evolving analysis. Many streaming databases also offer fewer of the traditional functions or APIs because their first job is taming the endless flow of data. They may not offer the complex views or elaborate joins, at least not for the incoming data. If the results are stored in a more traditional relational database, it will have all of the features associated with it. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/04/bounding-your-ml-models-dont-let-your-algorithms-run-wild/,Bounding your ML models: Don’t let your algorithms run wild,"The purpose of designing and training algorithms is to set them loose in the real world, where we expect performance to mimic that of our carefully curated training data set. But as Mike Tyson put it, “everyone has a plan, until they get punched in the face.” And in this case, your algorithm’s meticulously optimized performance may get punched in the face by a piece of data completely outside the scope of anything it encountered previously. When does this become a problem? To understand, we need to return to the basic concepts of interpolation vs. extrapolation. Interpolation is an estimation of a value within a sequence of values. Extrapolation estimates a value beyond a known range. If you’re a parent, you can probably recall your young child calling any small four-legged animal a cat, as their first classifier only used minimal features. Once they were taught to extrapolate and factor in additional features, they were able to correctly identify dogs too. Extrapolation is difficult, even for humans. Our models, smart as they might be, are interpolation machines. When you set them to an extrapolation task beyond the boundaries of their training data, even the most complex neural nets may fail. What are the consequences of this failure? Well, garbage in, garbage out. Beyond the deterioration of model results in the real world, the error can propagate back to training data in production models, reinforcing erroneous results and degrading model performance over time. In the case of mission critical algorithms, as in healthcare, even a single erroneous result should not be tolerated. What we need to adopt, and this is not a unique problem in the domain of machine learning, is data validation. Google engineers published their method of data validation in 2019 after running into a production bug. In a nutshell, every batch of incoming data is examined for anomalies, some of which can only be detected by comparing training and production data. Implementing a data validation pipeline had several positive outcomes. One example the authors present in the paper is the discovery of missing features within the Google Play store recommendation algorithm — when the bug was fixed, app install rates increased by 2 percent. Researchers from UC Berkeley evaluated the robustness of 204 image classification models in adapting to distribution shifts arising from natural variation in data. Despite the models being able to adapt to synthetic changes in data, the team found little to no adaptation in response to natural distribution shifts, and they consider this an open research problem. Clearly this is a problem for mission critical algorithms. Machine learning models in healthcare bear a responsibility to return the best possible results to patients, as do the clinicians evaluating their output. In such scenarios, a zero-tolerance approach to out-of-bounds data may be more appropriate. In essence, the algorithm should recognize an anomaly in the input data and return a null result. Given the tremendous variation in human health, along with possible coding and pipeline errors, we shouldn’t allow our models to extrapolate just yet. I’m the CTO at a health tech company, and we combine these approaches: We conduct a number of robustness tests on every model to determine whether model output has changed due to variation in the features of our training sets. This training step allows us to learn the model limitations, across multiple dimensions, and also uses explainable AI models for scientific validation. But we also set out of bound limitations on our models to ensure patients are protected. If there’s one takeaway here, it’s that you need to implement feature validation for your deployed algorithms. Every feature is ultimately a number, and the range of numbers encountered during training is known. At minimum, adding a validation step that ascertains whether a score in any given run is within the training range will increase model quality. Bounding models should be fundamental to trustworthy AI. There is much discussion on design robustness and testing with adversarial attacks (which are designed specifically to fool models). These tests can help harden models but only in response to known or foreseen examples. However, real world data can be unexpected, beyond the ranges of adversarial testing, making feature and data validation vital. Let’s design models smart enough to say “I know that I know nothing” rather than running wild. Niv Mizrahi is Co-founder and CTO of Emedgene and an expert in machine learning, big data, and large-scale distributed systems. He was previously Director of Engineering at Taykey, where he built an R&D organization from the ground up and managed the research, big data, automation, and operations teams."
https://venturebeat.com/2021/04/04/how-cloud-architectures-defend-against-the-cyber-attack-surge/,How cloud architectures defend against the cyber attack surge,"As we look to a post-pandemic world, we can expect to see companies invest in building resilience to destructive-type attacks. 2020 saw a record number of distributed denial-of-service (DDoS) and ransomware attacks, and the numbers are expected to remain high through the rest of this decade. The cloud — and cloud-native architectures — can help deliver resilience due to three key attributes: Distributed applications and services: If your applications are leveraging a distributed delivery model, for example leveraging cloud-based services such as content delivery networks (CDNs), then you have to worry less about DDoS attacks, as these attacks work best by concentrating their firepower in one direction. Immutable data sets: If your applications are leveraging solutions that do not modify records but rather are “append-on-write,” in other words your data set is immutable, then you have to worry less about attacks on the integrity of that data, as it is easier to detect and surface such attacks. Ephemeral workloads: Finally, if your applications are ephemeral in nature then you may worry less about attackers establishing persistence and moving laterally. And the value of confidential information (such as tokens associated with that application instance) is reduced, as those assets simply get decommissioned and new ones get instantiated within a relatively short frame of time. By leveraging modern cloud-native architectures that are distributed, immutable and ephemeral, you help address the issues of confidentiality, integrity and availability that have been the foundational triad of cybersecurity. So how are companies manifesting these attributes in their applications? Modern cloud architectures are moving from monolithic, tiered models to distributed microservices-based architectures, where each microservice can scale independently, within a geographic region or across regions. And each microservice can have its own, optimized storage and database, thereby allowing that service to run stateless (or perhaps more accurately using a shared-state model where the state is shared amongst the running instances via the storage/database layer). This allows those services to become truly ephemeral and distributed. This brings us to a concept that has seen quite a bit of discussion already in the context of the cloud — pets vs. cattle. Pets have a cute name and can be recognized individually. If a pet falls ill, the owner takes it to the vet. Owners give their pets a lifetime of caring and make sure they live healthy lives for as long as possible. Traditional applications are like pets. Each instance is unique. If the application gets infected, it is taken to the cyber vet. “Patch in place” is common with traditional applications, which make these instances unique. IT’s job is to keep the applications up and running for as long as possible. Cattle on the other hand, don’t have names, they have numbers. You generally cannot distinguish the cattle in the herd, and you don’t build relationships with them. If cattle fall ill or get infected, you cull the herd. Modern cloud applications are like cattle. You create many running instances of the services, and each instance is indistinguishable from the other. They are all manifested from a golden repository. You never patch-in-place, i.e. you never make the instances bespoke. Your job is to make the instances ephemeral, killing them quickly and creating new ones. In doing so, you build resilient systems rather than fragile ones. The cloud offers many tools to help build systems that follow this paradigm. For example, Amazon recently announced “chaos engineering” as-a-service, which allows organizations to introduce elements of chaos into their production workloads, such as taking down running instances, to ensure that the overall performance isn’t impacted and the workloads over time become resilient in the face of these types of operational setbacks. Getting to this point is a journey, and companies may need to take multiple steps to get there. For example, if you move your pets from an on-premises world to the cloud world without significantly altering the architecture of the applications, that’s just one step. The common term for this is “lift and shift.” Once your applications are in the cloud and you have started building familiarity with cloud native tools, you can work on re-architecting those pets into modern architectures that are distributed, immutable and ephemeral (i.e. cattle). In other words, you can move from pets-in-the-cloud to cattle-in-the-cloud. When you get to that point, you need to make sure you don’t regress and move back to creating pets again. In other words, don’t patch-in-place or keep instances up and running longer than necessary. Shehzad Merchant is CTO at Gigamon."
https://venturebeat.com/2021/04/03/ibm-bets-homomorphic-encryption-is-ready-to-deliver-stronger-data-security-for-early-adopters/,IBM bets homomorphic encryption is ready to deliver stronger data security for early adopters,"The topics of security and data have become almost inseparable as enterprises move more workloads to the cloud. But unlocking new uses for that data, particularly driving richer AI and machine learning, will require next-generation security. To that end, companies have been developing confidential computing to allow data to remain encrypted while it is being processed. But as a complement to that, a security process known as fully homomorphic encryption is now on the verge of making its way out of the labs and into the hands of early adopters after a long gestation period. Researchers like homomorphic encryption because it provides a certain type of security that can follow the data throughout its journey across systems. In contrast, confidential computing tends to be more reliant upon special hardware that can be powerful but is also limiting in some respects. Companies such as Microsoft and Intel have been big proponents of homomorphic encryption. Last December, IBM made a splash when it released its first homomorphic encryption services. That package included educational material, support, and prototyping environments for companies that want to experiment. In a recent media presentation on the future of cryptography, IBM director of strategy and emerging technology Eric Maass explained why the company is so bullish on “fully homomorphic encryption” (FHE). “FHE is a unique form of encryption, and it’s going to allow us to compute upon data that’s still in an encrypted state,” Maass said. First, some context. There are three general categories of encryption. The two classic ones are encryption for when data is at rest or stored and then “data in transit” that protects the confidentiality of data as it’s being transmitted over a network. The third one is the piece that has been missing: the ability to compute on that data while it’s still encrypted. That last one is key to unlocking all sorts of new use cases. That’s because until now, for someone to process that data, it would have to be unencrypted, which creates a window of vulnerability. That makes companies reluctant to share highly sensitive data involving finance or health. “With FHE, the ability to actually keep the data encrypted and never exposing it during the computation process, this has been somewhat akin to a missing leg in a three-legged crypto stool,” Maass said. “We’ve had the ability to encrypt the data at rest and in transit, but we have not historically had the ability to keep the data encrypted while it’s being utilized.” With FHE, the data can remain encrypted while being used by an application. Imagine, for instance, a navigation app on a phone that can give directions without actually being able to see any personal information or location. Companies are potentially interested in FHE because it would allow them to apply AI to data, such as from finance and health, while being able to promise users that the company has no way to actually view or access the underlying data. While the concept of homomorphic encryption has been of interest for decades, the problem is that FHE has taken a huge amount of compute power, so much so that it has been too expensive to be practicable. But researchers have made big advances in recent years. For instance, Maass noted that in 2011, it took 30 minutes to process a single bit using FHE. By 2015, researchers could compare two entire human genomes using FHE in less than an hour. “IBM has been working on FHE for more than a decade, and we’re finally reaching an apex where we believe this is ready for clients to begin adopting in a more widespread manner,” Maass said. “And that becomes the next challenge: widespread adoption. There are currently very few organizations here that have the skills and expertise to use FHE.” During the presentation, AI security group manager Omri Soceanu ran an FHE simulation involving health data being transferred to a hospital. In this scenario, an AI algorithm was used to analyze DNA for genetic issues that may reveal risks for prior medical conditions. That patient data would typically have to be decrypted first, which could raise both regulatory and privacy issues. But with FHE, it remains encrypted, thus avoiding those issues. In this case, the data is sent encrypted and remains so while being analyzed, and the results are also returned in an encrypted state. It’s important to note that this system was put in place using just a dozen lines of code, a big reduction from the hundreds of lines of code that have been required until recently. By reducing that complexity, IBM wants to make FHE more accessible to teams that don’t necessarily have cryptography expertise. Finally, Soceanu explained that the simulation was completed in .069 seconds. Just five years ago, the same simulation took a few hours, he said. “Working on FHE, we wanted to allow our customers to take advantage of all the benefits of working in the cloud while adhering to different privacy regulations and concerns,” he said. “What only a few years ago was only theoretically possible is becoming a reality. Our goal is to make this transition as seamless as possible, improving performance and allowing data scientists and developers, without any crypto skills, a frictionless move to analytics over encrypted data.” To accelerate that development, IBM Research has released open source toolkits, while IBM Security launched its first commercial FHE service in December. “This is aimed at helping our clients start to begin to prototype and experiment with fully homomorphic encryption with two primary goals,” Maass said. “First, getting our clients educated on how to build FHE-enabled applications and then giving them the tools and hosting environments in order to run those types of applications.” Maass said in the near term, IBM envisions FHE being attractive to highly regulated industries, such as financial services and health care. “They have both the need to unlock the value of that data, but also face extreme pressures to secure and preserve the privacy of the data that they’re computing upon,” he said. But he expects that over time a wider range of businesses will benefit from FHE. Many sectors want to improve their use of data, which is becoming a competitive differentiator. That includes using FHE to help drive new forms of collaboration and monetization. As this happens, IBM hopes these new security models will drive wider enterprise adoption of hybrid cloud platforms. The company sees a day, for instance, when due diligence for mergers and acquisitions is done online without violating the privacy of shareholders and when airlines, hotels, and restaurants use FHE to offer packages and promotions without giving their partners access to details of closely held customer datasets. “FHE will allow us to secure that type of collaboration, extracting the value of the data while still preserving the privacy of it,” Maass concluded."
https://venturebeat.com/2021/04/03/all-ai-driven-crm-platforms-are-not-created-equal/,All AI-driven CRM platforms are not created equal,"Customer relationships are changing in unpredictable ways compared to a year ago, creating unique data-driven challenges for marketers. The pandemic has made digital convenience a high priority with consumers who want a contextually rich, safe customer experience on any mobile device and are willing to switch brands and products to get it. The shift to ecommerce for everything was accompanied by the expectation of having all daily transactions be digital and touchless. As marketers struggle to decipher how changes in customer data can affect current and future campaigns, they are looking at what AI and machine learning can do to improve customer relationship management. Salesforce Research’s Sixth Annual State of Marketing Report found that 40% of B2B marketing leaders (manager level or higher) and 38% of B2C marketing leaders planned to increase their use of AI in 2020. That is on top of the 35% of B2B and B2C marketing leaders who said they are already using AI, according to the report. Salesforce surveyed 6,950 full-time marketing leaders from B2B (business-to-business), B2C (business-to-consumer), and B2B2C (business-to-business-to-consumer) companies around the world and found that 84% of marketers said they were using AI in 2020, up from 29% in 2018. CMOs and their marketing teams face the challenge of delivering results that drive revenue even as the markets redefine themselves. B2B marketing leaders rely on AI to improve their customer segmentation and lookalike audience modeling, according to Drift and the Marketing Artificial Intelligence Institute’s 2020 Marketing Leadership Benchmark Report. Other high priorities include personalizing channel experiences, discovering new data insights, and driving next-best actions, including offers in real time, automating customer interactions, and personalizing the overall customer journey. It stands to reason that AI-based CRM applications promise to improve existing processes’ speed and efficiency, increase revenue, and help find new services to sell. Too often, however, the apps are providing more process automation than AI-driven results. The demand for AI-based apps and platforms is extremely high, leading some CRM vendors to overstate the AI capabilities in their applications, which creates more hype across the CRM landscape. For example, process automation is often sold as AI, when what it does is independently perform simple, repetitive actions and tasks. If it can’t learn from datasets and initiate new workflows or ways of doing work, it isn’t AI. To provide some context, CRM is big business. Gartner’s latest market forecasts peg the worldwide CRM market at $56.5 billion in 2019, placing it as the largest segment of the enterprise software market, at 11.7% of global software revenue. Software rating site G2 Crowd lists 14 different types of AI-based CRM applications. One way to evaluate whether the CRM application actually utilizes AI or is just marketing hype is to look at feature areas individually. Each feature area is assigned a grade in the below list based on how much value AI delivers to marketers and the companies relying on these applications. AI-based sales assistants or bots
Despite an impressive amount of hype in the CRM community about AI-based sales assistants or bots increasing revenue, they are most often used to automate data entry and scheduling tasks and carry out routine sales force automation (SFA) tasks. Sales assistants or bots in their current generation often only rely on CRM datasets, drastically reducing their possible use cases. Bots are also purpose-built for specific tasks and are often process automation engines. Any organization considering these should give the product area a few generations to get a more integrated foundation in place. (Grade: B- / C) Configure, price, and quote (CPQ)
Dominated by rules- and constraint-based product configuration and process automation engines, CPQ is another area that’s overhyped when it comes to AI. CPQ benefits most from AI when it comes to guided selling and optimizing revenue management. For CPQ to deliver the maximum value it’s capable of, it needs to at least be integrated with an ERP and CRM system. Constraint-based configurators have been around for decades, as have process automation engines, two technologies that have at times been sold as low-end AI. It’s much easier to use product configuration rules from an existing configurator than to train configuration models, which is what a true AI-based configuration requires. (Grade: C- / D) Cross-sell and up-sell
Often sold as an integrated app within a configure, price, and quote (CPQ) or account-based marketing (ABM) system or platform, cross-sell and up-sell apps have progressed from relatively simple apps that integrate with product catalogs or product data to more advanced rules- and constraint-based scenarios, including AI-based apps that factor in customers’ personalized preferences. Cross-sell and up-sell apps have become the go-to option in ABM to expand sales into existing accounts yet are limited in how much business value they can deliver using AI. Process automation-based apps are sometimes sold as AI-based in this area of CRM. (Grade: B) Data intelligence solutions for sales
Vendors providing apps in this category move away from contact and company information and toward contextual intelligence using AI and ML. Vendors’ goals in transitioning to contextual intelligence include supporting sales prospects and selling scenarios with real-time data. Like many of the CRM apps mentioned, vendors in this category do not provide their own datasets. They have limited expertise in improving their data quality to get the most value out of this application. (Grade: B-) Sales predictive analytics (includes lead scoring)
AI’s impact on improving sales predictive analytics is evident in how effective these applications are in guiding sales rep, sales leader, and sales operations decision-making to improve margins and revenue. The best apps in this category are using machine learning to find new insights in account, sales history, and revenue data. Predictive forecasting, pipeline inspection, opportunity, and lead scoring are a few of the many areas where sales predictive analytics’ AI-based capabilities can contribute. (Grade: A+) Quota planning
AI- and ML-based quota planning apps are sold as part of an integrated sales performance management (SPM) platform or as a standalone product. The majority of apps in this category today support collaboration and workflows to define accurate, optimal sales quotas. The best apps in this category support various mathematical modeling approaches for assigning quotas across an organization. AI and ML algorithms are being used to set optimal quotas that are in turn distributed across an organization. (Grade: B) What’s limiting AI’s potential to deliver value in CRM today is the lack of consistent, high-quality data. How much of a contribution AI-based apps and platforms make as part of any CRM system is more dependent on the quality and availability of integrated data and less on the features of the app itself. Marketing organizations are renowned for having data quality problems, as data governance often isn’t a core strength of the department. There can be conflicting data structures, taxonomies, metatags, and a lack of consistency across all databases. Overcoming all of these obstacles and improving the data’s quality needs to come first, yet this can be a hurdle too high for marketers to clear. But each of these areas has the potential to deliver greater value in CRM once businesses overcome data quality challenges."
https://venturebeat.com/2021/04/03/pivoting-to-privacy-first-why-this-is-an-adapt-or-die-moment/,Pivoting to privacy-first: Why this is an adapt-or-die moment,"Operating in the digital advertising ecosystem isn’t for the faint of heart, and that’s never been truer than it is in 2021. The landscape is undergoing unprecedented transitions right now as we make a much-needed pivot to a privacy-first reality, and a lot of business models, practices, and technologies are not going to survive the upheaval. That said, I’m not here to make doomsday predictions. In fact, there are a lot of reasons for genuine optimism right now. As an industry, we’re heading in the right direction, and when we emerge on the other side of important transitions — including Google’s removal of third-party cookie support in Chrome and Apple’s limitations on IDFA — our industry will be stronger as a whole, as will consumer protections. Let’s take a look at the principles that will define the digital advertising and marketing world of the future, as well as the players that operate within it. Google gave the industry more than two years’ warning of its plans to end third-party cookie support on Chrome in 2022. Since then, a number of companies and industry organizations have rolled up their sleeves and started planning for what has long been an inevitability. Those that leaned into the conversation, digesting Google’s position and anticipating how the cookieless future would look, weren’t surprised when Google clarified in March 2021 that it isn’t planning to build or use alternate identifiers within its ecosystem. The simple fact is that burying your head in the sand or digging your heels in as it relates to changes of this magnitude isn’t an option. Industry consternation, and even legal pushbacks, might delay implementation of certain policy shifts, but that’s all they will do — delay the inevitable. The writing is on the wall: Greater privacy controls are coming to the digital landscape, and the companies that succeed in the future will be the ones that embrace — and even help to accelerate — this transition. If the panic that followed Google’s cookieless announcement taught us anything, it should have been this: The digital marketing ecosystem can’t allow itself to become overly reliant on any single technology or provider. The future belongs to those that put interoperability at the heart of their approach. Moving forward from the cookie, there are a few truths we must recognize. One is that there’s no single universal identifier that’s going to step forward to fill the entirety of the void left by third-party cookies. A number of companies are moving forward with plans for their own universal identifiers, and taken together, these identifiers will help to illuminate user identity on a portion of the open web (i.e., non-Google properties). They will be an important part of the ecosystem but by no means a silver bullet to comprehensive cross-channel, personalized advertising. Another massive component of the post-cookie landscape will be behavioral cohorts, embodied most prominently in Google’s Federated Learning of Cohorts (FLoC) construct. Through FLoC, Google will be creating targetable groups of anonymous users who navigate the internet in similar ways. The good news is that, through FLoC, nearly all of Chrome’s users will become addressable in a fully private manner, whereas only a portion of them were addressable via cookies. As such, marketers and their partners will need to build solutions that accommodate FLoC and other cohort-driven approaches. But at the same time, they also need to look beyond what Google’s putting into the marketplace in order to continue effective cross-channel marketing and personalization across the broader landscape. Ultimately, companies that can bring their own ground truth of consumer understanding to the table — and then extend their insights through the most important identifiers and behavioral cohort solutions — will prove the most adaptable to future marketplace shifts. The time of putting all your digital eggs into one ecosystem basket are long gone. The next 12 months are going to be transformative in our industry. In 24 months, we’ll all be a lot wiser. We will have taken universal IDs and behavioral cohorts for a few laps around the track, and we’ll have a much stronger sense of the role that they can and will play in furthering our consumer connections and understanding. Likewise, the innovators of our industry will have gotten to work on rewriting the internet economy around the new privacy-first reality, and we’ll all be reaping the benefits of their novel ideas and solutions. Along the way, of course, we will see a lot of companies pivoting. This might be a period of rapid transformation, but there’s no reason to believe a period of stagnation awaits us on the other side. The future, as always, belongs to the nimble — the ones that anticipate and adapt while others resist. Now is the time to be fearless in building the future of our industry in a way that is sustainable for companies and consumers alike. Tom Craig is CTO at Resonate."
https://venturebeat.com/2021/04/03/these-are-the-ai-risks-we-should-be-focusing-on/,These are the AI risks we should be focusing on,"Since the dawn of the computer age, humans have viewed the approach of artificial intelligence (AI) with some degree of apprehension. Popular AI depictions often involve killer robots or all-knowing, all-seeing systems bent on destroying the human race. These sentiments have similarly pervaded the news media, which tends to greet breakthroughs in AI with more alarm or hype than measured analysis. In reality, the true concern should be whether these overly-dramatized, dystopian visions pull our attention away from the more nuanced — yet equally dangerous — risks posed by the misuse of AI applications that are already available or being developed today. AI permeates our everyday lives, influencing which media we consume, what we buy, where and how we work, and more. AI technologies are sure to continue disrupting our world, from automating routine office tasks to solving urgent challenges like climate change and hunger. But as incidents such as wrongful arrests in the U.S. and the mass surveillance of China’s Uighur population demonstrate, we are also already seeing some negative impacts stemming from AI. Focused on pushing the boundaries of what’s possible, companies, governments, AI practitioners, and data scientists sometimes fail to see how their breakthroughs could cause social problems until it’s too late. Therefore, the time to be more intentional about how we use and develop AI is now. We need to integrate ethical and social impact considerations into the development process from the beginning, rather than grappling with these concerns after the fact. And most importantly, we need to recognize that even seemingly-benign algorithms and models can be used in negative ways. We’re a long way from Terminator-like AI threats — and that day may never come — but there is work happening today that merits equally serious consideration. Deepfakes are realistic-appearing artificial images, audio, and videos, typically created using machine learning methods. The technology to produce such “synthetic” media is advancing at breakneck speed, with sophisticated tools now freely and readily accessible, even to non-experts. Malicious actors already deploy such content to ruin reputations and commit fraud-based crimes, and it’s not difficult to imagine other injurious use cases. Deepfakes create a twofold danger: that the fake content will fool viewers into believing fabricated statements or events are real, and that their rising prevalence will undermine the public’s confidence in trusted sources of information. And while detection tools exist today, deepfake creators have shown they can learn from these defenses and quickly adapt. There are no easy solutions in this high-stakes game of cat and mouse. Even unsophisticated fake content can cause substantial damage, given the psychological power of confirmation bias and social media’s ability to rapidly disseminate fraudulent information. Deepfakes are just one example of AI technology that can have subtly insidious impacts on society. They showcase how important it is to think through potential consequences and harm-mitigation strategies from the outset of AI development. Large language models are another example of AI technology developed with non-negative intentions that still merits careful consideration from a social impact perspective. These models learn to write humanlike text using deep learning techniques that are trained by patterns in datasets, often scraped from the internet. Leading AI research company OpenAI’s latest model, GPT-3, boasts 175 billion parameters — 10 times greater than the previous iteration. This massive knowledge base allows GPT-3 to generate almost any text with minimal human input, including short stories, email replies, and technical documents. In fact, the statistical and probabilistic techniques that power these models improve so quickly that many of its use cases remain unknown. For example, initial users only inadvertently discovered that the model could also write code. However, the potential downsides are readily apparent. Like its predecessors, GPT-3 can produce sexist, racist, and discriminatory text because it learns from the internet content it was trained on. Furthermore, in a world where trolls already impact public opinion, large language models like GPT-3 could plague online conversations with divisive rhetoric and misinformation. Aware of the potential for misuse, OpenAI restricted access to GPT-3, first to select researchers and later as an exclusive license to Microsoft. But the genie is out of the bottle: Google unveiled a trillion-parameter model earlier this year, and OpenAI concedes that open source projects are on track to recreate GPT-3 soon. It appears our window to collectively address concerns around the design and use of this technology is quickly closing. AI may never reach the nightmare sci-fi scenarios of Skynet or the Terminator, but that doesn’t mean we can shy away from facing the real social risks today’s AI poses. By working with stakeholder groups, researchers and industry leaders can establish procedures for identifying and mitigating potential risks without overly hampering innovation. After all, AI itself is neither inherently good nor bad. There are many real potential benefits that it can unlock for society — we just need to be thoughtful and responsible in how we develop and deploy it. For example, we should strive for greater diversity within the data science and AI professions, including taking steps to consult with domain experts from relevant fields like social science and economics when developing certain technologies. The potential risks of AI extend beyond the purely technical; so too must the efforts to mitigate those risks. We must also collaborate to establish norms and shared practices around AI like GPT-3 and deepfake models, such as standardized impact assessments or external review periods. The industry can likewise ramp up efforts around countermeasures, such as the detection tools developed through Facebook’s Deepfake Detection Challenge or Microsoft’s Video Authenticator. Finally, it will be necessary to continually engage the general public through educational campaigns around AI so that people are aware of and can identify its misuses more easily. If as many people knew about GPT-3’s capabilities as know about The Terminator, we’d be better equipped to combat disinformation or other malicious use cases. We have the opportunity now to set incentives, rules, and limits on who has access to these technologies, their development, and in which settings and circumstances they are deployed. We must use this power wisely — before it slips out of our hands. Peter Wang is CEO and Co-founder of data science platform Anaconda. He’s also the creator of the PyData community and conferences and a member of the board at the Center for Human Technology."
https://venturebeat.com/2021/04/02/ai-weekly-heres-how-enterprises-say-theyre-deploying-ai-responsibly/,AI Weekly: Here’s how enterprises say they’re deploying AI responsibly,
https://venturebeat.com/2021/04/02/why-ai-cant-solve-unknown-problems/,Why AI can’t solve unknown problems,"When will we have artificial general intelligence, the kind of AI that can mimic the human mind in all aspect? Experts are divided on the topic, and answers range anywhere between a few decades and never. But what everyone agrees on is that current AI systems are a far shot from human intelligence. Humans can explore the world, discover unsolved problems, and think about their solutions. Meanwhile, the AI toolbox continues to grow with algorithms that can perform specific tasks but can’t generalize their capabilities beyond their narrow domains. We have programs that can beat world champions at StarCraft but can’t play a slightly different game at amateur level. We have artificial neural networks that can find signs of breast cancer in mammograms but can’t tell the difference between a cat and a dog. And we have complex language models that can spin thousands of seemingly coherent articles per hour but start to break when you ask them simple logical questions about the world. In short, each of our AI techniques manages to replicate some aspects of what we know about human intelligence. But putting it all together and filling the gaps remains a major challenge. In his book Algorithms Are Not Enough, data scientist Herbert Roitblat provides an in-depth review of different branches of AI and describes why each of them falls short of the dream of creating general intelligence. The common shortcoming across all AI algorithms is the need for predefined representations, Roitblat asserts. Once we discover a problem and can represent it in a computable way, we can create AI algorithms that can solve it, often more efficiently than ourselves. It is, however, the undiscovered and unrepresentable problems that continue to elude us. Throughout the history of artificial intelligence, scientists have regularly invented new ways to leverage advances in computers to solve problems in ingenious ways. The earlier decades of AI focused on symbolic systems. This branch of AI assumes human thinking is based on the manipulation of symbols, and any system that can compute symbols is intelligent. Symbolic AI requires human developers to meticulously specify the rules, facts, and structures that define the behavior of a computer program. Symbolic systems can perform remarkable feats, such as memorizing information, computing complex mathematical formulas at ultra-fast speeds, and emulating expert decision-making. Popular programming languages and most applications we use every day have their roots in the work that has been done on symbolic AI. But symbolic AI can only solve problems for which we can provide well-formed, step-by-step solutions. The problem is that most tasks humans and animals perform can’t be represented in clear-cut rules. “The intellectual tasks, such as chess playing, chemical structure analysis, and calculus are relatively easy to perform with a computer. Much harder are the kinds of activities that even a one-year-old human or a rat could do,” Roitblat writes in Algorithms Are Not Enough. This is called Moravec’s paradox, named after the scientist Hans Moravec, who stated that, in contrast to humans, computers can perform high-level reasoning tasks with very little effort but struggle at simple skills that humans and animals acquire naturally. “Human brains have evolved mechanisms over millions of years that let us perform basic sensorimotor functions. We catch balls, we recognize faces, we judge distance, all seemingly without effort,” Roitblat writes. “On the other hand, intellectual activities are a very recent development. We can perform these tasks with much effort and often a lot of training, but we should be suspicious if we think that these capacities are what makes intelligence, rather than that intelligence makes those capacities possible.” So, despite its remarkable reasoning capabilities, symbolic AI is strictly tied to representations provided by humans. Machine learning provides a different approach to AI. Instead of writing explicit rules, engineers “train” machine learning models through examples. “[Machine learning] systems could not only do what they had been specifically programmed to do but they could extend their capabilities to previously unseen events, at least those within a certain range,” Roitblat writes in Algorithms Are Not Enough. The most popular form of machine learning is supervised learning, in which a model is trained on a set of input data (e.g., humidity and temperature) and expected outcomes (e.g., probability of rain). The machine learning model uses this information to tune a set of parameters that map the inputs to outputs. When presented with previously unseen input, a well-trained machine learning model can predict the outcome with remarkable accuracy. There’s no need for explicit if-then rules. But supervised machine learning still builds on representations provided by human intelligence, albeit one that is more loose than symbolic AI. Here’s how Roitblat describes supervised learning: “[M]achine learning involves a representation of the problem it is set to solve as three sets of numbers. One set of numbers represents the inputs that the system receives, one set of numbers represents the outputs that the system produces, and the third set of numbers represents the machine learning model.” Therefore, while supervised machine learning is not tightly bound to rules like symbolic AI, it still requires strict representations created by human intelligence. Human operators must define a specific problem, curate a training dataset, and label the outcomes before they can create a machine learning model. Only when the problem has been strictly represented in its own way can the model start tuning its parameters. “The representation is chosen by the designer of the system,” Roitblat writes. “In many ways, the representation is the most crucial part of designing a machine learning system.” One branch of machine learning that has risen in popularity in the past decade is deep learning, which is often compared to the human brain. At the heart of deep learning is the deep neural network, which stacks layers upon layers of simple computational units to create machine learning models that can perform very complicated tasks such as classifying images or transcribing audio. But again, deep learning is largely dependent on architecture and representation. Most deep learning models needs labeled data, and there is no universal neural network architecture that can solve every possible problem. A machine learning engineer must first define the problem they want to solve, curate a large training dataset, and then figure out the deep learning architecture that can solve that problem. During training, the deep learning model will tune millions of parameters to map inputs to outputs. But it still needs machine learning engineers to decide the number and type of layers, learning rate, optimization function, loss function, and other unlearnable aspects of the neural network. “Like much of machine intelligence, the real genius [of deep learning] comes from how the system is designed, not from any autonomous intelligence of its own. Clever representations, including clever architecture, make clever machine intelligence,” Roitblat writes. “Deep learning networks are often described as learning their own representations, but this is incorrect. The structure of the network determines what representations it can derive from its inputs. How it represents inputs and how it represents the problem-solving process are just as determined for a deep learning network as for any other machine learning system.” Other branches of machine learning follow the same rule. Unsupervised learning, for example, does not require labeled examples. But it still requires a well-defined goal such as anomaly detection in cybersecurity, customer segmentation in marketing, dimensionality reduction, or embedding representations. Reinforcement learning, another popular branch of machine learning, is very similar to some aspects of human and animal intelligence. The AI agent doesn’t rely on labeled examples for training. Instead, it is given an environment (e.g., a chess or go board) and a set of actions it can perform (e.g., move pieces, place stones). At each step, the agent performs an action and receives feedback from its environment in the form of rewards and penalties. Through trial and error, the reinforcement learning agent finds sequences of actions that yield more rewards. Computer scientist Richard Sutton describes reinforcement learning as “the first computational theory of intelligence.” In recent years, it has become very popular for solving complicated problems such as mastering computer and board games and developing versatile robotic arms and hands. But reinforcement learning environments are typically very complex, and the number of possible actions an agent can perform is very large. Therefore, reinforcement learning agents need a lot of help from human intelligence to design the right rewards, simplify the problem, and choose the right architecture. For instance, OpenAI Five, the reinforcement learning system that mastered the online video game Dota 2, relied on its designers simplifying the rules of the game, such as reducing the number of playable characters. “It is impossible to check, in anything but trivial systems, all possible combinations of all possible actions that can lead to reward,” Roitblat writes. “As with other machine learning situations, heuristics are needed to simplify the problem into something more tractable, even if it cannot be guaranteed to produce the best possible answer.” Here’s how Roitblat summarizes the shortcomings of current AI systems in Algorithms Are Not Enough: “Current approaches to artificial intelligence work because their designers have figured out how to structure and simplify problems so that existing computers and processes can address them. To have a truly general intelligence, computers will need the capability to define and structure their own problems.” “Every classifier (in fact every machine learning system) can be described in terms of a representation, a method for measuring its success, and a method of updating,” Roitblat told TechTalks over email. “Learning is finding a path (a sequence of updates) through a space of parameter values. At this point, though, we don’t have any method for generating those representations, goals, and optimizations.” There are various efforts to address the challenges of current AI systems. One popular idea is to continue to scale deep learning. The general reasoning is that bigger neural networks will eventually crack the code of general intelligence. After all, the human brain has more than 100 trillion synapses. The biggest neural network to date, developed by AI researchers at Google, has one trillion parameters. And the evidence shows that adding more layers and parameters to neural networks yields incremental improvements, especially in language models such as GPT-3. But big neural networks do not address the fundamental problems of general intelligence. “These language models are significant achievements, but they are not general intelligence,” Roitblat says. “Essentially, they model the sequence of words in a language. They are plagiarists with a layer of abstraction. Give it a prompt and it will create a text that has the statistical properties of the pages it has read, but no relation to anything other than the language. It solves a specific problem, like all current artificial intelligence applications. It is just what it is advertised to be — a language model. That’s not nothing, but it is not general intelligence.” Other directions of research try to add structural improvements to current AI structures. For instance, hybrid artificial intelligence brings symbolic AI and neural networks together to combine the reasoning power of the former and the pattern recognition capabilities of the latter. There are already several implementations of hybrid AI, also referred to as “neuro-symbolic systems,” that show hybrid systems require less training data and are more stable at reasoning tasks than pure neural network approaches. System 2 deep learning, another direction of research proposed by deep learning pioneer Yoshua Bengio, tries to take neural networks beyond statistical learning. System 2 deep learning aims to enable neural networks to learn “high-level representations” without the need for explicit embedding of symbolic intelligence. Another research effort is self-supervised learning, proposed by Yann LeCun, another deep learning pioneer and the inventor of convolutional neural networks. Self-supervised learning aims to learn tasks without the need for labeled data and by exploring the world like a child would do. “I think that all of these make for more powerful problem solvers (for path problems), but none of them addresses the question of how these solutions are structured or generated,” Roitblat says. “They all still involve navigating within a pre-structured space. None of them addresses the question of where this space comes from. I think that these are really important ideas, just that they don’t address the specific needs of moving from narrow to general intelligence.” In Algorithms Are Not Enough, Roitblat provides ideas on what to look for to advance AI systems that can actively seek and solve problems that they have not been designed for. We still have a lot to learn from ourselves and how we apply our intelligence in the world. “Intelligent people can recognize the existence of a problem, define its nature, and represent it,” Roitblat writes. “They can recognize where knowledge is lacking and work to obtain that knowledge. Although intelligent people benefit from structured instructions, they are also capable of seeking out their own sources of information.” But observing intelligent behavior is easier than creating it, and, as Roitblat told me in our correspondence, “Humans do not always solve their problems in the way that they say/think that they do.” As we continue to explore artificial and human intelligence, we will continue to move toward AGI one step at a time. “Artificial intelligence is a work in progress. Some tasks have advanced further than others. Some have a way to go. The flaws of artificial intelligence tend to be the flaws of its creator rather than inherent properties of computational decision making. I would expect them to improve over time,” Roitblat said. Ben Dickson is a software engineer and the founder of TechTalks. He writes about technology, business, and politics. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/04/02/how-ai-powered-bi-tools-will-redefine-enterprise-decision-making/,How AI-powered BI tools will redefine enterprise decision-making,"Value-creation in business intelligence (BI) has followed a consistent pattern over the last few decades. The ability to democratize and expand the addressable user base of solutions has corresponded to large value increases. Enterprise BI arguably started with highly technical solutions like SAS in the mid-’70s, accessible only to a small fraction of highly specialized employees. The BI world began to open up in the ’90s with the advent of solutions like SAP Business Objects, which created an abstraction layer on top of query language to allow a broader swath of employees to run business intelligence. BI 3.0 came in the last decade, as solutions like Alteryx have provided WYSIWYG interfaces that further expanded both the sophistication and accessibility of BI. But in many cases, BI still involves analysts writing SQL queries to analyze large data sets so that they can provide intelligence for non-technical executives. While this paradigm for analysis continues to increase, I believe that a new BI paradigm will emerge and grow in importance over the next few years — one in which AI surfaces relevant questions and insights, and even proposes solutions. This fourth wave of BI will leverage powerful AI advancements to further democratize analytics so that any line of business specialist can supervise more insightful and prescriptive recommendations than ever before. In this fourth wave, the traditional order of BI will be inverted. The traditional method of BI generally begins with a technical analyst investigating a specific question. For example, an electronics retailer may wonder if a higher diversity of refrigerator models in specific geographies will likely increase sales. The analyst blends relevant data sources (perhaps an inventory management system and a billing system) and investigates whether there is a correlation. Once the analyst has completed the work, they present a conclusion about past behavior. They then create a visualization for business decision makers in a system like a Tableau or Looker, which can be revisited as the data changes. This investigation method works quite well, assuming the analyst asks the right questions, the number of variables is relatively well-understood and finite, and the future continues to look somewhat similar to the past. However, this paradigm presents several potential challenges in the future as companies continue to accumulate new types of data, business models and distribution channels evolve, and real-time consumer and competitive adjustments cause constant disruptions. Specifically: AI-enabled platforms that will define the fourth wave of BI start by crunching and blending massive amounts of data to find and surface patterns and relevant statistical insights. A data analyst applies judgment to these myriad insights to decide which patterns are truly meaningful or actionable for the business. After digging into areas of interest, the platform suggests potential actions based on correlations that have been seen over a more extended period — again validated by human judgment. The time is ripe for this methodology to proliferate — AI advancements are coming online in conjunction with the growth of cloud-native vendors like Snowflake. Simultaneously, businesses are increasingly feeling the strain that business complexity and data proliferation are putting on their traditional BI processes. The data analytics space has spawned some incredible companies capable of tackling this challenge. In the last six months, Snowflake vaulted into the top 10 cloud businesses with a valuation above $70 billion, and Databricks raised $1 billion at a $28 billion valuation. Both of these companies (along with similar offerings from AWS and Google Cloud) are vital enablers for modern data analytics, providing data warehouses where teams can leverage flexible, cloud-based storage and compute for analytics. Industry verticals such as ecommerce and retail that are under the most strain from the three challenges outlined above are starting to see industry-specific platforms emerge to deliver BI 4.0 capabilities — platforms like Tradeswell, Hypersonix, and Soundcommerce. In the energy and materials sector, platforms like Validere and Verusen are helping to address these challenges by using AI to boost margins of operators. In addition, broad technology platforms like Outlier, Unsupervised, and Sisu have demonstrated the power to pull exponentially more patterns from a dataset than a human analyst could. These are examples of intuitive BI platforms that are easing the strains, old and new, that data analysts face. And we can expect to see more of them emerging over the next couple of years. Steve Sloane is a Partner at Menlo Ventures."
https://venturebeat.com/2021/04/02/what-is-a-decentralized-database/,What is a decentralized database?,"A decentralized database splits the workload up among multiple machines and uses sophisticated algorithms to balance the incoming and outgoing requests for the best response time. This type of database is useful for those times when there is more data that needs to be stored in the database than can physically saved on one physical machine. The bits — like log files, data collected by tracking click-throughs in the application, and the data generated by internet of things devices — pile up and need to be stored somewhere. They are also frequently referred to as distributed databases. There are several good reasons for splitting up a database: One approach to simplify the architecture is to split the dataset into smaller parts and assign the parts to certain machines. One computer might handle all people whose last name begins with A through F, another G through M, etc. This splitting, often called “sharding,” can inspire strategies that range from simple to complex. The greatest challenge with splitting up the database is ensuring that the information remains consistent. For example, in the case of a hypothetical airline booking system, if one machine responds to a database query that an airplane seat has been sold, then another machine shouldn’t respond to a query by saying that the seat is open and available. Some distributed databases enforce the rules on consistency carefully so that all queries receive the same answer, regardless of which node in the cluster responded to the query. Other distributed databases relax the consistency requirement in favor of “eventual consistency.” With eventual consistency, the machines can be out-of-sync with each other and return different answers, so long as the machines eventually catch up to each other and return the same results. In some narrow cases, one machine may not hear about the new version of the data stored on another machine for some time. Machines in the same datacenter tend to reach consistency faster than those separated by longer distances or slower networks. Database developers must choose between fast responses and consistent answers. Tight synchronization between the distributed versions will increase the amount of computation and slow the responses, but the answers will be more accurate. Allowing data to be out of sync will speed up performance, but at the expense of accuracy. Choosing whether to prioritize speed or accuracy is a business decision that can be an art. Banks, for instance, know their customers want correct accounting more than split-second responses. Social media companies, however, may choose speed because most posts are rarely edited and small differences in propagation aren’t essential. The major database companies offer elaborate options for distributing data storage. Some support large machines with multiple processors, multiple disks, and large blocks of RAM. The machine is technically one computer, but the individual processors coordinate their responses in similar ways as if the processors were separated by continents. Many organizations run their Oracle and SAP deployments on Amazon Web Services in order to take advantage of the computing power. AWS’ u-24tb1.metal, for instance, may look like one machine on the invoice, but it has 448 processors inside, along with 24 terabytes of RAM. It is optimized for very large databases like SAP’s HANA, which stores the bulk of the information in RAM for fast response. All of the major databases have options for replicating the database to create distributed versions that are split between more distinct machines. Oracle’s database, for instance, has long supported a wide range of replication strategies across collections of machines that can even include non-Oracle databases. Lately, Oracle has been marketing a version with the name “autonomous” to signify that it’s able to scale and replicate itself automatically in response to loads. MariaDB, a fork of MySQL, also supports a variety of replication strategies that allow the data from one primary node to pass copies of all transactions to replicas that are commonly set up to be read-only. That is, the replica can answer queries for information, but it doesn’t store new data. In a recent presentation, Max Mether, one of the cofounders of MariaDB, says his company is working hard at adding autonomous abilities to its database. “The server should know how to tune itself better than you,” he explained. “That doesn’t mean you shouldn’t have the option to tune the server, but for many of these variables, it’s really hard as a user to figure out how to tune them optimally. Ideally you should just let the server choose, based on the current workload, what makes sense.” The rise of cloud services hides some of the complexity of distributing the databases, at least for configuring the server and arranging for the connection. DigitalOcean, for instance, offers managed versions of MySQL, PostgreSQL, and Redis. Clusters can be created with a certain size with a single control panel to offer storage and failover. Some providers have added the ability to spread out clusters in different datacenters around the world. Amazon’s RDS, for instance, can configure clusters that span multiple areas called “availability zones.” Online file storage is also starting to offer much of the same replication. While the services that offer to store blocks of data in buckets don’t provide the indexing or complex searching of databases, they do offer replication as part of the deal. Some approaches work to merge more complex calculations with distributed data sets. Tools like Hadoop and Spark, for instance, are just two of the popular open source constellations of tools that match distributed computation with distributed data. There are a number of companies that specialize in supporting versions that are installed in house or in cloud configurations. Databricks’ Delta Lake, for instance, is one product that supports complex data mining operations on distributed data. Groups that value privacy are also exploring complicated distributed operations like the Interplanetary File System, a project designed to spread web data out among multiple locations for speed and redundancy. Not all work requires the complexity of coordinating multiple machines. Some projects may be labeled “big data” by project managers who feel aspirational, even though the volume and computational load is easily handled by a single machine. If a fast response time is not essential and if the size is not too large and won’t grow in an unpredictable way, a simpler database with regular backups may be sufficient. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/04/02/nanoms-nanotech-makes-more-efficient-batteries-that-last-at-least-9-times-longer/,Nanom’s nanotech makes more efficient batteries that last at least 9 times longer,"Nanom is unveiling a patented technology that uses nanoparticles to make batteries that are far more efficient than the ubiquitous lithium-ion version used today. If it works, the technology could revolutionize batteries used in everything from electric cars to laptops and smartphones. Nanom claims its batteries last 9 times longer than the nickel-iron batteries Thomas Edison invented and more than than 9 times longer than lithium-ion batteries. They also weigh 5 times less and improve energy density, recharge rates, and battery disposal. Nanom isn’t disclosing the direct comparison to lithium-ion batteries yet. But it is saying that an electric car with Nanom tech could drive across the United States on a single battery charge. That’s pretty mind-boggling. And the company has raised $3 million from investors in Silicon Valley and the Nordics, thanks to backers like Village Global, whose network includes Bill Gates, Jeff Bezos, and Reid Hoffman. A big advantage of the nanotechnology Nanom uses is that it can be applied to existing lithium-ion batteries, creating more surface area to generate energy with the battery cells. Nanom can make the tech in its labs and easily add production to existing manufacturing plants. “The problem has always been that nanotechnology has been a little bit stuck in the lab,” CEO Armann Kojic said in an interview with VentureBeat. “It’s like a concept that has been great for making a million-dollar battery, but not one that actually goes in your car. And what’s unique about our approach is the fact that our core method of creating nanoparticles is a mass production method. This is very much about bringing it out of the lab and into a product.” The Edison nickel-iron battery is more than 120 years old. Using iron and nickel particles and carbon fiber, Nanom makes the battery usable again. The company says its materials are 80% cheaper than those in a regular battery — and more disposable. The nickel-iron battery is used in large-scale energy storage devices. “The Edison battery is kind of a showcase for us … to show what could be done to take a green battery and counter the weaknesses,” Kojic said. Nanom is talking to a wide range of transportation, stationary storage, and battery companies about immediately adopting its nanoparticles into their own proprietary designs. “We all know that this battery problem exists,” Kojic said. “And there’s a lot of factors that need to come together, like the environmental factor, the cost factor, and the effectiveness of the battery.” Nanom’s nanoparticle technology enables any battery formulation to be fundamentally altered for the better by dramatically increasing every aspect of its performance. It does so by immersing the battery materials in nanoparticles that increase the surface area that generates energy on a microscopic level. These particles are many orders of magnitude more effective in increasing energy surface area than the current solutions, which makes them ideal for this generation of battery energy storage. And Nanom immediately enhances the energy density of any battery, Kojic said. “It’s all about the ultra-high surface area, which affects the charging rates and the capacity,” Kojic said. “If I take a business card, I have the surface area as the front and the back of the business card. But if I cut it into a billion pieces, it becomes several football fields of a surface area, I’m using the sides and everything in between. And we can use all of that unused area for storing energy.” Nanom has the ability to convert existing battery materials into nanoparticle size, enabling the same benefits for any battery process without needing to retool — the nanoparticles are simply mixed into the slurry that is a standard part of all battery manufacturing lines. Nanom has achieved massive scale in its manufacturing process and can already satisfy the requirements of key battery markets. In that sense, Kojic said the invention is industrially scalable. The critical barrier to the utilization of the full potential of nanotechnology has always been the inability to economically manufacture nanoparticles in sufficient quantities. Nanom’s method allows for mass production of nanoparticles produced through friction by transverse introduction of feedstock material into alternating supersonic flow regimes in various controlled atmospheres. Controlling flow speed, atmosphere, repetitions, and after treatment allows the company to uniquely shape the particle’s size and surface areas, resulting in a bespoke nanostructure with unique properties. Nanom said it can convert micron-sized particles to nano-sized particles in high volume. This provides unique nanostructures that enable longer-lasting batteries without changing anything in the batteries’ manufacturing flow. By replacing key materials with nano-enhanced materials, Nanom says it gives customers a competitive advantage. On top of the other benefits, the company said it can build the batteries into the materials used to make products, like car bodies, airplane seats, boat hulls, and more. A structural battery is one that can be used in the material of a product by embedding the nanoparticles into a structure. In a car, it’s a fast-charging battery that is safe to use, as it doesn’t explode on impact. It also allows engineers to decentralize a power source. Even a wall in your house can become a giant, safe battery that takes you off the grid. Nanom enables any structure or surface to become a battery storage device. For example, the company has already created a pilot project in which an electric boat was constructed such that the actual hull of the boat became the battery (15 meters of that type of hull has the energy storage capacity of five Tesla vehicles). Any material or structure Nanom turns into a battery is completely solid, uses no harmful chemicals, is green for the planet, and does not explode upon impact (as lithium-ion batteries have a distressing tendency to do). And Nanom doesn’t have to convince customers to go with a different material. “You can take existing technologies and get more out of them so that we don’t deplete the environment of raw materials,” he said. “We can create these high surface area nanoparticles that are super relevant to the battery industry. You can make batteries with a lower cost per kilowatt-hour than you can do otherwise.” Kojic was born in Iceland and moved to the San Francisco Bay Area to join the StartX mentor labs incubator at Stanford University. He fell in love with the region and met his wife in the Bay Area. As he built the company, he also had an easier time finding investors in the region than he would have back home. Nanom has applied for patents on the tech. The company has 17 people working in Palo Alto and Reykjavik, Iceland and has raised $3 million to date. The employees have worked at companies including Decode, Maersk, and Total S.A. Backers include Village Global, Iceland Venture Studio, Perkins Coie, and the European Union’s Green Deal project. The EU Green Deal is a very ambitious action plan with the target of making Europe carbon-neutral by 2050. The Nanom Greenvolt project was selected as an EU Green Deal technology, which came with Green Deal funding. Kojic said the company will disclose more in a few months when it announces its partners. The name “Nanom” combines the words “nano” and “om,” and it reflects the company’s vision of using the power of nano, or matter, to impact the universe, or om, the entirety of the universe, truth, and knowledge."
https://venturebeat.com/2021/04/02/a-developers-inside-look-at-where-slack-started-and-where-its-headed/,A developer’s inside look at where Slack started — and where it’s headed,"Presented by OutSystems In 2020, Slack nearly doubled its paying customer base over the previous year, thanks to the pandemic, and was recently acquired by Salesforce for over $27 billion. But according to Justin Hardin, senior software engineer at Slack, the product originally started as a gaming platform that failed to take off. “They unfortunately ran out of money and had to lay people off, so they pivoted by asking, ‘Which piece of our product works?’ And that was the chat aspect,” Hardin says on the latest episode of Decoded, OutSystems’ podcast for the next generation of developers. But the app’s friendly human tone was inspired by its gaming roots. “They kept the writer on who was creating the dialogues for the games, and instead had her do the dialogues for the product,” he says. “That’s how you have this enterprise chat platform with help messages and onboarding that’s in a more conversational tone, which helped define the product experience.” Listen to the conversation with Justin Hardin right here. Since Slack’s early pivot, Hardin says that Slack has been very intentional in the way it has expanded internationally. In addition to creating server caches around the world to reduce latency, Slack localizes completely when entering new markets. The product team, marketing team, enterprise team, app store team, and others all localize the entire Slack experience to ensure it’s relevant to every user, no matter where they are. “We localize not just the language itself, but also the imagery and graphics to make sure it’s contextual to the countries we’re in,” he says. “We make sure the blog isn’t only localized, but empower marketers from each country to create their own articles targeted at their specific market.” Hardin believes this reflects the fact that technology is becoming less centered around Silicon Valley. “Slack is a very American-based company, but that’s not how the world is going to see tech moving on in the future,” he explains. “Tech should be a global entity, not just a Silicon Valley thing. In a post-pandemic world, the whole notion of Silicon Valley should exist on the internet. It shouldn’t be a specific place.” Thanks to its commitment to creating an intuitive UI and human experience, Slack originally found success among developer teams at companies. This led to adoption at businesses ranging from small startups to Amazon, and then spread through other functions like marketing, sales, and finance. To ensure Slack is relevant and useful to all types of users, Hardin says their team focuses on providing a consistent experience above all else. “How can we make our front end through the product and the marketing site consistent? How can we have a design system? How can we utilize components?” he says. “That’s where I see Slack as being kind of an innovator in this space: how can you bring consistency? Some of it is through getting everyone on the same page, but other times it’s through how you create toolings that allow developers to not think about this.” With more people than ever using Slack as they work from home, Hardin says this consistency, along with rigorous testing, is what has helped the company scale to meet growing demand. The result is a product that people love so much and has been so successful that it’s news when the product has an issue. “When things don’t go well, we’re trending on Twitter.” Check out this week’s Decoded podcast to learn much more about how Justin Hardin started his career in software development, his work at Slack, and his work as the co-founder of Climatebase.org, a platform for climate action, education, and impact. Listen now, and subscribe to future episodes today. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/04/02/american-construction-source-acquires-foley-lumber-milaca-building-center/,American Construction Source Acquires Foley Lumber & Milaca Building Center," The Ninth Transaction by ACS Expands LBM Distribution Capacity in Minnesota  SPRINGFIELD, Mo.–(BUSINESS WIRE)–April 2, 2021– American Construction Source (“ACS”), a leading national building materials distribution platform for custom home builders and repair and remodel contractors, backed by Angeles Equity Partners, LLC (“Angeles”) and Clearlake Capital Group, L.P. (together with its affiliates “Clearlake”), today announced the acquisition of Foley Lumber (“Foley”) and Milaca Building Center (“Milaca”). Foley and Milaca will operate under their existing local brand names already established in Minnesota. The transaction marks the ninth acquisition by ACS under sponsorship from Clearlake and Angeles. “The ACS team welcomes Foley and Milaca to our national platform of service-oriented lumber and building materials locations,” said James Drexinger, CEO of ACS. “The Kotsmith family has built a great business over generations, and we respect the customer relationships they have fostered for the last 90 years.” “We are confident ACS, Angeles, and Clearlake are the best partners to help accelerate growth by delivering more value-added products and an expanded portfolio of services in North Central Minnesota and the greater Minneapolis St. Paul metro area,” said Milaca and Foley owners Chris Kotsmith and Randy Kotsmith. “ACS’s technology investments and national scale will enable Foley and Milaca to better serve our customers for generations to come.” Simpson Thacher & Bartlett LLP provided legal counsel to ACS. CliftonLarsonAllen Wealth Advisors, LLC served as financial advisors to the sellers. The financial terms of the transaction were not disclosed. About Foley Lumber and Milaca Building Center Serving customers since 1932, Foley Lumber and Milaca Building Center are widely known for quality products, including common and not so common lumber and building materials. Our sales, support, and yard staff include knowledgeable, professional individuals with many years of hands-on experience. The most important aspect of our business is servicing our builder and contractor customers. About American Construction Source American Construction Source is an LBM distributor with 70+ locations in 9 states serving the needs of custom home builders, repair & remodel contractors, and DIY consumers. ACS provides lumber and building materials businesses the resources, leverage, and focus to make their ideas happen. Recognizing the value and heritage of deep, local customer relationships as a strong foundation for growth, ACS’s best practices are designed to leverage shared strengths, drive operational excellence, and motivate performance to create a leading building products distributor with a national footprint and the industry’s best customer experience. ACS is backed by Angeles Equity Partners and Clearlake Capital Group. Learn more online at www.acs-lbm.com. About Angeles Equity Partners, LLC Angeles Equity Partners, LLC is a private equity firm that invests in companies across a wide range of sectors and specifically targets businesses which it believes can directly benefit from the firm’s strategic, operational, and M&A capabilities. The Angeles skill set drives the firm’s investment philosophy and, in its view, can help businesses reach their full potential. Learn more online at www.angelesequity.com. About Clearlake Founded in 2006, Clearlake Capital Group, L.P. is an investment firm operating integrated businesses across private equity, credit and other related strategies. With a sector-focused approach, the firm seeks to partner with experienced management teams by providing patient, long term capital to dynamic businesses that can benefit from Clearlake’s operational improvement approach, O.P.S.® The firm’s core target sectors are technology, industrials and consumer. Clearlake currently has approximately $35 billion of assets under management, and its senior investment principals have led or co-led over 300 investments. The firm has offices in Santa Monica and Dallas. More information is available at www.clearlake.com and on Twitter @ClearlakeCap.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210402005103/en/ Trenton Waterhouse at +1 623 523 1672 or email trent@acs-lbm.com"
https://venturebeat.com/2021/04/01/optipulse-non-coherent-laser-offers-optical-fiber-installers-a-wireless-extension-cord/,OptiPulse Non-Coherent Laser Offers Optical Fiber Installers a Wireless Extension Cord,"ALBUQUERQUE, N.M.–(BUSINESS WIRE)–April 1, 2021– OptiPulse is testing a new light source that sends invisible eye-safe infrared light wirelessly from one building or pole to another at 10Gbps. The light source is a miniature chip that costs ~$1 to produce in large volumes and has tested error-free at 25Gbps. The Light is about the same wavelength as your TV remote. It is turning on and off at 25 billion times a second to send data over a beam of invisible light with extremely high bandwidth. The company has designed the compound semiconductor chip, developed, and produced it at a commercial foundry. The chips were installed in a “binocular” type 3rd prototype that emulates a wireless infrastructure link. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210401005871/en/ OptiPulse is trying to prove that light is superior to microwaves for sending high speed wireless data. Their first product will be beta tested within 6 months. The links use invisible light about the same wavelength as your TV remote’s infrared light. The demonstrated 10Gbps upload and download systems can reduce the cost and increase the performance of those links. The 3rd prototype used less than ¼ of the energy a typical microwave link. These combined factors may ultimately lead to a significant reduction in energy use worldwide even as the bandwidth demands increase. OptiPulse’s first goal is to make infrastructure links that can be a kind of wireless extension cord for fiber optic deployments. Many times, fiber installers run into rock, streams or highway crossings that are difficult to span. Plugging the fiber into one device at the obstacle can send the same bandwidth wirelessly across the difficult area much cheaper than the alternatives. It is hoped by many current clients waiting for the links that it will reduce the cost and time for fiber installs. The links can also be used in metro areas to not disturb businesses by tearing up roads in major metro areas. The founder, John Joseph, believes the way to prevent this work from being taken over is by getting the public to fund and oversee its development. The company has started with a WeFunder crowd funding campaign where the public can invest as little as $100. https://wefunder.com/optipulse  View source version on businesswire.com: https://www.businesswire.com/news/home/20210401005871/en/ John Joseph CEO OptiPulse480-652-0717jjoseph@optipulse.com https://optipulse.com"
https://venturebeat.com/2021/04/01/global-chip-shortage-affects-more-than-cars/,Global chip shortage affects more than cars,"(Reuters) — From delayed car deliveries to a supply shortfall in home appliances to costlier smartphones, businesses and consumers across the globe are facing the brunt of an unprecedented shortage in semiconductor microchips. The shortage stems from a confluence of factors as carmakers, which shut plants during the COVID-19 pandemic last year, compete against the sprawling consumer electronics industry for chip supplies. Consumers have stocked up on laptops, gaming consoles and other electronic products during the pandemic, leading to tighter inventory. They also bought more cars than industry officials expected last spring, further straining supplies. Sanctions against Chinese tech companies have further exacerbated the crisis. Originally concentrated in the auto industry, the shortage has now spread to a range of other consumer electronics, including smartphones, refrigerators and microwaves. With every company that uses chips in production panic buying to shore up stocks, the shortage has squeezed capacity and driven up costs of even the cheapest components of nearly all microchips, increasing prices of final products. Automobiles have become increasingly dependent on chips — for everything from computer management of engines for better fuel economy to driver-assistance features such as emergency braking. The crisis has forced many to curtail the production of less profitable vehicles. General Motors Co and Ford Motor Co are among the big carmakers who said they would scale down production, joining other automakers including Volkswagen AG, Subaru Corp, Toyota Motor Corp and Nissan Motor Co. A shortage of auto semiconductor chips could impact nearly 1.3 million units of global light vehicle production in the first quarter, according to data firm IHS Markit. IHS said a fire at a Japanese chip-making factory owned by Renesas Electronics Corp, which accounts for 30% of the global market for microcontroller units used in cars, has worsened the situation. Severe winter weather in Texas has also forced Samsung Electronics Co Ltd, NXP Semiconductors and Infineon to shut down factories temporarily. Infineon and NXP are major automotive chip suppliers, and analysts expect the disruptions to add to the shortfalls in the ailing sector. At the root of the squeeze is the under-investment in 8-inch chip manufacturing plants owned mostly by Asian firms, which means they have struggled to ramp up production as demand for 5G phones and laptops picked up faster than expected. Qualcomm Inc, whose chips feature in Samsung phones, is one major chipmaker struggling to keep up with demand. Apple Inc’s major supplier Foxconn also warned of the chip shortage affecting supply chains to clients. The majority of chip production occurs in Asia currently, where major contract manufacturers such as Taiwan Semiconductor Manufacturing Co Ltd (TSMC) and Samsung handle production for hundreds of different chip companies. U.S. semiconductor companies account for 47% of global chip sales, but only 12% of global manufacturing is done in the United States. Factories that produce wafers cost tens of billions of dollars to build, and expanding their capacity can take up to a year for testing and qualifying complex tools. U.S. President Joe Biden has sought $37 billion in funding for legislation to supercharge chip manufacturing in the country. Currently, four new factories are slated in the country, two by Intel Corp and one by TSMC in Arizona, and another by Samsung in Texas. China has also offered a myriad of subsidies to the chip industry as it tries to reduce its dependence on Western technology."
https://venturebeat.com/2021/04/01/study-finds-that-even-the-best-speech-recognition-systems-exhibit-bias/,Study finds that even the best speech recognition systems exhibit bias,"Even state-of-the-art automatic speech recognition (ASR) algorithms struggle to recognize the accents of people from certain regions of the world. That’s the top-line finding of a new study published by researchers at the University of Amsterdam, the Netherlands Cancer Institute, and the Delft University of Technology, which found that an ASR system for the Dutch language recognized speakers of specific age groups, genders, and countries of origin better than others. Speech recognition has come a long way since IBM’s Shoebox machine and Worlds of Wonder’s Julie doll. But despite progress made possible by AI, voice recognition systems today are at best imperfect — and at worst discriminatory. In a study commissioned by the Washington Post, popular smart speakers made by Google and Amazon were 30% less likely to understand non-American accents than those of native-born users. More recently, the Algorithmic Justice League’s Voice Erasure project found that that speech recognition systems from Apple, Amazon, Google, IBM, and Microsoft collectively achieve word error rates of 35% for African American voices versus 19% for white voices. The coauthors of this latest research set out to investigate how well an ASR system for Dutch recognizes speech from different groups of speakers. In a series of experiments, they observed whether the ASR system could contend with diversity in speech along the dimensions of gender, age, and accent. The researchers began by having an ASR system ingest sample data from CGN, an annotated corpus used to train AI language models to recognize the Dutch language. CGN contains recordings spoken by people ranging in age from 18 to 65 years old from Netherlands and the Flanders region of Belgium, covering speaking styles including broadcast news and telephone conversations. CGN has a whopping 483 hours of speech spoken by 1,185 women and 1,678 men. But to make the system even more robust, the coauthors applied data augmentation techniques to increase the total hours of training data “ninefold.” When the researchers ran the trained ASR system through a test set derived from the CGN, they found that it recognized female speech more reliably than male speech regardless of speaking style. Moreover, the system struggled to recognize speech from older people compared with younger, potentially because the former group wasn’t well-articulated. And it had an easier time detecting speech from native speakers versus non-native speakers. Indeed, the worst-recognized native speech — that of Dutch children — had a word error rate around 20% better than that of the best non-native age group. In general, the results suggest that teenagers’ speech was most accurately interpreted by the system, followed by seniors’ (over the age of 65) and children’s. This held even for non-native speakers who were highly proficient in Dutch vocabulary and grammar. As the researchers point out, while it’s to an extent impossible to remove the bias that creeps into datasets, one solution is mitigating this bias at the algorithmic level. “[We recommend] framing the problem, developing the team composition and the implementation process from a point of anticipating, proactively spotting, and developing mitigation strategies for affective prejudice [to address bias in ASR systems],” the researchers wrote in a paper detailing their work. “A direct bias mitigation strategy concerns diversifying and aiming for a balanced representation in the dataset. An indirect bias mitigation strategy deals with diverse team composition: the variety in age, regions, gender, and more provides additional lenses of spotting potential bias in design. Together, they can help ensure a more inclusive developmental environment for ASR.”"
https://venturebeat.com/2021/04/01/arms-confidential-computing-uses-hardware-to-ensure-security/,Arm’s confidential computing uses hardware to ensure security,"Arm introduced its Armv9 chip platform this week as the first major upgrade for its architecture in a decade. And one of the key pillars was “confidential computing,” a hardware-based security initiative. Arm is a chip architecture company that licenses its designs to others, and its customers have shipped more than 100 billion chips in the past five years. Nvidia is in the midst of acquiring Cambridge, United Kingdom-based Arm for $40 billion, but the deal is waiting on regulatory approvals. During Arm’s press event, CEO Simon Segars said that Armv9’s roadmap introduces the Arm Confidential Compute Architecture (CCA). Confidential computing shields portions of code and data from access or modification while in use, even from privileged software, by performing the computation in a hardware-based secure environment, he said. More details will be released over time. The processor can have secure enclaves, and that can create better security throughout the system. Usually, the model for software is to inherently trust the operating system and the hypervisor the software is running on, and that the highest tiers of software are allowed to see into the execution of the lower tiers. But if the operating system or hypervisor is compromised, that’s a risk. CCA introduces a new concept of dynamically created “realms,” which can be viewed as secured containerized execution environments that are completely opaque to the OS or hypervisor. The hypervisor would still exist, but be solely responsible for scheduling and resource allocation. The realms instead would be managed by a new entity called the realm manager, which is supposed to be a new piece of code roughly a tenth the size of a hypervisor. “The Arm Confidential Compute architecture will introduce the concept of dynamically created realms, usable by ordinary programs in a separate computation world from either the non-secure or secure world that we have today,” said Richard Grisenthwaite, chief architect at Arm, in a press briefing. “Realms use a small amount of trust and testable management software that is inherently separated from the operating system.” Segars said that Realms are much like software containers, which isolate code in certain ways, but with hardware support. “People are realizing that it matters,” said Mike Bursell, chief security architect at Red Hat, in a press briefing. “Confidential computing is about protecting your applications, your workloads from a host which is compromised or malicious or from external hackers. Keeping your workloads safe using hardware controls is how we think about confidential computing. People realize there are some workloads that they’re not happy about putting on the cloud or which are not safe on the edge, maybe because their boxes aren’t physically secure.” Realms can protect commercially sensitive data and code from the rest of the system while it is in use, at rest, and in transit. In a recent survey of enterprise executives, more than 90% of the respondents believe that if confidential computing were available, the cost of security could come down, enabling them to dramatically increase their investment in engineering innovation. Overall, the chain of trust required for an application to run can be more limited, protecting the overall system if part of the system is compromised. Henry Sanders, chief technology officer of Azure Edge and Platforms at Microsoft, said in a statement that the complexity of edge-to-cloud computing means that one-size-fits-all solutions don’t work. He believes more synergy between hardware and software with the Confidential Compute architecture is necessary to foster innovation. Lee Caswell, vice president of marketing at VMware’s cloud platform business, said in a statement that Arm’s SmartNICs with VMware Project Monterey introduce a zero-trust security model with the goal of both improved security and better performance across a hybrid cloud. “Arm is positioning itself as a high-performance and highly secure platform, stepping up its competition with x86 and to stay ahead of RISC-V,” said Kevin Krewell, an analyst with Tirias Research, in an email to VentureBeat. “The System Ready program is designed to improve the standardization of Arm-based chips to ease software compatibility. Arm is also preparing for an eventual merger with Nvidia, with its Mali graphics adding new features that mirror Nvidia’s RTX family.” Patrick Moorhead, an analyst at Moor Insights & Strategy, said confidential computing is the next frontier in datacenter security, where every link in the chain has “zero trust” in each other. Armv9 incorporates many elements of confidential computing, and so he thinks Realms is a differentiator. “It’s all about security against many different attack scenarios from a security perspective,” said Ron Martino, executive vice president and general manager of edge computing at NXP. “This includes both the data and the software IP, dealing with multiple entities, some trusted, some that aren’t trusted. And it also includes ensuring security against physical and remote attacks. So when you think about this whole computing concept and deploying devices, it’s this edge-to-cloud computing concept that is applying confidential computing.” Dave Kleidermacher at Google said that confidential computing applies both to the cloud as well as mobile devices. He said one of the uses for confidential computing in the cloud is to stop fraud: Data can be extracted from each domain in a chain of payments, and that data that can point to evidence of fraud in a privacy-preserving way. Richard Searle at Fortanix said the Linux Foundation has been trying to educate the tech community about confidential computing, but there’s still some confusion around it. “There’s still work to be done,” he said. “It’s a new market. But events like this can help get the message about what this new technology can bring to data and application security.”"
https://venturebeat.com/2021/04/01/fangraphs-advanced-baseball-analytics-has-a-new-cloud-home-mariadb/,FanGraphs’ advanced baseball analytics has a new cloud home: MariaDB,"With the 2021 Major League Baseball season opening today, fans will be filling out their scorecards as they return to stadiums for the first time since the COVID-19 pandemic took hold last spring. Of course, the data that is now regularly made available by the MLB goes well beyond the hits, runs, and errors fans typically record in a scorecard they purchase at a game. MLB has made the Statcast tool available since 2015. It analyzes player movements and athletic abilities. The Hawk-Eye service uses cameras installed at ballparks to provide access to instant video replays. Fans now regularly consult a raft of online sites that uses this data to analyze almost every aspect of baseball: top pitching prospects, players who hit the most consistently in a particular ballpark during a specific time of day, and so on. One of those sites is FanGraphs, which has transitioned the SQL relational database platform it relies on to process and analyze structured data to a curated instance of the open source MariaDB database that has been deployed on the Google Cloud Platform (GCP) as part of a MariaDB Sky cloud service. MariaDB provides IT organizations with an alternative to the open source MySQL database Oracle gained control over when it acquired Sun Microsystems in 2009. MariaDB is a fork of the MySQL database that is now managed under the auspices of a MariaDB Foundation that counts Microsoft, Alibaba, Tencent, ServiceNow, and IBM among its sponsors, alongside MariaDB itself. FanGraphs uses the data it collects to enable its editorial teams to deliver articles and podcasts that project, for example, playoff odds for a team based on the results of the SQL queries the company crafts. These insights might be of particular interest to a baseball fan participating in a fantasy league, someone who wants to place a more informed wager on a game at a venue where gambling is, hopefully, legalized, or those making baseball video games. The decision to move from MySQL to MariaDB running on GCP was made after a few false starts involving attempts to lift and shift the company’s MySQL database instance into the cloud, FanGraphs CEO David Appelman said. One of the things that attracted FanGraphs to MariaDB is the level of performance that it could attain using a database-as-a-service (DBaaS) platform based on MariaDB and that it provides access to a columnstore storage engine that might one day be employed to drive additional analytics, Appelman said. In addition, MariaDB now manages the underlying database FanGraphs uses. Appleman said he previously handled most of the IT functions for FanGraphs, including the crafting of SQL queries. Now he will have more time to create SQL queries and monitor the impact they have on the performance of the overall database, Appelman said. “I like to see where the bottlenecks created by a SQL query are,” he added. FanGraphs plans to eventually take advantage of the data warehouse service provided by MariaDB, Appelman noted. It’s not likely any of the analytics capabilities provided by FanGraphs and similar sites will one day be able to predict which baseball team will win on any given day. However, the insights they surface do serve to make the current generation of baseball fans a lot more informed about the nuances of the game than Abner Doubleday probably could have imagined."
https://venturebeat.com/2021/04/01/amazon-introduces-choreographed-motions-with-alexa-presentation-language-1-6/,Amazon introduces choreographed motions with Alexa Presentation Language 1.6,"Amazon today unveiled a new version of its Alexa Presentation Language (APL), the visual design framework that allows developers to build visual apps on Alexa-enabled devices. This new version includes support for three choreographed motions on the Echo Show 10, the first-party Alexa-enabled device with a motorized base that was released last month. Also in tow is improved support for Fire tablets and custom on-screen transitions, as well as faster animations. The pandemic appears to have supercharged voice app usage, which was already on an upswing. According to a study by NPR and Edison Research, the percentage of voice-enabled device owners who use commands at least once a day rose between the beginning of 2020 and the start of April. Just over a third of smart speaker owners say they listen to more music, entertainment, and news from their devices than they did before, and owners report requesting an average of 10.8 tasks per week from their assistant this year compared with 9.4 different tasks in 2019. According to a new report from Juniper Research, consumers will interact with voice assistants on 8.4 billion devices by 2024. APL became generally available in September 2019, and it supports all Alexa devices with screens, including Fire TV devices. This latest release better supports multimodal apps at a time when multimodal app usage on Alexa devices is on the rise. Amazon says that on average, APL-based multimodal apps are seeing more than 3 times the number of monthly active users compared with voice-only apps. Moreover, apps that have implemented APL video have nearly double the customer engagement of voice-only apps, according to the company.  The new choreographed motions in APL (version 1.6), which Amazon calls “choreos,” are: Amazon says the choreos were created by designers working from home during the pandemic. The inspiration, testing, and final designs were completed outside the office, starting from sketches, and came together in about six weeks. APL 1.6 also introduces a change to the APL authoring tool that enables it to convert Lottie files into Alexa Vector Graphics (AVG). (Lottie is an open source animation file format popular among web designers,)  Once converted, the AVGs can be used in an app’s visual responses. Amazon has also increased the limit for visual app responses from 24KB to 120KB to support “even more complex and engaging customer experiences.” Beyond this, the company says it has expanded APL beyond Show Mode on supported Fire tablet — the mode that effectively takes Fire tablet displays full-screen. APL 1.6 lets developers create custom layouts for new screen sizes and adapt their responses as devices flip between portrait and landscape orientations. APL 1.6 is generally available starting today."
https://venturebeat.com/2021/04/01/open-banking-is-big-heres-why-open-finance-is-bigger/,Open banking is big. Here’s why open finance is bigger.,"Presented by Envestnet | Yodlee The open banking movement is part of a much larger open finance trend. In this VB Live event, learn how open finance and access to alternative data can revolutionize customer experiences, spur innovation, improve efficiencies, increase demand, and more. Register here for free. Open banking starts with the premise that consumers have the right to access the data that’s held by their financial institutions — and permit that data to be used by third-parties for the consumers’ benefit. But this is only the beginning, says Brian Costello, VP, Data Strategy & Governance at Envestnet | Yodlee, as the open banking movement becomes part of a much larger open finance trend. In the U.S., open banking has been about giving third parties access to retail banking data, while in the U.K., it’s been limited to just payment accounts. Open finance is simply taking the notion of open banking a step further: giving consumers the ability to share access to all of their financial data online, including mortgages, savings accounts, 401Ks, bills, payroll data, insurance, and more. Envestnet | Yodlee has been doing a commercial model of open finance for 20 years, Costello explains, giving them a front-row seat to the benefits when this data can be shared. “We know how important access to all these data types is for good advice, for proper lending decisions, for helping consumers understand and nudge their financial behaviors in the right direction,” he explains. The first major government-backed implementation of open banking around the globe was in the U.K., coming at the same time as PSD2, the Revised Payment Service Directive, which meant only payment accounts were in its scope. Non-payment accounts were behind security controls, and financial institutions couldn’t make the data available via the API. That’s had unintended negative consequences for consumers in the U.K. — but the issue can be addressed with the shift to open finance, Costello says. “The Financial Conduct Authority in the U.K. is engaging, very sincerely, in the evolution to open finance based on the hard lessons learned from their open banking implementation,” he says. Australia started first with their Consumer Data Rights. The Austrailia Parliament passed a consumer data law, the CDR, which affirms the open data concept: a consumer has a right to their data, and the right to permission it to accredited, appropriate third parties. The institution holding the data has the obligation to protect it, and the obligation to release it, to participate in the scheme. They started a phased implementation of open banking, beginning with checking and savings accounts and credit cards. The second and third implementations of CDR are telecommunications and energy, two of the most impactful discretionary spends that people have. In the U.S., the technical standard used for the open banking initiative is being evangelized and implemented by the Financial Data Exchange. Their model includes mortgages, loans, and tax forms.  The Finacial Data and Technology Association (FDATA) advocates that policy makers and regulators begin with this  end in mind as they lobby regulators on Parliament Hill in Canada and Capitol Hill in the U.S. “An open banking regime in the U.S. that’s just limited to payment accounts would be a major step back,” Costello says. “We’re not going to repeat the mistake that was made in the U.K.” Open finance will improve the experience for customers in the U.S., but they won’t really notice it directly, Costello says. However, under the hood, they’ll benefit from more reliability and more symmetric customer protection end to end — and that will make a big difference. This move to a regulated open finance experience will give customers not just uniform access to all of the data, but under the same umbrella of symmetric customer protection, their payment account data will be as safe as their loan data, payroll data, and so on. “The customer experience for the person who needs to use these services but is reluctant to is going to be incredibly positively impacted,” he says. “Now they’re going to have enough trust in these tools and services to know that if they’re harmed in some way, if there’s a breach in the system or a bad actor, they’re going to be protected.” “As this ecosystem takes off, the data that is being generated, correlated, and used is beneficial not just for the consumer and their direct third-party service providers, but by all thirdparty service providers,” Costello says For the banking industry, open finance levels the playing field. It allows all entities to participate in the ecosystem offering products and services that will either make better customers, attract more deposits, attract more lenders, or at least make less risky customers. “They get top-line growth and they can have bottom-line savings,” Costello says. “As well, there are efficiencies in the infrastructure as we move from screen scraping to APIs. That’s a huge operational savings.” Broker-dealers, RIAs, lenders, lending networks, and insurance companies will all benefit from the operational cost benefits of open finance, as well as the ability to participate in the ecosystem and compete with the startups. Insurance companies in particular will benefit because they’re going to have access to more data and be able to make better underwriting decisions. “There’s an opportunity for improved financial well-being of consumers and families but also it’s going to be a virtuous cycle for competition and customer protection,” he says. “If you’re a bank or a brokerage, now is not the time to be limited by your current technical debt or your current legacy systems.” To learn more about how open finance is evolving around the globe, the benefits to consumers and companies alike, and what financial institutions and fintechs need to do now to prepare for the shift, don’t miss this VB Live event. Don’t miss out. Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/04/01/curri-nabs-6m-for-ai-powered-last-mile-logistics-for-construction/,Curri nabs $6M for tech-powered last-mile construction logistics,"Curri, a Y Combinator-backed logistics startup delivering construction supplies and materials, today announced the closing of a $6 million funding round. The company says the proceeds will be used to expand its services as well as its market reach. Last-mile delivery logistics tends to be the most expensive and time-consuming part of the shipping process. According to one estimate, last-mile accounts for 53% of total shipping costs and 41% of total supply chain costs. With the rise of ecommerce in the U.S., retail providers are increasingly focusing on fulfilment and distribution at the lowest cost. Particularly in the construction industry, the pandemic continues to disrupt wholesalers, highlighting the need for flexible and reliable delivery. Curri claims to solve this problem in construction with an “Uber-like” last-mile delivery model. The company makes available to customers a fleet of drivers with trucks, flatbeds, cars, and other vehicles who can deliver items like pipe bundles, water heaters, and lumber. Curri users arrange an order, open the Curri app, and enter pickup and dropoff locations to book the service. Curri’s drivers then pick up the supplies and ensure the order is correct before fulfilling the delivery. Curri offers live updates via the app to let customers follow and share the status of their deliveries. It also provides proof-of-delivery signature and photos for tracking, regulatory, and compliance purposes. Curri competes with a number of startups in a last-mile delivery market that’s anticipated to reach $66 billion by 2026, including Bond, Bringg, Onfleet, DispatchTrack, and Deliverr. But Curri claims its secret sauce is something that cofounder and CEO Matthew Lafferty calls “elastic scale.” Basically, it’s a concept where customers only pay for what they need. While traditional fleets can underutilize trucks or idle drivers as they wait for orders to come in, Curri says it delivers loads faster thanks to a deep layer of predictive machine learning. According to Lafferty, thousands of customers use Curri to deliver shipments throughout the U.S. “Suppliers who don’t have the ability to make urgent, on-demand, or long distance deliveries are leaving sales on the table and risk losing customers and business to suppliers who do,” he said in a press release. “Fleet augmentation is the secret weapon of suppliers who care about getting material in their customer’s hands, fast.” Los Angeles, California-based Curri’s series A funding announced today was led by Brick & Mortar Ventures and included participation from existing backer Initialized Capital in addition to new investor Rainfall Ventures. It brings four-year-old Curri’s total raised to date to nearly $9 million following a $150,000 seed round in August 2019."
https://venturebeat.com/2021/04/01/aclaimant-a-data-driven-safety-and-risk-management-platform-for-the-workplace-raises-15m/,"Aclaimant, a data-driven safety and risk management platform for the workplace, raises $15M","Aclaimant, an “insight-driven” safety and risk management platform businesses use to monitor and report injuries and incidents in the workplace, has raised $15 million in a series B round of equity and debt funding. While remote work has been a prominent fixture for most companies over the past year, many businesses are now preparing to return to physical offices in some capacity. This is the world Aclaimant and its investors — including Next Coast Ventures — are betting on. Founded in 2013, Chicago-based Aclaimant offers tools that support tracking and the follow-up actions required to improve safety in the workplace and serves clients in industries such as construction, hospitality, insurance, and HR. Aclaimant’s service includes electronically submitting incident reports with support for capturing videos, images, and other documentation. Underpinning all of this are analytics that deliver insights into incidents and related claims activity, organizing all the data into a structured format and helping companies create reports to share with management and other stakeholders. The company in January launched its AI-powered Aclaimant Insights feature, which it said uses predictive modeling to help organizations “evaluate and understand every facet of risk in their environment,” spanning safety, activities, incidents, and claims. Other notable players in the space include Riskconnect, which was acquired by private equity giant Thoma Bravo a few years back. Aclaimant had previously raised $13 million, and with its fresh $15 million cash injection the company is well-positioned to support businesses as they welcome staff back to the office in the coming months. Indeed, tech giants Google and Amazon both confirmed this week that they will shortly start transitioning their staff back to the office, with Amazon noting that it expects most of its staff to be back by this fall."
https://venturebeat.com/2021/04/01/device-monitoring-and-management-startup-memfault-nabs-8-5m/,Device monitoring and management startup Memfault nabs $8.5M,"Memfault, a startup developing software for consumer device firmware delivery, monitoring, and diagnostics, today closed an $8.5 million series A funding round. CEO François Baldassari says the capital will enable Memfault to scale its engineering team and make investments across product development and marketing. Slow, inefficient, costly, and reactive processes continue to plague firmware engineering teams. Often, companies recruit customers as product testers — the first indication of a device issue comes through users contacting customer service or voicing dissatisfaction on social media. With 30 billion internet of things (IoT) devices predicted to be in use by 2025, hardware monitoring and debugging methods could struggle to keep pace. As a case in point, Palo Alto Networks’ Unit 42 estimates that 98% of all IoT device traffic is unencrypted, exposing personal and confidential data on the network. Memfault, which was founded in 2019 by veterans of Oculus, Fitbit, and Pebble, offers a solution in a cloud-based firmware observability platform. Using the platform, customers can capture and remotely debug issues as well as continuously monitor fleets of connected devices. Memfault’s software development kit is designed to be deployed on devices to capture data and send it to the cloud for analysis. The backend identifies, classifies, and deduplicates error reports, spotlighting the issues likely to be most prevalent. Baldassari says that he, Tyler Hoffman, and Christopher Coleman first conceived of Memfault while working on the embedded software team at smartwatch startup Pebble. Every week, thousands of customers reached out to complain about Bluetooth connectivity issues, battery life regressions, and unexpected resets. Investigating these bugs was time-consuming — teams had to either reproduce issues on their own units or ask customers to mail their watches back so that they could crack them open and wire in debug probes. To improve the process, Baldassari and his cofounders drew inspiration from web development and infrastructure to build a framework that supported the management of fleets of millions of devices, which became Memfault. By aggregating bugs across software releases and hardware revisions, Memfault says its platform can determine which devices are impacted and what stack they’re running. Developers can inspect backtraces, variables, and registers when encountering an error, and for updates, they can split devices into cohorts to limit fleet-wide issues. Memfault also delivers real-time reports on device check-ins and notifications of unexpected connectivity inactivity. Teams can view device and fleet health data like battery life, connectivity state, and memory usage or track how many devices have installed a release — and how many have encountered problems. “We’re building feedback mechanisms into our software which allows our users to label an error we have not caught, to merge duplicate errors together, and to split up distinct errors which have been merged by mistake,” Baldassari told VentureBeat via email. “This data is a shoo-in for machine learning, and will allow us to automatically detect errors which cannot be identified with simple heuristics.”  IDC forecasts that global IoT revenue will reach $742 billion in 2020. But despite the industry’s long and continued growth, not all organizations think they’re ready for it — in a recent Kaspersky Lab survey, 54% said the risks associated with connectivity and integration of IoT ecosystems remained a major challenge. That’s perhaps why Memfault has competition in Amazon’s AWS IoT Device Management and Microsoft’s Azure IoT Edge, which support a full range of containerization and isolation features. Another heavyweight rival is Google’s Cloud IoT, a set of tools that connect, process, store, and analyze edge device data. Not to be outdone, startups like Balena, Zededa, Particle, and Axonius offer full-stack IoT device management and development tools. But Baldassari believes that Memfault’s automation features in particular give the platform a leg up from the rest of the pack. “Despite the ubiquity of connected devices, hardware teams are too often bound by a lack of visibility into device health and a reactive cycle of waiting to be notified of potential issues,” he said in a press release. “Memfault has reimagined hardware diagnostics to instead operate with the similar flexibility, speed, and innovation that has proven so successful with software development. Memfault has saved our customers millions of dollars and engineering hours, and empowered teams to approach product development with the confidence that they can ship better products, faster, with the knowledge they can fix bugs, patch, and update without ever disrupting the user experience.” Partech led Memfault’s series A raise with participation from Uncork Capital, bringing the San Francisco, California-based company’s total raised to $11 million. In addition to bolstering its existing initiatives, Memfault says it’ll use the funding to launch a self-service of its product for “bottom-up” adoption rather than the sales-driven, top-down approach it has today."
https://venturebeat.com/2021/04/01/scratchpad-command-lets-you-update-salesforce-from-anywhere-on-the-web/,Scratchpad Command lets you update Salesforce from anywhere on the web,"Scratchpad, a fledgling startup that has built a modern productivity workspace directly on top of the Salesforce CRM, has launched a new feature that lets users update Salesforce from whatever website they are currently on, without switching tabs. The problem Scratchpad is looking to fix is that while Salesforce is the dominant CRM tool for sales teams, it’s not always popular from a usability perspective. Scratchpad develops a suite of tools that integrate with Salesforce — spanning notes, spreadsheets, tasks, search, and more — bundles them under a nice interface, and sells them as a freemium SaaS subscription. Notable early enterprise customers include Adobe, Autodesk, Box, Snowflake, Splunk, and Twilio. With Scratchpad Command, which is available for free as of this week, the company is now setting out to help sales teams avoid having to constantly switch between Salesforce and the various web apps they need to do their job. The key benefit Scratchpad Command offers is that users can not only access Salesforce data from any tab (e.g. to check whether a LinkedIn prospect they’re researching is already in their Salesforce instance), they can now update their Salesforce instance from wherever they are. First, the user has to install the Scratchpad extension for Chrome and authenticate themselves with their Salesforce credentials. They then have to activate Scratchpad Command during the setup process by checking the box. After that, the user can hit Ctrl J (Windows), Command J (Mac) and the Scratchpad icon on the browser toolbar to be presented with a search bar that lets them search their Salesforce instance or create a new note, task, account, lead, opportunity, and more. Scratchpad Command more or less brings the full functionality of Salesforce to wherever you are on the web with a single click. So if you’re in Slack and a potential lead pops up, you can pounce on it without switching tabs. Scratchpad is just one of several startups in the Salesforce ecosystem to garner investor attention recently. According to some estimates, the burgeoning ecosystem is at least 4 times larger than Salesforce itself. Just this week, Sonar raised $12 million for a platform that monitors companies’ Salesforce tech stack for changes, and this came shortly after Copado’s $96 million raise for Salesforce-native DevOps. OwnBackup, meanwhile, secured $167.5 million at a $1.4 billion valuation in January to power cloud data backups for Salesforce. Founded out of San Francisco in 2019, Scratchpad recently raised $13 million in a series A round of funding from Accel and David Sacks’ Craft Ventures, with Sacks now sitting on Scratchpad’s board of directors."
https://venturebeat.com/2021/04/01/ibm-and-red-hat-form-strategic-partnership-with-process-mining-startup-celonis/,IBM and Red Hat form strategic partnership with process mining startup Celonis,"(Reuters) — Celonis, a fast-growing German process mining software startup, has struck a strategic partnership with IBM to help companies make the most of the digital transformation that many are undergoing at speed. IBM’s Global Business Services consulting arm will weave the Celonis Execution Management System into its offering, adding the ability to analyse data thrown off by processes like supply-chain management, finance, or procurement to identify weaknesses and recommend fixes. Celonis will also shift its software stack to IBM’s Red Hat OpenShift platform, which enables companies to operate in an open “hybrid” setting that can include public or private cloud datacenters, on-premise servers, and mainframe computers. That represents a big step for the Munich startup, which last raised funds from investors at a valuation of $2.5 billion in 2019 and counts Coca-Cola, Siemens, Uber, and Vodafone as clients. “This is the most comprehensive and committal partnership that I’ve been part of,” said Miguel Milano, chief revenue officer and co-owner of Celonis, who joined from Salesforce a year ago. “We took the risk because we saw the opportunity and the value to re-engineer our full stack to run on Red Hat OpenShift. That is massive,” Milano, a 28-year software industry veteran, told Reuters in an interview. A typical large company runs more than 1,000 processes with the use of technology, often in different computing environments, creating complexity that makes it hard to bring about change. Weaving Celonis into IBM’s consulting offering can help clients identify where the value can be found across a range of applications, making it possible to benchmark performance, deploy artificial intelligence, or trigger automation. “Every workflow in every organization is now fair game,” said Mark Foster, senior vice president at IBM Services. Celonis has doubled bookings and revenues over the past year, and Milano said the partnership offered an opportunity to accelerate its growth further. “Our breakthrough from a strategic perspective was to realise that Celonis alone cannot take our platform to every company, every industry, every process,” he said, explaining the attraction of the partnership."
https://venturebeat.com/2021/03/31/cisco-is-bringing-individual-and-team-insights-to-webex-video-calls/,Cisco is bringing individual and team insights to Webex video calls,"Starting this summer, Cisco’s Webex will begin serving up insights for video calls to a select group of individuals, teams, and organizations. Engagement insights include things like how often you had your video on or showed up on time and the people or teams within an organization you speak with most often. The goal, Cisco VP Jeetu Patel told VentureBeat in a phone interview, is to make video calls better for people living in the hybrid world between in-person meetings in the office and virtual meetings at home. The tricky part, he said, is considering what information is useful for an individual to know while not raising concern that Webex is, for example, alerting managers to employees who are routinely late to meetings.  “Let’s say you did 12 meetings today, and in six of those meetings with four people or fewer, you actually spoke for 90% of the time. That would be a really bad thing to give your boss but a really good thing for you to have so you can say, ‘Oh, I should probably do a better job listening,'” he said. “The privacy on that front is not at the organizational level. It’s at the individual level. So when we provide insights like that to an individual, the individual owns the data, not the organization, because we don’t believe that without your explicit permission, you’d want to have your boss see that.” Webex has introduced a series of new features in recent months, some powered by artificial intelligence, to change how people share information in video calls. Toward this end, Patel said, “We’ve probably invested about a billion dollars or so in the past two years in AI.” Gesture recognition means people in video calls can now raise their hand to ask to speak or give a thumbs up or thumbs down to register feedback. Another AI-powered feature on the way will crop the faces of people who attend in-person meetings for anyone who’s working remotely. “Even though there are three people sitting in a conference room, we’ll actually break the stream into three separate boxes and show it to you, and our hardware will actually do that,” Patel said. Patel has overseen the acquisition of three companies since joining Cisco last summer, after serving as chief product officer at Box. Last month, Cisco closed its acquisition of IMImobile for $730 million, in part to beef up its AI capabilities. Last summer, Cisco announced plans to acquire BabbleLabs, an AI startup focused on filtering audio so the sound of someone doing dishes, a lawnmower running, or other loud background noise can be reduced or eliminated. And earlier this year, Cisco acquired Slido, a startup that makes engagement features for video calls, like word clouds and upvoting questions. Such features can allow a meeting to take the structure of a town hall, with transparency around the top questions for employees within an organization, since everyone can see the questions being posted. But Patel acknowledges that there are limits to how far the technology should go. “Engagement should not be measured based on having a judgment on someone saying, ‘I’m judging that you look sad, and therefore I’m going to do certain things’ … at that point in time, in my mind, you could cross a boundary where there’s more bad that can come out of that than good,” he said. In 2019, Cisco acquired Voicea to power speech-to-text transcription of meetings. Closed captioning and live translation are also available in Webex calls. Deciding where to draw the line on which AI-powered features or insights to introduce in video calls can be a challenge. Earlier this year, Microsoft Research did a study with AffectiveSpotlight on AI for recognizing confusion, engagement, and head nods in meetings. If taken in the aggregate, picking up cues from the audience could be really helpful, particularly for large organizations. But if affective AI for video calls leads to a critique of how often a person smiles or has a certain expression on their face, it could be considered invasive or counterproductive or even biased toward groups of people. Video analysis of expression today can have major shortcomings. A group of journalists in Germany recently demonstrated that placing a bookshelf in the background or wearing glasses can change affective AI evaluations of a person in a video. And it shouldn’t matter whether a person is an extrovert or prefers not to talk in group settings as long as they fulfill their job duties. Some people talk a lot but have nothing much say, while others speak less often but deliver sharp insights or sage advice. It all depends on the team, role, and scenario. Monitoring such information also raises the question of consent. “There’s a fine line between ‘This is super productive’ and ‘We can’t do this because it violates my privacy or it’s just outright creepy,'” Patel said.  Cisco plans to roll out Webex People Insights globally over the next year, starting with select users in the U.S. this summer. The company announced the news today as part of its Cisco Live virtual event. In other Cisco Live news, on Tuesday the company announced plans to combine networking, security, and IT infrastructure offerings and to work with the Duo authentication platform it acquired in 2018."
https://venturebeat.com/2021/03/31/cresta-which-uses-ai-to-mentor-customer-service-agents-in-real-time-raises-50m/,"Cresta, which uses AI to mentor customer service agents in real time, raises $50M","Cresta, an AI-powered platform that offers real-time support to help customer service agents respond to inquiries on calls or in chats, has raised $50 million in a series B round of funding. The company’s latest investment, which was led by Sequoia Capital, with participation from Greylock Partners, Andreessen Horowitz, Allen & Company, and Porsche Ventures, comes after a year of growth that saw its revenues quadruple. It’s difficult to read too much into any first-year revenue growth metrics, but it’s clear that companies are hankering for technology that helps them optimize their customer-facing operations. Contact centers have proven fertile ground for AI, with a slew of companies emerging to offer their own take on how automation can improve companies’ interactions with their customers. Just today, Uniphore announced a fresh $140 million investment to analyze emotion and engagement in both voice and video-based calls, while Talkdesk launched a new “human-in-the-loop” AI trainer for contact centers. Cresta shares common ground with many of these companies, though it’s placing a specific focus on learning from what the best-performing agents do and passing this knowledge to colleagues while nudging them with suggested responses. The San Francisco-based company officially launched last year with $21 million in funding, and it has amassed an impressive roster of clients so far, including Intuit, Adobe, and Dropbox. Cresta recently introduced Cresta for Voice to target phone-based sales and contact center teams, and it launched an integration with Amazon Web Services’ Amazon Connect cloud contact service platform last year."
https://venturebeat.com/2021/03/31/cloudera-expands-enterprise-data-platform-to-google-cloud/,Cloudera expands enterprise data platform to Google Cloud,"Enterprise data cloud company Cloudera has announced that its data platform is now available on Google Cloud. The Palo Alto-based company launched the Cloudera Data Platform (CDP) back in 2019, following its merger with rival Hortonworks. The CDP combined technologies from both companies, serving to bring a powerful data and analytics platform to hybrid and multicloud enterprises. At launch, the only public cloud CDP supported was AWS, though Azure was added to the mix shortly afterward. So today’s news has been a long time coming. This is a big deal for several reasons, though perhaps chiefly because it positions Cloudera as a truly platform-agnostic data platform, one that could help it capture a bigger piece of the burgeoning multicloud market. While similar services from cloud rivals such as Microsoft and Google do offer some support for competing public (and private) clouds, they were ultimately designed for their respective owner’s cloud. Cloud infrastructure spending has skyrocketed over the past year, and reports suggest businesses are eager to segue from single cloud platform providers, lured by the promise of flexibility, a way to avoid vendor lock-in, and the inherent task-specific strengths different clouds offer. Flexera’s recent State of the Cloud report noted that 93% of enterprises now use multiple cloud service providers, a factor that has led Microsoft and Google to embrace a multicloud world, though Amazon’s AWS still has some way to go on that front. The CDP is generally available on Google Cloud from today."
https://venturebeat.com/2021/03/31/cloud-management-startup-striim-plans-new-integrations-and-streaming-analytics-with-50m-funding/,Cloud management startup Striim plans new integrations and streaming analytics with $50M funding,"Striim, a startup specializing in streaming data integration, today closed a $50 million series C funding round led by Goldman Sachs Growth Equity. Striim founder and CEO Ali Kutay says the funding will be used to scale the company’s offerings that power cloud integration, edge processing, and streaming analytics. Data modernization continues to be a major push, with Gartner predicting that over 75% of midsize and large companies will deploy a multi-cloud or hybrid IT strategy by 2021. Reflecting this uptick, Markets and Markets anticipates that the multi-cloud management sector will climb from $1.17 billion in value in 2017 to $4.49 billion by 2022. Kutay, Steve Wilkes, and Sami Akbay cofounded Palo Alto, California-based Striim in 2012 to address the needs of enterprises managing data across multiple clouds. Each had experience with deep data and infrastructure and shared successes building GoldenGate Software, which was acquired by Oracle in 2009, and WebLogic, which merged with BEA. Striim continuously ingests data from databases, log files, messaging systems, cloud apps, and internet of things devices, performing processing including filtering, transformations, aggregations, masking, and metadata enrichment. The platform offers monitoring and validation features that help to trace and confirm the collection and delivery of streaming data, as well as a wizard that leads customers through the process of defining data flows and connections.  Striim customers can connect with their data in public and private clouds and build custom pipelines with routing and rules defined in an SQL-like language. Out-of-the-box dashboards show table-level metrics and latency of data delivery, with tools admins can use to configure performance and uptime alerts and self-healing pipelines with remediation workflows. These dashboards can also incorporate AI for deeper insights and expose metrics on connector components including read and write rates, lag, latency, and CPU usage. Striim appears to be well-positioned for growth in an increasingly lucrative market. As cloud computing usage explodes during the pandemic, enterprises are embracing multi-cloud strategies. Ninety-three percent are using multiple clouds and 87% have a hybrid cloud setup, according to a Flexera survey. And while 20% of organizations already spend over $12 million a year on public clouds, 59% expect their cloud usage to exceed projections due to pandemic headwinds. Striim recently announced a preview release of StreamShift, a managed service offering that helps enterprises migrate databases from on-premise to the cloud. (A full release is expected in Q2 2021.) Building on this, the company says it’s continuing to hire “aggressively” and now has over 100 full-time employees across offices in California as well as Chennai, India. Striim’s customers include brands in financial services, health care, transportation and logistics, and retail, including UPS, Macy’s, Equifax, and HSBC. Strimm’s latest funding round brings the company’s total raised to date to over $108 million. It follows on the heels of a $16.5 million extension to a series B round that closed in March 2017."
https://venturebeat.com/2021/03/31/world-leading-ai-research-and-inclusion-at-the-forefront-of-this-years-nvidia-gtc/,World-leading AI research and inclusion at the forefront of this year’s NVIDIA GTC,"This article is part of the VB Lab / NVIDIA GTC insight series. “The story of GTC is in many ways the story of NVIDIA, and it’s also the story of what’s happening in technology,” says Greg Estes, VP of corporate marketing and developer programs at NVIDIA. Twelve years ago, GTC began as a conference focused squarely on GPUs, and at that time, that meant primarily graphics and gaming. “But then people figured out that GPUs are the perfect architecture for AI,” says Estes. GTC is now billed as the conference for AI innovators, developers, technologists, startups and creatives, and this year it will offer over 1,500 sessions covering breakthroughs in AI, data center, accelerated computing, autonomous vehicles, health care, intelligent networking, game development, and more. This year’s event will take place online April 12 – 16, 2021, free for registered participants. The week-long event kicks off with a keynote on April 12 at 8:30 a.m. PDT with NVIDIA’s CEO and founder Jensen Huang. Along with exciting announcements, he’s set to share the company’s vision for computing that scales from the edge to the data center to the cloud. Following the keynote are panels from the world’s leading researchers along with thought leaders from top companies, including Adobe, Amazon, Facebook, GE Renewable Energy, Google, MIT, Microsoft, Salesforce, and Stanford University. “We bring together the people who are taking the leading edge of technology and making it deployable across businesses and industries of all kinds,” Estes says. “You’ll hear everything from bleeding edge, just-invented-last-week technology to the platform solutions that can be securely deployed in your data center. There’s no other place you’ll find this huge range of insight and expertise.” That’s why GTC has grown so large, from its 2009 debut in a San Jose hotel ballroom to the sprawling behemoth that will attract more than 100,000 attendees and an A-list line-up of speakers this year. That includes Turing Award winners like Geoffrey Hinton, Yoshua Bengio, and Yann LeCun; AI pioneers like Juergen Schmidhuber; MacArthur Fellowship Award winner Daphne Koller; Gordon Bell Award winners for COVID research Rommie Amaro and Lillian Chong; autonomous vehicle pioneer Raquel Urtasun; and many more, in addition to a number of NVIDIA’s own researchers, including Anima Anandkumar and Sanja Fidler. “GTC has now become a magnet for business leaders, developers, and startups to talk about what they’re doing as they implement AI and data science across their work, from health care, automotive and transportation to energy research, retail, media, and entertainment,” says Estes. “The Da Vincis and Curies of our time are at GTC,” he adds. “Where else do you get to hang out with those people?” In addition to the vast breadth of topics, the conference is putting emphasis on two major tech topics this year: autonomous vehicle technology and health care. Autonomous vehicle tech is evolving rapidly, and so is the industry. AI is creating an opportunity for new entrants into the field, shaking up a playing ground that had long been dominated by the major auto players, and opening new business models to automakers. This year, leading thinkers from two dozen autonomous vehicle startups including Aurora, Cruise, and Zoox will be presenting, Estes says, alongside the world’s largest automakers, including Audi, Ford, Toyota, and Volkswagen. “When you think about the toughest problems in AI, autonomous vehicle technology is far and away at the top of the list,” he says. “As a research area, dealing with the complexity of the neural networks, sensors, and data, plus keeping things secure, is very rich.” In the health care arena, AI has played a major role in everything from COVID-19 research to cancer detection, genomics, and more, all the way through climate studies and other essential research on the many things that impact human health. Several NVIDIA Inception startups, the company’s AI startup acceleration platform, are leading the charge in health care. “The diversity of health care research comes together at GTC — the combination of researchers, doctors, and companies focused on drug discovery is unique compared with other health care conferences,” Estes says. NVIDIA is committed to making GTC a forum for all communities to engage with the leading-edge of AI, data science, and other ground-breaking technologies, Estes says. “These world-changing technologies affect everyone, which means it’s more important than ever to ensure everyone’s voices are heard.” Overall, GTC attracts attendees from 165 countries. “We’ve been doing a lot of work to make GTC more inclusive for women, underrepresented communities, and developers from emerging countries,” Estes says. “We have an amazing and diverse group of speakers to inspire the next generation regardless of what they look like or where they come from.” That includes several hundred women speakers, including Hildegard Wortmann, on the board of management at Audi; Victoria Uti, director and principal research engineer at Kroger Technology, and Amy Bunszel, senior vice president of product at Autodesk. NVIDIA is also creating pathways to learning for those new to AI and deep learning, which includes free seats for day-long certification classes at the NVIDIA Deep Learning Institute. Partners comprise minority-serving institutions such as Norfolk State, Howard University, North Carolina A&T University, San Jose State University, and Bowie State University, and tech organizations like Black in AI, Black Tech Nation, Data Science Salon, LatinX in AI, and Qwasar. NVIDIA’s inclusion efforts also encompass outreach to developers in emerging technology centers, including Africa, Latin America, and southeast Asia, for which they will feature the work of startups and rising developers building AI in their regions. GTC will also highlight innovative work being done by scientists such as Dr. Nashlie Sephus, who is transforming 12 acres of abandoned land in Mississippi into a $25M tech hub, as well as Black futurist Justin Shaifer, and the Hip Hop MD, Maynard Okereke, as they co-moderate a panel on the importance of educating youth to be more AI literate. There will also be networking opportunities for underrepresented communities, including GTC’s popular Dinner with Strangers series. The series provides an opportunity to meet new people who share similar interests, build connections, and foster career growth. Get all the details about GTC 2021 right here. Registration is free. VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/31/edge-computing-orchestration-startup-zededa-raises-12-5m/,Edge computing orchestration startup Zededa raises $12.5M,"Edge virtualization startup Zededa today announced it has raised $12.5 million in an oversubscribed extension to its series A round. The company says the funding will be used to bolster its customer acquisition efforts as it continues to expand the size of its workforce. Global internet of things (IoT) revenue hit an estimated $1.7 trillion in 2019, when the number of edge devices connected to the internet exceeded 23 billion, according to CB Insights. But despite the industry’s growth, not all organizations think they’re ready for it. In a recent Kaspersky Lab survey, 54% said the risks associated with connectivity and integration of IoT ecosystems remained a significant blocker. Zededa was cofounded in 2016 by Erik Nordmark, Roman Shaposhnik, Said Ouissal, and Vijay Tapaskar to solve the edge computing adoption challenges plaguing the enterprise. Based in Santa Clara, California and India, the company’s hardware- and cloud-agnostic software enables app deployment over most types of edge networks. “Edge computing is critical in not just driving efficiencies that may lead to cost savings, but more importantly in building new business models and creating new customer experiences,” Ouissal told VentureBeat via email. “Organizations are quickly identifying new use cases where edge computing will provide tangible benefits, such as remote orchestration to reduce the need for employees to travel to edge locations, self-service kiosks, and computer vision to maintain safe indoor experiences [during the pandemic].” Zededa says its centralized, subscription-based technology is built on open standards and plays nicely with devices from Advantech, Lanner, SuperMicro, Scalys, and other vendors. It supports Docker containers, Kubernetes clusters, and virtual machines and works in concert with Eve, a bare-metal Linux-based operating system designed for edge computing that counts among its contributors Intel and General Electric. Leveraging apps, their dependencies, and core operating system bits that periodically communicate with the cloud, Zededa’s platform attempts to ensure edge installations behave as they should, according to a spokesperson. “Zededa [addresses] the complexity of today’s solutions through an open and purpose-built IoT edge orchestration framework that breaks down silos and delivers the agility customers need to evolve their connected operations with a multicloud strategy,” they said. There’s an abundance of tools promising to simplify IoT analytics and management at the edge, including Google’s Cloud IoT Edge, Amazon’s Amazon Web Services (AWS) IoT, Microsoft’s Azure Sphere, and Baidu’s OpenEdge. But Zededa asserts that it has an advantage in the open source nature of its approach. “We made our product generally available earlier this year in January, after nearly a year of working with a very select group of advanced and leading customers and partners to build and refine the product features. Today we have over a dozen paying customers, including several Fortune 500 companies spread across verticals like manufacturing, oil and gas, and renewable energy, telco, and health care,” Ouissal said. “We anticipate bringing on several new high-profile customers this year and are excited to see customers begin to scale their deployments. We expect to see the number of edge nodes under management more than triple by the middle of 2021 from the beginning of the year.” Zededa recently launched a marketplace hosting data ingestion, AI and machine learning, networking, and security apps and joined the Linux Foundation’s EdgeX Foundry, an ongoing vendor-neutral open source IoT project. Zededa is also part of the Foundation’s LF Edge umbrella organization, where it’s incubating Eve and the telecom-oriented Akraino Edge Stack. New strategic investors Rockwell Automation, Juniper Networks, and EDF North America Ventures participated in Zededa’s expanded round, along with investors Almaz Capital, Energize Ventures, Lux Capital and HBAM. This brings the company’s total raised to date to $28.5 million."
https://venturebeat.com/2021/03/31/ai-experts-warn-facebooks-anti-bias-tool-is-completely-insufficient/,AI experts warn Facebook’s anti-bias tool is ‘completely insufficient’,"Facebook today published a blog post detailing Fairness Flow, an internal toolkit the company claims enables its teams to analyze how some types of AI models perform across different groups. Developed in 2018 by Facebook’s Interdisciplinary Responsible AI (RAI) team in consultation with Stanford University, the Center for Social Media Responsibility, the Brookings Institute, and the Better Business Bureau Institute for Marketplace Trust, Fairness Flow is designed to help engineers determine how the models powering Facebook’s products perform across groups of people. The post pushes back against the notion that the RAI team is “essentially irrelevant to fixing the bigger problems of misinformation, extremism, and political polarization [on Facebook’s platform],” as MIT Tech Review’s Karen Hao wrote in an investigative report earlier this month. Hao alleges that the RAI team’s work — mitigating bias in AI — helps Facebook avoid proposed regulation that might hamper its growth. The piece also claims that the company’s leadership has repeatedly weakened or halted initiatives meant to clean up misinformation on the platform because doing so would undermine that growth. According to Facebook, Fairness Flow works by detecting forms of statistical bias in some models and data labels commonly used at Facebook. Here, Facebook defines “bias” as systematically applying different standards to different groups of people, like when Facebook-owned Instagram’s system disabled the accounts of U.S.-based Black users 50% more often than accounts of those who were white. Given a dataset of predictions, labels, group membership (e.g., gender or age), and other information, Fairness Flow can divide the data a model uses into subsets and estimate its performance. The tool can determine whether a model accurately ranks content for people from a specific group, for example, or whether a model under-predicts for some groups relative to others. Fairness Flow can also be used to compare annotator-provided labels with expert labels, which yields metrics showing the difficulty in labeling content from groups and the criteria used by the original labelers. Facebook says its Equity Team, a product group within Instagram focused on addressing bias, uses “model cards” that leverage Fairness Flow to provide information potentially preventing models from being used “inappropriately.”  The cards include a bias assessment that could be applied to all Instagram models by the end of next year, although Facebook notes the use of Fairness Flow is currently optional. Mike Cook, an AI researcher at the Queen Mary University of London, told VentureBeat via email that Facebook’s blog post contains “very little information” about what Fairness Flow actually does. “While it seems that the main aim of the tool is to connect the Facebook engineers’ expectations with the model’s output, … the old adage ‘garbage in, garbage out’ still holds. This tool just confirms that the garbage you’ve gotten out is consistent with the garbage you’ve put in,” he said. “In order to fix these bigger problems, Facebook needs to address the garbage part.” Cook pointed to language in the post suggesting that because groups might have different positive rates in factual (or “ground truth”) data, bias isn’t necessarily present. In machine learning, a false positive is an outcome where a model incorrectly predicts something, while a true positive measures the percentage of the model’s correct predictions. “One interpretation of this is that Facebook is fine with bias or prejudice, as long as it’s sufficiently systemic,” Cook said. “For example, perhaps it’s reasonable to advertise technology jobs primarily to men, if Facebook finds that mostly men click on them? That’s consistent with the standards of fairness set here, to my mind, as the system doesn’t need to take into account who wrote the advert, what the tone or message of the advert is, what the state of the company it’s advertising is, or what the inherent problems in the industry the company is based in are. It’s simply reacting to the ‘ground truth’ observable in the world.” Indeed, a Carnegie Mellon University study published last August found evidence that Facebook’s ad platform discriminates against certain demographic groups. The company claims its written policies ban discrimination and that it uses automated controls — introduced as part of the 2019 settlement — to limit when and how advertisers target ads based on age, gender, and other attributes. But many previous studies have established that Facebook’s ad practices are at best problematic. Facebook says Fairness Flow is available to all product teams at the company and can be applied to models even after they’re deployed in production. But Facebook admits that Fairness Flow, the use of which is optional, can only analyze certain types of models — particularly supervised models that learn from a “sufficient volume” of labeled data. Facebook chief scientist Yann LeCun recently said in an interview that removing biases from self-supervised systems, which learn from unlabeled data, might require training the model with an additional dataset curated to unteach specific biases. “It’s a complicated issue,” he told Fortune. University of Washington AI researcher Os Keyes characterized Fairness Flow as “a very standard process,” as opposed to a novel way to address bias in models. They pointed out that Facebook’s post indicates the tool compares accuracy to a single version of “real truth” rather than assessing what “accuracy” might mean to, for instance, labelers in Dubai versus in Germany or Kosovo. “In other words, it’s nice that [Facebook is] assessing the accuracy of their ground truths … [but] I’m curious about where their ‘subject matter experts’ are from, or on what grounds they’re subject matter experts,” Keyes told VentureBeat via email. “It’s noticeable that [the company’s] solution to the fundamental flaws in the design of monolithic technologies is a new monolithic technology. To fix code, write more code. Any awareness of the fundamentally limited nature of fairness … It’s even unclear as to whether their system can recognise the intersecting nature of multiple group identities.” Exposés about Facebook’s approaches to fairness haven’t done much to engender trust within the AI community. A New York University study published in July 2020 estimated that Facebook’s machine learning systems make about 300,000 content moderation mistakes per day, and problematic posts continue to slip through Facebook’s filters. In one Facebook group that was created last November and rapidly grew to nearly 400,000 people, members calling for a nationwide recount of the 2020 U.S. presidential election swapped unfounded accusations about alleged election fraud and state vote counts every few seconds. Separately, a May 2020 Wall Street Journal article brought to light an internal Facebook study that found the majority of people who join extremist groups do so because of the company’s recommendation algorithms. And in an audit of the human rights impact assessments (HRIAs) Facebook performed regarding its product and presence in Myanmar following a genocide of the Rohingya people in that country, Carr Center at Harvard University coauthors concluded that the third-party HRIA largely omitted mention of the Rohingya and failed to assess whether algorithms played a role. Accusations of fueling political polarization and social division prompted Facebook to create a “playbook” to help its employees rebut criticism, BuzzFeed news reported in early March. In one example, Facebook CEO Mark Zuckerberg and COO Sheryl Sandberg have sought to deflect blame for the Capitol Hill riot in the U.S., with Sandberg noting the role of smaller, right-leaning platforms despite the circulation of hashtags on Facebook promoting the pro-Trump rally in the days and weeks beforehand. Facebook doesn’t perform systematic audits of its algorithms today, even though the step was recommended by a civil rights audit of Facebook completed last summer. “The whole [Fairness Flow] toolkit can basically be summarised as, ‘We did that thing people were suggesting three years ago, we don’t even make everyone do the thing, and the whole world knows the thing is completely insufficient,'” Keyes said. “If [the blog post] is an attempt to respond to [recent criticism], it reads as more of an effort to pretend it never happened than actually address it.”"
https://venturebeat.com/2021/03/31/windfall-nabs-21m-for-ai-that-aids-nonprofit-fundraising/,Windfall nabs $21M for AI that aids nonprofit fundraising,"Windfall, a startup developing an AI platform to help nonprofits engage donors, today raised $21 million in venture capital. The company says the funds will be used to invest in product R&D, the expansion of its team, and the scaling up of Windfall’s marketing and sales operations. Securing donations remains among the top challenges philanthropic organizations face on a regular basis. Eight percent of nonprofits saw the number of philanthropies competing for dollars as one of the biggest barriers to their charitable impact, a 2019 Statista survey found. Crises like the pandemic threaten to exacerbate this. According to the Association of Fundraising Professionals, more than half of U.S. nonprofits expected to raise less money in 2020 than they did in 2019, and an equal percentage believe the same will occur in 2021. Founded in 2016 by Arup Banerjee, Cory Tucker, and Dan Stevens, Windfall offers estimates of potential donors’ net worth at the household level, leveraging data and machine learning to make projections. The platform also delivers contextual analysis, putting donor databases in a framework that can then be used for data science. With Windfall, nonprofits can prioritize donors and apply insights to segment the donors and focus outreach. The company claims its platform can sync millions of records with Salesforce, HubSpot, Shopify, and other apps on a weekly basis. “With a focus on the affluent, Windfall delivers a precise net worth figure, not a range, and a spectrum of deterministic consumer attributes at the individual household level,” the company writes on its website. “Windfall’s proprietary data set is completely rebuilt on a weekly basis, giving customers the best and latest insights on their constituents.” Windfall’s current round follows a seed round in 2019 that raised $9 million and allowed the company to continue building its consumer financial data products. Windfalls says it had a 3 times year-over-year growth rate in 2020 and that its customer base exceeds 500 organizations, including the Environmental Defense Fund, Make-A-Wish, and the University of Michigan. “At Windfall, our focus is empowering our customers to access the most reliable data and insights possible, but that’s just the beginning,” CEO Banerjee said. “We’ve built artificial intelligence and machine learning into our platform to drive better decision-making, enable teams to develop comprehensive data-driven strategies and build that into their existing workflows. It’s a truly transformative approach for our customers.” David Lamond led San Francisco, California-based Windfall’s series A with participation from EPIQ Capital Group and existing investors Bonfire Ventures, Bullpen Capital, Cherubic Ventures, and ValueStream Ventures. It brings the company’s total raised to date to $30 million."
https://venturebeat.com/2021/03/31/dataikus-new-ai-tools-reduce-dependency-on-data-science-teams/,Dataiku’s new AI tools reduce dependency on data science teams,"Dataiku today expanded its effort to make AI accessible to the average business user with an update that makes it possible to run what-if simulations of AI models to determine how changes to the data they are based on will impact them. The goal is to make it easier for business analysts to experiment with AI models based on machine learning algorithms they can create with the help of a data scientist team, Dataiku CEO Florian Douetteau said. As part of that effort, Dataiku 9 adds a Model Assertions tool that enables a subject matter expert to inject a known condition or sanity-checks into a model to prevent a certain outcome or conclusion from ever being reached. There is also now a Visual ML Diagnostics tool that will generate error messages if the platform determines a model will fail and a Model Fairness Report tool that provides access to a dashboard through which companies can assess the bias or fairness of an AI model. Finally, there is also now a Smart Pattern Builder and Fuzzy Joins capability that makes it easier to work with more complex or even incomplete datasets without having to write code or manually clean or prep data. Most organizations are investing in AI to make better data-driven decisions, which Douetteau says makes it essential for business analysts to create AI models without having to wait for a data science team to construct them. “Business users should be able to create AI, not just consume it,” he said. That doesn’t necessarily mean an organization shouldn’t hire a data science team to address more complex challenges, but it does reduce the dependency an organization would otherwise have a on small team of specialists, Douetteau noted. When it comes to AI, most organizations are employing the approach they typically used to make previous generations of analytics applications available to end users. A team of IT specialists would create a series of dashboards based on data marts that would be exposed to business users via a self-service portal. But that approach is insufficient when it comes to AI because the rate of change in the underlying data has now substantially increased, Douetteau said. Business analysts need to be able to dynamically create AI models that enable organizations to respond faster to rapidly changing business conditions. Of course, there is no better example of the need for that capability than the COVID-19 pandemic. Most of the AI models organizations had in place prior to the arrival of the pandemic were rendered obsolete almost overnight. It has taken most of those organizations months to construct new AI models, but even now it’s difficult to make assumptions that are not subject to change as new data becomes available. Unfortunately, the average data science team today is fortunate if they can get an AI model into a production environment in a few months. Businesses clearly need to get data science tools into the hands of end users to enable AI to live up to its full potential, Douetteau said. The challenge is ensuring there are enough checks and balances in place to make sure any AI model that is implemented is properly vetted. After all, the scale at which an AI model typically works means that any potential error could represent a considerable risk for any business. At this point, however, the proverbial AI genie is out of the bottle, regardless of the level of risk. Tumultuous economic times are pushing many business executives to accept higher levels of risk in an effort to either reduce costs or maximize revenues. But whatever outcomes AI eventually enables, the way business processes are constructed and maintained is about to change utterly."
https://venturebeat.com/2021/03/31/moveworks-helps-enterprises-automate-it-self-service-tasks/,Moveworks helps enterprises automate IT self-service tasks,"IT issues take time to solve, which can cost enterprises money. A PagerDuty survey found 38.4% of organizations that take more than 30 minutes to resolve IT incidents see an impact on customer-facing services. Moreover, nearly one-third of departments regularly affected by technical problems say that an hour of downtime costs them $1 million or more. That’s where Moveworks aims to make a difference. The Mountain View, California-based company, which was founded in 2016, is developing an AI platform that can resolve IT support issues automatically. Today marks the launch of Moveworks’ newest product, the Employee Service Platform, which brings together AI and natural language understanding technologies to get employees help across departments. Moveworks says that the system can handle human resources, finance, and facilities issues end-to-end, from the initial request to the final resolution. According to CEO Bhavin Shah, Moveworks has been laying the groundwork for the Employee Service Platform since the company’s earliest days. Eighteen months ago, after experiencing success in the IT segment — Moveworks counts among its customers Palo Alto Networks, Slack, and LinkedIn — the company began building the platform. More recently, they started inviting customers in early access. “Everything we do at Moveworks is inspired by a simple idea: It shouldn’t take days to get help at work,” Shah said. “Today, after half a decade, Moveworks … delivers instant help to all lines of business.” Beyond answering questions about unlocking accounts, resetting passwords, and provisioning software, the Employee Service Platform helps surface forms, pull answers from knowledge bases, and route requests to the right subject-matter experts. The platform’s engine, which was trained on over 100 million real-world issues, combines domain recognition, semantic search, and deep integrations to address questions with answers from departments’ knowledge bases. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. “We engineered a unique approach to understanding the language used in the enterprise, which we deployed prior to this product expansion to resolve IT issues — without predefining specific intents or hard-coding rigid workflows. That approach is our multifaceted intent system,” CTO Vaibhav Nivargi told VentureBeat via email. “At a high level, it is a generalized natural language understanding system. Rather than predefining specific user intents, our multifaceted intent system determines the overarching action and resource type needed to resolve each issue. Once we’ve established this generalized intent, we then evaluate the utility of potential resources.” The Employee Service Platform also transforms resources to display information in a conversational format inside collaboration tools like Slack, Microsoft Teams, and more. For example, users can fill out IT forms without leaving the Moveworks interface in Teams or receive only the pertinent paragraph of a human resources policy after asking Moveworks a question in Slack. As part of the Employee Service Platform, Moveworks released the Employee Communications module, which enables company leaders to send messages via a cross-platform chatbot. The engine ingests knowledge articles and documents several times per day, enabling the chatbot to answer follow-up questions about messages autonomously. The chatbot market is expected to reach $1.23 billion by 2025, according to Grand View Research, and there’s reason for its continued growth. Fifty-three percent of service organizations expect to use chatbots within 18 months, according to a Salesforce survey. And Gartner estimates that chatbots were powering 85% of all customer service interactions as of last year. “Immediately following the pandemic, we saw a significant increase in the overall volume of tickets submitted to Moveworks — approximately twice as many in March 2020 than in February. Employees across industries needed to learn to use new collaboration tools, order new devices for the home office, look up colleagues’ contact information, troubleshoot Zoom, stay abreast of business continuity plans, and more,” Nivargi said. “Perhaps the most enduring challenge for companies in this work-from-anywhere economy is keeping their employees up-to-date and on the same page. … We responded to the demand by accelerating the creation of our new solution for employee communications. Our customers regularly achieve 50% to 70% engagement with communications campaigns done through Moveworks, compared to around 10% for the average mass email.”"
https://venturebeat.com/2021/03/31/oracle-offers-free-cloud-migration-to-lure-new-customers/,Oracle offers free cloud migration to lure new customers,"(Reuters) — Oracle announced on Wednesday it will migrate companies’ most complicated computer programs to its cloud for free as it tries to catch a new wave of potential cloud-computing clients by aiming to save them time and money. The pandemic prompted many businesses and governments to shift from in-house digital storage and computing to leased cloud servers from Amazon Web Services and other providers. A laggard in the cloud industry, Oracle is counting on its free support to persuade organizations that have not made a switch — or done so only partially — to opt for a move to its infrastructure. More than 100 customers have taken advantage of what Oracle is calling its Cloud Lift Services over the last six months, and the program is opening globally on Wednesday. “I’ll be surprised if any customer says ‘I don’t need this,'” Vinay Kumar, a senior vice president at Oracle, told Reuters. Over the last nine months, Oracle has shifted 1,000 workers to focus on Cloud Lift Services, Kumar said. He expects the offer will pay off for Oracle, but he declined to say how much the company has budgeted for it. Oracle’s expertise in using its own tools efficiently and setting up applications to work smoothly on the cloud speeds transitions and reduces the risk of customers encountering errors, he said. Rival cloud providers discount transition support but typically only for key customers or those that agree to long-term deals or minimum spending. Kumar said Oracle is not requiring commitments, but the program is limited to what he called the “handful of applications” that would be most difficult for a customer to migrate and excludes those requiring a thorough rewrite. Among early testers was new U.S. agribusiness client Cargill. With Oracle’s help, Cargill transitioned in weeks, not months, Kumar said."
https://venturebeat.com/2021/03/31/auditoria-raises-15-5m-to-automate-repetitive-accounting-tasks/,Auditoria raises $15.5M to expand AI platform automating repetitive accounting tasks,"Auditoria, a startup offering AI-driven automation products for corporate finance teams, today announced that it raised $15.5 million in series A funding. Cofounder and CEO Rohit Gupta says that the proceeds will shore up Auditoria’s investments in data science and the expansion of its sales, marketing, and customer success teams. . It’s estimated that accountants lose thousands of hours to repetitive tasks, follow-ups, error checking, and data entry. But studies show that the vast majority of these workloads can be automated. That may be why over 50% of respondents in a survey conducted by the Association of Chartered Certified Accountants said they anticipate the development of automated and intelligent systems will have an impact on finance over the next 30 years. Auditoria, which was founded in 2019, offers software that aims to streamline collections and procurement processes through the use of AI. A combination of machine learning, computer vision, robotic process automation, optical character recognition, and natural language processing enable Auditoria’s platform to augment the work of human accountants. The company claims it can handle certain tax obligations and expense management as well as vendor onboarding via auditable journal entries.  Auditoria’s workflow orchestration engine draws on data from clients’ email inboxes, apps, and other financial systems of record, leveraging algorithms to bring in human experts where necessary. Auditoria says its software can communicate with stakeholders using natural language while monitoring high-risk accounts with incentives and promotions for clients. Beyond this, Auditoria can respond to payment inquiries, capture supplier data, and automate the forecasting of financial key [performance indicators (KPIs). While 25-employee Auditoria competes with Botkeeper and Zeni in an accounting software market projected to be worth $19.59 billion by 2026, according to Mordor Intelligence, the company is among the first that’s enterprise resource planning-centric in Gupta’s mind. He says that more than 200 organizations are using Auditoria’s tools. “Despite launching out of stealth during the pandemic, Auditoria has seen incredible growth and traction,” Gupta said. “The team’s focused efforts to provide a purpose-built solution for corporate finance struck a chord with the forward-looking, next-gen finance executives looking to adopt software to drive greater efficiencies and increase resiliency across the back office.” Brian Ascher, partner at investor Venrock, added, “Finance operations automation is the future for corporate finance. We have been on the hunt to invest in the right team with the experience and innovative technology necessary to transform corporate finance, and we have found that in Auditoria. We envision a day when every global finance team is using Auditoria to streamline and automate back-office operations.” Venrock led Santa Clara, California-based Auditoria’s oversubscribed round announced today, which saw participation from Workday Ventures and existing investors including B Capital Group, Engineering Capital, Firebolt Ventures, and Neotribe Ventures."
https://venturebeat.com/2021/03/31/uniphore-nabs-140-million-for-automated-analysis-of-voice-and-video-calls/,Uniphore nabs $140 million for automated analysis of voice and video calls,"Uniphore, an AI-powered platform that helps businesses understand, analyze, and automate their voice-based customer service, has raised $140 million in a series D round of funding. The company said it plans to use the investment to expand its existing conversational AI and machine learning technologies deeper into the enterprise, with a particular focus on video-based applications. The genesis for this expansion actually dates back a couple of months to its acquisition of Emotion Research Lab, a Spanish startup that determines emotion and engagement levels through video-based interactions by tracking facial expressions and eye movement. Founded out of India in 2008, Uniphore offers a platform built around four core services: U-Self-Serve, designed to give businesses quick setup access to a conversational AI assistant; U-Analyze, which uses natural language processing (NLP) to glean insights and generate analytics from customer conversations; U-Trust, an automated voice authentication tool that helps companies verify an agent’s identity in the remote-working world; and U-Assist, which serves up real-time call transcriptions and in-call alerts. Uniphore, which opened a new U.S. HQ in Palo Alto in 2019, had previously raised $81 million and claims a roster of major enterprise clients, including BNP Paribas. Its latest investment was led by Sorenson Capital Partners, with participation from notable enterprise backers such as Cisco Investments. By adding video to its existing automated voice monitoring smarts, Uniphore is essentially looking beyond the customer service realm and into sales, marketing, and HR, among other business verticals. It’s focused anywhere companies may come face to face with people over video, which is particularly pertinent as the world has had to rapidly embrace remote work. In addition to expanding into video-based applications, Uniphore said it will invest in other areas around trust, security, and robotic process automation (RPA). This comes shortly after it acquired an exclusive third-party RPA license from NTT Data."
https://venturebeat.com/2021/03/31/productiv-which-develops-software-that-helps-enterprises-manage-saas-apps-raises-45m/,"Productiv, which develops software that helps enterprises manage SaaS apps, raises $45M","Productiv, a platform for aggregating real-time engagement data and insights for apps and management, today announced that it raised $45 million in a series C funding round, bringing its total raised to date to over $73 million. The company says it’ll use the funding to bolster growth after a year in which Productiv more than doubled its headcount and added customers including DocuSign, Kayak, PagerDuty, and Robinhood. Enterprise software-as-a-service (SaaS) adoption has never been higher. Companies use 16 SaaS apps on average, driving the global industry to an estimated $157 billion. But coinciding with this climb is a decline in app usage transparency. A recent survey of IT leaders conducted by Numerify found that 45% don’t have a complete picture of key apps and business health services, with 57% saying they lacked an overview of IT performance across projects and employees. The market’s relative opaqueness motivated Jody Shapiro, formerly head of product management for Google Analytics, to investigate a metrics-driven solution. Unable to find one, he developed his own in Palo Alto, California-based Productiv. “When I was at Google, I witnessed first-hand how Google Analytics’ quantification of marketing efforts transformed the CMO role, but marveled that nothing like that existed for CIOs,” Shapiro told VentureBeat via email. “I got together with my two cofounders Ashish Aggarwal, former engineering lead at Postmates and Amazon, and Munish Gandhi, former COO for LinkedIn Sales Navigator, to start Productiv.” Productiv’s cloud-based dashboard integrates with single sign-on tools to track login activity and extract purchase and license data from contracts, finance, and expense reporting systems. The platform surfaces usage data and over 50 different engagement dimensions that can highlight redundant apps, offering an organization-wide view of agreements and expired software. Configurable rules enable admins to reclaim licenses automatically, and usage logs — including charts that plot the number of engaged users, teams, and locations over time — make it easier to compare stats to industry benchmarks and to determine best practices that might boost productivity. “At the core of [the] platform is a complex, real-time analytics engine that automatically joins billions of data points from multiple sources, including user-level, feature-level engagement data from SaaS applications, login data from single sign-on providers, spend data from ERP providers, expense data from expense management providers, contract data from contract management providers, data from security providers, and more,” Shapiro explained. “Ingesting, normalizing, and joining all of this data is all automated.” The endgame is to empower teams to make profit-boosting rightsizing decisions from app analytics. Rather than determining whether a division has, say, dozens or hundreds of Dropbox licenses and how many team members used those licenses in the past fiscal quarter, with Productiv, CIOs can drill down into the productivity impact and estimate the potential cost savings of choosing to cancel, upgrade, or downgrade service. “Companies spend enormous amounts providing people with the tools they need to get their work done — more than $10,000 per employee annually — but these are open-loop investments without data to show how business value is delivered … Productiv gives CIOs actionable insights and intelligent recommendations, enabling them to focus on real, measurable business outcomes,” Shapiro continued. “Productiv helps enterprises understand exactly how their employees are using SaaS. Better usage and adoption data coupled with customizable automation of everyday tasks allow CIOs and their teams to focus on the projects that actually grow their business.” Productiv, which has 75 employees, claims to have hundreds of enterprise customers."
https://venturebeat.com/2021/03/31/opswat-expands-infrastructure-protection-with-anti-malware-tools-raises-125m/,"Opswat expands infrastructure protection with anti-malware tools, raises $125M","Infrastructure protection services company Opswat today announced that it raised $125 million from Brighton Park Capital, the startup’s first external funding round. Opswat says the capital will support its R&D and hiring efforts as it looks to acquire new customers. More than three quarters of IT security leaders anticipate a major breach involving a critical infrastructure organization in the near future, according to a Black Hat USA survey. It’s estimated that 31% of organizations have experienced cyberattacks on operational technology infrastructure. Perhaps unsurprisingly, Gartner found that organizations planned to invest $17.48 billion in infrastructure protection in 2020. San Francisco, California-based Opswat, which was founded in 2002,  protects critical infrastructure by attempting to eliminate malware and zero-day attacks. Its products focus on threat prevention and process creation for secure data transfer and safe device access. Toward this end, Opswat offers two platforms — MetaDefender for threat prevention and MetaAccess for cloud access control and endpoint compliance. MetaDefender leverages what Opswat calls content disarm and reconstruction — “deep CDR” – to remove threats from files by reconstructing the files while stripping out potentially malicious content. The platform also delivers multiscanning with over 30 antimalware engines, file-based vulnerability assessment, and proactive data loss prevention technologies, which spot and redact sensitive information like names, companies, subjects, GPS locations, authors, and more in emails and other messages. As for MetaAccess, it allows access to software-as-a-service apps and cloud data based on device health and compliance, helping to block potentially risky devices. MetaAccess and MetaDefender draw on Opswat’s endpoint compliance technology, which detects and classifies over 5,000 apps to enable monitoring, management, and real-time remediation. It’s complemented by anti-keylogger and screen capture prevention features that intercept and conceal keystrokes from malware and block unauthorized screengrabs by users, web collaboration tools, and apps. Opswat competes with Confluera, Erikos, IP Access, and others in a global cybersecurity market that’s valued at over $156.5 billion. But Opswat claims to have over 1,000 customers, including IBM, Comcast, American Express, the U.S. Department of Homeland Security, and 98% of U.S. nuclear power facilities. The company recently announced that its channel partner program now covers over 40 countries and will expand by 50% this year. Opswat would appear to be primed for growth when taking into account the recent spate of attacks on critical infrastructure including electricity, gas, and water systems,. Just last month, hackers attempted to poison a Florida city water supply by remotely accessing a server. And in 2015 and 2016, cyberattacks caused large-scale power outages in Ukraine. Founder and CEO Benny Czarny claims Opswat’s revenue grew 40% from 2019 to 2020 while annual recurring revenue hit 30%. With “record sales” in 2020, he says that Opswat intends to be ready for a stock market offering by 2021, which would make it among the first companies focused on infrastructure protection to go public. “Opswat’s mission is to protect the world’s critical infrastructure. This is an extremely important objective since it literally means helping ensure people worldwide can continue their daily lives uninterrupted,” Czarny told VentureBeat via email. “While we have built an amazing business over the past 18 years, I realize a financial partner can help us accelerate our progress toward achieving our mission.” Opswat employs over 400 employees across its 10 offices worldwide. Last December, the company relocated its headquarters to Tampa, Florida and plans to hire 100 employees in the city over the next three years."
https://venturebeat.com/2021/03/30/looking-for-a-new-job-then-you-need-to-check-these-out/,Looking for a new job? Then you need to check these out,"Are you on the hunt for a new job at the moment? First of all, good for you — it’s a pretty exciting decision to make for yourself. However, it can also be pretty scary, especially if you’re living through a global pandemic, am I right? The good news is that companies are very much hiring at the moment, despite everything that’s happening, so there’s no shortage of exciting roles to apply for. Here are just a handful of the kinds of roles up for grabs right now. This position is for a Staff Software Engineer with solid development experience who will focus on creating new capabilities for the Visa AI Platform while maturing the code base and development processes. In this position, you are first a passionate and talented developer that can work in a dynamic environment as a member of Agile Scrum teams. Your strong technical leadership, problem-solving abilities, coding, testing and debugging skills is just a start. You must be dedicated to filling product backlog and delivering production-ready code. You must be willing to go beyond the routine and prepared to do a little bit of everything. You will be an integral part of the development team, sometimes investigating new requirements and design and at times refactoring existing functionality for performance and maintainability, but always working on ways to make Visa more efficient and provide better solutions to customers. The Bizinfra team that is part of the core development of Outbrain’s Business Technology, is looking for a leader! As a Bizinfra Team Lead you will lead backend developers (local and offshore) and QA engineers. You will be working closely with Randamp;D, MIS (Information System), Solution Architect, Business and Finance stakeholders, data infra, and more. The successful candidate will be responsible for designing, developing, and maintaining processes that affect both customer-facing applications and integrations with back-office systems while also building and maintaining processes to support billing systems. The role of the Redbull Communications function is to develop the overarching messaging and communications strategy for the brand, and to get people to talk about Red Bull through media coverage, opinion leader engagement, and partner content amplification. The function grows media relationships, brings opinion leaders into the Red Bull world through experiences, and works with partners to share messages. Reporting to the Director of Marketing of the Southwest Region, you will lead the Communications efforts for the region, with the goal of increasing awareness of and affinity for the Red Bull brand and its local activations, events, athletes, and media projects. You will develop the annual Southwest Regional Communications plan, ensuring all communications efforts are accomplished on-budget and achieve their planned engagement targets. Zurich Instruments is the technology leader for advanced test and measurement instruments. As a marketing team, they are proud of their products and passionate about finding creative and effective ways of how to promote these to researchers and industrial customers around the world. Are you a self-starter keen to work in an international team? Then apply now as a Marketing Specialist who will strengthen the Marketing & Sales team in Cambridge, MA. Supported by teams from across three continents, you will become the first marketer in the U.S. office and will therefore have the unique chance to drive and further develop marketing initiatives in the region. You will join McKinsey’s Technology & Digital organization as a core member of both the Product Management function and as part of the Data, BI and Analytics Group committed to solving problems for colleagues and solutions using technology. You will focus on products designed to enable data-driven decisions and deliver insights. The Product Managers work closely in teams alongside experts in various disciplines — designers, researchers, engineers, analysts, and others — and together, the team relentlessly strives to deliver outcomes that matter to people. You and your team will work together to discover opportunities, experiment, test, and learn, and deliver solutions using a mix of methodologies, including design thinking to help understand people’s needs, wants, and problems; lean methodologies to experiment fast and learn a lot; and agile approaches to reduce uncertainty by working in short dev cycles and pausing to reflect and adapt. Head on over to VentureBeat jobs now for even more brilliant opportunities."
https://venturebeat.com/2021/03/30/armv9-is-arms-first-major-architectural-update-in-a-decade/,Armv9 is Arm’s first major architectural update in a decade,"Arm, the leader in chips used in everything from mobile devices to supercomputers, has unveiled Armv9, the company’s first major architectural change in a decade. The new designs should result in 30% faster performance over the next two chip generations. Arm is a chip architecture company that licenses its designs to others, and its customers have shipped more than 100 billion chips in the past five years. Nvidia is in the midst of acquiring Cambridge, United Kingdom-based Arm for $40 billion, but the deal is waiting on regulatory approvals. In a press briefing, Arm CEO Simon Segars said Armv9 will be the base for the next 300 billion Arm-based chips. Arm’s customers have shipped more than 180 billion chips to date, and those chips touch more than 70% of the world’s population, Segars said. “We’re extremely excited to be sharing Arm’s vision of the next decade of computing with you,” Segars said. The new architecture has processing that balances economics, design freedom, and accessibility advantages of general-purpose computing devices with specialized processors that handle tasks like digital signal processing and machine learning. The company says Armv9 also takes security and artificial intelligence features to new levels. Arm previously launched its Armv8 architecture in 2011, and that became its most successful platform in history as the foundation for smartphone chips, internet of things (IoT) devices, and a wide range of industrial devices. Arm has more than 6,500 employees, about 80% of whom are engineers. At the current rate, 100% of the world’s shared data will soon be processed on Arm; either at the endpoint, in the data networks or the cloud, Segars said. Such pervasiveness conveys a responsibility on Arm to deliver more security and performance, along with other new features in Armv9, he added. The new capabilities in Armv9 will accelerate the move from general-purpose to more specialized compute across every application as AI, IoT, and 5G gain momentum globally. Back in 2011, Arm launched its 64-bit processing architecture, enabling Arm devices to make the leap from low-power mobile devices to high-end supercomputers. “The Arm architecture is not a static thing. We keep on innovating and evolving to meet the ever changing needs of the computing world,” said Richard Grisenthwaite, chief architect, in a press briefing. “In our increasingly connected world, we’re seeing Arm processors being used at all stages. The collection of data often starts with ultra-low-power IoT devices based on the Arm profile processes, or from the Arm-based smartphones that virtually all of us carry all of the time. … It continues to be the processor of choice.” To address the greatest technology challenge today — securing the world’s data — the Armv9 roadmap introduces the Arm Confidential Compute Architecture (CCA). Confidential computing shields portions of code and data from access or modification while in use, even from privileged software, by performing computation in a hardware-based secure environment. The Arm CCA will introduce the concept of dynamically created Realms, usable by all applications, in a region that is separate from both the secure and non-secure worlds. Segars said that Realms are much like software containers, which isolate code in certain ways, but with hardware support. For example, in business applications, Realms can protect commercially sensitive data and code from the rest of the system while it is in use, at rest, and in transit. In a recent Pulse survey of enterprise executives, more than 90% of the respondents believe that if confidential computing were available, the cost of security could come down, enabling them to dramatically increase their investment in engineering innovation. “The Arm Confidential Compute architecture will introduce the concept of dynamically created Realms, usable by ordinary programs in a separate computation world from either the non-secure or secure world that we have today,” Grisenthwaite said. “Realms use a small amount of trust and a testable management software that is inherently separated from the operating system.” The ubiquity and range of AI workloads demands more diverse and specialized solutions. For example, it is estimated there will be more than eight billion AI-enabled voice-assisted devices in use by the mid-2020s, and 90% or more of on-device applications will contain AI elements along with AI-based interfaces like vision or voice. To address this need, Arm partnered with Fujitsu to create the Scalable Vector Extension (SVE) technology, which is at the heart of Fugaku, the world’s fastest supercomputer. Building on that work, Arm has developed SVE2 for Armv9 to enable enhanced machine learning (ML) and digital signal processing (DSP) capabilities across a wider range of applications. “I am excited about the new generation of Arm instruction sets and technological capabilities,” said Patrick Moorhead, an analyst at Moor Insights & Strategies. “Performance-wise, Arm is making it easier to integrate ML capabilities into the end product. It’s important to recognize that for most performance cases, especially CPU, it’s more about the architecture of the design versus the instruction set. So in other words, chip designers still need to architect something performant. Security is dramatically improving too, and if we had these technologies fully enabled today, it could ward off most all of the known attacks. I also think Arm thought about the future with ‘Realms’ even though it won’t be out day one.” SVE2 enhances the processing ability of 5G systems, virtual and augmented reality, and ML workloads running locally on CPUs, such as image processing and smart home applications. Over the next few years, Arm will further extend the AI capabilities of its technology with substantial enhancements in matrix multiplication within the CPU, in addition to ongoing AI innovations in its Mali graphics processing units (GPUs) and Ethos network processing units (NPUs). Segars noted that one customer, Johnson Controls, has been working on automation and control equipment in buildings for more than a century. “They’re a major user of Arm-based chips, and now we’re talking to them about the enhanced AI and security features coming with the new Armv9 architecture being launched today,” Segars said. “One upgrade JC is considering is the use of AI-powered digital twins monitoring key equipment in real time within the company, as well as aggregating data in the cloud.” Johnson Controls has already used Arm chips to manage chiller systems and cut energy use by more than 50%. Segars also said that Arm-based devices could prove that someone has been vaccinated against COVID-19. The smartphone could be used for that, and it could store medical information, but to be comfortable with that, Segars said he would want advanced encryption running on the device beyond what is possible today. He would want features like memory tagging to help eliminate memory cybersecurity issues. “Our first smartphone product with an Armv9 CPU will be commercially available by the end of this year,” Segars said. Besides security, Armv9 supports specialized AI, DSP, and XR workloads. Segars said he also expects Arm’s combination with Nvidia will advance areas such as graphics computing. Over the past five years, Arm designs have increased CPU performance annually at a rate that outpaces the industry, Segars said. He added that Arm will continue this momentum into the Armv9 generation with expected CPU performance increases of more than 30% over the next two generations of mobile and infrastructure CPUs. However, as the industry moves from general-purpose computing toward ubiquitous specialized processing, annual double-digit CPU performance gains are not enough. Along with enhancing specialized processing, Arm’s Total Compute design methodology will accelerate overall compute performance through focused system-level hardware and software optimizations and increases in use-case performance. By applying Total Compute design principles across its entire IP portfolio of automotive, client, infrastructure, and IoT solutions, Armv9 system-level technologies will span the entire IP solution, as well as improving individual IP. Additionally, Arm is developing several technologies to increase frequency, bandwidth, and cache size, and reduce memory latency to maximize the performance of Armv9-based CPUs. “There was very little detail in the disclosures. Realms should improve security, particularly for multiuser cloud systems (such as Amazon Web Services),” said Linley Gwennap, principal analyst at the Linley Group, in an email. “The memory protection stuff sounded interesting, but it seems like it might be years away. In summary, v9 offers a much smaller improvement than v8.” Grisenthwaite said that addressing the demand for more complex AI-based workloads is driving the need for more secure and specialized processing, which will be the key to unlocking new markets and opportunities. “It’s been an amazing, tragic, and enlightening year, no matter where we’ve been living or working,” Segars said. “Now, it’s time to rebuild a world that’s inherently more resilient. In computers, one of the most urgent needs is expanding the data processing capacity in the cloud. We can’t just do that at any cost. Transforming the cloud isn’t just about the more. It’s about different, especially when it comes to the performance per watt of traditionally power-hungry datacenter chips.” Arm collected supporting comments from customers including Ampere Computing, Cadence, Crytek, Foxconn, Fujitsu, Google, Marvell, MediaTek, Nvidia, NXP, Oppo, Red Hat, Renesas Electronics, Samsung, Siemens, Synopsys, Unity Technologies, Vivo, VMware, and Xiaomi Group. “We’re not just focusing on the CPU and GPU either, but looking at all of compute, as well as maximizing performance by deploying new system technologies that provide additional gains,” Segars said. “And we are broadening the architecture to execute even more compute, such as DSP and AI on the CPU.”"
https://venturebeat.com/2021/03/30/elistair-the-pioneer-in-tethered-uavs-announces-a-e5m-series-b-round-to-accelerate-its-international-expansion/,"Elistair, The Pioneer in Tethered UAVs, Announces a €5M Series B Round to Accelerate Its International Expansion","   LYON, France & BOSTON–(BUSINESS WIRE)–March 30, 2021– Elistair, a market leader in tethered drones, has raised €5M from Omnes and Starquest Capital, its historic investor. This Series B round follows an initial €2M funding round in 2018. Since its creation in 2014, Elistair’s solutions have already been deployed by military forces and national security agencies in over 65 countries. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210330005817/en/ Automated tethered drones to protect military bases, surveil borders and provide security for major events Elistair counts national security and defense organizations amongst its customers, offering them two patented product ranges: an automated tethered drone: Orion 2 and tether stations capable of transforming free-flying drones into tethered drones. With their ability to detect vehicles more than 6 miles away, The Orion 2 drone acts as an aerial reconnaissance mast, substantially increasing the field of vision of operating forces and reducing vulnerable entry points. Orion 2, provides clear, and live information which is key for border surveillance and protection of perimeters and events. They are also used to deploy tactical communication networks, greatly increasing the range and coverage of military radios. Thanks to its micro-tether technology, Elistair has solved the problem of a drones limited battery life. When connected to a generator or the electrical grid, the micro-tether powers the drone continuously, allowing it to function for 24 hours without interruption. It also provides secure data transfer between the drone and its operator through the tether. This means that the camera feed cannot be saturated and provides protection against hacking and other interference. “Our tethered drones let units on the ground expand their field of vision considerably, as well as their understanding of their nearby environment during operations, which is often a critical factor. This funding round will allow us to consolidate our range of products for the national security and defense segment and to handle the significant amount of orders from recent months.” – Guilhem de Marliave, Elistair CEO International presence and investment in R&D The reliability and sturdiness of Elistair’s products have already earned recognition in the US, where the company conducts more than 40% of its business. After opening a subsidiary in Boston, the French start-up will be increasing its production capacity in order to assert its foothold in the American market, while reinforcing its presence in the European market. “We plan to accelerate the industrialization of our production processes and build up our R&D capacity. Our technological roadmap aims to develop the next generations of tethered drones capable of responding to any and all kinds of situations and conditions. We will also be boosting our solutions’ intelligence and automation.” – Timothée Penet, Elistair CTO Elistair is launching a campaign to recruit around 20 people for its Lyon and Boston sites. The goal is to retain its initial approach – a combination of technological innovation and knowledge of conditions in the field – thanks to a multidisciplinary R&D team and former defense operatives (from the Royal Air Force, Navy SEALs and the French Navy). “Elistair is a leading name on the tethered drone market, which is still a relatively new market, but one that is growing quickly. Guilhem and Timothée have demonstrated an impressive execution capacity from the outset, and the company displays a rare level of business maturity after only a few years in operation. This funding will allow the company to shore up its technological leadership, and we are very eager to see the next innovations to be released on the market.” – Fabien Collangettes, Venture Capital Principal, Omnes About ElistairCreated in 2014 and based in Lyon (France), and Boston (USA), Elistair designs and manufactures tethered drone solutions for tactical surveillance and civil defense missions. Elistair’s products are used by armed forces, civil security services and a number of private companies, in more than 60 countries. Elistair was co-founded by Guilhem de Marliave and Timothée Penet, both of them alumni of the scientific graduate school Centrale Lyon.https://elistair.com/ About OmnesOmnes is a leading private equity and infrastructure investor. With €5 billion in assets under management, it provides companies with the capital they need to fund their growth, in three key areas: venture capital, growth & buyout capital and infrastructure. Omnes is wholly-owned by its employees. It is committed to ESG issues and has founded the Fondation Omnes to fund initiatives in favour of children and young people. It is a signatory to the United Nations Principles for Responsible Investment (PRI). www.omnescapital.com  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005817/en/ Press Contact – Elistair Gwenaelle Le Cocguen – g.lecocguen@elistair.com – +33 (0)6 67 10 62 22 Press Contact – Omnes Aurélie Blanchard-Massoni – aurelie.blanchard-massoni@omnescapital.com – +33 (0)7 63 13 65 74"
https://venturebeat.com/2021/03/30/cisco-integrates-networking-security-it-management-for-the-enterprise/,"Cisco integrates networking, security, IT management for the enterprise","At its an online Cisco Live! event, Cisco today advanced an ambitious effort to further unify the management of networking, security, and IT infrastructure across the extended enterprise using hardware and software that can now be acquired under a single subscription license dubbed Cisco Plus. In addition, Cisco is making it possible to acquire all the elements of its Secure Access Service Edge (SASE) portfolio via a single offering that will also soon be made available via a subscription. And Cisco announced that the Duo authentication platform it acquired in 2018 can now be configured to enable users to log into cloud applications using biometrics or security keys that eliminate the need to employ passwords, in addition to updating a Cisco Secure X platform through which it unifies the management of its security portfolio. Finally, Cisco has — as expected — integrated the ThousandEyes monitoring service it acquired last year with the Cisco Catalyst 9000 switches and the Cisco AppDynamics Dash Studio that provides visibility into an observability platform the company acquired in 2017. As the management of networking and security becomes flatter across an extended enterprise, it’s clear Cisco is making a case for integrating the management of networking and security to reduce total cost at a time when more highly distributed applications that tend to be latency-sensitive are being deployed. In some cases, networking platforms based on proprietary processors will be required to maximize throughput, while platforms based on commodity processors will suffice in others, said Todd Nightingale, senior vice president and general manager for the Enterprise Networking and Cloud business at Cisco. Cisco makes available a series of management overlays and control planes that — in addition to AppDynamics and SecureX — includes Cisco Intersight to manage IT operations, regardless of the underlying class of processors or platforms employed. The company, for example, has extended Cisco Intersight to also manage cloud infrastructure resources across a hybrid cloud computing environment that now includes a growing number of edge computing platforms. In effect, Cisco is making a case for lowering the total cost of IT by relying on a single vendor to unify as much of the underlying IT and security infrastructure as possible. ThousandEyes is a critical element of that strategy because the platform enables IT teams to gain insights into network bottlenecks that are impacting application performance even when that network infrastructure is managed by a third-party telecommunications carrier. “It provides full visibility from the user to the application,” Nightingale said. It’s not clear to what degree IT organizations are employing the entire Cisco portfolio. Organizations may, for example, employ Cisco networking hardware while relying on a platform other than AppDynamics to monitor applications. The decision to acquire those platforms is often made by completely different teams within an enterprise. Via various subscription services, Cisco is trying to entice organizations to standardize on a wider range of offerings. Those subscription offerings can over time reduce the total cost of IT. At the same time, it reduces any economic incentive an organization might have to swap in a rival platform because future Cisco upgrades are now included as part of the subscription service. It remains to be seen how many enterprise IT organizations are willing to subscribe to offerings from a single vendor. But there is no doubt more organizations are acquiring IT technologies via subscription services that make it possible to treat the acquisition of IT as an operating rather than capital equipment expense. In effect, the same consumption-based pricing models that are commonly found in the cloud are now being applied to all IT infrastructure and security offerings. The tradeoff is that while those underlying platforms and systems have arguably never been more open, the mechanism that locks an IT organization into one vendor versus another is now embedded within the subscription."
https://venturebeat.com/2021/03/30/github-launches-secret-scanning-for-private-repositories-into-general-availability/,GitHub’s secret scanning for private repositories enters general availability,"GitHub has announced that its enterprise-focused secret scanning tool for private repositories is now generally available. The Microsoft-owned code-hosting platform first debuted secret scanning for private repositories last May as part of its advanced security program. This was introduced in beta alongside a new native code-scanning tool that automatically scans every git push for vulnerabilities. Code scanner launched in general availability in September and is followed today by secret scanning. In related news, GitHub also announced the beta launch of a new “security overview” tool that gives security teams a single interface to view all the risks detected by GitHub’s advanced security tools. These span code scanning, secret scanning, and Dependabot. The overview highlights known and unknown security risks, where teams haven’t fully configured their security features. “Secrets” refers to authentication credentials such as API tokens, passwords, and keys that protect access to applications, services, and other sensitive areas of a company’s digital infrastructure. GitHub first launched secret scanning — then known as “token scanning” — for public repositories back in 2018. It’s designed to help companies identify sensitive data hidden inside their public code before it’s found by bad actors. There has been a flurry of activity in the secrets management space of late, with GitGuardian raising $12 million in funding a few months back to help companies detect sensitive data hidden in their code repositories and Doppler raising $6.5 million in a round of funding led by Alphabet’s GV to expand into the enterprise. Recent data from GitGuardian indicates a 20% rise in secrets hidden in public GitHub repositories last year, a trend driven in part by a broader push toward code collaboration platforms as developers and businesses rapidly embraced remote work. Businesses that use GitHub for private (i.e. non-open source) projects can buy a GitHub advanced security license as part of their Enterprise Cloud (hosted) or Enterprise Server (self-hosted) subscription, which gives them access to secrets scanning. In the 10 months since it first arrived in beta, GitHub said it has helped organizations find and revoke more than 5,000 secrets. Since its beta launch last year, GitHub has added a bunch of new features, though some are currently only available for the GitHub Enterprise Cloud edition. These include an API and support for webhooks to set up secret scanning alerts, while GitHub has also expanded its secret scanning pattern coverage to incorporate tokens from more than 35 companies, including Shopify, Stripe, AWS, Azure, SendGrid, Twilio, and Slack. Earlier today, GitHub also launched new granular controls for the GitHub mobile app, designed to boost developers’ productivity by helping them manage notifications and pause them at the end of a shift."
https://venturebeat.com/2021/03/30/pinterest-open-sources-big-data-analytics-tool-querybook/,Pinterest open-sources big data analytics tool Querybook,"Pinterest today open-sourced Querybook, a data management solution for enterprise-scale remote engineering collaboration. The company says the tool, which it uses internally, can help engineers compose queries, create analyses, and collaborate with one another via a notebook interface. Querybook started in 2017 as an intern project at Pinterest. The development team early on decided on a document-like interface where users could write queries and analyses in one place, with collocated metadata and the simplicity of a note-taking app. Released internally in March 2018, Querybook became the go-to solution for big data analytics at Pinterest. It now averages 500 daily active users and 7,000 daily query runs. “With Querybook, Pinterest engineers have brought together the power of metadata with the simplicity of a note-taking app for a better querying interface, where teams can compose queries and write analyses all in one place,” a spokesperson told VentureBeat. “Querybook can be set up and deployed in minutes.” Every query executed on Querybook gets analyzed to extract metadata like referenced tables and query runners. Querybook uses this information to automatically update its data schema and search ranking, as well as to show a table’s frequent users and query examples. The more queries in Querybook, the better documented the tables become. Querybook also features an admin interface that lets companies configure query engines, table metadata ingestion, and access permissions. From this interface, admins can make live Querybook changes without going through code or config files. And they can create visualizations, including lines, bars, stacked areas, pies, donuts, scatter charts, and table charts. “The common starting point for any analysis at Pinterest is an ad-hoc query that gets executed on the internal Hadoop or Presto cluster. To continuously make these improvements, especially in an increasingly remote environment, it’s more important than ever for teams to be able to compose queries, create analyses, and collaborate with one another,” Pinterest wrote in a blog post. “We built Querybook to provide a responsive and simple web user interface for such analysis so data scientists, product managers, and engineers can discover the right data, compose their queries, and share their findings.” Pinterest previously open-sourced Teletraan, a tool that can deploy code onto virtual machines, such as those available from public cloud Amazon Web Services. Prior to this, the company released Terrapin, software designed to more efficiently push data out of the Hadoop open source big data software and make it available for other systems to use."
https://venturebeat.com/2021/03/30/sonar-which-monitors-companies-salesforce-tech-stack-for-changes-raises-12m/,"Sonar, which monitors companies’ Salesforce tech stack for changes, raises $12M","Salesforce has emerged as a formidable force in the cloud-based enterprise software sphere — a $200 billion colossus upon which countless others have built their own billion-dollar businesses. In the past couple of months alone, we’ve seen OwnBackup raise $167.5 million at a $1.4 billion valuation to power cloud data backups for Salesforce; Scratchpad lock down $13 million to develop a productivity workspace for Salesforce teams; and Copado secure $96 million for Salesforce-native DevOps. According to some estimates, the Salesforce ecosystem could be at least 4 times larger than the company itself, a factor that has played no small part in Salesforce’s success over the past two decades. Against this backdrop, Sonar today announced it has raised $12 million from a slew of big-name investors, including David Sacks’ Craft Ventures and Slack’s venture capital fund, to bring “X-ray vision” to Salesforce by helping sales teams visualize how all their data is connected and used across related systems. “It’s essentially a living, searchable dictionary that shows how your entire tech stack works together and automatically documents every change to your data,” Sonar CEO and cofounder Brad Smith told VentureBeat. “This allows teams to scope and execute their work fast, work confidently without the risk of taking critical systems offline due to breakages, and collaborate effortlessly around change-management projects, digital transformation, systems integrations, building new processes, and more.” Although Sonar is now focused purely on Salesforce, and showing how other systems are mapped to Salesforce fields and processes, later this year it plans to “start serving more operations teams across the enterprise,” according to Smith. This will include expanding to marketing platforms such as Marketo, HubSpot, and Pardo, as well as finance systems such as Netsuite. “Change management” is perhaps the key to understanding what Sonar actually does. When someone in a company alters any fields or automations in a critical system, it can be difficult to see how this might impact other processes that rely on it — but the smallest changes can break integrations, lead to inaccurate reports, and — perhaps more importantly — prevent sales teams from closing deals. For a platform like Salesforce that thrives on integrations, Sonar shows its users how all their data is joined together and issues real-time alerts when anything that might impact them is changed. Sonar can be used before changes are made to determine how (or whether) they should be implemented, and it can also provide visibility into problems that occur as a result of changes after they have been made. This is particularly important for teams that are transitioning any of their existing workflows or integrations, as Sonar helps them reverse-engineer processes that may have been set up a while ago by someone who is no longer at the company and understand how everything fits together. “Sonar gives you complete situational awareness so you can understand the impact of changes before they happen and correct problems if and when they occur,” Smith said. “This helps teams prioritize their work, work faster, and execute changes safely. In the event someone does make a change that causes processes to break, Sonar will alert you to exactly what broke and why.” At its core, Sonar is designed to help revenue and operations teams avoid making blind changes and then scrambling to fix problems after. According to Smith, Sonar does something “similar to what GitHub does for software engineers,” in terms of how it documents every change you make and records the full history and context around it. But rather than focusing on codebases, Sonar is concerned with integrations across a company’s tech stack, allowing non-technical teams to handle at least some of the heavy lifting behind the scenes. Sonar is available as a standalone web application, though the company also offers a browser extension that overlays data on top of Salesforce to help visualize dependencies across every field. Founded out of Atlanta, Georgia in 2018, Sonar had previously raised $3.7 million. It said that over the past year its customer count and revenue have increased by 1,000% and 3,000%, respectively. It has also signed up notable enterprise clients, including OneLogin and Carta."
https://venturebeat.com/2021/03/30/docebo-launches-multi-product-learning-suite-to-address-challenges-across-entire-learning-lifecycle/,Docebo Launches Multi-Product Learning Suite to Address Challenges Across Entire Learning Lifecycle,"TORONTO–(BUSINESS WIRE)–March 30, 2021– Leading AI-powered learning platform, Docebo Inc. (“Docebo”) (Nasdaq:DCBO; TSX:DCBO), announced today the launch of a multi-product learning technology suite. Previously, Docebo has focused on solving the problem of how organizations deliver training with its flagship learning management system (LMS). With the launch of Docebo Learning Suite, Docebo will go beyond content delivery and address challenges across the entire learning lifecycle, from content creation and management to measuring learning impact and key business drivers. The launch of Docebo Learning Suite coincides with the launch of Docebo Shape, a content creation product built on AI. Developed internally, Docebo Shape enables businesses to bring more internal experts into their elearning content strategy by leveraging AI to create engaging learning content in minutes. Including Docebo Shape, the core products that come together to transform the company’s offering into a cohesive learning suite include: “Our vision is to build a learning suite for the future that addresses every enterprise learning requirement so our customers have a one-stop shop for all their learning needs,” said Docebo’s Chief Executive Officer, Claudio Erba. “Today’s announcement is the result of both a previous acquisition and internal R&D, and it’s a testament to the innovation that exists in Docebo’s DNA.” To learn more about Docebo’s Enterprise Learning Suite, visit www.docebo.com. About Docebo Docebo is redefining the way enterprises leverage technology to create content, deliver training, and understand the business impact of their learning experiences. With Docebo’s multi-product learning suite, enterprises around the world are equipped to tackle any learning challenge and create a true learning culture within their organization. Forward-Looking Information This press release may contain “forward-looking information” and “forward-looking statements” (collectively, “forward-looking information”) within the meaning of applicable securities laws, including, without limitation, statements regarding: Company’s business; future business strategy; the launch of new products and features; the learning management industry; our growth rates and growth strategies; addressable markets for our solutions; the achievement of advances in and expansion of our platform; expectations regarding our revenue and the revenue generation potential of our platform and other products; our business plans and strategies; and our competitive position in our industry. This forward-looking information is based on our opinions, estimates and assumptions that, while considered by the Company to be appropriate and reasonable as of the date of this press release, are subject to known and unknown risks, uncertainties, assumptions and other factors that may cause the actual results, level of activity, performance or achievements to be materially different from those expressed or implied by such forward-looking information, including, without limitation: risks related to the COVID-19 pandemic and its impact on Docebo, economic conditions, and global markets; and other unforeseen events, developments, or factors causing any of the aforesaid expectations, assumptions, and other factors ultimately being inaccurate or irrelevant, and those factors discussed in greater detail under the “Risk Factors” section of our Annual Information Form for the year ended December 31, 2020, available under our profile on SEDAR at www.sedar.com, and should be considered carefully by prospective investors. If any of these risks or uncertainties materialize, or if the opinions, estimates or assumptions underlying the forward-looking information prove incorrect, actual results or future events might vary materially from those anticipated in the forward-looking information. Although we have attempted to identify important risk factors that could cause actual results to differ materially from those contained in forward-looking information, there may be other risk factors not presently known to us or that we presently believe are not material that could also cause actual results or future events to differ materially from those expressed in such forward-looking information. There can be no assurance that such information will prove to be accurate, as actual results and future events could differ materially from those anticipated in such information. No forward-looking statement is a guarantee of future results. Accordingly, you should not place undue reliance on forward-looking information, which speaks only as of the date made. The forward-looking information contained in this press release represents our expectations as of the date specified herein, and are subject to change after such date. However, we disclaim any intention or obligation or undertaking to update or revise any forward-looking information whether as a result of new information, future events or otherwise, except as required under applicable securities laws. All of the forward-looking information contained in this press release is expressly qualified by the foregoing cautionary statements.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005315/en/ Press: Nesh PillayDirector of PR and Communicationspress@docebo.com Investors: Dennis FongInvestor Relationsinvestors@docebo.com"
https://venturebeat.com/2021/03/30/github-boosts-developer-productivity-with-new-mobile-notification-controls/,GitHub boosts developer productivity with new mobile notification controls,"GitHub is rolling out a handful of new updates to its mobile and desktop apps, including “enhanced” push notifications with more granular controls and the ability to pause them altogether. The Microsoft-owned code-hosting platform said the update is part of its push to support the burgeoning hybrid and remote workforce, which relies on asynchronous communications. Nicole Forsgren, VP of research and strategy at GitHub, recently wrote about developer productivity in a co-authored article published in ACM Queue. The paper notes that ensuring efficient software development and the well-being of developers has “never been more important,” with the rapid shift to remote work creating a potential disconnect between developers and their usual workspaces and teams. “This forced disruption and the future transition to hybrid remote/colocated work expedites the need to understand developer productivity and well-being, with wide agreement that doing so in an efficient and fair way is critical,” the coauthors wrote. GitHub launched its mobile app for Android and iOS a year ago, but at the time it only supported push notifications for messages that include a direct mention of the developer — and with good reason. “Push notifications [were] one of the very first features we added via a cross-team hack week with the GitHub notifications team,” Ryan Nystrom, senior director of engineering at GitHub, told VentureBeat. “From that work, we created early versions of pushes for any type of activity, but we knew that without controls this could overwhelm users. “Notification fatigue is real, so we decided to start at a very high signal with lower volume through the initial direct mentions notifications.” In other words, developers could end up drowning under a deluge of alerts, particularly when they’re supposed to be offline. And so over the past year, GitHub has been taking on feedback from developers to figure out what additional notifications and controls could help them manage their time and productivity. With this latest update, developers can toggle push notifications on and off not only for when they’ve been directly mentioned, but when they’ve been asked to review a pull request, assigned a task, or asked to approve a deployment for a protected branch. This is important because a manager or senior developer might need to approve key stages in a project when they’re on the move or otherwise not at their desktop. “One of the core principles of the mobile app is that we’re helping unblock people,” Nystrom said. “Deploy approvals are a new flow for GitHub — for developers using GitHub mobile, we knew immediately it’d be valuable to get notified when your review is requested so you can unblock a deploy without the need to be at your computer.” Related to this, GitHub for mobile also now lets developers set custom working hours, meaning users can specify when push notifications will be sent to their phone. This fits a push across the technology spectrum to foster a healthier work-life balance — Google, for example, rolled out “focus mode” in 2019 to help users minimize and control alerts on their mobile devices. Elsewhere, the GitHub mobile app now lets developers view releases natively inside the app, rather than linking the user through to a web view. “This was also one of our most-requested features,” Nystrom added. Along similar lines, GitHub users can also now customize their repository “watch” settings from mobile. Much as it works on the browser version, they can now opt in to a very specific subset of actions they’d like notifications for in their inbox, such as issues, pull requests, releases, and discussions. Over in the desktop realm, GitHub launched version 2.7 of its desktop app that makes it easier for developers to copy individual or multiple commits between branches (known as “cherry-picking”) using a drag-and-drop tool. According to GitHub staff engineering manager Billy Griffin, developers would previously have to go to the command line and look up the Git cherry-pick documents to remember the correct syntax to copy the commits, but the drag-and-drop option makes this process more visual and intuitive."
https://venturebeat.com/2021/03/30/cloudera-foundation-merger-strengthens-data-science-and-ai-support-for-nonprofits/,Cloudera Foundation merger strengthens data science and AI support for nonprofits,"Today, the Patrick J. McGovern Foundation and the Cloudera Foundation, software company Cloudera’s philanthropic arm, announced a merger that will launch an initiative to accelerate data and AI maturity within nonprofits. In a definitive agreement, Silicon Valley-based Cloudera Foundation will merge its staff, $9 million endowment, and $3 million in grants with the Patrick J. McGovern Foundation, a $1.5 billion data science and AI philanthropic organization. The companies say the transaction is expected to close in the second quarter of 2021, subject to regulatory approval. Nonprofits face significant barriers to leveraging big data analytics and AI tools for driving strategy, growth, and global impact. The gaps in knowledge and technical expertise translate to opportunity costs in efficiency, impact, and scalability. A 2019 survey by PwrdBy found that nonprofit-specific AI is reaching fewer than 23% of organizations worldwide. In that same report, 83% of respondents said they believe ethical frameworks need to be defined before the nonprofit sector sees “full adoption” of AI. “Data science and AI-based tools, deployed with purpose and social conscience, could improve nearly every part of the human experience. Enabling civil society to access these technologies will unlock transformational approaches to many of the world’s greatest challenges,” Patrick J. McGovern Foundation president Vilas Dhar said in a press release. “Backed by philanthropic capital and deep technical expertise, we are working together with social changemakers to advance a tech-enabled, human-centered future.” The Cloudera Foundation, which was founded in 2017 with the mission of providing technology and mentorship to foster data expertise in the civil sector, has piloted its approach with seven nonprofits. The Patrick J. McGovern Foundation plans to broaden the foundation’s efforts post-merger to up to 100 organizations addressing the United Nations (UN) Sustainable Development Goals, the collection of 2030 goals set by the UN General Assembly as “a blueprint to achieve a better and more sustainable future for all.” Cloudera Foundation CEO Claudia Juech will direct activities around data enablement for nonprofits as VP of Data and Society at the Patrick J. McGovern Foundation, a new program. Data and Society will offer a range of services, including public workshops, multi-year data science development collaborations, and accelerator partnerships focusing on extracting actionable insights from datasets. For example, Data and Society could help organizations combating climate change better manage their resources. In a report on the civil sector’s adoption of AI, the Brookings Institute cites a partnership between The Nature Conservancy and the Red Cross to create a dashboard incorporating social media data on floods for city planning — an effort that might assist cities in becoming more sustainable. “AI can enable nonprofits to manage a broad range of activities. In conjunction with machine learning and data analytics, it is a way to control costs, handle internal operations, and automate routine tasks within the organization,” the report’s coauthors wrote. “Adoption of these tools can help groups with limited resources streamline internal operations and external communications and thereby improve the manner in which they function.” Cloudera cofounder and Cloudera Foundation chair Mike Olson added: “Our missions couldn’t be better matched. We have seen how data and analytics enable nonprofits to work more effectively and increase their impact. With the leadership and resources of the Patrick J. McGovern Foundation, our team can do even more to help mission-driven organizations change the world.”"
https://venturebeat.com/2021/03/30/celential-ai-which-matches-software-engineers-with-jobs-raises-9-5m/,"Celential.ai, which matches software engineers with jobs, raises $9.5M","AI-powered software engineering recruitment platform Celential.ai today announced that it raised $9.5 million in series A funding. It comes as Celential appoints Amer Akhtar, former Yahoo small business president and ex-CEO of ADP China, as the company’s new CEO. Ncube estimates that sometime this year, the software industry will experience a shortage of 1.4 million engineers. Meanwhile, the employment of software developers is projected to grow 21% from 2018 to 2028. The cost of hiring will likely remain substantial as a result. On average, it takes 42 days to fill a job position, during which companies can face a productivity loss of $33,251. Celential, which is headquartered in Sunnyvale, California and was founded by former product leads at Salesforce, VMware, and Zynga, develops a platform designed to match hiring managers with engineering candidates by essentially “simulating” human experts. The startup’s AI-driven virtual recruiter finds, vets, and engages passive candidates behind the scenes, resulting in what Celential claims is a 70% present-to-interview ratio. Celential clients upload the job description for their engineering, data science, or project manager roles to the platform. Then, Celential lines up candidates by engaging them with an email engine that tracks open rates, automatically follows up, and gauges sentiment. To personalize the emails, Celential’s algorithms draw on talent and company graphs vertically focused on over 3 million engineers and the employers for whom they’ve worked. The company says it pulls in thousands of data sources to train its natural language processing, computer vision, and machine learning recommendation algorithms, aiming for a mutual fit between engineers and hiring companies, teams, and opportunities.  For job seekers, Celential searches job boards and career sites. Because it’s referral-based, only matches with roles and companies that are actively hiring bubble up to the top. Celential’s competitors include Plum, which has job candidates fill out problem-solving and personality tests that award points for “talents” like adaptation, communication, inclusion, and innovation. Another rival, Vervoe, offers AI tools that test would-be employees’ on-the-job skills with a mix of general assessments, coding challenges, and personality quizzes. There’s also Headstart, which recently raised $7 million for AI that can mitigate recruitment bias; Xor, a startup developing an AI chatbot platform for recruiters and job seekers; and Phenom People, a human resources platform that taps AI to help companies attract new talent. But 40-employee Celential’s focus on software engineering talent recruitment has allowed it to stand out in the crowded field. It counts among its over 50 customers Affinity, Paradigm, Twin Health, and more than 100 other companies. And Celential says it’s on track to notch “double digit millions” in annual recurring revenue by 2022, after growing revenue by 7 times in 2021. “Celential is on a mission to fundamentally transform recruiting and career development,” Akhtar told VentureBeat via email. “With proven customer success, our vertically focused virtual recruiter, powered by AI, enables fast-growing companies to compete for talent cost-effectively, with little friction, and without having to build and scale up a recruiting team and infrastructure. Over time, we’ll leverage the same vertical approach and deep insights into talent, industry, companies and opportunities to accelerate talent’s career development, through a virtual personal career agent solution.” GSR Ventures led Celential’s investment round announced today. Spider Capital, TSVC, and undisclosed individual investors participated."
https://venturebeat.com/2021/03/30/gryps-an-rpa-platform-focusing-on-construction-raises-1-5m/,"Gryps, an RPA platform focusing on construction, raises $1.5M","Gryps, a robotic process automation (RPA) startup focused on the construction industry, today announced it has raised $1.5 million. The company says it will use the funding to support product R&D and hire new employees, particularly engineers. RPA — technology that automates monotonous, repetitive chores traditionally performed by human workers — is big business. Forrester estimates that RPA and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce found that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than four hours savings per employee each week. Gryps, which was founded in 2020, connects to multiple email, project management database, and other systems to automatically scrape and organize documents during the construction process. The platform ingests things like manuals, warranty certificates, contracts, change orders, invoices, lien waivers, and other close-out documents from different sources and then applies machine learning to categorize the files and label them correctly so that they can be shared with various stakeholders. “We met in 2013 working on the same team at a construction management firm in New York City,” Gryps founders Dareen Salama and Amir Tasbihi told VentureBeat. “One thing was always top of mind: The industry is ready for and needs AI that helps process the volume of data generated, and it must be easy to adopt for users. We started Gryps to provide an amazing product with the best user experience and to create an environment where young professionals in construction can grow and find a fulfilling career.” Gryps employs APIs and digital robots to process the documents it collects. Leveraging a combination of natural language processing, machine learning, computer vision, and document understanding, the platform canvasses, ingests, and transforms construction project data. “We use cutting-edge transfer learning techniques to transfer AI knowledge from best-in-class models to boost our accuracy. We also use layout-based, Transformer-based deep learning models to extract information from documents,” Salama and Tasbihi explained. “Our RPA agents are rule-based software robots performing actions to get our information ingestion jobs done more accurately and faster than people can … For example, [we apply] computer vision and machine learning to extract and analyze trends such as companies, products used, services rendered, and costs involved from documents.” Gryps has a number of competitors in a global intelligent process automation sector that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere last secured a $290 million investment from SoftBank at a $6.8 billion valuation. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. But Gryps, which has three paying customers and several in pilots, asserts that specializing in construction gives it a leg up over rivals focused on the broader market. To this end, one of the company’s first clients was the Javits Convention Center in New York. Gryps claims its software automatically ingested over 20,000 documents and 100,000 data points, collated them, and handed them over to the Javits team, with estimates putting the savings at hundreds of hours of staff time. Salama and Tasbihi say the pandemic made apparent the need for and speed of digitization adoption, with construction teams desiring faster access to information remotely, in-office, and on-site. “With teams working from home, they needed more robust tools than what exists today to access project data such as contracts, financials, and other documentation as fast as possible,” they continued. “Project managers are typically extremely busy responding to project needs, which typically slows down technology adoption because they have no time to spend on digitizing their processes. The pandemic paused a lot of projects and provided executives and teams time to rethink their policies, procedures, and ways of doing business. Now these projects are coming back and the teams are determined to increase efficiency, and integrating new technologies is key to that goal.” LDV Capital led Gryps’ seed round with participation from Pear VC and Harvard Business School Graduate Syndicate. A group of angel investors also contributed."
https://venturebeat.com/2021/03/30/6sense-which-uses-ai-to-power-account-engagement-raises-125m/,"6Sense, which uses AI to power account engagement, raises $125M","Account engagement startup 6Sense today announced that it closed a $125 million series D funding round led by D1 Capital Partners, valuing the company at $2.1 billion post-money. 6Sense says that the investment will bolster its growth and product initiatives, particularly in the areas of machine next-best-action prediction, data insights, and AI-powered orchestration capabilities. Business-to-business buyers are typically 57% of the way to a buying decision before they engage with sales departments. Moreover, only 23% of executives are confident in the speed at which they’re gaining accurate insights. Motivated by the idea that AI might have a role to play in helping seal the deal, five entrepreneurs — Amanda Kahlow, Dustin Chang, Premal Shah, Shane Moriah, and Viral Bajaria — cofounded 6Sense in 2013. 6Sense’s product captures intent signals from known and anonymous sources including the web, creating customer segments by account, behavioral intent, or a combination of those two factors. 6Sense identifies contacts and builds out targeted buyer lists, helping to prioritize outreach sales efforts and boost conversions with machine learning-based fit scores. The platform also triggers marketing communications through apps like Marketo and Eloqua in response to sales prospects’ demands. Moreover, it enables salespeople to engage with buying teams via multichannel, multitouch campaigns. Automation is now seen as essential among marketers to bolster the outreach of campaigns, in part because of its ability to better target customer communications. According to a recent HubSpot survey, email automation campaigns are among the top three tactics used by marketers to improve performance. And in 2017, Salesforce reported that 67% of sales leaders used a marketing automation platform. Under the hood of 6Sense’s platform is a demand graph that captures signals and automatically connects them to sales prospects. Algorithms ingest historical intent data to reconstruct account-based buyer journeys for any given business, monitoring the demand graph and analyzing changes in intent to score hundreds of millions of accounts and people every day. 6Sense recently launched Segment Performance Reports and Custom Talking Points, two features that enable marketers to analyze changes in account engagement and progression through buying stages and provide guided conversation points based on buyer intent, role, and fit. February 2020 saw the introduction of Next Best Actions, which leverages AI to present business development representatives with a prioritized list of actions to engage buying teams within a target account. 6Sense competes to a degree with ZoomInfo. Other startups operating in the segment include Demandbase, which has raised over $150 million with backing from high-profile investors, as well as Lattice Engines and Leadspace. But 6Sense backers aren’t concerned, and they have some reason to be optimistic. The year 2020 was the company’s third straight year of 100% revenue growth, 6Sense says, driven by “significant increases” in pipeline, revenue, average sale price, and deal velocity and a twofold increase in customer base size. (Brightcove and Cognizant are among 6Sense’s clients.) While global ad spend was predicted to have fallen 10.2% year-on-year in 2020, ad agencies including Magna say they expect to see ad spend to rise over 7% in 2021 to around $612 billion total, with digital media seeing growth exceeding 10%. “We’re grateful for our success leading the account-based sales and marketing category — and humbled by the confidence our customers and investors have in 6Sense — but our vision has always been bigger and bolder,” said CEO Jason Zintak. “There is an enormous opportunity to redesign the way business-to-business companies go to market. We believe we have the platform, data, team, and investment partners to be the foundation for business-to-business revenue technology.” Forrester predicts that spend for marketing automation tools will grow “vigorously” over the next few years, reaching $25.1 billion annually by 2023 from $11.4 billion in 2017. It’s estimated that 55% of marketing decision-makers plan to increase their spending on marketing technology including AI and machine learning, with one-fifth of the respondents expecting to increase by 10% or more. “We invest heavily in sales and marketing technology, and 6Sense is truly one-of-a-kind,” Sapphire Ventures partner Rajeev Dham, a 6Sense investor, told VentureBeat. “We’ve always viewed 6Sense as a market leader with the ability to execute on their bold vision of transforming sales and marketing with data-driven insights and orchestration capabilities. 6Sense is already the leading account-based sales and marketing platform, and they are poised to define and deliver the future of revenue technology that every B2B organization needs.” Beyond D1 Capital Partners and Sapphire Ventures, Insight Partners and Tiger Global participated in 6Sense’s latest funding round. It brings the 300-employee, San Francisco, California-based company’s total raised to date to over $225 million."
https://venturebeat.com/2021/03/30/rivery-raises-16m-to-help-enterprises-manage-and-transform-data/,Rivery raises $16M to help enterprises manage and transform data,"Rivery, a data management platform geared toward enterprises, today closed a $16 million series A round, bringing its total raised to date to over $22 million. The company says the funds will be put toward expanding the size of its workforce and growing its international footprint. Enterprise data collection is expected to increase substantially over the next few years, but a chunk of data remains unleveraged, due to challenges in management and security. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends, while a separate Veritas report found that 52% of all information stored by organizations is of unknown value. Rivery, which was founded in 2018 and has offices in New York and Tel Aviv, offers a fully managed, serverless product that lets teams ingest, extract, and transform raw data. The platform offers APIs and connectors that allow customers to automate orchestration in the cloud, from both in-house and third-party databases. The idea is to turn data into business-ready inputs inside data stores to power insights and analysis. “Our mission is to give 100% of staff in organizations control over 100% of their data. In return, this gives teams the freedom to access the data they need to make incisive business decisions, whenever they need it,” cofounder and CEO Itamar Ben Hemo told VentureBeat via email. “By automating and streamlining data processes, we democratize access to data with operational efficiency that creates a single source of truth … Just like DevOps change the way businesses synchronize IT and development processes, DataOps will be essential to running an efficient data operation that is perfectly automated and connected to the business.” With Rivery, organizations can build, test, and deploy multiple data models, as well as aggregating data from sources such as social media data lakes and on-premise datacenters. The opportunity cost of this data, if unused, is substantial. The aforementioned Veritas report pegged it at a cumulative $3.3 trillion in 2020. That’s perhaps why the global enterprise data management market is expected to surpass $136.4 billion by 2026, according to a Research Dive report. Since launching in 2019, 40-employee Rivery says it has seen 500% growth with enterprise companies, including Bayer and the American Cancer Society. Hemo says the company’s annual recurring revenue stands at seven figures and is projected to reach eight figures within the next year. In the near-term, Rivery plans to turn its focus to a feature called Rivery Kits, which is designed to give data teams access to ready-made machine learning models. Rivery Kits are predefined templates for common use cases that live under a marketplace. Hemo describes this as a repository of data model templates with pipelines, connections, and logic. “[We have] hundreds of customers worldwide, ranging from data-driven tech unicorns like WalkMe or Riskified to established enterprises such as Bayer or American Cancer society … We’re also seeing increasing interest from finance and insurance companies and direct-to-consumer brands like NectarSleep,” Hemo said. “As a cloud-native company, Rivery works alongside all main cloud data warehouses with enterprise-grade solutions for businesses that need to unify, align, and automate all their data processes, ultimately to control and centralize the management of data flows.” Entree Capital led the series A round in Rivery, with participation from existing investor State of Mind Ventures."
https://venturebeat.com/2021/03/30/what-you-need-to-know-about-your-thought-leadership-strategy-post-pandemic/,"What you need to know about your thought leadership strategy, post pandemic","The term “thought leadership” was coined decades ago by Joel Kurtzman, former editor-in-chief of Strategy and Business magazine and Harvard Business Review and was defined as one who “possesses a distinctively original idea, a unique point of view, or an unprecedented insight into their industry.” Since then, it’s been proven that it’s not just a buzzword, but that it has real impact on influencing decisions. I asked Michael Norton, Harvard Business School Professor, how he would define thought leadership in today’s digital age. Michael shared, “Unfortunately, most self-proclaimed thought leaders tend to be thought followers — they reflect back the current zeitgeist and conventional wisdom rather than shine new light. Especially in times of dramatic social change, thought leaders who actually lead our thinking in new directions are sorely needed.” If new ways of thinking is the new way of owning thought leadership, how can you ensure your strategy is effective? If done right, thought leaders can be powerfully influential. Studies show that B2B companies with demonstrable expertise are those that will get noticed by other influencers and their core audience, and will realize real ROI. Now more than ever, in a post-pandemic world, business leaders are facing many unknowns, and thought leaders have an opportunity to influence and help guide their decision-making process. So what does this mean for B2B and B2BC marketers? I connected with industry-leading marketing and PR experts who have partnered with VentureBeat Lab, a thought leadership consultancy, and asked them to share how they defined thought leadership and how it’s become even more important post pandemic. Here’s what they shared: “Thought leadership is a powerful way to engage an audience that prefers to be educated versus sold to. Companies often make the mistake of trying to sell through the lens of what’s best for them, versus what’s best for their audience. Thought leadership helps bring your audience on a journey of shared understanding and insights, in order to drive toward impactful solutions. Born from necessity, the pandemic catalyzed the shift to a more digitally driven selling process, and consequently thought leadership has become an increasingly important top of the funnel marketing tool.” –Leah Hardy, Head of Gaming Marketing, Americas, Facebook “The pandemic has forced everyone to rethink how they do business, from doctors to data centers. After a year of fear and uncertainty, people will look to those who clearly understand how technology can fuel the recovery and deliver exciting new capabilities. Thought leadership is all the more important during these transitional times, to show the way forward.” — Ken Brown, Director of Corporate Communications at Nvidia. In this digital age, marketers are continuously adapting to the changing needs of the marketplace, and the continually evolving cycle of consumer behavior. The ways content is consumed is also constantly changing, and your thought leadership strategy should recognize this evolution. Whether you’re a marketer for an emerging start-up that’s looking to find its way into building awareness, or you’re representing a Fortune 500 company that’s globally well- known, thought leadership is an ongoing venture that keeps delivering results. For marketers in the start-up stage, your go-to-market strategy falls in what we can call “Phase 1” of your marketing efforts. Phase 1 means your company, services, and story is still “unknown” and has limited exposure and visibility in the marketplace, and your goal should be  focusing on building awareness thoughtfully before you ramp up any lead generation efforts. The takeaway: make sure your audience gets to know you organically first before you attempt to schedule a meeting with prospects. High-impact brand awareness and digital marketing is going to be part of the strategy, but a combination of branding and a well-conceived thought leadership strategy is what will really build meaningful awareness — you can’t do one without the other if you want to establish your brand. To summarize, in Phase 1, thought leadership is fundamental to taking the steps towards getting noticed and getting in front your top prospects — and again, if done right, that strategy is going to drive real ROI. For marketers who are developing a strategy for a corporation that is already well known, the need for thought leadership is continuous as it’s important to stay top of mind and highly engaged with your community, in an authentic way. Building authority as a thought leader is never ending, and those who have a thoughtful approach that puts the audience’s needs first will experience real turn-key engagement points with their target markets. This is why we still see experts from big brands and some of the most innovative companies speaking at events, publishing new content, and engaging with new and growing communities — and those who are authentic in their communication and approach will create value for those who are in the market for their services. So how can you stand out as an authentic thought leader and bring new value? For B2B companies, you want to create value by showing how your expertise is helping business decision-makers make more informed decisions. According to a 2020 Edelman study, 89% of decision-makers surveyed believe that thought leadership is effective in enhancing their perceptions of an organization — yet only 17% of them rate the quality of most of the thought leadership they read as very good or excellent. This is a real challenge for marketers, particularly in the B2B space, where marketers are facing a growing saturated market where it’s difficult to get noticed. As the marketplace evolves, it may seem hard to get it right. This is what inspired the creation of VentureBeat Lab, a thought leadership consultancy for brand partners. VB Lab has a proven process that helps brand partners establish their authority and influence as a global thought leader. Because VentureBeat reaches an influential audience of key enterprise business decision makers, partners have a real chance of influencing decision-makers and engaging with top prospects. VB Lab’s Thought Leadership Platform helps B2B marketers identify the most innovative go-to-market solutions and get in front of influencers and their core audience in a thoughtful way. It starts with identifying what makes you different. Hone in on your differentiator, offer new insights, and understand where the value lies in your target audience. Through high-touch collaboration, at VB Lab we’re focused on marketing strategies that nail your most important differentiator — and from there, we create new, disruptive partnership solutions, and custom content strategies, and then thoughtfully execute on your key core objectives. Thought leadership isn’t just the core part of building awareness and staying relevant, but a necessary tool for quality lead generation. When done right, the opportunities with thought leadership are endless. Visit VB Lab to learn more about VentureBeat Lab’s Thought Leadership Platform or email us at partners@venturebeat.com. Gina Joseph is Co-Founder of VB Lab and VP of Strategic Partnerships, VentureBeat"
https://venturebeat.com/2021/03/30/recuro-health-announces-oversubscribed-funding-round-founding-ceo-of-teladoc-leads-launch-of-new-integrated-digital-solutions-company/,Recuro Health Announces Oversubscribed Funding Round: Founding CEO of Teladoc Leads Launch of New Integrated Digital Solutions Company,"DALLAS–(BUSINESS WIRE)–March 30, 2021– Recuro Health (Recuro), an integrated digital health solution that transitions the U.S. healthcare system from a reactive, disease-focused model to a population health, outcomes approach, today announces the closing of its oversubscribed funding round led by OLSF Ventures. Michael Gorton, founding CEO of Teladoc, leads the Recuro team which includes several Teladoc leaders: the original Chief Technology Officer, along with the marketing team that perfected Teladoc’s go-to-market strategy. The company is also adding healthcare luminaries and experienced business executives such as Jay Sanders, MD, founder of the American Telemedicine Association and acknowledged as the “Father of Telemedicine.” “This oversubscribed round of funding and interest in our subsequent round demonstrate the support of our investors and confidence in our management team to build yet another industry-transforming company in the digital health space,” says Gorton, CEO and founder of Recuro Health. “The platform combines digital health with a patient-centered medical home. We are creating an easy, personal and economic solution for patients, employers and payers.” According to Dr. William Paiva, managing partner at OLSF Ventures and lead investor in the round, “Given the unprecedented interest in the Recuro business model, we will quickly follow this company formation and funding round with a larger round of funding which will further accelerate Recuro’s journey to build a new industry category.” Recuro is focused on empowering patients with tools, education and guidance to live healthier, longer and happier lives. Grounded in a suite of digital health offerings, analytics and services, Recuro touches patients along their entire health journeys. Gorton says, “We intend to disrupt the traditional healthcare delivery system by bridging the gap between the current system and digital health. This requires a deep understanding of the current challenges facing healthcare, along with a proven ability to impact the industry. We are building infrastructure and partnerships to support care delivery in every setting, across the entire patient journey, minimizing the need for acute, episodic care by providing the digital keys to member health.” Dr. Paiva continues, “There are rare moments in a venture capitalist career when you are presented an industry transforming opportunity, at the right moment in time, with a management team that has created the industry category. This is one of those moments.” In addition to OLSF, participants in the current round of funding include Cortado Ventures, 1843 Capital and Sage Venture Partners. About Recuro Health Serving employers, providers and managed care organizations, Recuro Health delivers value throughout the healthcare ecosystem. Digital and virtual services leverage aggregated data to direct users to achieve effective and efficient offerings. By evaluating outcomes and compliance together, Recuro provides appropriate referrals and recommendations. www.recurohealth.com About OLSF Ventures With over 20 years of industry experience working in management consulting, large pharma and venture capital, OLSF contributes many diverse perspectives to our portfolio companies. OLSF I is invested in five start-ups that have created over 100 jobs with a mean income that is three times greater than the state average. These companies have attracted over $210 million in venture capital from regional and national venture capital firms after OLSF’s first investment. These entrepreneurial success stories have continued with our second round, OLSF II, which has already invested in five promising start-ups in Oklahoma, which have attracted over $74.3 million in outside venture capital. Since its inception in 2000, The Oklahoma Life Science Fund has been the single largest driver of private equity capital to Oklahoma. www.olsfventures.com  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005442/en/ Media: CPR for Recuro HealthHolly McKennaHmckenna@cpronline.com 1.518.461.8207"
https://venturebeat.com/2021/03/30/climacell-rebrands-as-tomorrow-io-and-raises-77m-to-bring-weather-data-intelligence-to-enterprises/,ClimaCell rebrands as Tomorrow.io and raises $77M to bring weather data intelligence to enterprises,"Weather data holds a degree of interest for most people — it informs our plans for the weekend, whether to take an umbrella to the local grocery store or to pack sunscreen on a day walking in the hills. But for enterprises, procuring accurate weather data can be mission critical, which is why ClimaCell has built a platform over the past five years that enables companies across insurance, supply chain, energy, and more to predict future weather events and limit the impact it might have on their business. The Boston-based company today announced it has raised $77 million in a series D round of funding, which it said it will use to accelerate its SaaS growth and invest in its recently announced venture into outer space, which will see it build proprietary radar-equipped satellites to improve its weather monitoring and forecasting abilities. Alongside the investment, ClimaCell announced that it’s changing its name to Tomorrow.io (we’ll use this name from here on) to reflect the new direction. Founded in 2016, Tomorrow.io provides real-time weather forecasts that uses data garnered from sources such as wireless communication infrastructure, connected cars, drones, airplanes, and more. This is what the company refers to as the “weather of things,” and it’s designed to bring super-accurate, localized weather data to businesses that need it — this replaces traditional models that typically use government data gleaned from satellites. “The government systems were built for major events — they were not built for day-to-day operations of a company, and most players in the weather space are simply repackaging government models and selling the data along with a consulting services package,” Tomorrow.io CMO Dan Slagen told VentureBeat. Tomorrow.io counts customers across the consumer and enterprise sphere, including transport and logistics giants such as Ford, JetBlue, and Delta Air Lines, while last year Uber signed up as a customer to improve its ETA travel estimates. The correlation between weather and traffic flow is no secret, with rain alone estimated to reduce traffic speed by up to 12%, while snow and fog can also have similar repercussions. “A railroad operations executive uses us to know that ‘in New Mexico on Tuesday at 3 p.m. ET at mile marker 27, slow your trains down to 10 miles-per-hour to avoid a derailment due to crosswinds that exceed your safety protocol,'” Slagen said. “The weather forecast they used to get was ‘high winds across New Mexico on Tuesday,’ and they would then have to go figure out what that means.” For context, a single train derailment causes around $250,000 in damage on average on U.S. railroads, according to a study by the Volpe National Transportation Systems Center. Weather can cause many other periphery issues in the transport and logistics sphere, such as companies having to cancel staff shifts or pay overtime if hazardous weather means having to delay loading cargo, for example. “Between optimizing for staffing costs and improving delays, the cost savings are in the tens of millions per year for an individual company,” Slagen added. The current Suez Canal debacle is a good example of how weather events can wreak havoc — the Ever Given container ship became wedged after being blown off course by a gust of wind. The subsequent blockade it created is thought to be holding up $10 billion in global trade. Using archive data, Tomorrow.io has shown when the ship ran aground and the associated wind status. “Our software showed this risk well in advance of impact,” Slagen said. “Our system would have recommended that due to the wind speed, the vessel should delay passing or perform other safety procedures.” Such is the potential impact of weather events, Tomorrow.io CEO and cofounder Shimon Elkabetz thinks that businesses should hold weather intelligence in the same regard as cybersecurity. “Climate change makes weather events more frequent and more volatile, and CEOs should start thinking about weather intelligence in the same way they think about their cybersecurity strategy,” he wrote in a blog post today. “They need to think of tomorrow.” ClimaCell had previously raised around $107 million, and with a fresh $77 million in the bank from backers including Stonecourt Capital and Highline Capital, the company said that it will continue to invest in its current product that covers weather intelligence and air quality, as well as support investments across AI, machine learning, and its upcoming space operations. The company’s new radar-enabled satellites are scheduled to go live in 2022 and will serve to improve its weather data by garnering more detailed information about precipitation and cloud structure that “no other sensors can see,” according to the company. The launch comes as many existing satellites are scheduled to be retired, leading Tomorrow.io to develop its own version — one that’s roughly the size of a mini-fridge. So why change its name when “ClimaCell” seemed to fit the bill pretty well, and is arguably more unique than the generic name it will now use? “The truth is, we didn’t choose our new name — it chose us,” Elkabetz wrote. “Tomorrow.io perfectly captures everything that businesses, individuals, and countries want from the weather industry.”"
https://venturebeat.com/2021/03/30/linkedin-launches-video-cover-stories-and-other-profile-features-for-job-seekers/,LinkedIn launches video cover stories and other profile features for job seekers,"LinkedIn today announced new features designed to make profiles on the platform more expressive and inclusive as the pandemic continues to roil the job market. Video cover stories let members showcase their skills, while a creator mode helps them build a following by more prominently highlighting content on their profile. In addition, LinkedIn says it will expand its free LinkedIn Learning and Microsoft Learn courses aligned with the 10 most in-demand jobs through December 2021 and pilot Skills Path, which the company describes as a “skills-first” program for job seekers. According to LinkedIn, video cover stories, which will roll out within weeks, are a sought-after tool for both job candidates and recruiters. Sixty-one percent of job seekers believe recorded video could be the next iteration of the traditional cover letter, according to a survey conducted by the company, while almost 80% of hiring managers say video has become more important for vetting candidates. Complementing the video cover stories feature is a new field for pronouns on profiles, which is intended to let members communicate how they want to be seen. Seventy percent of job seekers believe it’s important that hirers know their gender pronouns, the above-mentioned survey found, and 72% of hiring managers believe having clarity about gender pronouns can help others be respectful.  Beyond cover stories and the gender pronouns field, LinkedIn is introducing creator mode and service pages for candidates and organizations. The creator mode enables members to highlight their recent job-relevant work, while the service page allows freelancers and small business owners to create dedicated pages listing the services they offer. Beyond the extended availability of free online learning courses, LinkedIn said it will partner with companies, including Gap, TaskRabbit, and Twitter on Skills Path, which aims to help job seekers learn the skills required for roles with free lessons. Skills Path will also give them an opportunity to demonstrate their skills with assessments and land a recruiter conversation with one of the participating companies.  Lastly, starting in May, LinkedIn says it will power a new Microsoft Teams app called Career Coach for institutions to support students pre- and post-graduation. The app will help students discover goals, interests, and skills using an AI-based identifier that aligns their profiles with job market trends. “Every day, we’re seeing our members share and connect like never before, with nearly 5 billion connections made last year [and] conversations on the platform [increasing by 50%],” a LinkedIn spokesperson told VentureBeat. “We … know that there’s no one-size-fits-all for someone’s career journey and that not everyone has the same identity or goals. That’s why we’re unveiling these new features and updates.”"
https://venturebeat.com/2021/03/30/cloud-backup-and-recovery-company-hycu-raises-87-5m/,Cloud backup and recovery company HYCU raises $87.5M,"HYCU, a company developing data backup and recovery solutions for enterprises, today announced that it closed a $87.5 million series A funding round led by Bain Capital Ventures. With the introduction of HYCU Protégé, a disaster recovery solution for enterprise apps, HYCU says it will use the funding to expand and grow its app, public cloud, and software-as-a-service-based innovations as well as hire aggressively in Boston and North America to meet growth goals. There are few catastrophes more disruptive to an enterprise than data loss, and the causes are unfortunately myriad. In a recent survey of IT professionals, about a third pegged the blame on hardware or system failure, while 29% said their companies lost data because of human error or ransomware. It’s estimated that upwards of 93% of organizations that lose servers for 10 days or more during a disaster filed for bankruptcy within the next 12 months, with 43% never reopening. Those statistics are more alarming in light of high-profile outages like that of OVHCloud earlier this month, which took down 3.6 million websites ranging from government agencies to financial institutions to computer gaming companies. Headquartered in Boston, Massachusetts, HYCU, which was founded in 2018, offers modular data management services designed to simplify multi-cloud data migration, disaster recovery, and data protection management. It aims to bring software-as-a-service-based data backup to both on-premises and cloud-native environments, in part via support for platforms including VMware, Amazon Web Services, Nutanix, Google Cloud Platform, and Microsoft Azure. “HYCU believes in leveraging the power of AI and making it transparent for the user. The way it manifests for the end user is in terms of what we call Intelligent Simplicity,” CEO Simon Taylor explained to VentureBeat via email. “For example, unlike a number of other solutions, with HYCU, our customer does not have to tell the software where to store the backups; it automatically matches the customer’s service-level agreement with the capabilities of the network and backup targets to find the right place. This approach reduces effort and keeps cost at the optimal level.” According to Gartner, data-driven downtime costs the average company $300,000 per hour — or $5,600 every minute. That’s perhaps why Markets and Markets predicts that the data and backup recovery market will be worth well over $11 billion by 2022.  HYCU competes to a degree with San Francisco-based Rubrik, which has raised $553 million in venture capital to date for its live data access and recovery offerings, and Cohesity, which bills itself as the industry’s first hyperconverged secondary storage for backup, development, file services, and analytics. That’s not to mention data recovery juggernaut Veeam, which now serves 80% of the Fortune 500 and 58% of the Global 5000; Acronis, which raised $147 million in September for its suite of data backup, protection, and restoration tools; and cloud data backup and recovery company Clumio. “Many use cases for our customers center around being able to backup and recover with specific on-premises environments like Nutanix and VMware. Or, they may need a solution they can easily run and deploy from a specific cloud platform like Google Cloud or Azure Cloud,” Taylor said. “For Nutanix environments in particular we have a long-established and rich pedigree of support for their solutions.” HYCU, which has over 200 employees, claims to have over 2,000 customers worldwide. ACrew Capital also participated in the company’s latest funding round."
https://venturebeat.com/2021/03/30/zoomin-which-helps-enterprises-extract-answers-from-siloed-technical-documents-raises-52m/,"Zoomin, which helps enterprises extract answers from siloed technical documents, raises $52M","Zoomin, a platform that helps enterprises extract answers from across their product content, has raised $52 million in a series C round of funding. The “knowledge orchestration” platform has amassed an impressive roster of clients since its inception six years ago, including Dell, McAfee, Imperva, and the now Adobe-owned Workfront, helping make their vast pools of content easier to search and extract answers from. Businesses may have thousands of manuals, guides, training documents, online community discussions, and more, each created and managed by different teams in silos. Zoomin unifies all of these parts, serving as a sort of white-label search engine that delivers answers wherever a company needs them. “Every enterprise generates hundreds to hundreds of thousands of pages of product content meant to help customers utilize products to their fullest potential,” Zoomin CEO and cofounder Gal Oron told VentureBeat. “This content is constantly evolving and growing in volume alongside the products it supports. But most of the time, the product content experience is severely lacking — it fails to provide customers product information in an easily accessible, seamless way across the range of channels where they are looking for it. When customers can’t find relevant information quickly and easily, this limits product adoption and onboarding and leads to frustrated customers, creating churn and burdening customer support teams.” For example, a company might use Zoomin to build a technical resource center similar to an intranet — where employees can search for all manner of business documentation, such as financials or how-to guides that are spread across different locations. Zoomin bakes in a bunch of useful features, including content filters, auto-suggestions, recommendations, and other tools users are likely accustomed to from other search platforms they use. Alternatively, a company may elect to deploy Zoomin directly inside one of their own applications as a widget that offers context-specific content. Zoomin also offers prebuilt apps that can be customized and integrated with Salesforce or ServiceNow, while companies can deploy their own integrations via Zoomin’s APIs. Other similar products that enterprises might use include Adobe Experience Manager or Liferay, but Zoomin aims to set itself apart with a focus on its “bespoke” solution built specifically for complex, technical content. Moreover, Oron argues that enterprises value Zoomin’s inherent agility. “We can get them up and running in weeks and enable full customization with minimal added costs and no reliance on IT resources while connecting to any system they’re already using,” he said. “Plus, we offer out-of-the-box actionable analytics.” There are also existing universal search tools for enterprises, and although they may bring search results into a single interface, they typically direct the user to the original source of the content. Zoomin brings all the answers into a single channel and saves users having to navigate across different destinations. Founded in 2015, Zoomin has been on a tear over the past year, claiming to have “more than doubled” its new customers since 2019. Moreover, the company announced a $21 million investment in December, although that was from a previously undisclosed round. Since then, Zoomin has introduced offline support, which Oron said is particularly relevant for field technical support: “for example, when field engineers need to be able to instantly access critical information as they solve hardware issues, often operating with limited network connection.” With another $52 million in the bank, Oron said it’s now well-financed to “meet the growing demand from enterprises across industries,” spanning a wider gamut of hardware and software companies. “While our sweet spot has historically been SaaS and hardware companies, a diverse mix of new industries are now recognizing the strategic value of investing in their product information experience,” he said. “We’re seeing strong demand from fintech and health care industries, in particular. In addition, while the majority of our customers are large U.S-based enterprises, we’re seeing that mid-market and growth companies are facing the same pain points that we are solving for enterprises.” While Zoomin already offers a range of analytics, such as “traffic insights” that show where traffic is being referred from and “content insights” that highlight which topics or documents garner the greatest engagement, Oron said the company plans to use some of its fresh cash injection to develop “new in-product and sales analytics features and case deflection analytics tools” and improve its platform’s existing search and prediction capabilities. Underpinning much of this growth is a desire to alleviate some of the burden from companies’ technical or customer support teams — Zoomin basically helps companies build their own self-serve product support portals, leaning on machine learning models to develop a knowledge graph that bridges enterprise content and the end users that are likely to find most use from it. “Customers are expressing a strong desire to self-serve product answers rather than rely on customer support teams and, secondly, more and more businesses are experiencing remote work,” Oron said. “As a result, [they are] increasingly relying on digital support channels. This makes the need and demand for a self-service product content experience more urgent than ever.”"
https://venturebeat.com/2021/03/30/nimble-crowned-crm-industry-leader-and-top-5-sales-intelligence-tool-for-small-business-teams-on-g2/,Nimble Crowned CRM Industry Leader and Top 5 Sales Intelligence Tool for Small Business Teams on G2," For 9 Years in a Row, G2 Reviewers Have Chosen Nimble As Their Fan-Favorite Simple Smart CRM for Microsoft 365 and Google Workspace  SANTA MONICA, Calif.–(BUSINESS WIRE)–March 30, 2021– Nimble, the pioneering Social Sales and Marketing CRM built for Microsoft 365 and Google Workspace, announced today that it has been named one of the Top Five Sales Intelligence Software Tools for Small Business and an overall CRM Industry Leader by G2, the world’s leading business software review platform. Nimble achieved top customer satisfaction rankings according to verified user reviews and a market presence in multiple categories, including: “We believe that lack of use is one of the biggest sins of CRMs,” explains Sergey Shvets, Nimble’s Head of Product. “That’s why we prioritize the end-user experience in everything we do. Our system is the only CRM on the market that works on every website in the world, allowing users to have a CRM always a click away; whenever they want to profile someone, capture a lead, or prepare for a meeting. We’re happy to see that our efforts are appreciated by our customers and we have many more improvements coming soon.” Nimble Sets the Bar for Simple, Smart CRM with Sales Intelligence Built-in Nimble is the only CRM that has built-in sales intelligence and has consecutively been ranked as a market-leading CRM in sales intelligence. Although most sales intelligence tools can cost over $100 per user/month in addition to the costs of your CRM, Nimble starts as low as $19 per user/month when billed annually. Nimble Scales Worldwide as the Simple CRM for Microsoft 365 Microsoft is now reselling Nimble+Microsoft 365 worldwide and their distributors and partners are reselling, implementing, and providing customized Nimble solutions to Microsoft 365 users 24/7 across the globe. Additional Resources: Get Started With Nimble Today! We invite you to try it for free for 14 days. Stay tuned for more product announcements as we evolve Nimble into the best CRM for Microsoft 365 and Google Workspace. ABOUT NIMBLE – Nimble is the leading global provider of simple, smart CRM for small business teams using Microsoft 365 or Google Workspace. It combines the strength of traditional CRM, classic contact management, social media, sales intelligence, pipeline management, and marketing automation into one powerful relationship management platform that delivers valuable company and contact insights – everywhere you work. Nimble has been named “Market Leading CRM for Customer Satisfaction and Ease of Use” and CRM Market Leader by G2 for nine consecutive years, CRM Watchlist Winner for three consecutive years, #1 Sales Intelligence Tool for Customer Satisfaction by G2 for the ninth consecutive time, and users’ choice award winner by Fit Small Business. Try Nimble’s 14-day free trial today. For more information, visit http://www.nimble.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210330005138/en/ Megan Ranger – megan.ranger@nimble.com"
https://venturebeat.com/2021/03/30/cleveland-clinic-will-be-ibms-first-private-sector-customer-to-install-a-quantum-computer-on-premise/,Cleveland Clinic will be IBM’s first private sector customer to install a quantum computer on premises,"IBM today announced it is installing a quantum computer at the Cleveland Clinic, marking the first time the company has physically placed this next-generation system on the premises of a private sector client. The move marks yet another step forward for quantum computing. It comes as part of a broader 10-year partnership between IBM and the clinic that includes hybrid cloud service and AI. According to IBM Quantum Network director Dr. Anthony J. Annunziata, including a quantum computer as part of that suite of tools is critical because the company wants to understand which tasks are best suited to quantum computations. Despite rapid advances, quantum computers are still in their infancy, but it’s still possible they could be more efficient at limited tasks. “The Cleveland Clinic will have the full capacity of a quantum system we purpose-built for them,” Annunziata said. “We’ll have a much better ability to integrate it into their existing infrastructure. There will be benefits in doing that as we figure out how quantum can address these really tough problems and also how it can accelerate the application of AI.” The partners have dubbed the program the Discovery Accelerator, and its overall goal is to power new breakthroughs in health care and life sciences. IBM’s computing tools are being leveraged to better harness the clinic’s wealth of data, including “genomics, single-cell transcriptomics, population health, clinical applications, and chemical and drug discovery,” according to a press release. The eye-catching part of the announcement, however, is the move to physically place a quantum computer at the clinic. Until now, the company has been focused on its IBM Q Network, a consortium of research and business partners who can experiment with quantum computing via a cloud-based service. IBM has grown increasingly optimistic about quantum’s potential and has laid out an ambitious timetable for expanding commercial applications. That will now include its first on-premises Quantum System One in the United States outside of an IBM computation center. IBM currently has a quantum computer on its own campus, as well as one at Germany’s Fraunhofer Institute and the University of Tokyo. The Cleveland Clinic is the first private sector client and the first in the U.S. Annunziata said the clinic will make for a good first private partner, thanks to its recently announced Global Center for Pathogen Research & Human Health. The new center will assemble teams to focus on viral pathogens, virus-induced cancers, genomics, immunology, and immunotherapies. “If there is anything that we can do as a technology partner to help institutions with the mission to advance life sciences and health care, we’re very happy to do it,” he said. In many cases, researchers feel progress in these areas is being limited by the ability to gather and analyze massive datasets. The clinic is betting that a system that combines AI, quantum computing, and hybrid
cloud technologies will remove those hurdles and unleash new health care innovation. Annunziata said part of the work will be to learn just where quantum computing sits in that computing system. Quantum is not robust enough to replace all computing functions. And even in many best-case scenarios, researchers believe quantum computing will be best suited for particular functions. Health care has long been touted as a strong potential use case. Quantum proponents are betting that such computers will be able to develop more sophisticated models of the human body, allowing for the development of better hypotheses for designing experiments, as well as models that speed the testing of new drugs. The key is learning which tasks in the Cleveland system can be offloaded to the quantum computer — with the results then fed back into the classic computing architecture, Annunziata said. At the same time, the Cleveland Clinic partnership will provide an opportunity to train a quantum workforce for the coming years as more commercial partners look for such skillsets."
https://venturebeat.com/2021/03/29/cloudera-adds-sql-tool-to-query-streaming-data/,Cloudera adds SQL tool to query streaming data,"Cloudera announced today it has added to its portfolio a Cloudera SQL Stream Builder tool based on technology it gained with the acquisition of Eventador that makes it possible to employ SQL to query streams of data in real time. That Eventador tool is now integrated with a Cloudera DataFlow (CDF) streaming platform that provides a common framework for processing streaming data using open source Apache Flink, Kafka Streams, or Spark Structured Streaming engines. Previously, the only way to query that data was using programming tools based on Java or Scala. Now data analysts can now query CDF data without having to know how to write code, said Dinesh Chandrasekhar, head of product marketing for Cloudera. SQL Stream Builder also enables analysts to create views of query results that can be exposed to other applications via REST application programming interfaces (APIs). It has also been integrated with the Shared Data Experience (SDX) framework Cloudera created to enforce governance and security policies across CDF. Despite the rise of a wide range of programming languages employed to analyze data, the dominant lingua franca for querying data in the enterprise remains SQL. However, as the need to query data as it streams in real time becomes larger, organizations want to be able to extend SQL to, for example, potentially identify anomalies in processes that would be indicative of potential fraud, Chandrasekhar said. Much of the increased need to query streaming data is being driven by digital business transformation initiatives that process and analyze data in real time using platforms such as Spark and Kafka. At some point, an analyst is going to need to launch an ad hoc query against that data to resolve a pressing issue long before the data is eventually stored in a relational database. “Data has a shelf life,” said Chandrasekhar. Rather than having to find a developer to write that query in Java or some other programming language to achieve that goal, it’s now possible for an analyst to immediately launch a SQL query themselves. Previously, that query might not have ever been launched simply because it would have taken too much time and effort to find a developer to write the code. In general, more data than ever is being processed and analyzed at both the points where it is created and consumed and where it moves between applications in real time. Cloudera is betting much of that data will ultimately land in a data warehouse based on the open source distribution of Hadoop that it provides. However, in the last few years, rival SQL-compatible data lakes based on proprietary platforms managed by cloud service providers have been gaining traction at the expense of provider of platforms based on Hadoop. Cloudera, with the launch of Cloudera SQL Stream Builder, is adding one more SQL-compatible tool to a portfolio that makes it possible to query data residing in Hadoop and other frameworks such as Apache Spark that are typically deployed on top of Hadoop. It’s not clear just yet to what degree those capabilities will enable Cloudera to counter the recent successes of its rivals. However, as a provider of a data warehouse platform based on open source software, Cloudera does appeal to IT organizations that have decided to avoid proprietary software whenever possible. Regardless of what tool is employed to analyze data, there’s more of it than ever being generated faster. The degree to which humans will be able to analyze data that is generated in real time remains to be seen. Many of the digital processes that organizations are trying to analyze occur in milliseconds, which is too fast for a human being to catch without help from some form of AI. Nevertheless, there’s a lot data residing in streaming platforms that can be queried. The challenge now is knowing how to first structure those SQL queries and, just as importantly, when to launch them."
https://venturebeat.com/2021/03/29/adversarial-training-reduces-safety-of-neural-networks-in-robots-research/,Adversarial training reduces safety of neural networks in robots: Research,"This article is part of our reviews of AI research papers, a series of posts that explore the latest findings in artificial intelligence. There’s a growing interest in employing autonomous mobile robots in open work environments such as warehouses, especially with the constraints posed by the global pandemic. And thanks to advances in deep learning algorithms and sensor technology, industrial robots are becoming more versatile and less costly. But safety and security remain two major concerns in robotics. And the current methods used to address these two issues can produce conflicting results, researchers at the Institute of Science and Technology Austria, the Massachusetts Institute of Technology, and Technische Universitat Wien, Austria have found. On the one hand, machine learning engineers must train their deep learning models on many natural examples to make sure they operate safely under different environmental conditions. On the other, they must train those same models on adversarial examples to make sure malicious actors can’t compromise their behavior with manipulated images. But adversarial training can have a significantly negative impact on the safety of robots, the researchers at IST Austria, MIT, and TU Wien discuss in a paper titled “Adversarial Training is Not Ready for Robot Learning.” Their paper, which has been accepted at the International Conference on Robotics and Automation (ICRA 2021), shows that the field needs new ways to improve adversarial robustness in deep neural networks used in robotics without reducing their accuracy and safety. Deep neural networks exploit statistical regularities in data to carry out prediction or classification tasks. This makes them very good at handling computer vision tasks such as detecting objects. But reliance on statistical patterns also makes neural networks sensitive to adversarial examples. An adversarial example is an image that has been subtly modified to cause a deep learning model to misclassify it. This usually happens by adding a layer of noise to a normal image. Each noise pixel changes the numerical values of the image very slightly, enough to be imperceptible to the human eye. But when added together, the noise values disrupt the statistical patterns of the image, which then causes a neural network to mistake it for something else. Adversarial examples and attacks have become a hot topic of discussion at artificial intelligence and security conferences. And there’s concern that adversarial attacks can become a serious security concern as deep learning becomes more prominent in physical tasks such as robotics and self-driving cars. However, dealing with adversarial vulnerabilities remains a challenge. One of the best-known methods of defense is “adversarial training,” a process that fine-tunes a previously trained deep learning model on adversarial examples. In adversarial training, a program generates a set of adversarial examples that are misclassified by a target neural network. The neural network is then retrained on those examples and their correct labels. Fine-tuning the neural network on many adversarial examples will make it more robust against adversarial attacks. Adversarial training results in a slight drop in the accuracy of a deep learning model’s predictions. But the degradation is considered an acceptable tradeoff for the robustness it offers against adversarial attacks. In robotics applications, however, adversarial training can cause unwanted side effects. “In a lot of deep learning, machine learning, and artificial intelligence literature, we often see claims that ‘neural networks are not safe for robotics because they are vulnerable to adversarial attacks’ for justifying some new verification or adversarial training method,” Mathias Lechner, Ph.D. student at IST Austria and lead author of the paper, told TechTalks in written comments. “While intuitively, such claims sound about right, these ‘robustification methods’ do not come for free, but with a loss in model capacity or clean (standard) accuracy.” Lechner and the other coauthors of the paper wanted to verify whether the clean-vs-robust accuracy tradeoff in adversarial training is always justified in robotics. They found that while the practice improves the adversarial robustness of deep learning models in vision-based classification tasks, it can introduce novel error profiles in robot learning. Say you have a trained convolutional neural network and want to use it to classify a bunch of images stored in a folder. If the neural network is well trained, it will classify most of them correctly and might get a few of them wrong. Now imagine that someone inserts two dozen adversarial examples in the images folder. A malicious actor has intentionally manipulated these images to cause the neural network to misclassify them. A normal neural network would fall into the trap and give the wrong output. But a neural network that has undergone adversarial training will classify most of them correctly. It might, however, see a slight performance drop and misclassify some of the other images. In static classification tasks, where each input image is independent of others, this performance drop is not much of a problem as long as errors don’t occur too frequently. But in robotic applications, the deep learning model is interacting with a dynamic environment. Images fed into the neural network come in continuous sequences that are dependent on each other. In turn, the robot is physically manipulating its environment. “In robotics, it matters ‘where’ errors occur, compared to computer vision which primarily concerns the amount of errors,” Lechner says. For instance, consider two neural networks, A and B, each with a 5% error rate. From a pure learning perspective, both networks are equally good. But in a robotic task, where the network runs in a loop and makes several predictions per second, one network could outperform the other. For example, network A’s errors might happen sporadically, which will not be very problematic. In contrast, network B might make several errors consecutively and cause the robot to crash. While both neural networks have equal error rates, one is safe and the other isn’t. Another problem with classic evaluation metrics is that they only measure the number of incorrect misclassifications introduced by adversarial training and don’t account for error margins. “In robotics, it matters how much errors deviate from their correct prediction,” Lechner says. “For instance, let’s say our network misclassifies a truck as a car or as a pedestrian. From a pure learning perspective, both scenarios are counted as misclassifications, but from a robotics perspective the misclassification as a pedestrian could have much worse consequences than the misclassification as a car.” The researchers found that “domain safety training,” a more general form of adversarial training, introduces three types of errors in neural networks used in robotics: systemic, transient, and conditional. Transient errors cause sudden shifts in the accuracy of the neural network. Conditional errors will cause the deep learning model to deviate from the ground truth in specific areas. And systemic errors create domain-wide shifts in the accuracy of the model. All three types of errors can cause safety risks. To test the effect of their findings, the researchers created an experimental robot that is supposed to monitor its environment, read gesture commands, and move around without running into obstacles. The robot uses two neural networks. A convolutional neural network detects gesture commands through video input coming from a camera attached to the front side of the robot. A second neural network processes data coming from a lidar sensor installed on the robot and sends commands to the motor and steering system. The researchers tested the video-processing neural network with three different levels of adversarial training. Their findings show that the clean accuracy of the neural network decreases considerably as the level of adversarial training increases. “Our results indicate that current training methods are unable to enforce non-trivial adversarial robustness on an image classifier in a robotic learning context,” the researchers write. “We observed that our adversarially trained vision network behaves really opposite of what we typically understand as ‘robust,'” Lechner says. “For instance, it sporadically turned the robot on and off without any clear command from the human operator to do so. In the best case, this behavior is annoying, in the worst case it makes the robot crash.” The lidar-based neural network did not undergo adversarial training, but it was trained to be extra safe and prevent the robot from moving forward if there was an object in its path. This resulted in the neural network being too defensive and avoiding benign scenarios such as narrow hallways. “For the standard trained network, the same narrow hallway was no problem,” Lechner said. “Also, we never observed the standard trained network to crash the robot, which again questions the whole point of why we are doing the adversarial training in the first place.” “Our theoretical contributions, although limited, suggest that adversarial training is essentially re-weighting the importance of different parts of the data domain,” Lechner says, adding that to overcome the negative side-effects of adversarial training methods, researchers must first acknowledge that adversarial robustness is a secondary objective, and a high standard accuracy should be the primary goal in most applications. Adversarial machine learning remains an active area of research. AI scientists have developed various methods to protect machine learning models against adversarial attacks, including neuroscience-inspired architectures, modal generalization methods, and random switching between different neural networks. Time will tell whether any of these or future methods will become the golden standard of adversarial robustness. A more fundamental problem, also confirmed by Lechner and his coauthors, is the lack of causality in machine learning systems. As long as neural networks focus on learning superficial statistical patterns in data, they will remain vulnerable to different forms of adversarial attacks. Learning causal representations might be the key to protecting neural networks against adversarial attacks. But learning causal representations itself is a major challenge and scientists are still trying to figure out how to solve it. “Lack of causality is how the adversarial vulnerabilities end up in the network in the first place,” Lechner says. “So, learning better causal structures will definitely help with adversarial robustness.” “However,” he adds, “we might run into a situation where we have to decide between a causal model with less accuracy and a big standard network. So, the dilemma our paper describes also needs to be addressed when looking at methods from the causal learning domain.” Ben Dickson is a software engineer and the founder of TechTalks. He writes about technology, business, and politics. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/03/29/canalys-more-data-breaches-in-2020-than-previous-15-years-despite-10-growth-in-cybersecurity-spending/,Canalys: More data breaches in 2020 than previous 15 years despite 10% growth in cybersecurity spending,"As cyberattacks increase, it’s hard not to wonder if enterprises are fighting a losing battle. According to a new report by Canalys, companies are spending record sums on cybersecurity, and yet the number of successful attacks is higher than ever. Canalys’ report noted that “more records were compromised in just 12 months than in the previous 15 years combined.” These are being driven in particular by ransomware attacks that have become more severe, in some cases disrupting hospitals. These attacks have also caused some companies to shut down and others to put emergency response plans in place to avoid being shuttered. This carnage is happening despite the fact that cybersecurity investment grew 10% in 2020 to $53 billion. So what’s going on? Canalys believes that companies are still under-investing in cybersecurity. During the pandemic, other areas of IT grew faster, signaling that enterprises were placing an emphasis on services that would help them remain stable during the pandemic or even grow rather than protect their infrastructure from attack. Indeed, some enterprises may have increased their vulnerability by responding to the pandemic in ways that ignored their safety policies. But the real culprit may simply have been failing to make security a top priority. Compared to the 10% growth in cybersecurity spending, cloud infrastructure services grew 33% in 2020, cloud software services rose 20%, notebook PC shipments jumped 17%, Logitech’s webcam sales increased 138%, and wi-fi router sales surged 40%. “Cybersecurity must be front and center of digital plans, otherwise there will be a mass extinction of organizations, which will threaten the post-COVID-19 economic recovery,” said Canalys Chief Analyst Matthew Ball in a statement. “A lapse in focus on cybersecurity is already having major repercussions, resulting in the escalation of the current data breach crisis and acceleration of ransomware attacks.”"
https://venturebeat.com/2021/03/29/solarwinds-hackers-accessed-emails-from-u-s-department-of-homeland-security/,SolarWinds hackers accessed emails from U.S. Department of Homeland Security,"(Reuters) — Hackers suspected of working for Russia got access to an email account belonging to the former head of the U.S. Department of Homeland Security, which is responsible for cybersecurity, in the SolarWinds hack, the Associated Press reported here on Monday. The AP report said the intelligence value of the hacking of Chad Wolf, the former acting secretary of the DHS, and of email accounts belonging to officials in the department’s cybersecurity staff, was not publicly known. The DHS did not immediately respond to a request for comment. In the security breach at SolarWinds which came to light in December, hackers infiltrated the U.S. tech company’s network management software and added code that allowed them to spy on end users. The hackers penetrated nine federal agencies and 100 companies. Last week, Reuters reported that a planned Biden administration executive order would require many software vendors to notify their federal government customers when the companies have a cybersecurity breach."
https://venturebeat.com/2021/03/29/5g-will-inspire-new-kinds-of-games-not-just-better-ones/,"5G will inspire new kinds of games, not just better ones","Presented by Verizon It’s no surprise that the latest generation of video game consoles is more popular than the average collectible sneaker launch. The wave of excitement that comes with each shoe launch inspires competition among collectors, and those same forces push gamers to scour retailers in hopes of unlocking hours of entertainment with a new gaming console. Now imagine if after getting that coveted game, you still have to fight to play. Imagine all that excitement has to be tempered because the next-gen game relied on last-gen infrastructure that wasn’t made to handle it. Mobile gamers and developers don’t need to imagine that; that’s been their reality — until now. I spent much of 2020 speaking to people who wondered, “Why do I need 5G?” “Gaming” was one of my favorite replies. 5G can offer speeds significantly faster than 4G LTE and, more importantly, it can enable ultra-low lag and massive capacity. Players can tap a button and get an almost instant response, which makes all the difference when a split second can influence who wins and who waits for the next round, lamenting their loss. Niantic Founder and CEO John Hanke recently shared that 100,000 players at a Pokémon GO event in Germany once pushed local capacity to hang on “by the skin of our teeth.” The promise of 5G is made clear in those moments. It’s not simply because current games and players demand better performance, but because new capabilities can empower bolder ideas to bring the real world and virtual together. The 5G era presents an opportunity for truly disruptive, immersive experiences that require more than building a faster horse. This is a moment for automobiles, or better yet, spaceships. 5G is a supercharged rocket ready to transform mobile and cloud gaming, and we should think bigger about what comes next. 5G can enable faster response times, so let’s build games that support advanced AR gaming competitions anywhere at any time. Cloud-based gaming can be vastly superior with 5G, so now is as good a time as any to blur the lines between console and mobile even further. Let’s reimagine virtual drone racing or cross-platform showdowns between friends in the park. Multiplayer is a must-have experience for many gamers, and 5G offers an opportunity to invite more players and create more challenging scenarios. John Hanke and I explored what that might mean when discussing 5G and the metaverse, the convergence of the physical and virtual worlds, but there’s so much more to discover and build to get towards that future. While gamers might crave better graphics and faster load times, titles are ultimately judged by the experiences they create. What experiences can we deliver with 5G that weren’t previously available? I’d be interested in knowing what choices a developer might make if network performance was no longer a major concern. If we can bring the computing power to the edge of the network, as we’re doing with 5G Edge, there’s more room for development, creativity and capability. Games can be more immersive and dynamic in ways we probably can’t even imagine. When I think about 5G’s impact on every industry, this illustration showing the difference in developer perception drives the point home beautifully:  5G has the potential to nurture more creativity, so Verizon built 5G for gamers. We worked closely with esports pros, device manufacturers, and game developers to create the infrastructure necessary for next-gen gaming. Building our 5G Labs has taught us how to create a superior connection that empowers developers and players, and we’re constantly seeking ways to advance our edge on network experience. We often host events to showcase the power of our 5G Ultra Wideband network, but a switch to virtual events inspired us to think bigger. In February, Verizon hosted the largest activation ever built in Fortnite’s Creative Mode, and 5G was at the heart of its development. More than 40 million Fortnite players entered the Verizon 5G Stadium to explore the unique environment, play games, and meet star athletes and pro gamers. It was the biggest virtual event we’ve done yet, and it showed us there’s so much more we can do. Today, Verizon 5G Ultra Wideband is available in 67 cities and 54 stadiums and arenas, and we plan to grow those numbers by the end of 2021. In fact, over the next 12 months, we expect to have incremental 5G bandwidth via new C-band spectrum available to 100 million people in 46 markets, delivering 5G on C-Band spectrum. As we continue to build the infrastructure that enables new thinking in gaming, we seek games that truly take advantage of 5G’s promise. There could be games with more dynamic AR layers unlike what we’ve seen; games that can handle 100,000 players in one area and thousands more in another nearby location. The network that can meet the needs of tomorrow’s gamers is already here and rapidly expanding. Now it’s time to see an ecosystem built on 5G grow even faster. At Verizon, we’re eager to hear from game developers and other industry players ready to level up, so we encourage them to visit verizon5glabs.com/gaming/ and explore what’s possible. We look forward to seeing the ambition 5G inspires once mobile and cloud gaming developers have the freedom to create without limits. I can’t wait to put on my far-from-collectible shoes and venture out to explore the next gaming experience. Ronan Dunne is EVP & CEO, Verizon Consumer Group. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/29/cere-network-raises-5-million-to-create-decentralized-data-cloud-platform/,Cere Network raises $5 million to create decentralized data cloud platform,"Cere Network has raised $5 million for its decentralized data cloud (DDC) platform, which is launching today for developers. The company’s ambition is to take on data cloud leader Snowflake. The investment was led by Republic Labs, the investment arm of crowdsourced funding platform Republic. Other investors include Woodstock Fund, JRR Capital, Ledger Prime, G1 Ventures, ZB exchange, and Gate.io exchange. Cere Network previously raised $5 million from Binance Labs and Arrington XRP Capital, amongst others, bringing its total raised to $10 million. “Enterprises using Snowflake are still constrained by bureaucratic data acquisition processes, complex and insufficient cloud security practices, and poor AI/ML governance,” Cere Network CEO Fred Jin said in an email to VentureBeat. “Cere’s technology allows more data agility and data interoperability across different datasets and partners, which extracts more value from the data faster compared to traditional compartmentalized setup.” The Cere DDC platform launches to developers today, which allows thousands of data queries to be hosted on the blockchain, the transparent and secure digital ledger. The platform offers a more secure first-party data foundation in the cloud by using blockchain identity and data encryption to onboard and segment individual consumer data. This data is then automated into highly customizable and interoperable virtual datasets, directly accessible in near real time by all business units, partners/vendors, and machine-learning processes. The Cere token will be used to power its decentralized data cloud and fuel Cere’s open data marketplace that allows for trustless data-sharing among businesses and external data specialists, as well as staking and governance. The public sale of the Cere token will be held on Republic, the first token sale on the platform. “We’ve been following Cere Network for some time and have been impressed with the team and the market fit – and need – for a decentralized data cloud,” said Boris Revsin, managing director of Republic Labs, in a statement. “We’re very excited to host Cere Network’s token sale on Republic, which will ensure a decentralized network and faster adoption in the enterprise space of blockchain technology. Their DDC improves upon Snowflake using blockchain identity and data encryption to onboard and segment individual consumer data.” Developers can access the Cere DDC here. The public sale for Cere token is scheduled for March 31 on Republic. The company said it is working with a number of Fortune 1,000 customers. “There’s a huge amount of opportunities in this rapidly shifting space for the coming years. We don’t plan to take on the likes of Snowflake head on, yet, but rather focus on specific solutions and verticals where we can bring more customization and efficiency. We are ok with chipping away at their lead while doing this,” Jin said. “We are bringing an open data marketplace which will open up data access beyond the limitation of traditional silo’d data ecosystems, which include Snowflake, and the likes of Salesforce.”"
https://venturebeat.com/2021/03/29/honeywell-says-quantum-computers-will-outpace-standard-verification-in-18-to-24-months/,Honeywell says quantum computers will outpace standard verification in ’18 to 24 months’,"Honeywell expects that as advances in quantum computing continue to accelerate over the next 18 to 24 months, the ability to replicate the results of a quantum computing application workload using a conventional computing platform simulation will come to an end. The company’s System Model H1 has now quadrupled its performance capabilities to become the first commercial quantum computer to attain a 512 quantum volume. Ascertaining quantum volume requires running a complex set of statistical tests that are influenced by the number of qubits, error rates, connectivity of qubits, and cross-talk between qubits. That approach provides a more accurate assessment of a quantum computer’s processing capability that goes beyond simply counting the number of qubits that can be employed. Honeywell today provides access to a set of simulation tools that make it possible to validate the results delivered on its quantum computers on a conventional machine. Those simulations give organizations more confidence in quantum computing platforms by allowing them to compare results. However, quantum computers are now approaching a level where at some point between 2022 and 2023 that will no longer be possible, Honeywell Quantum Solutions president Tony Uttley said. Honeywell has pursued an approach to quantum computing that differs from those of rivals by focusing its efforts on a narrower range of more stable qubits. Each system is based on a trapped-ion architecture that leverages numerous individual charged atoms (ions) to hold information. It then applies electromagnetic fields to hold (trap) each ion in a way that allows it to be manipulated and encoded using laser pulses. The company makes its quantum computers available via a subscription to a cloud service and counts BMW, DHL, JP Morgan Chase, and Samsung among its customers. Systems residing outside of Boulder, Colorado and Minneapolis are made available to customers for up to two weeks at a time before being taken offline for two weeks to add additional capacity. Subscriptions for the System Model H1 service are currently sold out, and each Honeywell quantum computing customer has previously tried to employ a different platform before switching to Honeywell, Uttley said. The company is now moving toward making a third-generation System Model H2 service available that will offer higher levels of unspecified quantum volume, Uttley added. Honeywell has committed to delivering a tenfold increase in quantum volume every five years. The company has been able to deliver a fourfold increase in the amount of quantum volume it can make available in the last five months alone, Uttley said. Quantum computers can process bits that have a value of both 0 and 1 at the same time, which makes them more powerful than conventional computing platforms. Advances in quantum computing, however, will by no means signal the demise of conventional computers, Uttley added. Instead, it’s becoming apparent that quantum computers and conventional computers are simply going to be better suited to running different classes of workloads, Uttley said. “These systems will run side by side for decades,” Uttley added. “Conventional computing platforms are not going to be replaced anytime soon.” Quantum computers, however, are better suited to addressing complex computational challenges involving chemistry, routing optimizations using, for example, logistics and traffic management applications, and even the training of AI models. In the latter case, a quantum computer can identify the starting point for the training of an AI model that would then be completed by a conventional computer. Other more intractable problems involving, for example, applications for ways to reduce the level of carbon in the atmosphere are only feasible to run on a quantum computing platform. It may still be a while before quantum computing delivers on its full promise, but while the way quantum systems work may not be widely understood, there is now no turning back."
https://venturebeat.com/2021/03/29/how-tiktok-is-evolving-sequential-marketing/,How TikTok is evolving sequential marketing,"Presented by GrowMojo Intelligent Tracking Prevention (ITP) is here, but fortunately there is a solution to allow businesses to continue to effectively target their ideal customers. In the past, marketers have thrived with advertising on Facebook and Google platforms by using cookies and pixels to target potential customers. Unfortunately, cookies and pixels have been abused by bad actors, so the push for more privacy has put cookies and pixels in the crosshairs of privacy advocates. Apple has reacted with their iOS 14 update that will make it much harder for tracking across websites. Google is now being forced to follow the lead of the other browsers, so Chrome will soon be blocking third-party cookies. Facebook is begrudgingly following along and is forcing their advertisers to phase out their Facebook pixel. Since the marketing strategy of following potential customers wherever they might be on the web is going to disappear (yes, that means you aren’t going to be seeing those shoes you once looked at in ads everywhere you go), marketers will have to develop new ways to target and track future customers. Fortunately, there is a new marketing opportunity that can solve this challenge for marketers: advertising on TikTok using observable signals to build a direct relationship with potential customers. If you’re a marketer, you’re probably asking, “How do they know my customer is on TikTok?” The answer is simple. Everyone’s customer is on TikTok. TikTok started in 2020 with about 500 million users, and about 40 million of them are in the U.S. Now TikTok has well over a billion users with 200 million of them in the U.S. The TikTok app was the number-one most-downloaded app in 2020, and the TikTok community continues to grow at an amazing rate. The average TikTok user spends 28 minutes per day on the app. That is only six minutes less than the average amount of time spent on Facebook each day. New, fast-growing advertising platforms come around once every ten years or so. The emergence of Google and then Facebook accelerated the growth of businesses that were early adopters, but the new advertising opportunities were initially trials by fire. Leading marketing agencies partnered with fast-growing Google and Facebook to help brands quickly find a return on investment to make advertising on these new platforms successful. There is now a small group of TikTok-authorized agencies like GrowMojo that helps brands find quick success on TikTok. So how will TikTok help you convert those users into customers? To answer that question, we first need to talk about sequential marketing which is the process of walking a potential customer down your purchase journey before they ever reach your website.  The main reason why TikTok leads the evolution of sequential marketing is because nothing works better in sequential marketing than video — and TikTok is all about video. TikTok’s other main advantage is that your advertising on TikTok is mixed in with all the other organic TikToks. The best TikTok ads are the ones that blend in well with the organic content, so that your TikTok ads receive comments, likes, and shares at the same rate as organic TikToks. Then, when users have liked or watched 75%+ of your TikTok ad, you can easily retarget the users and walk them down your purchase journey. Sequential marketing is a combination of art and science. The artistic component is the marketer’s talent in creating an ad that feels organic to the TikTok user at first glance. The science involves measuring the engagement of your first qualifying video to determine which users are showing the level of interest that warrants showing them a second video. Once you know they are a potential customer, because they engaged with your TikTok ad, you can use sequential marketing to target them with more sales-oriented ads and move them down your product or service purchase journey. So, if you are the CMO of a B2B company or you target enterprise customers, you probably are thinking that this sequential marketing TikTok strategy doesn’t apply to you. Wrong. This strategy works for all types of products and services whether you are targeting consumers or enterprise decision-makers. It also works well for franchise marketing, B2B, and even long sales cycle products. You just need an organic-feeling TikTok video advertisement and then you will be able to start qualifying your potential customers. Additionally, if you have a list of leads, there is no cheaper and easier way to target people most similar to your target customers than to do it on TikTok. You simply need to upload your target list into TikTok and you are ready to sequentially market to the most similar prospects. There is nothing wrong with inserting your ad, whatever it might be, among the TikTok dance challenges and funny pranks. If you do it the right way, users will raise their hands to show their interest in your product or service, qualifying them for the next step in the marketing sequence. Finding new prospects is far more cost effective on TikTok than on Facebook, Google, or LinkedIn because TikTok’s paid ads can blend in more organically, leading to relevant connections when sharing your brand’s story. The solution to reaching potential customers hidden in a pool of a billion users is to aggressively filter them and funnel those most enthusiastic for your product or service. Success comes from getting the details right with sequential marketing. Not many brands have figured it out yet. If you want to identify and convert qualified customers in TikTok’s huge user base, use GrowMojo TikTok Marketing Agency to help you implement a successful sequential marketing program. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/29/facebook-and-google-partner-on-undersea-cables-to-link-north-america-with-southeast-asia/,Facebook and Google partner on undersea cables to link North America with Southeast Asia,"(Reuters) — Facebook said today it is planning two new undersea cables to connect Singapore, Indonesia, and North America in a project with Google and regional telecommunication companies to boost internet connection capacity between the regions. “Named Echo and Bifrost, those will be the first two cables to go through a new diverse route crossing the Java Sea, and they will increase overall subsea capacity in the trans-pacific by about 70%,” Facebook VP of Network Investments Kevin Salvadori told Reuters. He declined to specify the size of the investment but said it was “a very material investment for us in Southeast Asia.” The cables, according to Salvadori, will be the first to directly connect North America to some of the main parts of Indonesia and will increase connectivity for the central and eastern provinces of the world’s fourth most populous country. Salvadori said Echo is being built in partnership with Alphabet’s Google and Indonesian telecommunications company XL Axiata and should be completed by 2023. Bifrost is being done in partnership with Telin, a subsidiary of Indonesia’s Telkom, and Singaporean conglomerate Keppel and is due to be completed by 2024. The two cables, which will need regulatory approval, follow previous investments by Facebook to build up connectivity in Indonesia, one of its top five markets globally. While 73% of Indonesia’s population of 270 million are online, the majority access the web through mobile data, with less than 10% using a broadband connection, according to a 2020 survey by the Indonesian Internet Providers Association. Swathes of the country remain without any internet access. Facebook said last year it would deploy 3,000 km (1,8641 miles) of fiber optic cable in Indonesia across 20 cities, in addition to a previous deal to develop public Wi-Fi hot spots. In addition to the Southeast Asian cables, Facebook is continuing with its broader subsea plans in Asia and globally, including the Pacific Light Cable Network (PLCN), Salvadori said. “We are working with partners and regulators to meet all of the concerns that people have, and we look forward to that cable being a valuable, productive transpacific cable going forward in the near future,” he said. The 12,800 km PLCN, which is being funded by Facebook and Alphabet, had met U.S government resistance over plans for a Hong Kong conduit. It was originally intended to link the United States, Taiwan, Hong Kong, and the Philippines. Facebook said earlier this month it would drop efforts to connect the cable between California and Hong Kong due to “ongoing concerns from the U.S. government about direct communication links between the United States and Hong Kong.”"
https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/,MIT study finds ‘systematic’ labeling errors in popular AI benchmark datasets,"The field of AI and machine learning is arguably built on the shoulders of a few hundred papers, many of which draw conclusions using data from a subset of public datasets. Large, labeled corpora have been critical to the success of AI in domains ranging from image classification to audio classification. That’s because their annotations expose comprehensible patterns to machine learning algorithms, in effect telling machines what to look for in future datasets so they’re able to make predictions. But while labeled data is usually equated with ground truth, datasets can — and do — contain errors. The processes used to construct corpora often involve some degree of automatic annotation or crowdsourcing techniques that are inherently error-prone. This becomes especially problematic when these errors reach test sets, the subsets of datasets researchers use to compare progress and validate their findings. Labeling errors here could lead scientists to draw incorrect conclusions about which models perform best in the real world, potentially undermining the framework by which the community benchmarks machine learning systems. A new paper and website published by researchers at MIT instill little confidence that popular test sets in machine learning are immune to labeling errors. In an analysis of 10 test sets from datasets that include ImageNet, an image database used to train countless computer vision algorithms, the coauthors found an average of 3.4% errors across all of the datasets. The quantities ranged from just over 2,900 errors in the ImageNet validation set to over 5 million errors in QuickDraw, a Google-maintained collection of 50 million drawings contributed by players of the game Quick, Draw! The researchers say the mislabelings make benchmark results from the test sets unstable. For example, when ImageNet and another image dataset, CIFAR-10, were corrected for labeling errors, larger models performed worse than their lower-capacity counterparts. That’s because the higher-capacity models reflected the distribution of labeling errors in their predictions to a greater degree than smaller models — an effect that increased with the prevalence of mislabeled test data. In choosing which datasets to audit, the researchers looked at the most-used open source datasets created in the last 20 years, with a preference for diversity across computer vision, natural language processing, sentiment analysis, and audio modalities. In total, they evaluated six image datasets (MNIST, CIFAR-10, CIFAR-100, Caltech-256, and ImageNet), three text datasets (20news, IMDB, and Amazon Reviews), and one audio dataset (AudioSet). The researchers estimate that QuickDraw had the highest percentage of errors in its test set, at 10.12% of the total labels. CIFAR was second, with around 5.85% incorrect labels, while ImageNet was close behind, with 5.83%. And 390,000 label errors make up roughly 4% of the Amazon Reviews dataset. Errors included: A previous study out of MIT found that ImageNet has “systematic annotation issues” and is misaligned with ground truth or direct observation when used as a benchmark dataset. The coauthors of that research concluded that about 20% of ImageNet photos contain multiple objects, leading to a drop in accuracy as high as 10% among models trained on the dataset. In an experiment, the researchers filtered out the erroneous labels in ImageNet and benchmarked a number of models on the corrected set. The results were largely unchanged, but when the models were evaluated only on the erroneous data, those that performed best on the original, incorrect labels were found to perform the worst on the correct labels. The implication is that the models learned to capture systematic patterns of label error in order to improve their original test accuracy. In a follow-up experiment, the coauthors created an error-free CIFAR-10 test set to measure AI models for “corrected” accuracy. The results show that powerful models didn’t reliably perform better than their simpler counterparts because performance was correlated with the degree of labeling errors. For datasets where errors are common, data scientists might be misled to select a model that isn’t actually the best model in terms of corrected accuracy, the study’s coauthors say. “Traditionally, machine learning practitioners choose which model to deploy based on test accuracy — our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets,” the researchers wrote. “It is imperative to be cognizant of the distinction between corrected versus original test accuracy and to follow dataset curation practices that maximize high-quality test labels.” To promote more accurate benchmarks, the researchers have released a cleaned version of each test set in which a large portion of the label errors have been corrected. The team recommends that data scientists measure the real-world accuracy they care about in practice and consider using simpler models for datasets with error-prone labels, especially for algorithms trained or evaluated with noisy labeled data. Creating datasets in a privacy-preserving, ethical way remains a major blocker for researchers in the AI community, particularly those who specialize in computer vision. In January 2019, IBM released a corpus designed to mitigate bias in facial recognition algorithms that contained nearly a million photos of people from Flickr. But IBM failed to notify either the photographers or the subjects of the photos that their work would be canvassed. Separately, an earlier version of ImageNet, a dataset used to train AI systems around the world, was found to contain photos of naked children, porn actresses, college parties, and more — all scraped from the web without those individuals’ consent. In July 2020, the creators of the 80 Million Tiny Images dataset from MIT and NYU took the collection offline, apologized, and asked other researchers to refrain from using the dataset and to delete any existing copies. Introduced in 2006 and containing photos scraped from internet search engines, 80 Million Tiny Images was found to have a range of racist, sexist, and otherwise offensive annotations, such as nearly 2,000 images labeled with the N-word, and labels like “rape suspect” and “child molester.” The dataset also contained pornographic content like nonconsensual photos taken up women’s skirts. Biases in these datasets not uncommonly find their way into trained, commercially available AI systems. Back in 2015, a software engineer pointed out that the image recognition algorithms in Google Photos were labeling his Black friends as “gorillas.” Nonprofit AlgorithmWatch showed Cloud Vision API automatically labeled a thermometer held by a dark-skinned person as a “gun” while labeling a thermometer held by a light-skinned person as an “electronic device.” And benchmarks of major vendors’ systems by the Gender Shades project and the National Institute of Standards and Technology (NIST) suggest facial recognition technology exhibits racial and gender bias and facial recognition programs can be wildly inaccurate, misclassifying people upwards of 96% of the time. Some in the AI community are taking steps to build less problematic corpora. The ImageNet creators said they plan to remove virtually all of about 2,800 categories in the “person” subtree of the dataset, which were found to poorly represent people from the Global South. And this week, the group released a version of the dataset that blurs people’s faces in order to support privacy experimentation."
https://venturebeat.com/2021/03/28/ai-could-help-advertisers-recover-from-loss-of-third-party-cookies/,AI could help advertisers recover from loss of third-party cookies,"Options for targeting digital advertising in a way that doesn’t rely on cookies are increasing, thanks to advances in predictive analytics and AI that will ultimately lessen the current dominance of Google, Facebook, and other large-scale content aggregators. Google announced earlier this month that it will no longer allow third-party cookies to collect data via its Chrome browser. Many companies have historically relied on those cookies to better target their digital advertising, as the cookies enable digital ad networks and social media sites to create a profile of an end user without knowing specifically who that individual is. While that approach doesn’t necessarily breach anyone’s privacy, it does give many users the feeling that some entity is tracking the sites they visit in a way that makes them uncomfortable. Providers of other browsers, such as Safari from Apple and the open source Firefox browser, have already abandoned third-party cookies. To be clear, Google isn’t walking away from tracking user behavior. Instead, the company has created a Federated Learning of Cohorts (FLoC) mechanism to track user behavior that doesn’t depend on cookies to collect data. Instead of being able to target an ad to a specific anonymous user, advertisers are presented with an opportunity to target groups of end users that are now organized into cohorts based on data Google still collects. It remains to be seen how these initiatives might substantially change the user experience. However, some advertisers are now looking to employ machine learning algorithms and other forms of advanced analytics being made available via digital advertising networks to reduce their dependency on Google, Facebook, Twitter, Microsoft, and other entities that control massive online communities. For example, Equifax, a credit reporting agency, is working with Quantcast to place advertising closer to where relevant content is being originally created and consumed, said Joella Duncan, director of media strategy for North America at Equifax. “We want our marketing teams to be able to pull more levers,” Duncan said. “Third-party cookies are stale.” That approach provides the added benefit of lessening an advertiser’s dependency on walled online gardens dominated by a handful of companies, Quantcast CEO Konrad Feldman said. At the core of the Quantcast platform is an Ara engine that applies machine learning algorithms to data collected from 100 million online destinations in real time. That data is then analyzed using a set of predictive models that surface the behavioral patterns that make it possible to target ad campaigns. Those predictive models are scored a million times per second, in addition to being continuously updated to reflect recent events across the internet. “We’re not dependent on only one technique,” Feldman said. That capability not only benefits clients such as Equifax, it also enables publishers of original content to retain a larger share of the advertising revenue generated. Google, Facebook, and Microsoft are all now moving toward compensating publishers for content that appears on their sites, but the bulk of the advertising revenue will still wind up in their coffers. Quantcast is making a case for an alternative approach to digital advertising that would make it more evenly distributed. Advertisers are not likely to walk away from walled online gardens that make it cost-efficient for them to target millions of users. However, many of those same advertisers are looking for a way to more efficiently target narrower audience segments that might have a greater affinity for their products and services based on the content they regularly consume. The AI and advanced analytics capabilities being embedded within digital advertising platforms may not upend the business models used by Google, Facebook, and others and based on walled gardens that themselves were constructed using algorithms. But it’s becoming apparent that fissures in the walls of those gardens are starting to appear as other entities in the world of advertising apply their own AI countermeasures."
https://venturebeat.com/2021/03/28/reinforcement-learning-the-next-great-ai-tech-moving-from-the-lab-to-the-real-world/,Reinforcement learning: The next great AI tech moving from the lab to the real world,"Reinforcement learning (RL) is a powerful type of artificial intelligence technology that can be used to learn strategies to optimally control large, complex systems such as manufacturing plants, traffic control systems (road/train/aircraft), financial portfolios, robots, etc. It is currently transitioning from research labs to highly impactful, real world applications. For example, self-driving car companies like Wayve and Waymo are using reinforcement learning to develop the control systems for their cars. AI systems that are typically used in industry perform pattern recognition to make a prediction. For instance, they may recognize patterns in images to detect faces (face detection), or recognize patterns in sales data to predict a change in demand (demand forecasting), and so on. Reinforcement learning methods, on the other hand, are used to make optimal decisions or take optimal actions in applications where there is a feedback loop. An example where both traditional AI methods and RL may be used, but for different purposes, will make the distinction clearer. Say we are using AI to help operate a manufacturing plant. Pattern recognition may be used for quality assurance, where the AI system uses images and scans of the finished product to detect any imperfections or flaws. An RL system, on the other hand, would compute and execute the strategy for controlling the manufacturing process itself (by, for example, deciding which lines to run, controlling machines/robots, deciding which product to manufacture, and so on). The RL system will also try to ensure that the strategy is optimal in that it maximizes some metric of interest — such as the output volume — while maintaining a certain level of product quality. The problem of computing the optimal control strategy, which RL solves, is very difficult for some subtle reasons (often much more difficult than pattern recognition). In computing the optimal strategy, or policy in RL parlance, the main challenge an RL learning algorithm faces is the so-called “temporal credit assignment” problem. That is, the impact of an action (e.g. “run line 1 on Wednesday”) in a given system state (e.g. “current output level of machines, how busy each line is,” etc.) on the overall performance (e.g. “total output volume”) is not known until after (potentially) a long time. To make matters worse, the overall performance also depends on all the actions that are taken subsequent to the action being evaluated. Together, this implies that, when a candidate policy is executed for evaluation, it is difficult to know which actions were the good ones and which were the bad ones — in other words, it is very difficult to assign credit to the different actions appropriately. The large number of potential system states in these complex problems further exacerbates the situation via the dreaded “curse of dimensionality.”  A good way to get an intuition for how an RL system solves all these problems at the same time is by looking at the recent spectacular successes they have had in the lab. Many of the recent, prominent demonstrations of the power of RL come from applying them to board games and video games. The first RL system to impress the global AI community was able to learn to outplay humans in different Atari games when only given as input the images on screen and the scores received by playing the game. This was created in 2013 by London-based AI research lab Deepmind (now part of Alphabet Inc.). The same lab later created a series of RL systems (or agents), starting with the AlphaGo agent, which were able to defeat the top players in the world in the board game Go. These impressive feats, which occurred between 2015 and 2017, took the world by storm because Go is a very complex game, with millions of fans and players around the world, that requires intricate, long-term strategic thinking involving both the local and global board configurations. Subsequently, Deepmind and the AI research lab OpenAI have released systems for playing the video games Starcraft and DOTA 2 that can defeat the top human players around the world. These games are challenging because they require strategic thinking, resource management, and control and coordination of multiple entities within the game. All the agents mentioned above were trained by letting the RL algorithm play the games many many times (e.g. millions or more) and learning which policies work and which do not against different kinds of opponents and players. The large number of trials were possible because these were all games running on a computer. In determining the usefulness of various policies, the RL algorithm often employed a complex mix of ideas. These include hill climbing in policy space, playing against itself, running leagues internally amongst candidate policies or using policies used by humans as a starting point and properly balancing exploration of the policy space vs. exploiting the good policies found so far. Roughly speaking, the large number of trials enabled exploring many different game states that could plausibly be reached, while the complex evaluation methods enabled the AI system to determine which actions are useful in the long term, under plausible plays of the games, in these different states. A key blocker in using these algorithms in the real world is that it is not possible to run millions of trials. Fortunately, a workaround immediately suggests itself: First, create a computer simulation of the application (a manufacturing plant simulation, or market simulation etc.), then learn the optimal policy in the simulation using RL algorithms, and finally adapt the learned optimal policy to the real world by running it a few times and tweaking some parameters. Famously, in a very compelling 2019 demo, OpenAI showed the effectiveness of this approach by training a robot arm to solve the Rubik’s cube puzzle one-handed. For this approach to work, your simulation has to represent the underlying problem with a high degree of accuracy. The problem you’re trying to solve also has to be “closed” in a certain sense — there cannot be arbitrary or unseen external effects that may impact the performance of the system. For example, the OpenAI solution would not work if the simulated robot arm was too different from the real robot arm or if there were attempts to knock the Rubik’s cube out of the real robot arm (though it may naturally be — or be explicitly trained to be — robust to certain kinds of obstructions and interferences). These limitations will sound acceptable to most people. However, in real applications it is tricky to properly circumscribe the competence of an RL system, and this can lead to unpleasant surprises. In our earlier manufacturing plant example, if a machine is replaced with one that is a lot faster or slower, it may change the plant dynamics enough that it becomes necessary to retrain the RL system. Again, this is not unreasonable for any automated controller, but stakeholders may have far loftier expectations from a system that is artificially intelligent, and such expectations will need to be managed. Regardless, at this point in time, the future of reinforcement learning in the real world does seem very bright. There are many startups offering reinforcement learning products for controlling manufacturing robots (Covariant, Osaro, Luffy), managing production schedules (Instadeep), enterprise decision making (Secondmind), logistics (Dorabot), circuit design (Instadeep), controlling autonomous cars (Wayve, Waymo, Five AI), controlling drones (Amazon), running hedge funds (Piit.ai), and many other applications that are beyond the reach of pattern recognition based AI systems. Each of the Big Tech companies has made heavy investments in RL research — e.g. Google acquiring Deepmind for a reported £400 million (approx $525 million) in 2015. So it is reasonable to assume that RL is either already in use internally at these companies or is in the pipeline; but they’re keeping the details pretty quiet for competitive advantage reasons. We should expect to see some hiccups as promising applications for RL falter, but it will likely claim its place as a technology to reckon with in the near future. M M Hassan Mahmud is a Senior AI and Machine Learning Technologist at Digital Catapult, with a background in machine learning within academia and industry."
https://venturebeat.com/2021/03/28/industry-clouds-could-be-the-next-big-thing/,Industry clouds could be the next big thing,"Despite predictions of a cloud shift accelerated by the pandemic and Gartner projecting a $651 billion public cloud market in 2024, organizations have barely scratched the surface of public cloud adoption. So it might seem odd at this stage to ask, “What’s the next big thing in public clouds?” The war between traditional on-premises data center infrastructure providers such as Dell, HPE, and Cisco and the public cloud providers such as Amazon Web Services, Microsoft Azure, and Google Cloud is far from over. However, one opportunity worth examining is industry clouds. Industry clouds are collections of cloud services, tools, and applications optimized for the most important use cases in a specific industry. APIs, common data models and workflows, and other components are available to customize capabilities. Industry cloud solutions from major public cloud providers also typically offer a variety of software and services, including industry-specific applications, from partners. For example, Microsoft and SAP partner to deliver SAP supply chain solutions through Microsoft Cloud for Manufacturing. Industry clouds are of interest because of their potential to create value for both customers and public cloud providers. Established companies in industries feeling the sting of competition from cloud-native disrupters are especially good prospects for these types of solutions. For these companies, moving their core business applications to general-purpose public clouds can be challenging because they often rely on homegrown legacy applications or industry-specific software designed for on-premise data centers. These companies face a difficult choice. Simply “lifting and shifting” applications to the cloud could result in sub-optimal performance. Yet rewriting or optimizing them for the cloud would be time consuming and costly. Industry clouds have the potential to accelerate and take the risk out of their cloud migrations. An essential component of an industry cloud is that it must address the specific requirements of the industry it is designed to serve. For example, healthcare providers place a high priority on improving the patient experience but also require high levels of security, data protection, and privacy. These are necessary to demonstrate compliance with Health Insurance Portability and Accountability Act (HIPAA) regulations. Financial services companies value data analytics and AI for customer insights and new product development, and trading applications require latency measured in fractions of a second. Like healthcare, the financial services industry is a highly regulated industry. Specific characteristics of the retail industry include the need to continually collect and analyze large sets of data to improve inventory management. For some of these requirements — and especially when there are several in combination — general-purpose cloud solutions might not be enough. And given this has been the focus of most cloud migrations thus far, many traditional companies in highly competitive industries have fallen behind in the race to the cloud. This means they are not realizing anywhere near the value they could from adding public clouds to their IT infrastructures. In addition to public cloud, there are many industry specific SaaS options and new ones emerging. For example, in the healthcare industry, there are electronic health record (EHR) SaaS options available. Healthcare SaaS offerings include critical functions such as billing and supply chain. Another example is the pharmaceutical and life sciences vertical. Pharmaceutical SaaS offerings support clinical, medical, and compliance functions. The important point to highlight is that SaaS has been and continues to be the top cloud migration choice with a projected market spend for 2021 of  $117.7 billion according to Gartner. SaaS is an excellent choice for supporting industry specific needs, and the big hyperscalers have taken notice. Given the opportunity in industry clouds, it’s not surprising that Amazon, Microsoft, Google, and IBM now all offer a broad range of industry-specific cloud solutions. For long-established companies such as IBM and Microsoft, this development mirrors that of their computer and software businesses, which evolved from providing customers with technology to solve their business problems. Both IBM and Microsoft have long histories of vertical market experience and large vertical market customer bases they can leverage to build and support industry clouds. This gives them an advantage with some customers. But for all public cloud providers, industry clouds are a logical next step in the ongoing maturation of public clouds. The industry cloud opportunity has also attracted the attention of cloud service providers that offer support for migrating industry-specific applications to public clouds. Before the cloud, cottage industries sprung up to help customers of industry-standard applications deploy and maintain them. Some of these companies have evolved their businesses to support cloud migrations of these applications. For example, in healthcare, where Epic and Cerner dominate the U.S. hospital EMR market, with 29% and 26% of the market, respectively, numerous firms exist to help companies bring these applications to the cloud. Given regulations and the business imperative to protect data in EMRs, most of these companies support a hybrid cloud approach, a solution that combines on-site data centers with public clouds. They also provide solutions and special expertise in privacy, security, and disaster recovery. While some host the applications on private clouds, many form partnerships with one or more public cloud providers. At the same time, Epic and Cerner are establishing their own relationships with public cloud providers. It’s worth noting that a failed relationship between Epic and Google offers an object lesson for cloud providers seeking to make their mark in specific industries. Epic severed ties with Google after it came to light that in its work with Ascension, a large Missouri-based health system, Google employees gained access to patient information without consent when information was being transferred from on-site servers to Google servers. Even though Ascension has continued to work with Google, Epic shifted its focus to Microsoft. Cerner, too, moved away from Google in favor of Amazon. It’s still early days for industry clouds, and no doubt some are more marketing strategies than offerings tailored for specific industries in meaningful ways. That will change. However, in the meantime, companies evaluating industry clouds from public cloud providers should do so carefully, taking care to compare not just the industry cloud offerings from different providers but also the industry cloud of each provider to their general-purpose solution. There might not yet be that much of a difference. Kash Shaikh is CEO and President of Virtana, a cloud platform with AI-powered observability for migrating, optimizing, and monitoring cloud applications."
https://venturebeat.com/2021/03/27/covid-has-moved-us-closer-to-an-industrial-ar-and-vr-revolution/,COVID has moved us closer to an industrial AR and VR revolution,"During the pandemic, many of us have gotten used to working from our laptop at the dining room table and dialing in to meetings on Zoom and Microsoft Teams. But for those in an industrial occupation, working in the field, and often executing critical tasks, table-bound remote working tools fail to fulfill the promise of a connected, collaborative future. I work for telecoms network provider Ciena, and our employees often need to share visual information, coach each other, and diagnose issues while working with both hands on the subject in question. In other instances, we need to provide live point-of-view information while touring facilities or demoing our equipment. If you’ve ever tried to do this with technology that’s commonly available, you know that it simply won’t do to walk around holding up a laptop or tablet all the time. So we’ve become early adopters of AR and VR technology and developed our own extended reality solution. I have seen the future, and I am here to tell you, the industrial AR and VR revolution is primed. We’re at a tipping point where this technology that started as a consumer curiosity is quickly filling an essential enterprise need. Extended reality solves the problem of not being able to physically see or touch what our colleagues are working on, particularly when teaching or instructing on highly technical systems. Until now, AR and VR have been rather niche and experimental. Pokémon GO and VR games have given us a taste of what the technology can do. But in these consumer use cases, slower networks with high latency never posed the hazards of a life-threatening situation or potential failure of a business-critical application. Industrial applications, like allowing first responders to see their colleagues through the walls of a burning building or overlaying parts with assembly instructions, simply won’t work without a sufficiently robust underlying communication network with high speeds and low latency. When employee safety and business continuity are on the line, these applications don’t have room for a jittery network. The sheer number and density of connected devices in these applications can make them a challenge to deploy, which is why 5G and optical networking will play such an important role in their adoption. Future networks will also require additional attention in terms of monitoring and managing the quality of connections to ensure that hiccups in service don’t have dramatic consequences. As for our own deployment, there was no one specific solution that perfectly fit our needs. Many of the vendors developing industry-grade solutions are focused on defense/military. In order to suit our needs, we needed to buy hardware and software from a variety of sources and knit them together to create our ideal solution. While it did take some effort to customize the component mix, the benefits we realized were immense. For one, extended reality offers more immersive and engaging learning experiences. When you’re in a virtual environment, you’re ensconced and there are few external distractions — it’s almost impossible to not be engaged. Of course, we’re still discovering new ways to use extended reality in the work environment, but we’ve had great success at my company getting employees to buy into the technology because we let them experiment and create their own use cases (outlined below). We challenge them to try new approaches to engage with our customers and offer internal education/suggestions. As an example, we recently used AR headsets to instruct our partners on product design and quality assurance at some of our manufacturing sites. By equipping our own employees with headsets, our customers have the option of viewing a stream of what our employees are seeing through their computers at home. The presentation is so compelling that we have even sent equipment to our customers to get the full experience. We have also used AR to give customers virtual/interactive demos from our labs and to equip our IT team to support our remote offices without needing to physically travel. From a usage standpoint, this makes sense when you look at Zoom over the last year. A year ago, many people were skeptical about using Zoom to hold in-depth, inclusive and collaborative meetings in a remote setting. Fast forward to today, and we’ve realized that the technology has enabled us to work from home, reduce the need for travel, and enjoy more time with our families. I see the same epiphany happening with extended reality. At this early stage, we have approximately two dozen employees involved in extended reality projects, with headsets distributed to users as needed. Each headset has cost us roughly the equivalent of a laptop. Considering that the hardware is a one-time cost, the price is eventually negated by not needing to have subject matter experts physically present. What we have realized is that the important investment at this stage is in user experience. I think the best advice I can offer to organizations considering extended reality right now is that you need to be thinking about the layers that bring the whole experience together. For example, you’ll need experts in UI design who will be mindful of where notifications and heads-up displays are placed within the field of view. Chances are your application of extended reality will be unique, and you’ll need a programmer who can build in features and functions to the solution that cater to your specific business need, like the ability to recognize objects or impose information on other users’ views. Perhaps most importantly, you’ll need to ensure smooth functionality, both for continuity and user comfort. Extended reality solutions use a lot of data, and nothing is more distracting than visuals that skip or lag. Even worse, visual information that isn’t synchronized with the user’s mental ability to process that image will cause a nauseating effect that will ruin the experience completely. Although there are only a handful of headset providers today, our own experience with AR and VR makes me believe that the market is about to bust open. Much like with the adoption of Zoom, we’ll soon be wondering what took us all so long to embrace the technology, and the market for solutions will be as diverse as the market for cameras or laptops are today. Craig Williams is CIO at Ciena."
https://venturebeat.com/2021/03/27/does-your-enterprise-plan-to-try-out-gpt-3-heres-what-you-should-know/,Does your enterprise plan to try out GPT-3? Here’s what you should know,"In a previous article, I talked about the market advantages enterprises could reap by developing applications using OpenAI’s GPT-3 natural language model. Here I want to provide a bit of a primer for companies taking a first look at the technology. There’s currently a waiting list to gain access to the GPT-3 API, but I’ve had an opportunity to play around in the system. For those who haven’t tried it out yet, here are a few things to be prepared for: The input you give GPT-3 is some seed text that you want to train the model on. This is the context you’re setting for GPT-3’s response. But you also provide a “prefix” to its response. This prefix is a direction that controls the text generated by the model, and it’s marked with a colon at the end. For example, you can give a paragraph as context and use a prefix like “Explain to a 5-year-old:” to generate a simple explanation. (It is highly recommended not to add any space after the prefix). Below is a sample response from GPT-3.  As you can see in the above example, your prefix doesn’t need to follow any complex machine-readable encoding. It is just a simple human-readable phrase. You can use multiple prefixes to describe a larger or extended context — as in a chatbot example. You want to provide a history of chat to help the bot generate responses. This context is used to tune the output of GPT-3 and generate response. For instance, you could make the chatbot helpful and friendly, or you could make it assertive and unfriendly. In the example below, I’ve given GPT-3 four prefixes. I’ve provided sample output for the first three and then left GPT-3 to continue from there.  Since the output you get from the model depends entirely on the context you provide, it’s important to construct these elements carefully. Configurations are the settings shown at right in the examples above. These are parameters that you include with your API call that help tune the response. For example, you can change the randomness of responses using the Temperature configuration setting, which has a range from 0 to 1. If Temperature is set to 0, every time you make a call with some context you will get the same response. If the Temperature is 1 then the response will be highly randomized. Another configurable you can tune is Response Length, which limits the text sent back by the API. Keep in mind that OpenAI charges for use of the platform on a token basis rather than a per-word basis. And a token will usually cover you for four characters. So, in the testing phase, make sure to tune your response length so you don’t use all of your tokens right away. With the 3 month free trail of GPT-3 you get $18 worth of tokens. I ended up consuming almost 75% of mine just with some experimentation with the API. There are actually four different versions of the GPT-3 model available as “engines,” and each of them has a different pricing model. The usual cost for tokens as of today is $0.06 per thousand tokens for the DaVinci engine, which is best-performing of the four. The less user-friendly engines, Curie, Babbage, and Ada, are $.006, $0.0012, and $0.0008 per thousand tokens respectively. GPT-3 is probably the most famous example of an advanced natural-language-processing API, but it’s likely to become one of many as the NLP ecosystem matures. Machine learning as a service (MLaaS) is a powerful business model because you can either spend the time and money to pre-train a model yourself (for context, GPT-3 cost OpenAI nearly $12 million to train), or you can purchase a pre-trained model for pennies on the dollar. In GPT-3’s case, every call you make to the API is routed to some shared instance of the GPT-3 model running in OpenAI’s cloud. As mentioned earlier, the DaVinci engine performs best, but you should experiment for yourself with each engine for specific use cases. DaVinci is forgiving if your input context has spelling mistakes or extra/missing spaces, and it gives a very readable response. You can sense it has been trained on a larger corpus and is resilient to errors. The cheaper engines will need you to do more work to frame the context and usually will need tuning to get exactly kind of response expected. Below is an example of classification of companies with misspelled name FedExt in the context. DaVinci is able to get right response while Ada gets it wrong. Again, when we look up a specific drug interaction example, DaVinci gets to the point and answers the question much better than Ada or Babbage:  GPT-3 is a stateless language model, which means it doesn’t remember your previous requests or learn from them. It relies solely on its original training (which pretty much constitutes all the text on the internet) and the context and configuration you provide it. This is the major hurdle for enterprises in adoption. You can generate some very interesting demos, but for GPT-3 to be a serious contender for real-world use cases in banking, healthcare, industrial, etc. we will need to train models that are domain specific. For example, you would want a model trained on your company’s internal policy documents or patient health records or machinery manuals. So, applications built directly on top of GPT-3 may not have actual use to enterprises. A more lucrative monetization scheme could be to host GPT-3-like models as an API specialized for specific problems like drug discovery, insurance policy recommendation, financial reports summarization, planning machinery maintenance, etc. The end use would be to leverage an application built on a model built on top of another model. A specialized model built by an enterprise on its proprietary data will also need to be able to adapt based on new knowledge obtained from business documents in order to stay relevant. In the future, we will see more domain language models with an active learning capability. And we will most likely see an active learning business model from GPT-3 eventually, too, where organizations will be able to train an instance incrementally on their custom data. However, this will come at a significant price point since it will require OpenAI to host a unique instance for that customer. Dattaraj Rao is Innovation and R&D Architect at Persistent Systems and author of the book Keras to Kubernetes: The Journey of a Machine Learning Model to Production. At Persistent Systems, he leads the AI Research Lab. He has 11 patents in machine learning and computer vision."
https://venturebeat.com/2021/03/27/what-is-dataops-and-why-its-a-top-trend/,"What‌ ‌is‌ ‌DataOps,‌ ‌and‌ ‌why‌ ‌it’s‌ ‌a‌ ‌top‌ ‌trend‌","Enterprises‌ ‌have‌ ‌struggled‌ ‌to‌ ‌collaborate‌ ‌well ‌around‌ ‌their‌ ‌data, which hinders their ability to adopt‌ ‌transformative‌ ‌applications‌ ‌like‌ ‌AI.‌ ‌ ‌The‌ ‌evolution‌ ‌of‌ ‌‌DataOps‌ ‌could‌ ‌fix that problem. The‌ ‌term‌ ‌DataOps‌ ‌emerged‌ ‌seven‌ ‌years‌ ‌ago‌ to refer to ‌best‌ ‌practices‌ for ‌getting‌ ‌proper‌ ‌analytics,‌ ‌and research firm Gartner calls it a major trend encompassing several steps in the data lifecycle. Just‌ as‌ ‌the‌ ‌DevOps‌ ‌trend‌ ‌led‌ ‌to‌ ‌a‌ ‌better‌ ‌process‌ ‌for‌ ‌collaboration‌ ‌between‌ ‌‌developers‌ ‌and‌ ‌operations‌ ‌teams,‌ ‌DataOps‌ ‌refers‌ ‌to closer collaboration between various teams handling data and operations teams deploying data into applications. ‌Getting‌ ‌DataOps‌ ‌right‌ ‌is‌ ‌a‌ ‌significant‌ ‌challenge‌ ‌because‌ ‌of‌ ‌the‌ ‌multiple‌ ‌stakeholders‌ ‌and‌ ‌processes‌ ‌involved‌ ‌in‌ ‌the‌ ‌data‌ ‌lifecycle.‌ In the DevOps world, enterprises ‌can‌ ‌develop,‌ ‌test,‌ ‌and‌ ‌deploy‌ ‌app‌ ‌updates‌ ‌in‌ ‌a‌ ‌matter‌ ‌of‌ ‌hours.‌ It is harder to move that fast in the data world, as it‌ can ‌take‌ ‌eight‌ ‌months‌ ‌to‌ integrate ‌an‌ ‌ML‌ ‌model‌ ‌into‌ business‌ ‌workflows‌ ‌and‌ deliver tangible value. ‌”[Creating]‌ ‌a‌ ‌common‌ ‌architecture‌ ‌pattern‌‌ helps‌ ‌with‌ ‌operationalizing‌ ‌data‌ ‌science‌ ‌and‌ ‌ML‌ ‌pipelines‌ ‌and‌ ‌has‌ ‌been‌ ‌identified‌ ‌as‌ ‌one‌ ‌of‌ ‌the‌ ‌major‌ ‌trends‌ ‌for‌ ‌2021,” Gartner research director ‌Soyeb‌ ‌Barot said.‌ ‌
‌
Gartner‌ ‌‌predicts ‌enterprises‌ ‌will‌ ‌begin‌ ‌to‌ ‌see‌ ‌real‌ ‌gains‌ ‌in‌ ‌these‌ ‌efforts‌ ‌through‌ ‌the‌ ‌evolution‌ ‌and‌ ‌extension‌ ‌of‌ ‌DataOps‌ ‌to‌ ‌support‌ ‌trusted‌ ‌AI.‌ ‌The research firm ‌predicts‌ ‌the‌ ‌number‌ ‌of‌ ‌enterprises‌ ‌that‌ ‌have‌ ‌operationalized‌ their‌ ‌AI‌ ‌efforts‌ ‌will‌ ‌grow‌ ‌from‌ ‌8%‌ ‌in‌ ‌2020‌ ‌to‌ ‌70%‌ ‌in‌ ‌2025‌ ‌due‌ ‌to‌ ‌the‌ ‌maturity‌ ‌of‌ ‌AI‌ ‌orchestration‌ ‌platforms.‌ ‌ ‌
‌ Even so, ‌enterprises‌ ‌will‌ ‌struggle‌ ‌to‌ ‌move‌ ‌their‌ ‌AI‌ ‌predictive‌ ‌projects‌ ‌past‌ the‌ ‌proof‌ ‌of‌ ‌concept‌ stage ‌because‌ ‌they‌ ‌have‌ ‌not‌ ‌addressed‌ ‌the‌ ‌full‌ ‌range‌ ‌of‌ ‌processes‌ ‌for‌ ‌collaborating‌ ‌across‌ ‌the‌ ‌AI‌ ‌lifecycle.‌ ‌A‌ 2019‌ ‌Gartner‌ ‌survey‌ ‌found‌ ‌that‌ ‌the‌ ‌top‌ ‌four‌ ‌challenges‌ ‌companies‌ ‌face‌ were ‌security‌ ‌or‌ ‌privacy‌ ‌concerns‌ ‌(30%)‌ ,‌ ‌complexity‌ ‌of‌ ‌AI‌ ‌integration‌ ‌with‌ ‌existing‌ ‌infrastructure‌ ‌(30%)‌, ‌data‌ ‌volume‌ ‌or‌ ‌complexity‌ ‌(22%),‌ ‌and‌ ‌potential‌ ‌risks‌ ‌or‌ ‌liabilities‌ ‌(22%).‌ ‌
‌
Gartner‌ ‌argues‌ ‌that‌ ‌a‌ ‌more‌ ‌nuanced‌ ‌way‌ ‌of‌ ‌thinking‌ ‌about‌ ‌different‌ ‌types‌ ‌of‌ ‌collaboration‌ ‌can‌ ‌improve‌ ‌this‌ ‌transition.‌ ‌This‌ ‌includes‌ ‌extending‌ ‌the‌ ‌older‌ ‌idea‌ ‌of‌ ‌DataOps‌ ‌(data‌ ‌engineering)‌ ‌to‌ ‌include‌ ‌MLOps‌ ‌(machine‌ ‌learning‌ ‌development),‌ ‌ModelOps‌ ‌(AI‌ ‌governance),‌ ‌and‌ ‌Platform‌ ‌Ops‌ ‌(overarching‌ ‌AI‌ ‌platform‌ ‌management).‌ ‌It‌ ‌has‌ ‌characterized‌ ‌this‌ ‌entire‌ ‌collection‌ ‌of‌ ‌capabilities‌ ‌as‌ ‌XOps.‌ ‌ ‌
‌
‌”These‌ ‌frameworks‌ ‌can‌ ‌help‌ ‌implement‌ ‌a‌ ‌structured‌ ‌process‌ ‌for‌ ‌the‌ ‌people‌ ‌involved‌ ‌to‌ ‌productionalize‌ ‌AI.‌ ‌Think‌ ‌of‌ ‌it‌ ‌as‌ ‌the‌ ‌assembly‌ ‌line‌ ‌of‌ ‌an‌ ‌automobile‌ ‌manufacturing‌ ‌plant,‌ ‌but‌ ‌for‌ ‌data,” Barot said.‌ ‌ ‌Software development was historically a slow plodding process in which developers spent months or even years working on new updates that were collectively thrown over the wall to testing and operations teams. In 2008, Andrew Clay and Patrick Debois began discussing how to streamline this process through better collaboration between developers, testers, and operation teams. This came to be known as DevOps since it improved the handoff between development and operations teams. As the movement took hold, it led to the creation of a variety of platforms, tools, and processes that allowed teams to continuously integrate and deploy applications in small bits that could be rolled back if problems occurred. But these same kinds of innovations eluded efforts to create value from the growing volume, variety, and velocity of big data. As much as pundits predicted that big data was the new oil, companies struggled to operationalize big data in the way DevOps improved the deployment of code. Value is gleaned from data by creating artifacts like analytics, machine learning models, and data-driven applications. But doing these things introduced a variety of new challenges and bottlenecks outside the scope of DevOps practices. In a blog post for IBM in 2014, Lenny Liebmann, then a contributing editor at InformationWeek, introduced the notion of DataOps to characterize these challenges and suggest a path forward. In an interview with VentureBeat, Liebmann, who is now a founding partner of technology adoption consultancy Morgan Armstrong, said that at the time a lot of enterprises were struggling to solve big data problems using improved technology without addressing the organizational and process side. He said, “People thought you could just throw big data into a magic bucket and it would work.” But they bumped up against a variety of issues connecting disparate sources and types of data to new applications and analytics. One of the main issues he saw was that businesses would focus on the functional aspects, like moving the actual data around through better data engineering tools, without addressing non-functional issues like performance, availability, quality, scalability, security, and governance. A lot of the fundamental data engineering challenges have been solved as enterprises begin moving their infrastructure to the cloud. “This is less a problem today than when I first talked about it,” Liebmann said. The next step lies in mapping out a strategy to address security, governance, and quality issues as companies scale their data operations. Barot has had many conversations with enterprises asking for DataOps tools only to discover they already had a strong DataOps framework. They really needed more help in operationalizing their AI processes. This is where Gartner’s model of XOps emerges to provide the foundation for a more comprehensive set of distinctions. “We were looking at all these ‘ops’ terminologies in the marketplace, and there was ambiguity about what they were for and the relationship between them,” Barot said. “We wanted to set the record straight as to what they stand for and how they are related to each other as part of bigger strategic initiatives in the enterprise.” In this expanded taxonomy, Gartner constrains DataOps to the challenges associated with building, managing, and scaling data pipelines in a way that promotes reusability, reproducibility, and rolling back changes if problems occur. Some of these key capabilities include data extraction, integration, transformation, and analysis. Governance is constrained to the data itself. MLOps focuses on improving the collaboration across development and operationalization of the machine learning model development lifecycle. These activities are typically performed outside of the purview of traditional data engineering practices. Data scientists are often tasked with a process called feature engineering for tuning ML models to improve decision-making, discover insight, or enable a new application feature. MLOps makes it easier to tie these efforts in with teams on the operations side that are responsible for deploying the models into production. ModelOps is an extension of MLOps to help companies work with third-party AI models that may be baked into enterprise applications or improve decision-making using tools like knowledge graphs, rules engines, or new optimization algorithms. The biggest differentiation is that MLOps makes it easier for business experts to manage AI models with less reliance on data engineering and for data science teams to implement changes. Platform Ops provides an overarching framework to help organizations manage activities that span all of these different kinds of activities, as well as DevOps. It is also the youngest and most immature market. AIOps would probably have been a better term to describe this overall way of thinking about AI management, Barot said. However, the term was already widely used to describe the use of AI to improve IT operations management. While there are dozens of commercial products for the other domains, Barot said there are only four commercial Platform Ops tools today: Amazon SageMaker, Cloudera SDC, ForePaas, and OneLogic. There are also a variety of open source Platform Ops tools that are being championed by commercial vendors as part of their larger portfolio of AI tools. Barot expects to see intense competition among vendors rushing to become the AI orchestration platform other things get plugged into. Barot cautions that there are no silver bullet products. Every enterprise will need to adopt the best capabilities suited to their existing development practices and industry niche."
https://venturebeat.com/2021/03/27/the-enterprise-guide-to-experimenting-with-data-streaming/,The enterprise guide to experimenting with data streaming ,"Streaming data into your enterprise analytics systems in real time rather than loading it in batches can increase your ability to make time-sensitive decisions. Over the last few years, more and more enterprises and industries have started experimenting with data streaming, including the healthcare and financial services sectors. The global streaming analytics market size is expected to grow from $12.5 billion in 2020 to $38.6 billion by 2025, thanks to adoption in industries like manufacturing, government, energy and utilities, media and more. A company that is looking to explore data streaming capabilities does not need to go “all-in.” In fact, it’s best if you don’t. What’s becoming clear is that you can reap the benefits of data streaming without building out a fully mature solution. Limited projects and proof-of-concept work with data streaming can prove incredibly valuable for your organization. Data streaming concepts are highly transferrable. Learning one platform enables you to adopt other tools and capabilities with ease. So the key is not to start dabbling with data streaming early and often so that your engineering teams can start developing the necessary skillsets related to resilient, distributed system design and development. Adopting a data streaming architecture will help solve a number of challenges that will surface due to the increasing volume and scale of information organizations are able to tap into as a result of digitization. Getting started requires a shift in data strategy and implementation. Data strategy for many businesses, such as brick and mortar stores, manufacturers, and logistics firms, is grounded in core processes oriented to weekly or monthly batch calculations. Often, supporting applications using modern, cloud-based technology stacks are tailored to process data using a monthly ETL load — an inherent limitation to real-time enterprise insights. When you begin prototyping for data streaming, you will quickly uncover technical limitations and hidden requirements that will impact your ability to scale your model. So it’s important to make a deliberate investment in this kind of prototyping so that you can assess any roadblocks to a long-term strategy while creating tangible short-term opportunities to pilot streaming tactics and technologies. Embracing the incremental failures of prototyping is an effective path to a scalable data streaming architecture. Your best prototypes can scale into industry-leading competitive advantages. Failed prototypes, on the other hand, can be shut down after minimal investment and maximum learning. For example, my team built one proof of concept for a client to collect and correlate WiFi, authentication gateway, and endpoint protection platform (EPP) logs. We shut it down due to a lack of any data science models able to correlate events across these sources, but we were able to take away the learning that Syslog, Kafka, Confluent Kafka Connect, and Flink are capable of solving similar integration challenges in the future. Building a POC (proof of concept) or MVP (minimum viable product) always doubles as a risk management strategy by establishing technical feasibility and product viability with minimal investment. Let’s explore ways a data streaming prototype can add value. Start with a small team and a targeted goal of creating a POC solution to solve a particular business and technical problem. Then, evaluate the results to decide how best to scale the POC. Teams should approach prototyping with an exploratory mindset vs. executing a preconceived outcome on a small scale. Embrace failure and learnings when validating your streaming model with prototypes. POC, MVP, pilot — whatever name it goes by, prototyping will stop teams from creating products that don’t (or can’t) meet the business’s needs. You will learn a lot and mitigate a lot of risk by taking this “build, measure, learn” approach to validating your data streaming model before you try to scale it. Apache Kafka is a great place to start as it is the most widely adopted platform. Its cloud counterparts, Microsoft Azure Event Hub and AWS Kinesis, are either 100% compatible at a protocol level or operate using very similar concepts. Apache Kafka, Azure Event Hub, and AWS Kinesis are products focused on data ingestion. Google Dataflow and IBM Streaming Analytics are also popular options that act as a superset — bigger platforms with more capabilities. Since the POC has few risks related to scalability and data retention, you can even deploy a small Kafka cluster on premises. Several Kafka stack distributions such as Confluent, Bitnami, and Cloudera, provide an easy way to launch Kafka and its dependencies on container systems, virtual machines, or even spare PC desktop boxes. A team will want to tap into relational data and push relational data records to a low-latency data stream on Kafka. They will explore Change Data Capture (CDC) protocol and find out it works similarly for both a MS SQL-based warehouse and inventory system and a PostgreSQL-based e-commerce site. Both of these data sources are easily streamed into a Kafka feed category (or “topic”) as events. A modern single-page application (SPA) where customers can manage their personal profile and preferences can be also enriched to emit events to another data topic once relevant customer information is updated. After this analysis, the team will explore how they can aggregate and analyze streaming data. The data streaming and processing landscape (and big data in general) may seem daunting at first. There are many well-known players in the space, such as Flink and Spark for stream processing, MapReduce for batch processing, and Cassandra, HBase, and MariaDB for storing large volumes of data in a read-optimized columnar format. All of the technologies I’ve just mentioned work best to crunch specialized, massive data loads, and the POC does not operate at such a scale. Therefore, your prototype team will want to choose a data ingestion and aggregation platform with a user-friendly interface and SQL-like data retrieval support; it will likely be Confluent Kafka Connect, Lenses.io, Striim, or a similar commercial platform. All of these data sources, when combined, can provide timely insights via custom reports and real-time alerts. For example, if a B2B account has updated its credit limit in a self-service single page app, this event, pushed to a data stream, is available to an e-commerce site right away. Analytics on most products in the highest demand, busiest shopping hours, and even alerts on fraudulent activity (unusually high order amounts) can be produced by aggregating and processing windowed data streams from inventory and e-commerce. Even though the POC does not introduce complex, scalable data processing platforms such as Spark or Hadoop, you will be getting new reports and alerts in near real-time, meaning that the duration to obtain insight is reduced from weeks to minutes or even seconds. The POC will allow you to consider what other processes would benefit from real-time reporting and analytics. Meanwhile, the POC engineering team will learn important lessons about data model design. Poor design will lead to data duplication, which could become expensive and challenging when a POC is scaled to production levels, so it’s important to use these learnings when moving on to future iterations. IT and operations will also have learned that traditional concepts such as “database rollback” are not present in the streaming world. Monitoring is a must for a data streaming platform, as are support personnel with the appropriate skills. You can reduce the cost and complexity of operational support if you choose AWS Kinesis or Azure Event Hub instead of Apache Kafka, since cloud platforms are simpler to maintain. Data streaming provides a natural design for decoupling integrated systems. As data flows, it becomes available to all of its stakeholders independently, enabling services written for isolated use cases like data persistence, aggregate functions, anomaly detection, and many others. All of these are independent in terms of development and deployment. The benefits of having decoupled integrated systems is that each of these pieces can be introduced incrementally. This also allows you to scope your POC and focus on pieces that are important for your organization independently. Once you execute on a POC, there is a decision point: continue iterating, shut it down, or restart. Questions related to data modeling, integrations between systems, and potential AI/ML opportunities should surface at this point, giving your organization better insight into how to staff your development and operations teams for the future of streaming. Lastly, increased awareness of distributed systems will enable your technical teams to improve current back-office systems and map a modernization path for your organization. Bottom line: Your organization has a lot to gain and little to lose by piloting data streaming. Aurimas Adomavicius is President of DevBridge, a tech consultancy specialized in designing and implementing custom software products for companies across many industries."
https://venturebeat.com/2021/03/26/ai-weekly-algorithms-accountability-and-regulating-big-tech/,"AI Weekly: Algorithms, accountability, and regulating Big Tech","This week, Facebook CEO Mark Zuckerberg, Google CEO Sundar Pichai, and Twitter CEO Jack Dorsey went back to Congress, the first hearing with Big Tech executives since the January 6 insurrection led by white supremacists that directly threatened the lives of lawmakers. The main topic of discussion was the role social media plays in the spread of extremism and disinformation. The end of liability protections granted by Section 230 of the Communications Decency Act (CDA), disinformation, and how tech can harm the mental health of children were discussed, but artificial intelligence took center stage. The word “algorithm” alone was used more than 50 times. Whereas previous hearings involved more exploratory questions and took on a feeling of Geek Squad tech repair meets policy, in this hearing lawmakers asked questions based on evidence and seemed to treat tech CEOs like hostile witnesses. Representatives repeatedly cited a May 2020 Wall Street Journal article about an internal Facebook study that found that the majority of people who join extremist groups do so because the Facebook recommendation algorithm proposed that they do so. A recent MIT Tech Review article about focusing bias detection to appease conservative lawmakers instead of to reduce disinformation also came up, as lawmakers repeatedly asserted that self regulation was no longer an option. Virtually throughout the entirety of the more than five-hour long hearing, there was a tone of unvarnished repulsion and disdain for exploitative business models and willingness to sell addictive algorithms to children. “Big Tech is essentially handing our children a lit cigarette and hoping they stay addicted for life,” Rep. Bill Johnson (R-OH) said. In his comparison of Big Tech companies to Big Tobacco — a parallel drawn at Facebook and a recent AI research paper — Johnson quotes then-Rep. Henry Waxman (D-CA), who stated in 1994 that Big Tobacco had been “exempt from standards of responsibility and accountability that apply to all other American corporations.” Some congresspeople suggested laws to require tech companies to publicly report diversity data at all levels of a company and to prevent targeted ads that push misinformation to marginalized communities including veterans. Rep. Debbie Dingell (D-MI) suggested a law that would establish an independent organization of researchers and computer scientists to identify misinformation before it goes viral. Pointing to YouTube’s recommendation algorithm and its known propensity to radicalize people, Reps. Anna Eshoo (D-CA) and Tom Malinowski (D-NJ) introduced the Protecting Americans from Dangerous Algorithms Act back in October to amend Section 230 and allow courts to examine the role of algorithmic amplification that leads to violence. Next to Section 230 reform, one of the most popular solutions lawmakers proposed was a law requiring tech companies to perform civil rights audits or algorithm audits for performance. It might be cathartic seeing tech CEOs whose attitudes are described by lawmakers as smug and arrogant get their come-uppances for inaction on systemic issues that threaten human lives and democracy because they’d rather make more money. But after the bombast and bipartisan recognition of how AI can harm people on display Thursday, the pressure is on Washington, not Silicon Valley. I mean, of course Zuckerberg or Pichai will still need to answer for it when the next white supremacist terrorist action happens and it’s again drawn directly back to a Facebook group or YouTube indoctrination, but to date, lawmakers have no record of passing sweeping legislation to regulate the use of algorithms. Bipartisan agreement for regulation of facial recognition and data privacy has also not yet paid off with comprehensive legislation. Mentions of artificial intelligence and machine learning in Congress are at an all-time high. And in recent weeks, a national panel of industry experts have urged AI policy action to protect the national security interests of the United States, and Google employees have implored Congress to pass stronger laws to protect people who come forward to reveal ways AI is being used to harm people. The details of any proposed legislation will reveal just how serious lawmakers are about bringing accountability to those who make the algorithms. For example, diversity reporting requirements should include breakdowns of specific teams working with AI at Big Tech companies. Facebook and Google release diversity reports today, but those reports do not break down AI team diversity. Testing and agreed-upon standards are table stakes in industries where products and services can harm people. You can’t break ground on a construction project without an environmental impact report, and you can’t sell people medicine without going through the Food and Drug Administration, so you probably shouldn’t be able to freely deploy AI that reaches billions of people that’s discriminatory or peddles extremism for profit. Of course, accountability mechanisms meant to increase public trust can fail. Remember Bell, the California city that regularly underwent financial audits but still turned out to be corrupt? And algorithm audits don’t always assess performance. Even if researchers document a propensity to do harm, like analysis of Amazon’s Rekognition or YouTube radicalization showed in 2019, that doesn’t mean that AI won’t be used in production today. Regulation of some kind is coming, but the unanswered question is whether that legislation will go beyond the solutions tech CEOs endorse. Zuckerberg voiced support for federal privacy legislation, just as Microsoft has done in fights with state legislatures attempting to pass data privacy laws. Zuckerberg also expressed some backing for algorithm auditing as an “important area of study”; however, Facebook does not perform systematic audits of its algorithms today, even though that’s recommended by a civil rights audit of Facebook completed last summer. Last week, the Carr Center at Harvard University published an analysis of the human rights impact assessments (HRIAs) Facebook performed regarding its product and presence in Myanmar following a genocide in that country. That analysis found that a third-party HRIA largely omits mention of the Rohingya and fails to assess if algorithms played a role. “What is the link between the algorithm and genocide? That’s the crux of it. The U.N. report claims there is a relationship,” coauthor Mark Latonero told VentureBeat. “They said essentially Facebook contributed to the environment where hateful speech was normalized and amplified in society.” The Carr report states that any policy demanding human rights impact assessments should be wary of such reports from the companies, since they tend to engage in ethics washing and to “hide behind a veneer of human rights due diligence and accountability.” To prevent this, researchers suggest performing analysis throughout the lifecycle of AI products and services, and attest that to center the impact of AI requires viewing algorithms as sociotechnical systems deserving of evaluation by social and computer scientists. This is in line with a previous research that insists AI be looked at like a bureaucracy, as well as AI researchers working with critical race theory. “Determining whether or not an AI system contributed to a human rights harm is not obvious to those without the appropriate expertise and methodologies,” the Carr report reads. “Furthermore, without additional technical expertise, those conducting HRIAs would not be able to recommend potential changes to AI products and algorithmic processes themselves in order to mitigate existing and future harms.” Evidenced by the fact that multiple members of Congress talked about the perseverance of evil in Big Tech this week, policymakers seem aware AI can harm people, from spreading disinformation and hate for profit to endangering children, democracy, and economic competition. If we all agree that Big Tech is in fact a threat to children, competitive business practices, and democracy, if Democrats and Republicans fail to take sufficient action, in time it could be lawmakers who are labeled untrustworthy. For AI coverage, send news tips to Khari Johnson and Kyle Wiggers — and be sure to subscribe to the AI Weekly newsletter and bookmark The Machine. Thanks for reading, Khari Johnson Senior AI Staff Writer"
https://venturebeat.com/2021/03/26/fable-studio-opens-its-virtual-beings-wizard-engine-to-collaborators/,Fable Studio opens its virtual beings Wizard Engine to collaborators,"Fable Studio is opening its virtual beings Wizard Engine to collaborators in fashion and nursing. The San Francisco company built the engine to manage its virtual character Lucy, an 8-year-old animated girl from the Wolves in the Walls virtual reality experience. Lately, Lucy has made appearances at the Sundance Film Festival and SXSW as a virtual character. She sang songs and talked with the audience, using her AI-based personality, on Twitch. The Wizard Engine enables Fable to manage Lucy’s appearances on different social media platforms. Fable CEO Edward Saatchi said in an interview with GamesBeat that Fable envisions a world where young people have virtual beings as close friends. It is opening up the Wizard Engine that it uses to make virtual beings such as Lucy, as well as the newer Charlie and Beck characters. The Wizard Engine lets creators schedule events on different platforms for a virtual being and create new milestones in the lives of the characters, Saatchi said. “We’re announcing that we’re opening up the Wizard Engine for people to use themselves,” Saatchi said. “We’re going to open it up for signups. You’ll be able to create the AI characters, distribute them on Twitch, Instagram, and YouTube. We use it to create these monthly beats in Lucy’s life.” The Wizard Engine is the tool that Fable uses to bring to life to its virtual beings. It works on three things: generation, distribution, and memory. The tool lets Fable take a virtual being as conceived — the bible and backstory of their life, a synopsis of what will happen to them — and uses that to generate on an ongoing basis the content of their life. It generates the voice, animation, text dialogue, and video. These things would realistically make up the virtual beings life (what their knowledge base about their past is, what happened yesterday, what’s happening right now, what are they looking forward to etc). One of the tools the Wizard Engine uses is OpenAI’s GPT3 technology to generate Lucy’s responses. The Wizard Engine also distribute the virtual being’s life. It allows Fable and collaborators to distribute virtual beings across many media platforms: on Twitch, text messaging apps, photo sharing apps, in videogames, and eventually in metaverses. This is important because virtual beings should be like us, Saatchi said. There’s no Edward app to chat with Edward and similarly virtual beings shouldn’t be confined to an app download, but live across different media platforms, Saatchi said. And the engine gives your virtual being memories. It allows Fable and collaborators to build virtual beings with memory so that the beings can remember their own lives, what they are learning about their friends, and combine those to create empathetic moments where users feel “seen.” You could almost think of the Wizard Engine as an AI game engine, he said. It allows collaborators to generate and deploy interactive experiences with virtual beings to delight millions of users. So far the interactive/gaming and deep learning worlds have remained very separate, but virtual beings will only exist if they can work together. By opening up the opportunity to work with the Wizard Engine to a small number of collaborators, Fable hopes to show that the intersection of AI and interactivity can be a powerful one. To date, nonplayer interactive characters have been centralized — usually in a videogame. With the Wizard Engine, Fable is decentralizing nonplayer characters and virtual beings. “We need virtual beings to be seen in lots of places. That’s why we had Lucy on Twitch at SXSW. Right now, NPCs exist only in games. But they can be on other platforms,” Saatchi said. Collaborators will be able to work with Fable to create a virtual being in one of these two spaces — fashion and health care, creating a backstory, life, and aesthetic for the character. The creators will be able to plan what will happen to the character over the course of their first year, how they’ll interact with friends. Collaborators can monetize through the careers of the virtual beings they create with Fable. For a fashion virtual being, imagine a young man leaving design school and while working in physical fashion by day working on nights and weekends to build digital fashion items that can be sold as nonfungible tokens (NFTs). “We’re building the Wizard Engine with that decentralized future in mind, and the character might spend some time as an NPC in a game or texting or chatting or broadcasting on Twitch,” Saatchi said. But making money isn’t the only purpose here. “This is less about making money and more about finding early collaborators in fields of fashion and health,” Saatchi said. “There might be great ideas for virtual beings in that space. I think these digital characters pretty quickly start to have digital work products that could be purchased.” Separately, a nurse who struggles with compassion fatigue with friends and relatives can, after a long day of work, unwind by playing games with friends and hanging out in virtual concerts with friends. That nurse could perform some simple healthcare checks online and make money doing it — as a virtual character. At SXSW last week, Lucy played her first virtual concert on Twitch to show off the Wizard engine. She interacted with viewers, most of whom have been friends with Lucy for months. She sang three songs: Toys, My Friends, and Nana’s song. The lyrics unlocked parts of her story for users who text and call with her. And now Lucy will be able to talk to friends about the concert and remember it when they talk. Potential collaborators need to answer a series of questions about the virtual being they would like to build — the career the virtual being would pursue, a bit about the backstory of the virtual being, and what they plan to happen to the virtual being. Fable envisions a world where virtual beings are paid for their digital work. “We are really interested in the idea of just telling a story with intimacy and connection,” Saatchi said. “We want the Wizard Engine to be something that can empower them.”"
https://venturebeat.com/2021/03/26/cloudflare-goes-deep-on-api-abuse-detection/,Cloudflare goes deep on API abuse detection,"APIs (application programming interfaces) have emerged as the cornerstone of most modern, agile software companies, powering the shift from monolithic on-premises software to the cloud and microservices-based applications. Smaller, function-based components that connect via APIs are easier to maintain, with individual developers or teams taking charge of a single element. There are many reasons why the API economy is booming, of course, but this proliferation potentially serves bad actors with unfettered access to companies’ internal systems and infrastructure. Many businesses have hundreds or even thousands of APIs to monitor, some of which they might not even know exist. And that is why web infrastructure and security company Cloudflare is introducing new ways to secure API endpoints beyond standard DDoS protection tools. Cloudflare’s new API abuse detection toolset constitutes several elements. The first part relates to API discovery, with Cloudflare developing a system that builds a “trustworthy map of APIs” that gives businesses an accurate picture of their API landscape. With the APIs “discovered,” Cloudflare’s abuse detection smarts first target what it calls “volumetric anomalies,” which sets an API call threshold to manage abuse by guessing how often each path should be reached legitimately. It’s worth noting that existing security tools can already set “rate limits” to prevent an API from becoming overwhelmed, which can help thwart automated bad actors from repeating the same breach tactic. But with so many potential unknown APIs in a company, it’s difficult to allocate realistic thresholds for each scenario automatically without causing problems. For example, it’s easy to set a threshold that blocks an IP after it exceeds 100 requests, but what if those requests are legitimate? Ultimately, it all boils down the purpose of the API. As Cloudflare notes, the problem “demands a more subjective arbiter,” which Cloudflare is attempting with what it refers to as an “adaptive rate-limiting” technique. Using unsupervised machine learning, Cloudflare can determine APIs that will likely require frequent calls from an end user and set an appropriate threshold. A sports betting website, for example, might have an API that serves real-time soccer score updates — this will likely have to refresh multiple times each minute to ensure that the information is up-to-date. But that same betting website might also have an API for resetting passwords, and it’s unlikely that a user would make nearly as many calls to that API as they would for soccer scores. When Cloudflare maps out a company’s APIs, it establishes unique baselines for each one and predicts the intent of requests as they are made. “If we see 150 sudden attempts to reset a password, our systems immediately suspect an account takeover,” the company wrote in a blog post. Additionally, Cloudflare said that it can change thresholds if, for example, it detects that there should be a good reason for a sudden spike in traffic, such as a major sporting event is taking place. In addition to detecting volumetric anomalies, Cloudflare is also applying an additional layer of security it refers to as “sequential anomaly detection,” where it figures out the most likely or common paths a user might take through a website, and flags any deviation from that. For example, it could be that a typical sequence involves a user logging in, verifying themselves, and then successfully entering the website. But if any steps in that typical process fall out of sync — e.g., if the “user” ends up directly at the third stage — then Cloudflare sounds the alarm. Cloudflare’s new API abuse detection tools are available now through a request-only early access program for existing customers."
https://venturebeat.com/2021/03/26/data-centric-security-is-key-to-resiliency-cyber-risk-report-says/,"Data-centric security is key to resiliency, cyber risk report says","The COVID-19 pandemic changed more than how people work. Organizations boosted their security investments in response to an increase in cyberattacks and adopted a “data-centric” mindset to protect their information investments over the past year, according to a study Capgemini and Forrester released today. With this data-centric approach, organizations are trying to reduce cyber risk and remove internal business growth barriers, according to the study, titled Making Your Business Cyber-Resilient In 2021. Based on interviews with 215 IT decision-makers — 63% were directors and 29% were vice presidents — the report found that IT leaders were trying to balance cyber risks with the demands of keeping their business running. Enterprise IT teams need to protect the organization’s data from accidental breaches and cyberattacks, but they can’t use security controls that make it impossible for the business to keep operating. Data-centric security prioritizes securing data where it’s stored and processed, instead of focusing on security controls for hardware and network infrastructure. Data-centric models also take a zero trust approach by applying the principle of least privilege to determine user access. Zero trust reduces cyber risk because the user requests privileged access only when needed. The proliferation of cyberattacks during the pandemic is one of several factors making data-centric security a priority today. It’s also proving to be a catalyst that’s putting greater emphasis on identity and access management, cybersecurity controls, and endpoint security. Even the National Security Agency (NSA) emphasized zero trust and data-centric security in recent guidance. The “data-centric security model allows the concept of least-privileged access to be applied for every access decision, allowing or denying access to resources based on the combination of several contextual factors,” the NSA said. The Capgemini and Forrester study showed how organizations are prioritizing cyber-resilience as a way to scale data-centric security. Data-centric security dominates budgets. Of the 75% of companies planning to increase their cybersecurity budgets in response to COVID-19, the majority of the respondents (71%) said they prioritized data-centric security. A health care CISO told VentureBeat they accelerated their plans to build out zero trust frameworks in response to new security concerns and requirements over the past year. Protecting existing and new digital sales channels was always important, but it became urgent during the pandemic because of changes in customer buying patterns, the CISO said. Cyber-resiliency protects revenue. Threats to organizations’ channels, operations, and revenue increased over the past year in comparison to levels seen before the pandemic, according to 68% of the survey respondents. A little over three-quarters of the survey participants (79%) said their digital business grew too quickly to keep up with, making it harder to keep their organizations cyber-resilient. Six in 10 organizations don’t have the right tools or technology they need, which explains why the report found that cybersecurity spending increased by 66% last year. Defining a cyber-resilient roadmap. Organizations are developing cyber-resiliency roadmaps as part of their zero trust initiatives. They want greater visibility across every endpoint enterprise-wide and to be able to enforce least privileged access to every data asset. They also want to reduce risk and internal barriers to online revenue growth. The Capgemini/Forrester study found two other things IT leaders want: skilled employees and foundational security controls.Getting started with data-centric security. By definition, data-centric security is a framework with supporting methodologies and classification taxonomies that need to be an organization-wide initiative to succeed. With cybersecurity attacks concentrating on data assets, endpoints, and identities during the pandemic, organizations opt for the zero trust framework as their data-centric security strategy. Capgemini and Forrester’s report provides a timely glimpse into how organizations translate their data-centric investments into concrete security plans. Data-centric models deliver the most value when they improve application, tool, and device visibility across organizations. Identity and access management (IAM), cybersecurity controls, and endpoint security provide that level of visibility. Leading vendors in IAM include BeyondTrust, Centrify, CyberArk, Ivanti, and Thycotic. Leading endpoint security vendors include Absolute Software, CrowdStrike, Cybereason, and Ivanti. The study provides compelling results that reflect how cyber-resilience is essential to protecting revenue, defining a data-centric security roadmap, and managing cybersecurity decisions from a business perspective first. Investing in cybersecurity is now primarily a business decision, rather than a technological one. The increasing cyberattacks on data assets, endpoints, and identities directly impact revenue and can thwart new online sales channel growth. The NSA’s prescriptive guidance on zero trust comes just as many organizations are struggling to define their data-centric security strategies. “Zero trust is a security model, a set of system design principles, and a coordinated cybersecurity and system management strategy based on an acknowledgment that threats exist both inside and outside traditional network boundaries,” the NSA said in its guidance."
https://venturebeat.com/2021/03/26/avvir-raises-10m-for-ai-that-spots-construction-site-errors/,Avvir raises $10M for AI that spots construction site errors,"Avvir, a startup using laser scans and AI to catch construction mistakes, today announced that it raised $10 million in a funding round led by Trust Ventures. The New York-based startup, which is valued at $40 million, says it’ll use the funds to expand its workforce while improve its technology platform. Mistakes often prove to be costly in the construction industry. According to a study commissioned by Autodesk, 5% of construction professionals’ time is spent on nonproductive activities including looking for project information, conflict resolution, and dealing with errors and rework. It’s estimated that these activities cost the U.S. construction sector alone over $177 billion in labor in 2018. Avvir, which was founded in 2017 by CEO Raffi Holzer and CTO Tira Odhner, provides a platform that monitors construction progress for budget and schedule overruns. Customers send Avvir fabrication models and then deploy mobile lasers that can scan up to 30 square kilometers in an hour, even while construction is ongoing. Custom computer vision algorithms compare photographs and the scans with the building plans and catch errors and delays. Avvir claims it can identify construction problems like unlevel floors to one-eighth inch of accuracy. Avvir can refresh bills of materials with site laser scans to document real-world modifications as they occur. The platform automatically pushes updates to models and enables monitoring of the percent completion of projects. Avvir tracks the progress on systems with color renderings that show which portions of the building geometry are finished and which aren’t — actual progress can be mapped against the schedule to highlight where the timeline might be off target.  Avvir competes with companies including Swapp, which automates construction planning using AI. Another rival is SiteAware, a startup that tracks construction zone progress using a combination of drones and machine learning systems. Buildots, OnSiteIQ, Disperse, OpenSpace, and Indus.ai are among the other firms vying for a slice of the the AI in construction market, which is projected to be worth $2.31 billion by 2026, Mordor Intelligence reports. But according to Holzer, Avvir, which has 18 full-time employees, has gone from $300,000 in annual recurring revenue to $1.4 million and roughly 12 customers and partners — including robotics firm Boston Dynamics — in less than 12 months. He expects that number to reach $4.4 million by 2022. “Facility managers and building owners typically [don’t] have an accurate set of building plans off of which they could manage or plan renovations of their built assets,” Holzer told VentureBeat in an interview over email. “Avvir plans to become a true construction information platform, taking unstructured information from the field, structuring and analyzing it, and then enabling clients to take action on it either within Avvir or in any of the software tools they already use.” Tekfen Ventures, Khosla Ventures, and MetaProp also participated in Avvir’s fundraising round announced today. It brings the company’s total raised to date to nearly $15 million."
https://venturebeat.com/2021/03/26/meet-the-newest-fintech-unicorn-bezos-expeditions-whale-rock-join-pilots-push-to-modernize-the-back-office/,Meet the Newest Fintech Unicorn: Bezos Expeditions & Whale Rock join Pilot’s Push to Modernize the Back Office," Additional investment doubles Pilot’s valuation to $1.2Bn and increases Series C to $100M raised as Pilot rapidly scales “big-company superpowers” for SMBs  SAN FRANCISCO–(BUSINESS WIRE)–March 26, 2021– Pilot, the largest provider of back office services for startups and small businesses in the U.S., announced today that it has closed on additional funding led by Bezos Expeditions (the personal investment company of Jeff Bezos) and Whale Rock Capital, with participation from Sequoia Capital and Index Ventures, that increases the Series C to $100 million and doubles the company’s valuation to $1.2Bn. The additional funding enables Pilot to further expand its breadth of back office services that offload day-to-day distractions and allow small business owners and founders to focus on growth. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210326005284/en/ “Since the earliest days of my career, including an internship at Amazon in 2005, I’ve been inspired by Jeff’s relentless focus on delivering an exceptional customer experience. We’ve brought that same DNA to Pilot’s customer-centric approach: over 80% of our business is driven by customer referrals and organic interest,” said Waseem Daher, Founder & CEO. “Pilot is revolutionizing the back office by combining white-glove customer service with carefully-engineered software,” said Kristov Paulus, Partner at Whale Rock. “We look forward to supporting Pilot in their vision to make back office services as easy-to-use, scalable, and ubiquitous as AWS has with the cloud.” In 2020, Pilot completed more than $3 billion in bookkeeping transactions for its customers, which range from pre-revenue startups to established companies generating more than $30M of revenue a year. It has also established co-marketing partnerships with other industry leaders including American Express, Bill.com, Brex, Carta, Gusto, Rippling, Stripe, SVB, and Techstars. Pilot continues to see strong momentum on its mission to bring big-company finance superpowers to SMBs. With Pilot, founders, business owners and finance teams are able to: Started by three-time co-founders Waseem Daher, Jeff Arnold, and Jessica McKellar, Pilot was created to solve problems they’d faced themselves while building their two previous companies. About Pilot Pilot launched in 2017 to bring the back office into the modern era. The company provides small businesses with dedicated finance experts-which Pilot hires as full-time, U.S.-based employees-who learn the ins-and-outs of their business. Pilot integrates directly with the billing, banking, expense and payroll systems customers already use. With a special blend of custom software and expert bookkeepers, Pilot delivers accurate, consistent bookkeeping that gives entrepreneurs the freedom to focus on their business, as well as an array of additional financial services. Pilot was founded by Waseem Daher, CEO; Jeff Arnold, COO; and Jessica McKellar, CTO. It is their third startup, after two successful exits. Pilot has over 1,000 customers and has raised over $150 million to date. Investors include Sequoia Capital, Index Ventures, Stripe, Whale Rock, and many world-class entrepreneurs, including Jeff Bezos, Patrick and John Collison, Drew Houston, Diane Greene, Frederic Kerrest, Hans Robertson, Adam D’Angelo, Paul English, Howard Lerman, Joshua Reeves and Tien Tzuo. Learn more at https://pilot.com/  View source version on businesswire.com: https://www.businesswire.com/news/home/20210326005284/en/ Lisa Tarter press@pilot.com"
https://venturebeat.com/2021/03/26/aunalytics-unifies-siloed-bank-customer-data-with-ai-driven-data-mart-and-nlp/,Aunalytics unifies siloed bank customer data with AI-driven data mart and NLP,"Aunalytics announced an update to its Daybreak for Financial Services platform that employs machine learning algorithms to enable midrange banks and credit unions to more easily analyze data. The latest update adds a data mart that automatically discovers and aggregates customer data residing in siloed lending, mobile banking, automated teller machine (ATM), customer relationship management (CRM), wealth management, and trust applications. The platform has also added support for a natural language processing (NLP) engine that eliminates the need to know SQL to query data. Companies can automatically create visualizations of those query results as well. Finally, Aunalytics made it simpler to access external data via connectors and added a “smart features” capability that will, for example, automatically generate alerts anytime a customer’s credit score changes. Midrange banks and credit unions are at a distinct AI disadvantage compared to larger financial services rivals that can afford to build their own AI models with specialists who know how to program in Python or R programming languages, Aunalytics president Rich Carlton said. “They can’t afford to hire a team of data scientists,” he added. Aunalytics is making a case for a platform that automates low-level data science tasks in a way that enables either end users or a small team of data scientists to maximize the value of the data any midrange bank or credit union routinely collects, Carlton said. The Daybreak for Financial Services platform is based on cloud-native technologies such as Hadoop, containers, and Kubernetes clusters that he said enable it to be deployed in the cloud or an on-premises IT environment. Midrange financial services providers have realized they are losing touch with customers in the wake of the COVID-19 pandemic as they rely more on digital services. The number of banking customers that visit their local bank has sharply declined as reliance on web and mobile applications increases. The challenge midrange financial services face today is that they already rely on a disjointed suite of applications to manage their business. Mobile applications in particular have added yet another silo that makes it difficult for financial services providers to correlate customer activity across a portfolio of services. Awareness of AI and data science has never been higher. The issue organizations are trying to come to terms with is to what degree they are now at a competitive disadvantage because they lack these capabilities. Platforms and applications that embed AI capabilities may provide a way to close that gap at a time when many smaller financial services firms need to operate as efficiently as possible just to stay afloat. As data science and AI continue to evolve, organizations will soon need to decide when it makes sense to employ advanced analytics that are baked into a platform such as Daybreak for Financial Services versus building and maintaining their own AI models. Given the general shortage of data science professionals, it’s especially difficult for smaller organizations to hire and retain in-house talent. At the same time, it usually takes a data science team several months to successfully deploy an AI model in a production environment. Providers of applications and platforms may very well have added similar capabilities to their offerings before that custom AI project ever comes to fruition. In many cases, organizations will find they are gaining access to advanced analytics capabilities at no extra cost as new updates are made available under a subscription license. Most end users, of course, are a lot more interested in the business outcomes AI models and data science enable than they are in the processes employed to build them. The fact that an independent provider of a platform or application is willing to vouch for the accuracy of those AI models adds yet another perceived level of comfort."
https://venturebeat.com/2021/03/26/the-open-source-index-showcases-githubs-most-popular-projects-right-now/,The Open Source Index showcases GitHub’s most popular projects right now,"It intersects with just about every piece of software, from systems architecture to APIs, and enterprises are adopting it more than ever. Open source software, judging by just about every estimation in recent years, is eating the world. But sifting through the vast array of open source projects out there, sorting the wheat from the chaff, can be a challenge, which is partly why early stage VC firm Two Sigma Ventures has launched a new index designed to surface “high-level trends” in the open source sphere. It’s worth noting that there are already all manner of indexes and charts out there that deliver useful insights for the open source world, such as the Open Source Contributor Index, which ranks commercial organizations by their employees’ open source contributions (Google’s in the lead). And GitHub itself charts things like trending repositories. It’s possible to slice, dice, and present GitHub data in any way you see fit through its publicly available API, which is exactly what Two Sigma Ventures has done with the Open Source Index. But rather than relying on “stars,” it uses “watchers,” which it argues provides a more accurate reflection of a project’s true popularity. GitHub, for the uninitiated, allows logged-in users to either “star” or “watch” a project — the former can perhaps best be likened to bookmarking, as the user saves the project to their profile so they can easily check in on it without having to search. It can also be used as a show of respect, similar to how someone might “like” a Facebook post or tweet — “I dig what you’re doing for open source, keep up the good work.” When someone chooses to “watch” a project, however, they are likely taking a more active interest, as they essentially sign up to receive project notifications. As such, the Open Source Index is based on the top GitHub projects as per the number of people that are “watching” a project. While there are of course broad correlations between “stars” and “watchers,” i.e. top projects will likely have a high number of both, they aren’t always totally aligned. Moreover, Two Sigma Ventures wanted to showcase what’s popular today, rather than what has built a high “vanity metric” by virtue of having launched 10 years ago. “A stars-based ranking tends to prioritize older projects that have been around for a while, since stars are more cumulative in nature,” Two Sigma Ventures VC Vinay Iyengar told VentureBeat. “With watchers, we believe we have a better sense of the projects that are ‘hot’ right now, as opposed to those that have been around for a while.” And so the Open Source Index, which is continuously updated, showcases the 100 “most popular and fastest-growing” open source projects, allowing users to sort and filter by various criteria (Two Sigma Ventures filtered out all the non-technical projects, such as books and educational content from the index). For the index, Two Sigma Ventures has produced its own TSV (Two Sigma Ventures) ranking, which is weighted as an average of five variables: Watchers (40%); Watcher growth (25%), which considers the variance in watchers over the past quarter; Contributors (15%); Release cadence (10%), which is the number of commits over a project’s lifetime; and Community health score (10%), which is based on GitHub’s own metric for how well-maintained a repository is. “We think our TSV Score metric is somewhat of a ‘super’ metric, in that it takes into account several factors that we believe lead to building a great open source project/community,” Iyengar said. None of this is an exact science of course, and Iyengar acknowledges that these weightings are somewhat “arbitrary,” reflecting “just one perspective on what’s important in building a great open source community.” The index defaults to the TSV score ranking (highest to lowest), and doesn’t reveal too many surprises — TensorFlow, React, Vue, Angular, and Kubernetes all rank highly, and they all have high numbers of stars and watchers. But playing around with the various filters is where things start to get a little more interesting. Chinese tech titan Baidu’s open source autonomous driving project Apollo, for example, ranks 41st when using the TSV ranking and 72nd by number of watchers. And in terms of stars, Apollo comes in last at 100th. However, if you filter the index by the quarterly watcher growth metric, Apollo is in pole position. There could be a number of reasons for this surge in interest. Two months ago, Baidu’s Apollo became the sixth company in California to get approval to test fully driverless cars on public roads, while the company has launched all manner of autonomous vehicle programs and projects in its domestic China too. Whatever the reason behind this surge, it serves as an interesting data point for any developer, company, or entrepreneur wanting to keep their finger on the open source pulse. “It [watcher growth metric] gives us an important signal on which projects have momentum in the developer ecosystem,” Iyengar noted. Other interesting observations including Bitcoin, which is ranked 40th in the index by number of stars (48,000 stars) and 33rd by TSV ranking. However, it’s in seventh place by number of watchers, ahead of JQuery, Kubernetes, and Visual Studio Code, among other arguably “more relevant” projects. So why has Two Sigma Ventures taken the time to create this list, and what relevance does it hold? Well, as an investor, the firm has backed several startups that commercialize open source projects, such as GitLab, Timescale, Radar Labs, NS1, and Replicated. Playing around with the various menus and filters on the index reveals some interesting insights related to this, such as that seven of the top 100 projects were either created by private VC-backed startups or are maintained by commercial companies created by the original project creators — these are Redis, Grafana, Vercel, Hashicorp, Confluent, Databricks, and Preset. But Two Sigma Ventures also has a separate business simply called Two Sigma, which is an investment management company that applies “cutting-edge technology to the data-rich world of finance,” according to Iyengar. It counts 1,700 employees — more than half of whom are software developers and use open source software on a daily basis. They are also creators of a number of open source projects, such as Flint and BeakerX. “We have seen firsthand how software created by developers, for developers, leveraging community-based development, leads to incredible innovation,” Iyengar said in a separate blog post announcing the index. “Moreover, we are excited about how enterprise software is moving toward bottoms-up adoption, and how an open core business can lead to remarkably efficient customer acquisition and growth.” This new index also constitutes part of a growing trend in the technology realm that strives to make sense of the open source world. Just last week, OpenLogic launched an upgraded tool it calls Stack Builder, which helps enterprises choose the right open source software. And earlier this year, Openbase emerged out of the ether to serve as a sort of Yelp for open source software packages. If nothing else, the Open Source Index serves as a useful accompaniment to these other efforts, helping companies and developers dig down into the best — or most popular — open source projects on GitHub right now. There are plans to add more data to the mix in the future, according to Iyengar, such as downloads, community engagement in external channels such as Slack or Discord, and even mentions in job advertisements. The Open Source Index is available now and free to use for anyone."
https://venturebeat.com/2021/03/26/how-yandex-is-expanding-its-autonomous-robot-delivery-service/,How Yandex plans to expand its autonomous robot delivery service,"The COVID-19 health crisis in much of the U.S. seems likely to hasten the adoption of self-guided robots and drones for goods transportation. They require disinfection, which companies like Kiwibot, Starship Technologies, and Postmates are conducting manually with sanitation teams. But in some cases, delivery rovers like Refraction’s could minimize the risk of spreading disease. Recent market reports from Allied Market Research and Infiniti estimate that annual growth in the last-mile delivery sector over the next 10 years will exceed 14%, with the autonomous delivery segment projected to grow at over 24%, from $11.9 billion in 2021 to more than $84 billion globally by 2031. As something of a case in point, Yandex says that in early February its Yandex.Rovers autonomous delivery program reached a milestone: 4,000 deliveries since April 2020. After launching at Yandex’s headquarters and the Moscow district of Skolkovo last year, Yandex says its fleets of robots have delivered documents, packages, and more to customers. Yandex soon plans to pilot the service in Ann Arbor, Michigan, which will be the company’s first autonomous delivery site in the U.S. Ahead of the deployment, VentureBeat spoke with Dmitry Polishchuk, head of driverless cars, about how Yandex is approaching the technology stack for its delivery network and the innovations that might be applied to projects elsewhere at the company. Yandex first took the wraps off Yandex.Rover in November 2019. A project within its self-driving division, Yandex.Rover was created to start by delivering small packages before eventually handling food as part of Yandex’s Yandex.Eat platform and groceries from Yandex.Lavka. Yandex’s six-wheeled rover, which is roughly the size of a small suitcase, taps some of the tech at the core of the company’s autonomous cars to travel “at the speed of a pedestrian.” It’s capable of navigating around obstacles in most weather conditions during daylight or darkness, but in the areas where it’s currently deployed, Yandex says its activities can be monitored by a remote operator.  Customers use an app to arrange deliveries by setting a dropoff destination, after which one of Yandex’s rovers travels on sidewalks and crosses intersections within a radius of “several kilometers.” From the same app, shoppers can remotely open the robot’s storage compartment and monitor its movements on a map or connect with a Yandex engineer for assistance. “The rovers are operated by the same technology stack as our self-driving cars. We have adopted it for this new platform that utilizes the same approaches and solutions,” Polishchuk told VentureBeat via email. “The sensor configuration on the rover is different than the cars, but these are still the same types of sensors. This allows us to reuse perception models from cars to detect vehicles, pedestrians, static obstacles, and other road objects, as well as use, existing labeling, and training pipelines for developing new models.” According to Polishchuk, Yandex draws on a number of tools developed in-house to refine the performance of its delivery rovers. One is Yandex.Toloka, a crowdsourced dataset labeling service that provides the ability to annotate data at scale. Another is a web app — Nirvana — that creates reproducible training and testing pipelines for machine learning models, which feeds into a storage and computation cloud called Yt that’s designed for analytics. “Using proprietary solutions allows us to have these tools be quickly adjusted or fine-tuned for the specific needs of the self-driving team,” Polishchuk explained. “For example, in Toloka, we’ve created special tools for labeling objects in 3D using joint data from lidars, cameras, and radars. And of course, using our own cloud infrastructure is significantly less expensive than using external services.” In the case of Yandex.Rover, Yandex’s self-driving team was able to repurpose some of the prediction models developed for the company’s cars to its robots, for purposes like predicting where pedestrians might walk. The team also uses the simulator originally developed to test the cars’ software for validating and fine-tuning the rovers’ algorithms.  “The machine learning-based solutions greatly help our cars [and rovers] predict other agents’ behavior in situations that are not strictly regulated by traffic rules. In comparison to cars, human traffic on sidewalks is less structured and thus creates even more uncertainty,” Polishchuk said. “We are using the latest machine learning-based motion-planning solutions to help robots safely and intelligently navigate busy sidewalks.” Mapping is another area where Yandex’s autonomous cars have helped its robots. The rovers need high-definition maps in order to position themselves in the real world, and portions of the maps can be constructed with the help of the cars’ sensors. The sensors are sensitive enough to capture sidewalk surfaces — the robots only need to enhance the map where the view from the street is obscured by other objects. Yandex’s delivery fleet currently comprises about 35 robots, most of which are making deliveries from Yandex.Eats and Yandex.Lavka in four Moscow districts and the town of Innopolis. A few additional robots are being used for testing purposes and to map new delivery locations, Polishchuk says. During the first few months of 2020, Yandex’s self-driving team began testing various hardware and sensor configurations for the rovers, eventually settling on an ARM platform equipped with a lidar sensor, sonar, cameras, and radars on the sides to detect vehicles when crossing a street. Deliveries during the winter months revealed new challenges that led to upgrades, including an adjustment to the rovers’ speed-planning system that could better handle icy walkways. The team also improved power consumption so that the robots could drive up to 10 hours on a charge, even in cold, snowy wintery weather.  The rovers’ form factor also saw some changes in response to customer feedback. For example, its storage unit expanded in size and gained a lid that opens and closes automatically so that each delivery is contactless. Beyond Ann Arbor, Yandex says it is shipping rovers to Israel and South Korea for autonomous delivery testing. In the future, Yandex envisions the robots making their way into the ecommerce platforms for order fulfillment and delivery or the company’s own warehouses and datacenters to transport cargo. Yandex is now in “active talks” about partnerships with companies in Russia and abroad, according to Polishchuk. Beyond Yandex, startups like Marble, Starship, Nuro, Robomart, Boxbot, FedEx, Refraction AI, Dispatch, and Robby are vying for a slice of the self-driving robot delivery market. Amazon is testing its Scout robots in parts of Southern California, expanding the tech giant’s pilot program from Snohomish County, Washington. And in a sign of the segment’s competitiveness, Uber-owned Postmates X, the division of Postmates developing autonomous delivery robots, recently spun off into a separate company. “Pandemic is changing last-mile delivery both on the side of customers and business. Demand for contactless delivery changed the protocols of couriers’ interaction with customers, and robots perfectly fit into this new reality,” Polishchuk said. “We think that the ideal combination of human couriers and rovers is in an efficient split of the orders. Rovers can do hyperlocal orders close to a shop or a restaurant, while humans can deliver the rest — as they cover long distances faster — using bicycles, scooters, or public transportation … Such a split of the orders between humans and robots can help services improve current last-mile delivery time, even with [the] constantly growing demand.”"
https://venturebeat.com/2021/03/26/hacktivism-adds-twist-to-cybersecurity-woes/,‘Hacktivism’ adds twist to cybersecurity woes,"(Reuters) — At a time when U.S. agencies and thousands of companies are fighting off major hacking campaigns originating in Russia and China, a different kind of cyberthreat is reemerging: activist hackers looking to make a political point. Three major hacks show the power of this new wave of “hacktivism” — the exposure of AI-driven video surveillance being conducted by the startup Verkada, a collection of January 6 riot videos from the right-wing social network Parler, and disclosure of the Myanmar military junta’s high-tech surveillance apparatus. The U.S. government’s response shows that officials regard the return of hacktivism with alarm. An indictment last week accused 21-year-old Tillie Kottmann, a Swiss hacker who took credit for the Verkada breach, of a broad conspiracy. “Wrapping oneself in an allegedly altruistic motive does not remove the criminal stench from such intrusion, theft, and fraud,” Seattle-based Acting U.S. Attorney Tessa Gorman said. According to a U.S. counter-intelligence strategy released a year ago, “ideologically motivated entities such as hacktivists, leaktivists, and public disclosure organizations” are now viewed as “significant threats,” alongside five countries, three terrorist groups, and “transnational criminal organizations.” Earlier waves of hacktivism, notably by the amorphous collective known as Anonymous in the early 2010s, largely faded away under law enforcement pressure. But now a new generation of youthful hackers, many angry about how the cybersecurity world operates and upset about the role of tech companies in spreading propaganda, is joining the fray. And some former Anonymous members are returning to the field, including Aubrey Cottle, who helped revive the group’s Twitter presence last year in support of the Black Lives Matter protests. Anonymous followers drew attention for disrupting an app that the Dallas police department was using to field complaints about protesters by flooding it with nonsense traffic. They also wrested control of Twitter hashtags promoted by police supporters. “What’s interesting about the current wave of the Parler archive and Gab hack and leak is that the hacktivism is supporting antiracist politics or antifascism politics,” said Gabriella Coleman, an anthropologist at McGill University, Montreal, who wrote a book on Anonymous. Gab, a social network favored by white nationalists and other right-wing extremists, has also been hurt by the hacktivist campaign and had to shut down for brief periods after breaches. Most recently, Cottle has been focused on QAnon and hate groups. “QAnon trying to adopt Anonymous and merge itself into Anonymous proper, that was the straw that broke the camel’s back,” said Cottle, who has held a number of web development and engineering jobs, including a stint at Ericsson. He found email data showing that people in charge of the 8kun image board, where the persona known as Q posted, were in steady contact with major promoters of QAnon conspiracies. The new-wave hacktivists also have a preferred place for putting materials they want to make public — Distributed Denial of Secrets, a transparency site that took up the mantle of WikiLeaks with less geopolitical bias. The site’s collective is led by Emma Best, an American known for filing prolific freedom of information requests. Best’s two-year-old site coordinating access by researchers and media to a hoard of posts taken from Gab by unidentified hackers. In an essay this week, Best praised Kottmann and said leaks would keep coming, not just from hacktivists but insiders and the ransomware operators who publish files when companies don’t pay them off. “Indictments like Tillie’s show just how scared the government is, and just how many corporations consider embarrassment a greater threat than insecurity,” Best wrote here. The events covered by the Kottmann indictment took place from November 2019 through January 2021. The core allegation is that the Lucerne software developer and associates broke into a number of companies, removed computer code, and published it. The indictment also said Kottmann spoke to the media about poor security practices by the victims and stood to profit, if only by selling shirts saying things like “venture anticapitalist” and “catgirl hacker.” But it was only after Kottmann publicly took credit for breaching Verkada and posted alarming videos from inside big companies, medical facilities, and a jail that Swiss authorities raided their home at the behest of the U.S. government. “This move by the U.S. government is clearly not only an attempt to disrupt the freedom of information, but also primarily to intimidate and silence this newly emerging wave of hacktivists and leaktivists,” Kottmann said in an interview with Reuters. Kottmann and their lawyer declined to discuss the U.S. charges of wire fraud for some of Kottmann’s online statements, aggravated identity theft for using employee credentials, and conspiracy, which together are enough for a lengthy prison sentence. The FBI declined an interview request. If it seeks extradition, the Swiss would determine whether Kottmann’s purported actions would have violated that country’s laws. Kottmann was open about their disdain for the law and corporate powers-that-be. “Like many people, I’ve always been opposed to intellectual property as a concept and specifically how it’s used to limit our understanding of the systems that run our daily lives,” Kottmann said. A European friend of Kottmann’s known as “donk_enby,” a reference to being non-binary in gender, is another major figure in the hacktivism revival. Donk grew angry about conspiracy theories spread by QAnon followers on the social media app Parler that drove protests against COVID-19 health measures. Following a Cottle post about a leak from Parler in November, Donk dissected the iOS version of Parler’s app and found a poor design choice. Each post bore an assigned number, and she could use a program to keep adding 1 to that number and download every single post in sequence. After the January 6 U.S. Capitol riots, Donk shared links to the web addresses of a million Parler video posts and asked her Twitter followers to download them before rioters who recorded themselves inside the building deleted the evidence. The trove included not just footage but exact locations and timestamps, allowing members of Congress to catalog the violence and the FBI to identify more suspects. Popular with far-right figures, Parler has struggled to stay online after being dropped by Google and Amazon. Donk’s actions alarmed users who thought some videos would remain private, hindering an attempt at a comeback. In the meantime, protesters in Myanmar asked Donk for help, leading to file dumps that prompted Google to pull its blogging platform and email accounts from leaders of the February 1 coup. Donk’s identification of numerous other military contractors helped fuel sanctions that continue to pile up. One big change from the earlier era of hacktivisim is that hackers can now make money legally by reporting the security weaknesses they find to the companies involved or by taking jobs with cybersecurity firms. But some view so-called bug bounty programs, and the hiring of hackers to break into systems to find weaknesses, as mechanisms for protecting companies who should be exposed. “We’re not going to hack and help secure anyone we think is doing something extremely unethical,” said John Jackson, an American researcher who works with Cottle on above-ground projects. “We’re not going to hack surveillance companies and help them secure their infrastructure.”"
https://venturebeat.com/2021/03/25/amazon-launches-lookout-for-metrics-an-aws-service-to-monitor-business-performance/,"Amazon launches Lookout for Metrics, an AWS service to monitor business performance","Amazon today announced the general availability of Lookout for Metrics, a fully managed service that uses machine learning to monitor key factors impacting the health of enterprises. Launched at re:Invent 2020 last December in preview, Lookout for Metrics can now be accessed by most Amazon Web Services (AWS) customers via the AWS console and through supporting partners. Organizations analyze metrics or key performance indicators to help their businesses run effectively and efficiently. Traditionally, business intelligence tools are used to manage this data across disparate sources, but identifying these anomalies is challenging. Traditional rule-based methods look for data that falls outside of numerical ranges. Problematically, these ranges tend to be static and unchanging in response to conditions like the time of the day, day of the week, seasons, or business cycles. Using the same machine learning technology behind Amazon, Lookout for Metrics ostensibly solves this problem by automatically inspecting business health indicators including revenue, web page views, active users, transaction volume, and mobile app installations. The service also helps to diagnose the root cause of anomalies like unexpected dips in revenue, high rates of abandoned shopping carts, spikes in payment transaction failures, increases in new user sign-ups, and more. Customers can connect Lookout for Metrics to 19 popular data sources like Amazon CloudWatch and Amazon Redshift as well as software-as-a-service apps like Salesforce, Marketo, and Zendesk. Lookout for Metrics scrapes and prepares the data, selects the best-suited machine learning algorithm, begins detecting anomalies, groups related anomalies together, and summarizes the possible causes. For example, if a customer’s website traffic dropped suddenly, Lookout for Metrics would help them determine if an unintentional deactivation of a marketing campaign was the cause. Lookout for Metrics also connects to notification and event services, allowing customers to create alerts or actions like filing tickets or removing an incorrectly priced product from a retail website. As the service begins returning results, developers have the ability to provide feedback on the relevancy of detected anomalies, which the service uses to improve its accuracy. “From marketing and sales to telecom and gaming, customers in all industries have KPIs that they need to be able to monitor for potential spikes, dips, and other anomalies outside of normal bounds across their business functions. But catching and diagnosing anomalies in metrics can be challenging, and by the time a root cause has been determined, much more damage has been done than if it had been identified earlier,” Swami Sivasubramanian, VP of Amazon AI, said in a press release. “We’re excited to deliver Amazon Lookout for Metrics to help customers monitor the metrics that are important to their business using an easy-to-use machine learning service that takes advantage of Amazon’s own experience in detecting anomalies at scale and with great accuracy and speed.” Lookout for Metrics is initially available in the US East (N. Virginia), US East (Ohio), US West (Oregon), EU (Ireland), EU (Frankfurt), EU (Stockholm), Asia Pacific (Singapore), Asia Pacific (Sydney), and Asia Pacific (Tokyo) AWS regions. It’ll come to additional regions in the next few months, Amazon says."
https://venturebeat.com/2021/03/25/when-ai-flags-the-ruler-not-the-tumor-and-other-arguments-for-abolishing-the-black-box-vb-live/,"When AI flags the ruler, not the tumor — and other arguments for abolishing the black box (VB Live)","AI helps health care experts do their jobs efficiently and effectively, but it needs to be used responsibly, ethically, and equitably. In this VB Live event, get an in-depth perspective on the strengths and limitations of data, AI methodology and more. Hear more from Brian Christian during our VB Live event on March 31. Register here for free. One of the big issues that exists within AI generally, but is particularly acute in health care settings, is the issue of transparency. AI models — for example, deep neural networks — have a reputation for being black boxes. That’s particularly concerning in a medical setting, where caregivers and patients alike need to understand why recommendations are being made. “That’s both because it’s integral to the trust in the doctor-patient relationship, but also as a sanity check, to make sure these models are, in fact, learning things they’re supposed to be learning and functioning the way we would expect,” says Brian Christian, author of The Alignment Problem, Algorithms to Live By and The Most Human Human. He points to the example of the neural network that famously had reached a level of accuracy comparable to human dermatologists at diagnosing malignant skin lesions. However, a closer examination of the model’s saliency methods revealed that the single most influential thing this model was looking for in a picture of someone’s skin was the presence of a ruler. Because medical images of cancerous lesions include a ruler for scale, the model learned to identify the presence of a ruler as a marker of malignancy, because that’s much easier than telling the difference between different kinds of lesions. “It’s precisely this kind of thing which explains remarkable accuracy in a test setting, but is completely useless in the real world, because patients don’t come with rulers helpfully pre-attached when [a tumor] is malignant,” Christian says. “That’s a perfect example, and it’s one of many for why transparency is essential in this setting in particular.” At a conceptual level, one of the biggest issues in all machine learning is that there’s almost always a gap between the thing that you can readily measure and the thing you actually care about. He points to the model developed in the 1990s by a group of researchers in Pittsburgh to estimate the severity of patients with pneumonia to triage inpatient vs outpatient treatment. One thing this model learned was that, on average, people with asthma who come in with pneumonia have better health outcomes as a group than non-asthmatics. However, this wasn’t because having asthma is the great health bonus it was flagged as, but because patients with asthma get higher priority care, and also asthma patients are on high alert to go to their doctor as soon as they start to have pulmonary symptoms. “If all you measure is patient mortality, the asthmatics look like they come out ahead,” he says. “But if you measure things like cost, or days in hospital, or comorbidities, you would notice that maybe they have better mortality, but there’s a lot more going on. They’re survivors, but they’re high-risk survivors, and that becomes clear when you start expanding the scope of what your model is predicting.” The Pittsburgh team was using a rule-based model, which enabled them to see this asthma connection and immediately flag it. They were able to share that the model had learned a possibly bogus correlation with the doctors participating in the project. But if it had simply been a giant neural network, they might not have known that this problematic association had been learned. One of the researchers on that project in the 1990s, Rich Caruana from Microsoft, went back 20 years later with a modern set of tools and examined the neural network he helped developed and found a number of equally terrifying associations, such as thinking that being over 100 was good for you, or having high blood pressure was a benefit. All for the same reason — that those people were given higher-priority care. “Looking back, Caruana says thank God we didn’t use this neural net on patients,” Christian says. “That was the fear he had at the time, and it turns out, 20 years later, to have been fully justified. That all speaks to the importance of having transparent models.” Algorithms that aren’t transparent, or that are biased, have resulted in a variety of horror stories, which have led to some saying these systems have no place in health care, but that’s a bridge too far, Christian says. There’s an enormous body of evidence that shows that when done properly, these models are an enormous asset, and often better than individual expert judgments, as well as providing a host of other advantages. “On the other hand,” explains Christian, “some are overly enthusiastic about the embrace of technology, who say, let’s take our hands off the wheel, let the algorithms do it, let our computer overlords tell us what to do and let the system run on autopilot. And I think that is also going too far, because of the many examples we’ve discussed. As I say, we want to thread that needle.” In other words, AI can’t be used blindly. It requires a data-driven process of building provably optimal, transparent models, from data, in an iterative process that pulls together an interdisciplinary team of computer scientists, clinicians, patient advocates, as well as social scientists that are committed to an iterative and inclusive process. That also includes audits once these systems go into production, since certain correlations may break over time, certain assumptions may no longer hold, and we may learn more — the last thing you want to do is just flip the switch and come back 10 years later. “For me, a diverse group of stakeholders with different expertise, representing different interests, coming together at the table to do this in a thoughtful, careful way, is the way forward,” he says. “That’s what I feel the most optimistic about in health care.” Hear more from Brian Christian during our VB Live event, “In Pursuit of Parity: A guide to the responsible use of AI in health care” on March 31. Register here for free. Presented by Optum You’ll learn: Speakers:"
https://venturebeat.com/2021/03/25/nexo-awarded-california-finance-lender-license-from-the-department-of-business-oversight/,Nexo Awarded California Finance Lender License from the Department of Business Oversight,"  LONDON–(BUSINESS WIRE)–March 25, 2021– Nexo, the leading regulated financial institution for digital assets with over $5 billion in assets under management, today announced it has obtained a California Finance Lender (CFL) License, applicable to lenders and brokers of commercial loans in the United States, from the California Department of Business Oversight (DBO). “The CFL license is a significant milestone in serving both our expanding client base and the next generation of crypto adopters within California State’s regulatory framework,” said Antoni Trenchev, Co-founder and Managing Partner of Nexo. “Our latest lender’s license in the U.S., it further showcases our commitment to building a fully compliant and sustainable business globally and sets the bar high for the industry.” Given the increased demand for Nexo services from California-based retail customers and institutions, the company expects the CFL license to play an immediate role in driving further interest to its flagship Instant Crypto Credit Lines™. California is among the states which hold sway over the regulation of commercial lending activities in the U.S. and is considered the one with the most onerous licensing requirements. As well as the most populous state and an economic powerhouse that would be the fifth largest economy in the world were it a country, California is a technological hub crucial to accelerating the acceptance and adoption of digital assets. Since pioneering crypto-collateralized credit lines in 2018, Nexo has become the leading regulated provider of liquidity and security for digital assets, offering: About Nexo Nexo is the world’s leading regulated financial institution for digital assets with $5+ billion in assets under management. The company’s mission is to maximize the value and utility of cryptocurrencies by offering tax-efficient Instant Crypto Credit Lines™, a high-yield Earn on Crypto & Fiat suite, and sophisticated trading and OTC capabilities, while providing the top-tier custodial insurance and military-grade security of the Nexo Wallet. Nexo manages assets for 1,000,000+ users across more than 200 jurisdictions.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210325005756/en/ Stella ZlatarevaPR Manager, Nexostella@nexo.io"
https://venturebeat.com/2021/03/25/meetinvr-launches-collaborative-meetings-app-in-oculus-store/,MeetinVR launches collaborative meetings app in Oculus Store,"MeetinVR has launched its app for collaborative meetings in virtual reality. It is available in the Oculus Store and can run on the Oculus Quest and Quest 2 platforms. Cristian-Emanuel Anton, CEO of MeetinVR, said in an email to VentureBeat that the app costs 35 euros a month ($41.38) and can accommodate as many as 35 people in the same room. You can try it out for free on the Oculus Store, but the full version is available on www.meetinvr.com. It’s a solution if you want to escape Zoom during the ongoing pandemic. Anton’s vision is to make both customer and internal business meetings better than in real life. The app does this by making human interaction more intuitive and effective, according to Anton, and by giving participants “superpowers.” The need for remote working solutions continues to rise and is creating a permanent change in enterprise meeting environments across the globe. Because of this change, Fortune 500 companies and private users are keen to explore the possibilities of VR collaboration. Within the past five years, MeetinVR has signed many multinational corporations. I’ve tried it out, and the VR application is designed in a very intuitive manner, with attention to detail such as fun backgrounds where you can look out of windows that are in outer space. The solution is available on stand-alone VR headsets including the Oculus Quest 1 and 2, Pico Neo 2, and computer-tethered HMDs. Since not everyone has their own virtual reality headset yet, MeetinVR provides a desktop solution that allows anyone to interact with their virtual reality environments using the hardware they already have. The app has VR collaboration tools such as spatial sound, 2D/3D drawing, and speech-to-text note-taking. MeetinVR also has distinctive features such as being able to shake hands virtually, 360 media casting, Microsoft OneDrive integration, meeting management, and a set of tools for workshop participants, facilitators, and moderators. Last year, the company announced partnerships with Wolf3D and Varjo. The Wolf3D integration has brought personalized selfie-based avatars to the platform. This feature, as well as content upload, meeting scheduler, and team management, is available for registered users only. MeetinVR and Varjo have a bundle offer combining the Varjo mixed-reality headset and the MeetinVR app. If you’re used to working with video calls and prefer sticking to what’s familiar, MeetinVR will soon have a solution for that as well. MeetinVR will feature essential integrations, allowing you to peer into the world of VR via the video conferencing solution within just a few simple clicks. And non-VR users can do more than just listen — they can actively participate in VR meetings. In the upcoming update, desktop users will see more features and functionality without any additional hardware costs. The company has 14 employees and it raised $2.35 million. Copenhagen-based MeetinVR offers a platform similar to Spatial, Breakroom, and Arthur, all designed to use the 360-degree immersion of VR for meetings and events."
https://venturebeat.com/2021/03/25/green-transformative-tech-startup-fuelgems-raises-900k/,Green & Transformative Tech Startup FuelGems Raises $900K,"AUSTIN, Texas–(BUSINESS WIRE)–March 25, 2021– GreenTech startup FuelGems announced the completion of a successful funding round that raised a total of $928,690. The fundraising was carried out through Wefunder, with primary investment from Austin-based venture capital fund and startup accelerator Sputnik ATX. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210325005690/en/ FuelGems’ revolutionary fuel additive uses nanoparticles to transform the operation of diesel and gasoline-powered engines. With the FuelGems additive, carbon emissions drop by 9%, poisonous carbon monoxide emissions by 15%, and the release of unburnt hydrocarbons and other dangerous particulate matter falls by up to 50%. Fuel efficiency also rises by up to 9%. FuelGems’ additive is cost-effective, adding just 2 cents onto the cost of a gallon of gasoline, and requires only a microdose of 1-5 grams for 260 gallons of fuel. The company forecasts $400+ million revenue by year 6 of operations, thanks to widespread public concern about the impact of vehicle fumes on climate change and public health. The strength of FuelGems’ solution is underscored by the amount of interest and traction the startup is already seeing from multi-billion-dollar oil and gas corporations like BP British Petroleum, Suncor, and Marubeni, as well as from the Australian Department of Defense. Sputnik invested $50,000 in FuelGems before the Wefunder got underway, and then added another $35,000 during the course of the Wefunder, bringing their total investment to $85,000. The partners at Sputnik unanimously agreed to support FuelGems after evaluating the company’s unique value proposition. They are in a unique position to assess FuelGems’ technology and capabilities, thanks to Dr. Oksana Malysheva, managing partner at Sputnik, who has a PhD in physics. It was Dr. Malysheva who evaluated FuelGems’ technical abilities, and she was convinced by the scientific efficacy of the FuelGems solution. Volodymyr Khmurych, an angel investor and COO of UFuture, made a personal investment in FuelGems by taking a lead investor role in the Wefunder campaign. He said, “Their team and product will change the world when it comes to using gasoline and diesel. FuelGems will save lives…What I love even more is the fact that such a small amount of nanoparticles is needed to treat fuel. This lets them price multiple times cheaper than other fuel additives and win a huge market share very quickly…I believe the company has incredible growth ahead and I am very excited to get in with FuelGems at an early stage and to be the lead investor in this deal.” Additional angel investors in FuelGems have backgrounds from Shell, Tesla, Battery Ventures and Sierra Ventures. Serial entrepreneur and investor Jeff Sudman adds,”I am strongly passionate about investing my capital into startups that have a potential massive impact on our planet and people. I have a very strict criteria that I look for, few things to mention such as a dedicated and talented team with proven track record, unique and protected IP, extensive testing of this IP, ability to have a massive impact to that industry, and vision to the investor to see an actual future return. The remaining 50% of my decision falls upon how big the impact will be on our planet and people. I feel that FuelGems has hit all of my requirements and more.” About the founder: Kirill Gichunts, CEO of FuelGems, is a seasoned entrepreneur and investor who has invested in and supported over 15 startups and has achieved one previous exit as a venture capitalist and one previous exit as a member of a founding management team. He discovered the value of clean technology during his studies at UC Berkeley.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210325005690/en/ Kirill Gichunts1-415-251-0250team@fuelgems.com"
https://venturebeat.com/2021/03/25/what-is-a-nosql-database/,What is a NoSQL database?,"The NoSQL database gets its name from what it isn’t: It’s a database that does not use Structured Query Language (SQL) to access the data. Some of the well-known databases, such as Oracle and PostgreSQL, are SQL databases, but most new databases that have been introduced over the past few years are considered NoSQL databases. Some people insist that NoSQL is not the exact opposite of SQL and argue that the name really stands for “Not Only SQL.” Either way, what’s important is that NoSQL databases relaxed many of the requirements that defined earlier SQL databases. While some NoSQL databases support SQL queries, most of them are built around engines that offer better performance and more flexibility for certain use cases. The most prominent difference between NoSQL and SQL lies in how the data is structured in the database. SQL databases organize the information into rectangular tables with columns that are predefined and populated with set datatypes such as integers and dates. NoSQL databases, on the other hand, store pairs of data: a key holding the name of the field and the value connected to that field. This flexibility allows some entries to have a few keys and other entries to have completely different sets of keys. For example, one entry may have the keys “name,” “rank,” and “serial number,” while another could store just the “name,” and a third might hold “name,” “age,” “home town,” and “height.” This flexibility is a blessing and a curse for developers. Adding special fields for particular rows is simple. The type of data being stored can evolve over time, and the database can adapt to the changes because it can handle new entries carrying a set of data that differs from older entries. But that freedom can wreak havoc if developers lose track of what data may or may not be stored. The code can’t rely on any predefined structure to simplify the processing, and data must often be checked and double-checked after being retrieved. The raw storage for the database can often be larger because each entry keeps a set of keys to unpack it, something that can be quite wasteful if multiple entries have the same keys. Some common use cases tend to be open-ended systems that will evolve over time. A customer management system, for instance, may start out tracking bare essentials like name and phone number. Over time, the sales team may want to store more useful information about customer preferences, like their favorite products or their particular business strategy. A NoSQL database makes it simpler to add new fields for the entries that need them. Some NoSQL databases use a “document” model, where sets of keys and their values are grouped into documents. Sometimes the values can hold other documents, allowing elaborate nested hierarchies of documents. Some simpler NoSQL databases don’t allow this, and sometimes they’re just described as “key-value” stores. Transaction support is another major difference between the two types of databases. Many early NoSQL databases did not use the most sophisticated algorithms for ensuring consistency between entries and tables. The earliest NoSQL databases used simpler algorithms because they were focused on speed, making them attractive to developers who were more concerned about database performance and less about achieving perfect consistency. Traditional SQL databases make better promises about preventing mistakes, which is an important feature in case of power outages, transaction errors, or hardware failures. A social media company, for instance, may not worry if some posts don’t post correctly. But a bank would be very concerned if there were inconsistencies in account balances because a deposit transaction failed. Over the years, the distinctions between the two databases have narrowed, as some NoSQL databases have adopted better algorithms to match the accuracy provided by earlier SQL databases. In general, developers tend to prefer traditional SQL relational databases for applications with well-defined data structures that must be carefully enforced. Financial records and scientific results, for instance, benefit from rules on data types and formatting. A less obvious, but still significant, difference between SQL and NoSQL databases is the format the databases use for their responses. While SQL databases used a spare format to return answers to queries, some NoSQL databases formatted their responses in JSON. Developers like JSON because it makes it easier to write code for the browser. Several SQL databases have also adopted JSON to take advantage of this convenience. The big database companies have adopted several NoSQL database features. As mentioned earlier, newer versions of SQL databases adopted JSON for the response format. PostgreSQL, Oracle, IBM, and most others have added extra responses that preformat the data in JSON to make it simpler for developers to switch between NoSQL and SQL databases. Microsoft’s CosmosDB is said to be a “multimodel” database because it offers two APIs, one that speaks traditional SQL and another that speaks NoSQL. The data underneath is stored in a NoSQL format that is a superset of the tabular model, and the API interprets SQL requests when necessary. Oracle offers its own NoSQL database as both a product and a service, and it smoothly scales to distribute data over multiple nodes. While most of the NoSQL databases are relatively new, at least compared to SQL databases, many are well-established in the enterprise. MongoDB, for instance, is a publicly traded company offering a number of different versions of its core database, both as a service and on-premise. The open source edition is frequently installed as a core component of web applications. Couchbase is another independent company that began more than a decade ago but hasn’t gone public. Its core NoSQL engine has expanded over the years, and the company now offers other services, like full-text, mobile support, and an SQL-like API for more complex queries. Cassandra began as a project inside Facebook to support the social media giant’s vast collection of data. Social media sites are good examples of applications that work well with the unstructured freedom of document-style databases. The tool is now released as open source, and companies like Datastax have sprouted up to support cloud and on-premise installations. The cloud companies offer a variety of tools that vary from their own proprietary versions to curated versions of open source tools. Google, for instance, started building Bigtable for its internal use and later started reselling it as a service on the company’s own cloud platform. Another product, Firebase, is designed to integrate a document-style API with communication software to make it simpler for data to be synchronized between mobile devices and the centralized cloud. Amazon offers two options. DynamoDB is optimized to support large, enterprise-scale collections of data that need a fast response. Data is encrypted for security as a default and ACID-level transactions are supported. A second option called DocumentDB is built to be compatible with MongoDB. Some of the popular NoSQL databases are tightly coupled with support for distributed analysis. HBase and Accumulo are two options that are integrated with the Hadoop world for big data processing. Many of the other types of databases share some structural similarities with NoSQL. Graph databases like Neo4J and ArrangoDB are mainly designed to store networks or interconnected nodes, but they often also use NoSQL’s simple model for the data stored at these nodes. A number of databases are following the NoSQL tradition of relaxing some of the structural rules that defined the SQL generation while retaining elements of SQL. EraDB’s tool for searching time-series log data, for instance, is said to be “schema-free” because there are no predefined rules for the structure of the data. The company’s query language is SQL, however, and so it straddles both camps. The document or key-value model is a pure superset of a tabular model, and so every set of rows and columns can be easily stored as pairs of keys and associated values. Still, this flexibility comes with a cost in time and sometimes efficiency. Each entry must track the keys and also be ready to search them for the matches. This can be very repetitive and consume more disk space in cases where most or all of the entries have the same fields with the same names. Relational databases can also split data into multiple tables, a process that can dramatically reduce the number of repeated values. Some NoSQL databases still don’t offer the best algorithms for ensuring consistency. These are poor choices for applications that require the best levels of accuracy like, say, banks or reservation applications that can sell only one seat on a flight. The early versions traded off this security for speed and attracted applications that didn’t need absolute consistency. Many of the newest versions of the NoSQL use better algorithms now, making this difference less pronounced. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/03/25/datarobot-ceo-calls-for-a-new-era-of-democratization-of-ai/,DataRobot CEO calls for ‘a new era of democratization of AI’,"Dan Wright just became CEO of DataRobot, a company valued at more than $2.7 billion that is promising to automate the building, deployment, and management of AI models in a way that makes AI accessible to every organization. Following the release of version 7.0 of the DataRobot platform, Wright told VentureBeat that the industry requires a new era of democratization of AI that eliminates dependencies on data science teams. He explained that manual machine learning operations (MLOps) processes are simply not able to keep pace with changing business conditions. This interview has been edited for brevity and clarity. VentureBeat: Now that you’re the CEO, what is the primary mission? Dan Wright: What I’m trying to drive is the democratization of AI. In the past, AI has been some kind of buzzword. It’s been mainly experimental. You had data scientists who are working on different data science projects. But a lot of the models that they were working on never actually made it in production or added any value. What we’re doing now is allowing our platform to be used by people who are not data scientists, as well as data scientists, to create business insights and make better decisions on an ongoing basis. That kind of opportunity is limitless right now so we’re really focused on doing that. VentureBeat: DataRobot just released a version 7.0 update to the platform. What are the highlights? Dan Wright: We have enhancements to every one of our products within the platform. We can monitor and manage all of your models, regardless of where they live. They can be completely outside of DataRobot and still provide alerts if there’s any sort of accuracy or model drift. Another thing is anomaly detection. One thing that’s happened in the past is a model would get thrown off when there was some sort of anomalous piece of data. Now we’re actually able to tell you this is an anomaly and ask if it should be disregarded. That way you don’t throw off your models. The other thing that we’ve done is we’ve created what we call our app builder, which makes it much easier for us to build applications on top of the platform for different use cases. We’re going to create an ecosystem of these AI-powered applications. Then there were some additional features around bias and fairness detection. Our philosophy is that we need to alert you if there’s any sort of bias or fairness issues with respect to your model, and then allow you to configure the model as you deem fit based on your own ethics and your own values. VentureBeat: Most AI models require a lot of manual effort to build and maintain. Are we on the cusp of moving beyond that? Are we looking at the industrialization of AI? Wright: I think that’s spot on. We have seen a lot of what I refer to as experimental AI, where people are using disjointed point solutions and open source tools. It’s been a little bit of a black box. Those days are over. Now it’s about the industrialization of AI using an end-to-end system all the way from data prep to monitoring and managing all of your models in production. It’s decision intelligence around specific use cases. I think we’re really going to see AI take off and become real, even for people who may have failed in the past. VentureBeat: How much data science expertise will ultimately be required? Do organizations need a data scientist? Wright: The whole idea with DataRobot is to automate a lot of the things that data scientists had previously done manually. You don’t need to be a very highly skilled data scientist to create value with AI to drive insights. A business analyst, engineers, and executives can all get models into production and then monitor and manage all those models. It’s really important that you build data science best practices into the platform, and that everything is fully explainable with trust and governance. It’s democratizing AI, but with guardrails to make sure that people don’t get in trouble. VentureBeat: What impact did the economic downturn brought on by the COVID-19 pandemic have on AI adoption? Wright: I think there were a couple of ways. One is because there’s been so much volatility a human can’t take in all of this data when it’s changing that rapidly. You need AI to actually understand what’s happening in the future. If you’re a big retailer trying to determine how many jars of peanut butter are needed in a particular store, that’s incredibly complex when you layer in the pandemic and all of a sudden you have stores opening and then closing. The other thing that we really saw with the pandemic was that there were already AI models being used in production. People woke up and realized they had no idea what was going on with those models. They had no visibility into them. All they knew is that they were very likely to be inaccurate because all the data had completely changed. We’ve seen really broad adoption of our machine learning operations (MLOps), which is the part of our platform that allows you to monitor and manage all of your different models, including a model that’s created manually with Python or any sort of open source tool. If there is any kind of drift, you can actually run challenger models in the background. It’s no longer acceptable to just say I’m going to get a model in production and come back in six months and see if it’s still accurate. You need to be managing it in real time and updating it as the data is changing. VentureBeat; Will MLOps eventually just become an element of existing IT operations? Wright: What we’re really starting to see is an end-to-end system. I don’t think it’s going to be so much about just MLOps in the future, I think it’s going to be about monitoring the entire lifecycle of a model and continually updating it as data is changing. What makes what we do really powerful is we don’t just have MLOps. We have MLOps for all of your models, but most importantly we combine that with automated machine learning. We’re constantly running challenger models in the background and updating the models as the data is changing to do continuous learning. That’s what you’re going to see in the future. It’s not going to be about working for six months to get a model into production. VentureBeat: It seems like MLOps borrows concepts that were originally pioneered by DevOps practitioners. What’s going to be the relationship? Wright: I think it’s similar but more powerful. The platform automates many of the things that were previously done manually. VentureBeat: Most AI models are dependent on the quality of the data, and yet the quality of the data in the enterprise is often suspect. Is there some way to address that fundamental problem? Wright: You need to be able to automate the process to tag and clean your data to apply machine learning in the first place. We acquired Paxata in December of 2019, which was a company focused on data preparation. We’ve now integrated that into our platform. The other thing that’s really important is being able to take the data in from wherever it resides. One thing that we’ve really focused on is being able to plug into any data source, whether it’s saved locally or in any cloud. We have a great partnership with Snowflake, which made its first strategic investment ever in DataRobot. That is a major pain point for a lot of companies. A lot of companies previously tried AI, but they never got past the step of Data Prep. We’re really solving that by automating a lot of the process related to Data Prep. VentureBeat: Most AI training today occurs in the cloud. Will training of AI models soon be moving all the way out to edge computing platforms? Wright: We’re already seeing that, and it’s opening up new possibilities. The other thing that we’re seeing is AI is being used now on different types of data sources that were never previously possible. We have the ability now to take not just text data, but also image data, geospatial data, and many other types of data. You can combine them all into one model and generate predictions and decision intelligence. Humans have all of these different senses. Now AI is going to have all of those different senses, and the edge is definitely a direction that this technology is moving. VentureBeat: Will the algorithms ever get smart enough to tell us not the answer to a question but also the right questions to ask? Wright: How we look at it is you want the AI to get as smart as possible. That requires that you have as much data as possible and that you’re continually improving your algorithms. But it’s not going to be about just AI or machine intelligence. It’s this combination of human intelligence with machine intelligence. That’s what’s going to create amazing opportunities in every industry in the future. There’s always going to be a human in the loop. I don’t think AI can be too smart so long as you’ve got that human in the loop. VentureBeat: Is it possible one day AI models created for conflicting purposes ultimately just nullify each other? Wright: I’ll answer that question in a couple of ways. We are seeing kind of a rush to adopt this technology. Many people have referred to this as a fourth industrial revolution, but there’s always going to be a first mover advantage. With AI, that is even greater because of the feedback loop you get with algorithms that are constantly getting better and better. The leaders when it comes to AI are going to be the big winners over the next decade, and the losers really may never catch up. There is a very large sense of urgency to adopt the technology. But it’s unlikely that people will adopt it exactly at the same rate, but let’s just say for argument’s sake they do. You’ll end up getting a much more efficient market. VentureBeat: What’s your best AI advice to organizations right now? Wright: Too few companies are actually asking what should be an obvious question. What value is actually being delivered from my AI? A lot of people have big budgets and have been spending tens of millions of dollars for years with some of the legacy vendors in the space. They’re not getting any value, and they’re not even actually looking to see if they’re getting any value. That’s no longer acceptable. You need to know in real time what is the value that you’re getting from all of the models in production, and where are opportunities to drive more value? This is a race, and whoever is able to get value fastest is likely going to win in the market. The other thing that has flown a little under the radar is this idea of trust. It’s not enough to just use open source tools or a bunch of disjointed solutions to try to experiment with AI. You actually need a system that has trust built into the very foundation so it’s not a black box."
https://venturebeat.com/2021/03/25/8-ways-ai-is-transforming-talent-management/,8 ways AI is transforming talent management in 2021,"The pandemic has transformed how people work, forcing human resources leaders to bet on AI and other new technologies and processes that support a more adaptive, flexible, and fluid workforce. There have been “seismic shifts” in the way organizations operate, according to Sage’s recent survey of 500 senior HR and people leaders. A third of the HR leaders said they are changing how they hire by building better candidate experiences for applicants, on-boarders, and new joiners and focusing on workforce experiences. While 24% of companies are currently using AI for recruitment, that number is expected to grow, with 56% reporting they plan to adopt AI in the next year. Sage’s findings suggest a steady growth over the years, while Gartner’s Artificial Intelligence Survey from March 2020 found that 17% of organizations used AI-based talent management systems in 2019. The pandemic has likely accelerated the pace of change for many of these organizations. Evaluating job candidates on potential. Organizations are streamlining recruiting by using AI to match employees’ capabilities and potential with the skills required for a new position. Talent intelligence platforms typically rely on augmented and autonomous AI levels to personalize the candidate experience and guide the job seeker to the role that best matches their capabilities. Most talent intelligence platforms anonymize applicants’ data in an attempt to remove biases from recruiting, evaluating candidates only on skills and capabilities. But improvements in training algorithms can give organizations some insight into the biases in their data and how to correct them. According to a recent AI study from PwC, augmented AI systems support human decision-making and continuously learn from their interactions with humans and the environment. Autonomous AI systems adapt to different situations and can act without human assistance. Leading companies providing talent intelligence platforms include Claro, Eightfold, SeekOut, and Stratigens. Josh Bersin, a noted HR industry analyst, educator, and technologist, recently published an interesting report on this area titled The Rise of the Talent Intelligence Platform. Improving virtual recruiting event results. HR leaders create a series of virtual recruiting events and use AI to guide applicants to jobs with the most value. This is especially useful for enterprises wanting to hire new talent, including graduating seniors. Recruiters use virtual event apps to upload the resume books they receive from college and university recruitment programs into a talent intelligence platform. The platform analyzes each resume and classifies it based on candidates’ skills and capabilities. Instead of just doing keyword matching the way an ATS (Application Tracking System) would, virtual event apps designed into the talent intelligence platform use machine learning to match candidates with available positions based on their skills and potential. One example is Eightfold’s Virtual Event Recruiting Module. Reducing biased language. Organizations are adopting AI-enabled chatbots and augmented writing in collaboration and workflow platforms. The fact that these platforms integrate with enterprise applications such as Microsoft Outlook, Office, and Teams, as well as Slack/Salesforce, can reduce the likelihood of using biased language and support inclusive messaging. AI-based chatbots and writing assistants are trained to look for problematic terms, concepts, and sentences and recommend alternative words, concepts, and sentences that reflect less bias. HR teams say AI-augmented writing is useful for job descriptions, letters of recommendation, and improved marketing content. Leading AI chatbot vendors include Allie, Catalyst, and #Biascorrect, while AI-augmented writing leaders include Textio and Datapeople. Career planning and mobility. AI can also be used in career path planning and to create mentorship opportunities. HR teams use AI features in talent intelligence platforms to provide personalized career guidance to employees based on their innate capabilities, potential, and future positions of interest. Companies are also able to match employees just starting their career with mentors, sponsors, and executive coaches. The idea of combining AI and human judgement to improve diversity and facilitate more inclusive leadership development is still in the early stages, however. Key vendors in this area include Landit and Guild, as well as Talent Intelligence Platforms, Claro, Eightfold, SeekOut, and Stratigens. Improving new employee experiences. People leaders are relying on AI to improve the new hire onboarding process by delivering a more personalized experience. This could help the new employees get up to speed more quickly and improve retention. For example, AI could match a new employee’s preferences with recommendations about which health insurance benefit to select. Onboarding new employees during the pandemic comes with its own challenges and has underscored the need for greater process consistency and cross-functional communication because every onboarding process needed to be personalized. Key players in this field include Axonify and Kea. Retaining high performers. Organizations can also use AI to improve internal mobility and reduce the chance of high performers leaving. AI models can be trained to spot patterns that suggest when employees will decide to leave for another job. HR teams can use the talent intelligence platform to identify when a top performer may be ready to leave and provide them with information about new roles within the company and networking contacts who can talk about the department and the role. Vendors providing this capability include Ascendify, Aimee, and Gloat. Achieving more equitable compensation. As HR focuses on equal pay for equal work, AI tools can use market factors, employee performance, and job achievement to help improve compensation programs. Organizations can define sales quotas, monitor progress toward those quotas, and suggest new strategies to improve results. For sales quotas to be effective, they need to be based on historical sales trends, the latest market data by territory, a salesperson’s experience level, the pace of new product introductions, and many other factors. Leading companies in this area include Workday HCM, Paycom, and SAP SuccessFactors. Listening to employees. Employee surveys give HR leaders insight into what aspects of the organization’s culture are working and which need improvement. AI and Natural Language Processing (NLP) tools can look at chatbot sessions and quarterly surveys to gauge employee sentiment. During the course of the pandemic, many organizations have focused more on employee health, wellness, stress, welfare, and how they are coping with uncertainty. Chief human resource officers and their teams need to get up to speed on AI today if they’re going to stay competitive in the future. Automation combined with assisted, augmented, and autonomous intelligence has the potential to improve employee experiences, delivering personalization at scale. HR professionals would do well to start preparing for how their roles and long-term goals will change as automation and AI gain greater adoption."
https://venturebeat.com/2021/03/25/the-pandemic-and-gender-inequality-how-ai-is-helping-companies-hire-women/,The pandemic and gender inequality: How AI is helping companies hire women,"Presented by Hiretual Before COVID hit, women in the U.S. had made significant progress towards overcoming gender inequality. Representation was on the rise in male-dominated industries, and women were outnumbering men in the workforce for the first time since 2010. Unfortunately, 2020 would undo that short-lived victory. In December, the U.S. economy lost around 140,000 jobs — all of which belonged to women. Beyond the U.S., women accounted for 54% of job losses worldwide, even though they only made up 39% of the global workforce. Before women suffer even greater gender inequality setbacks, we have to pick up the pace in achieving workplace equality and inclusivity. With an imminent all-out-war for talent, known as the Great Rehire, employers have a chance to re-evaluate the way they hire women talent this year. However, many recruitment professionals still face two major challenges to their diversity hiring efforts: a time-consuming sourcing process and a lack of proper tools.  So how will organizations and hiring teams rise up to the occasion and respond? Companies have made well-intentioned efforts to boost women applicants by investing in hiring-bias training and prioritizing gender-neutral job descriptions. Unfortunately, these methods alone will not adequately increase diverse representation at scale across all sectors. Over the past five years, employers have gained more ways to be proactive and efficient at hiring women through the use of AI-powered talent sourcing. Recruiters on Hiretual conduct holistic, contextual searches through millions of candidate profiles across the open web backed by a self-expanding and self-learning Knowledge Graph. In 2020, I saw recruiters across all company sizes use AI sourcing to increase talent visibility with searches and fill their pipelines with more women and underrepresented minorities. In fact, from July 2020 to January 2021, searches for women candidates increased by 15%, according to recruiter activity on Hiretual’s platform. For women in the STEM workforce, having their resumes overlooked and hidden behind male counterparts is a common dilemma. Recruiters use our AI sourcing technology to address gender inequality and close gender gaps in engineering teams. Out of all the women sourced on our platform, 67% of those searches were for software engineering and software development roles. Location has also played a factor in this increase. Historically, searches for women on Hiretual were concentrated in San Francisco, Seattle, and New York City. In 2020, we saw a huge spike in searches for women engineers in Detroit, Cincinnati, and St. Louis, overtaking searches in Seattle and New York City.  “Before AI-powered tools like Hiretual, creating a go-to-market strategy took a lot of time with a high margin of error. These tools not only solve these errors but also allow talent professionals to focus on top-of-the-funnel strategies which is especially important when targeting any underrepresented group,” said Trent Cotton, a vice president of talent acquisition and author of Sprint Recruiting. “Now I know what percentage of the talent pool belong to these groups and where they are located. With this information, I can create an effective and efficient strategy to bring more diverse top talent into the organization.” Communication with women candidates has been a lingering issue for many years. As Director of B2B Marketing at Fairygodboss, Micole Garatti explains, “The current one-size-fits-all recruiting messaging doesn’t help women [applicants] see that they’ll get what’s important to them.” The added challenges of remote work hiring haven’t helped this issue, either. Garatti points out that, “A leading insurance provider in the U.K., Zurich, tested out writing ‘remote work policy’ on 80% of their job descriptions — and significantly increased applications from women and achieved gender parity in one year. But, in our survey, only 27% of organizations were publishing those policies.” To fix these communication disconnects between companies and women candidates, employers use AI-driven recruitment technology to vet communication at each point of contact in the recruitment process. We’ve seen this work with tools like Textio, which increased the number of women applicants at enterprises like Johnson & Johnson by checking their job descriptions for non-inclusive language. Enterprises with high-volume hiring and big diversity recruitment expectations use Hiretual to identify potential communication problems in their pipeline with our AI-powered reports page. On average, hiring teams on Hiretual see a 27% drop-off in women candidates at each hiring process stage. If a team analyzes their performance reports and notices drop-off rates double after the first engagement touchpoint, they could reevaluate language in outreach templates for messaging that may be offending women. At that point, a team now has the data to explain candidate activity and identify bottlenecks preventing them from successfully hiring women. When arguing against AI’s productivity for hiring women, some refer to AI platforms that used algorithms to rank candidates based on everything from “word choice to facial movements” during video interviews. As a result, there were concerns that “traditional applicants (white, male) would rank more employable than others. If your company’s goal is to overcome gender inequality, this use case for AI would likely impede your efforts. Technology may take the blame for these situations, but employers must recognize their responsibility to be cognizant of their intentions during the hiring process. When teams use AI to automate interviews, subjectively judge candidate intelligence, and rank candidates based on potentially biased algorithms, they risk using technology to step in for a recruiter’s critical-thinking skills. Instead, teams should use AI to automate objective assessments, measure team performance, and increase data transparency for employers to meet workplace equality goals. As a professor of innovation at IMD, Michael Wade explains, “Tools at our fingertips are only one piece of the puzzle. What we’re missing is the right mindset.” This lesson is one we can apply towards using technology to further workplace equality. In addition to tech and data-driven DE&I processes, professionals at every level will have to prioritize the right mindset and attitudes in the workplace. This includes giving women and underrepresented talent equal respect, confidence, and compensation to do their job. It involves trusting them the same way you would any employee. It also ensures that those candidates feel supported, whether through mental health check-ins, employee resource groups, or safe workplace communication channels. When we combine the right mindset with the right tools, we can overcome any challenge. I hope that soon I can look back on this article with joy, knowing that AI technology helped us achieve workplace equality once and for all. Steven Jiang is CEO/Co-Founder at Hiretual. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/25/coscreen-exits-stealth-to-bring-multi-user-screen-sharing-and-editing-to-remote-engineering-teams/,CoScreen exits stealth to bring multi-user screen sharing and editing to remote engineering teams,"“Collaboration” has emerged as a key theme over the past year, with businesses forced to rapidly embrace remote work and figure out how to manage colleagues scattered over disparate locations. In the software development sphere, this provided a boost for tools such as Microsoft’s Visual Studio Live Share, while upstarts such as CodeSandbox and Replit raised sizable investments to further develop their various browser-based code collaboration platforms. Despite the broad embrace of cloud-based integrated development environments (IDEs), the software collaboration process extends far beyond coding, which is where a new startup called CoScreen is hoping to carve out a niche. Founded last April, CoScreen touts itself as a “deep collaboration” platform for engineering teams, encapsulating next-gen screen sharing, editing, and communication. The Menlo Park, California-based company officially launches today with $4.6 million in funding from institutional and angel investors, including Unusual Ventures, GitHub CTO Jason Warner, Google subsidiary Kaggle’s CEO Anthony Goldbloom, and former Mozilla CEO-turned VC John Lilly. “We believe that collaborative IDEs will play an increasingly important role in developer toolchains,” CoScreen cofounder and CEO Till Pieper told VentureBeat. “But developers still need to be able to collaborate on more than just on code. They need to review front end specs together in Figma, run commands in Terminal, debug in various browsers, test their prototypes in mobile emulators. That’s where CoScreen comes in.” CoScreen enables multiple team members to share and edit specific app windows simultaneously on a joint workspace, bypassing the need to share their entire screen with each other. This means confidential notifications that appear on one person’s screen won’t be displayed to others. In effect, CoScreen meshes the real-time collaboration functionalities of Google Docs with the video and screen-sharing smarts of Zoom. “To our knowledge, there is no other product that enables multiple team members to share applications at the same time and to control them at the same time,” Pieper said. “The recent hype around video chat is missing the point for what remote teams actually need and is over-indexing on talking about work and under-indexing on the need for a shared interactive context. Remote teams don’t require fancier ways to communicate with each other or to understand what everyone else is up to — they actually require a much better way to get things done together.” With CoScreen, which is available as a desktop app for Windows or Mac, users click whatever app window they want to collaborate on and all colleagues on that session can view and edit each other’s shared windows, which are each marked and labeled with the owner’s name. For example, users can copy and paste text between shared windows. A built-in audio and video-chat feature also allows them to communicate in real time. The platform comes with Slack and Google Calendar integration so teams can easily jump into a CoScreen session, and the company is gearing up to release integrations with the broader developer tool ecosystem in the future, including IDEs such as Visual Studio Code. CoScreen could be used for any number of scenarios in software development, such as pair programming (an agile software development technique); debugging; or interviewing prospective hires and onboarding new hires. Although it’s clear CoScreen holds potential as a utility for other departments and disciplines, software development is the company’s sole focus for now. “Developer teams are our initial core audience because they need to collaborate across a multitude of apps and often have personal preferences in what tooling to use,” said Pieper, who left a role as a Google product manager to found CoScreen. “There is no other tool that enables them to do that as interactively and flexibly as CoScreen.” That said, there isn’t anything to stop teams from using CoScreen for any internal project that requires multiple team members to collaborate from different locations. “Customers frequently approach us about using the platform for new employee onboarding, support, education, marketing, and other agile team activities like sprint and product planning,” Pieper added. “We’ve also been approached about other collaborative use cases like music production, video editing, and even financial planning and analytics.” In its short life so far, CoScreen has managed to attract engineering teams from esteemed enterprises such as Slack, Okta, Salesforce, and SAP, though it might be something of a stretch to call these companies “customers,” given that CoScreen remains an entirely free product through at least May. At some point, CoScreen will activate its Pro and Enterprise pricing plans, which will serve as the real testbed for how attractive the company’s proposition is. CoScreen’s three cofounders have fairly extensive enterprise experience, having worked at Google Cloud, SAP, and Nestlé, which should serve CoScreen well as it builds out its platform for bigger companies. “We are seeing considerable interest from large enterprise customers in extending their usage, as well as organically growing usage within organizations,” Pieper noted. “We have selected several of those enterprises as design partners to further build out our roadmap.”"
https://venturebeat.com/2021/03/25/morphisec-snags-31m-for-moving-target-defense-that-protects-mid-sized-companies/,Morphisec snags $31M for moving target defense that protects mid-sized companies,"With cybersecurity companies going after the big enterprise fish, mid-sized companies can get lost in the shuffle. But Morphisec has developed an endpoint protection service that’s designed to be cost-effective enough to serve them high-quality protection. As the pandemic stretches these companies even further, Morphisec has found itself in high demand. Today, the company announced it has raised $31 million in a funding round led by JVP, with participation from Orange and Deutsche Telekom Capital Partners. Morphisec CEO Ronen Yehoshua said security for mid-sized companies is taking on greater urgency as they struggle with migration to the cloud and other aspects of their digital transformation. By automating security, Morphisec believes it can help them overcome some of those hurdles. “If you are able to help them to protect and prevent before the attack comes and does damage, then we solve 90% of the problem in terms of people and cost,” he said. Morphisec relies on a technique called “moving target defense” that constantly changes the security parameters of a system to thwart cyberattacks. This enables a kind of zero trust defense that doesn’t require employees to have deep security expertise in order to combat threats. In addition, the company helps leverage existing security tools to make them more effective. That includes a partnership for a service that makes the latest security features in Microsoft Windows 10 easier for clients to use. By blocking attacks, Morphisec helps reduce the costs of security tools, but also the expense involved in recovering from such attacks. And companies have to spend less time and money chasing security personnel in an extremely competitive talent market, Yehoshua said. The company says it is currently protecting 7 million endpoints and plans to use the latest funding to expand its team in the U.S. and Israel."
https://venturebeat.com/2021/03/24/which-bi-apps-do-enterprise-users-most-admire/,Which BI apps do enterprise users most admire?,"Enterprises rely on business intelligence applications to identify and predict the potential outcomes of their business strategies. Whether or not the business intelligence application is effective at delivering measurable business value depends on the accuracy of the application’s predictions, and how those predictions affect the organization. SoftwareReview’s 2021 Business Intelligence Data Quadrant Report asked 1,234 IT decision makers to weigh in on the factors that separated the most and least admired BI vendors. Of the 16 vendors evaluated for the report, respondents identified Zoho Analytics, Tableau, Dundas BI, TIBCO Spotfire, and Qlik Sense as delivering the greatest business value to their users. Zoho Tableau, Qlik Sense, and Yellowfin received the highest scores for reusability and intuitiveness, and Board, Qlik Sense, and Looker were rated as being the most customizable, according to the survey. Emotional response ratings across 25 questions were aggregated to create an indicator of overall user feeling toward the vendor and product. Two of the metrics in the survey indicated how favorably the respondents viewed the BI applications: Value Index, or user satisfaction given the costs paid; and Net Emotional Footprint, or high-level user sentiment about the application. Dundas BI, Tableau, Board, Looker, and Zoho Analytics had the highest combined Value Index and Net Emotional Footprint scores across 16 BI vendors included in the study. When asked how well BI vendors supported advanced analytics and data science, the respondents rated TIBCO Spotfire the highest, at 83%, followed by Qlik Sense, Domo, and Tableau. Surprisingly, the aggregate level of customer satisfaction with advanced analytics and data science support was 76%, which is actually pretty low considering that BI vendors are extensively hyping these capabilities. Users were vocal about their satisfaction levels regarding features in BI apps, such as advanced analytics and data science. Tableau received 158 survey responses and a satisfaction score of 79%, the second-highest in the survey. Microsoft Power BI received the most survey responses with 207 and had a satisfaction score of 75% on this attribute. The chart below ranks how respondents ranked vendor support for advanced analytics and data science in BI applications. Survey participants were also asked about their impressions of the applications’ product strategy and rate of improvement. Tableau, Dundas BI, and Sisense were the most respected, according to the survey. The survey results reflect Tableau’s efforts to build a self-service tool that addresses business users’ needs across an enterprise. The survey respondents considered application and platform security as the highest priority feature, tied with operational reporting capabilities. To be considered in this category, the BI platform had to support data access control management, including access permissions management, user authentication, and enforcement of access permissions via technology. Domo was the highest-rated BI vendor for applications and platform security. The high rating reflects Domo’s support for advanced security features, including multiple logical and physical security layers, least privilege and separation of duties access model, and transport layer encryption and encryption at rest, allowing customers to manage their encryption keys. Enterprise leaders evaluating which BI application to buy need to consider their most important use case. For example, self-service BI, where business analysts connect with, and aggregate data from, diverse data sources that drive visualizations, might dominate business needs. The most important criteria in selecting a self-service BI application include data integration, data preparation expertise, and intuitive data visualization workflows. Another use case is enterprise-wide BI deployment. In that scenario, key criteria to consider include support for governance, centralized manageability, and scale to deliver access and content to a broad community of analytics users. A third use case is augmented BI, defined as automating manual processes involved in data analysis, integration, and visualization using machine learning. The most important criterion for evaluating augmented BI applications is the automated insights module, which includes machine learning algorithms trainable by an organization. Additional key criteria include natural language query, natural language generation, and support for data storytelling. Depending on which use case is most important, different BI applications might work better for an organization. Regardless, such applications need to progress beyond data visualization and dashboards by adopting machine learning techniques to generate insights that deliver more business value."
https://venturebeat.com/2021/03/24/rivervest-venture-partners-closes-275-million-life-sciences-fund/,RiverVest Venture Partners Closes $275 Million Life Sciences Fund," Oversubscribed fund to continue investing in high-impact biopharma and medical device innovation  ST. LOUIS–(BUSINESS WIRE)–March 24, 2021– RiverVest Venture Partners, a leading venture capital firm, announced today the closing of its RiverVest Venture Fund V, L.P. (“Fund V”), with $275 million of capital commitments in an oversubscribed fundraise. Fund V brings the firm’s total assets under management to more than $1.6 billion. RiverVest invests in early-stage biopharma and medical device companies addressing significant unmet medical needs and has delivered consistently strong results to investors. Fund V is RiverVest’s largest fund to date, reflecting commitments from a wide range of institutional investors, as well as family offices and individual investors. Fund V’s limited partners include most major investors from earlier RiverVest funds and several new institutional investors, enabled by the larger fund size. “With RiverVest Venture Fund V, we will continue our investment strategy grounded in close collaboration with entrepreneurs and academic investigators to develop products for the most pressing challenges patients face today,” said Jay Schmelter, RiverVest’s co-founder and managing director. “Fund V’s larger size will enable RiverVest to participate more fully in later equity rounds of portfolio companies which have the greatest potential.” RiverVest has a 20-year track record of success. Of the 55 companies in which RiverVest has invested, 18 have been successfully sold and eight have gone public, including Allakos (NASDAQ: ALLK), Mirum Pharmaceuticals (NASDAQ: MIRM) and most recently Spruce Biosciences (NASDAQ: SPRB) in October 2020. Having founded 15 companies to date, RiverVest is adept at founding companies with technology that originates from academic labs or that has been spun out from larger companies. RiverVest joins syndicates with peer venture firms globally to propel early-stage companies. Today, there are at least 27 commercial products treating patients and many in development from companies in which RiverVest has invested. They include drugs such as Lokelma, a treatment for hyperkalemia, a life-threatening condition caused by elevated potassium levels, developed by ZS Pharma and commercialized by AstraZeneca in 2018, and medical devices such as the Supera™ stent, used to treat peripheral artery disease, developed by IDEV Technologies and acquired by Abbott in 2013. “RiverVest approaches each investment in our concentrated portfolio with high conviction,” said Schmelter. “Working as a team, we aggressively identify and create investment opportunities, we are thorough in our due diligence, and we aim to provide our portfolio companies collaborative scientific, operational, financial and business development expertise.” With its disciplined investment approach, RiverVest will continue to create value for patients, entrepreneurs and investors. Headquartered in St. Louis and with offices in San Diego and Cleveland, RiverVest is at the intersection of outstanding medical research universities and flourishing innovation ecosystems, reviewing a diverse opportunity set and investing nationally. About RiverVest RiverVest is a leading venture capital firm building life science companies to address significant unmet needs of patients and deliver consistently strong returns to investors. With headquarters in St. Louis and offices in San Diego and Cleveland, RiverVest accesses forward-thinking research and clinical expertise at leading institutions across the country to found and fund biopharma and medical device companies. Select Commercial Products Developed by RiverVest’s Portfolio Companies:   View source version on businesswire.com: https://www.businesswire.com/news/home/20210324005870/en/ Sara Mesiano314-726-6700smesiano@rivervest.com"
https://venturebeat.com/2021/03/24/how-data-and-automation-make-indie-game-developers-competitive-vb-live/,How data and automation make indie game developers competitive (VB Live),"Presented by Snowflake Gaming is a tough — but lucrative — market. Data levels the playing field for developers, fueling effective user acquisition, monetization, and more for companies of every size. Join this VB Live event featuring GamesBeat’s Dean Takahashi in discussion with of experts from EA, Jam City, and Snowflake, and learn how to leverage data and cash in on growth opportunities. Register here for free. Mobile gaming has been a data-driven business since the start, but the biggest story over the past few years is the advent of machine learning and automation. Marketing teams in mobile have relied on data for a long time, and now machine learning has expanded their ability to leverage that data, says Brian Sapp, SVP of user acquisition, marketing, and ads at Jam City. “Automation has risen with Google and Facebook on the network side, but now it’s coming into the buyer side and the marketing side,” Sapp says. “Automation is allowing us to hand metrics over to machines to optimize campaigns with way more data than any human ever could.” As an example, the main metric that humans have optimized toward in UA marketing for years has been day one or day seven return on ad spend, or ROAS. It’s a metric that allows marketing teams to balance cost with user quality. It’s been a popular metric to optimize to simply because in terms of cognitive load, ROAS was the easiest for teams to understand and optimize across a variety of campaigns. But machine learning and automation lets marketers step away from the single metric that humans can understand, and hand it over to the algorithm that can take into account a lot more data than just day one, day seven ROAS. Now Jam City is looking at account cost, CPM, cost to the market, CPI elasticity, the amount of payers per campaign versus the amount of risk, predictive LTVs, long-term payback and so much more. Automation and machine learning has also had a major impact on personalization, Sapp says. With machine learning, you can look at all of a player’s journey, whether they’re new or they’ve been in the game for five years, and create segments of users in an unbiased way. We’re only at the tip of the iceberg with what can be done, Sapp says, but there are challenges as companies try to wrap their arms around the technology. One of the biggest will always be the data itself. “Make sure that you get your instrumentation right, because getting bad data and trying to clean it up later is very challenging,” Sapp says. “And make sure you’re working with products that are nimble, and give you flexibility to leverage new technologies when they come.” Indie developers need a solution for in-game data, for marketing — which pulls from a variety of different data sources — and a data warehouse solution that makes it easy to query and visualize that data. “As an indie dev, you’re probably looking at those three buckets, and you need to find companies that give you pretty flexible solutions that can handle each of those areas,” he says. “If your job is making games, data is not your area of expertise. It’s worthwhile to do your homework and do a lot of research to get it right.” It’s also important for studios and businesses in a data-driven world to find the right KPIs to focus on — and you don’t want to focus on one KPI to the detriment of others. Sapp recommends finding at least two to four KPIs that are important to your business, whether that’s long-term retention or daily active payers and so on, and make sure you’re instrumenting your data that gives you the answers around those KPIs. “That would be first and foremost, because that’s what’s going to drive the business, and drive the long-term results the business is looking for,” he says. Second, he notes that humans have a tendency to over-optimize their way into poor performance. Have a good understanding of how to use the data to optimize your product, and don’t get too narrowly focused. Or in other words, don’t get too granular or take too short-term a view of your data without taking into account the long-term view, longer-term trends. And learn to understand what is standard deviation and what is noise, or you can get yourself in trouble, he says. In terms of how marketing operates, not much will actually change with the advent of IDFA, he believes. Giving users more choice in their privacy is a good thing for the industry, and will go a long way to building trust between developers and users. “The key to marketing is getting your product in front of users and selling the message of that product and why it’s important,” he says. “Aligning with audience motivations, finding the right creative messaging to get that point across, that’s not changing for us.” With the advent of automation, creative has become more important than ever in marketing, and Post-IDFA, that won’t change. “Making sure we’re making creative that resonates with the right audiences, making sure we understand what those audience motivations are, is still going to be the most important part of our marketing machine,” he says. “We’ll continue to be successful following those fundamentals.” Don’t miss out! Register for free here. Participants will learn how to: Speakers:"
https://venturebeat.com/2021/03/24/stampli-named-ap-automation-leader-in-g2s-grid-for-seventh-consecutive-quarter/,Stampli Named AP Automation Leader in G2’s Grid® for Seventh Consecutive Quarter," #1 in All AP Automation Index Reports, Momentum Grid® Leader in Multiple Categories, Grid® Leader in AP Automation and Invoice Management  MOUNTAIN VIEW, Calif.–(BUSINESS WIRE)–March 24, 2021– Stampli, a complete accounts payable (AP) automation platform that brings together AP communications, documentation, and payments in one place, has been named Leader in AP Automation, as well as Invoice Management by G2, the world’s leading business solutions review website. In addition to securing its placement as an overall G2 Grid® Leader in AP Automation for the seventh consecutive quarter, G2’s Spring 2021 scores also mark this as the fifth quarter in a row the company was named an overall Grid® Leader in Invoice Management. Stampli is also recognized as a Leader in AP Automation for mid-market businesses (companies with 51-1,000 employees). As part of Stampli’s continued success in multiple software categorizations, the company is placed as overall Leader in G2’s Momentum Grid® across AP Automation, Invoice Management, and Billing categories. The Momentum Grid® is measured by Momentum scores on the vertical axis which represents overall company growth along with review growth, and Satisfaction scores on the horizontal axis which represents overall customer satisfaction. As with previous quarters, Stampli received its Leader status across multiple categories based on high ratings of customer satisfaction, support and usability according to reviews submitted to G2 by Stampli users. User-contributed ratings of Stampli led to a number one overall rank in Spring 2021 for all four AP Automation Index Reports – Implementation, Relationship, Usability, and Results – as well as first in the same four Index Reports for mid-market companies. “From the beginning, Stampli has put customers front and center,” said Eyal Feldman, CEO of Stampli. “This has resulted in everything from our industry-leading response times to the way we’ve evolved the Stampli product (our introduction of Stampli Direct Pay being an example of this). We’re thrilled to be recognized by our customers on G2 Crowd and excited to continue building the best AP Automation platform for our users.” “We hold an incredible amount of respect for the G2 Crowd because they’re real-life indicators of customer satisfaction. Being able to see unbiased user feedback is invaluable. It’s also incredibly fulfilling to see reviewers holding Stampli in such high regard, with things like user friendliness, fast integration times, supportive customer success and deep integrations being common themes in the reviews.” G2 reviewers praised Stampli for its intuitive design, extensive training and support capabilities, and seamless integrations with existing processes and financial systems. These traits are especially important when many AP teams are operating remotely. According to one Stampli user in 2021: “Stampli’s simple to use interface makes it easy for our employees to review, process and pay invoices. By implementing Stampli, we were able to streamline our AP process and cut down on the amount of time it took to process invoices. In addition, their on boarding process was easy and they took great care making sure we were 100% satisfied before they went “hands off”. Stampli’s support has been on point and they are quickly able to answer any questions that we have.” In addition to being a G2 Leader in AP Automation, Stampli was named “Best Accounts Payable Solution” in the 2021 FinTech Breakthrough Awards just last week. About Stampli Stampli is intuitive with role-specific experiences, carefully tailoring the invoice processing experience by stakeholders – AP Staff, Management, Approvers, and Vendors – giving each individual an experience based on their unique workflows and needs. Additionally, Stampli turns the invoice into a communication tool by offering an in-app communications hub connected to the invoice. All conversations are connected to the invoice – getting questions answered quickly and available for audit. Stampli is also payment agnostic, giving customers the freedom to choose whichever payment method or service they prefer. Stampli Direct Pay, is an optional service, where Stampli users can pay vendors directly inside of Stampli with ACH and Check payments. Stampli seamlessly integrates with ERPs, including Oracle NetSuite, Sage Intacct, QuickBooks Desktop, QuickBooks Online, Microsoft Dynamics and more. Fast, easy setup without the need for IT. For more information, visit stampli.com. About G2 G2, the world’s leading business solution review platform, leverages more than 1,000,000 user reviews to drive better purchasing decisions. Business professionals, buyers, investors, and analysts use the site to compare and select the best software and services based on peer reviews and synthesized social data. Every month, more than one million people visit G2’s site to gain unique insights. Co-founded by the founder and former executives of SaaS leaders like BigMachines (acquired by Oracle) and SteelBrick (acquired by Salesforce) and backed by more than $100 million in capital, G2 aims to bring authenticity and transparency to the business marketplace. For more information, go to G2.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210324005098/en/ Tiffaney Fox QuintanaStampli408-910-9571tiff@stampli.com"
https://venturebeat.com/2021/03/24/sourcegraph-now-lets-enterprises-automate-large-scale-code-changes-across-repositories/,Sourcegraph now lets enterprises automate large-scale code changes across repositories,"Sourcegraph, a universal code search platform used by companies such as Amazon, Uber, Atlassian, PayPal, Qualtrics, and Cloudflare, has launched a new feature designed to help enterprises accelerate their development “velocity,” a metric used in agile development to represent the amount of work a team handles during a sprint. With batch changes, which officially rolls out today, companies can automate and track large-scale code changes across all their repositories and code hosts, saving up to 80% of their time versus existing manual methods, according to Sourcegraph. Founded out of San Francisco in 2013, Sourcegraph straddles the various strands that constitute modern developer operations (DevOps) teams, including editors, repositories, programming languages, file formats, and more. It’s ultimately designed to fix the “big code” problem, which is characterized by the sheer volume and variety of source code, systems, and tools that companies have to manage across projects. As expectations around speedy software cycles increases, this puts extra pressure on developers to optimize how they work. This is where Sourcegraph comes into play, serving to help developer teams find and fix things across all their code. So if a developer needs to know how to use a particular function or service, or where the correct library is for a particular task, they turn to Sourcegraph. While Sourcegraph has always allowed its users to search for every repository file and line of code that might need to be modified as part of a large-scale refactor, it didn’t previously offer any functionality to execute this — and that is what batch changes is all about. “Sourcegraph now enables companies to automate and support large-scale code changes end-to-end,” CEO and cofounder Quinn Slack told VentureBeat. With batch changes, Sourcegraph is looking to fix a problem that is more inclined to impact larger businesses with multiple developer teams and departments, though in truth it could prove useful for any project that shares code across multiple repositories. Companies will often create boilerplate code and components that can be reused by different teams and projects, but when it comes to modifying that code, they then have to relay those changes to all the other teams that use it — this can be “slow, painful, and have a low completion rate,” according to Slack. “This might result in a lot of legacy code just not being updated, which creates technical debt.” And so with batch changes, companies can alter the boilerplate code and apply the changes to whichever repository the code may reside in. “They can then track the progress of that large-scale change as repository owners review and accept it,” Slack added. Batch changes could prove useful in myriad scenarios, such as issuing security fixes. The code to fix a flaw might be straightforward to write, but if the issue exists in hundreds of repositories, it could be a ghastly time-consuming nightmare deploying the update and then ensuring each pull request is accepted into the main codebase. Major technology companies such as Facebook and Google have developed their own proprietary universal code search platforms for use internally, and these often include tools to automate code changes — Google has built a tool called Rosie for its “find-and-replace” functionality. “Instead of building this functionality internally, which would be extremely time and cost intensive, businesses can now use batch changes,” Slack said. Sourcegraph has raised $73 million across two rounds of funding in the past 12 months, with big-name backers including Sequoia, Redpoint, and David Sacks’ Craft Ventures."
https://venturebeat.com/2021/03/24/cibc-innovation-banking-provides-maystreet-with-10-million-in-growth-financing/,CIBC Innovation Banking Provides MayStreet with $10 Million in Growth Financing,"NEW YORK & DENVER–(BUSINESS WIRE)–March 24, 2021– CIBC Innovation Banking is pleased to announce a $10 million credit facility for New York-based MayStreet Inc. (“MayStreet”), an industry-leading market data technology and content provider. The credit facility is available to help the rapidly growing fintech firm further scale its business. Founded in 2012, MayStreet offers market data technology that delivers high-quality global market data to enable data-driven decision-making. Combining ultra-low latency software with consolidated, top-of-book and full depth-of-book data, MayStreet empowers its clients – including the sell side, buy side, vendors, regulators and academics – to gain deeper insights to drive investing, trading, execution analytics and compliance. “Ultra-high-quality data is critical in powering the most innovative technologies being developed on Wall Street and beyond, and MayStreet is at the forefront for market data with its end-to-end platform,” said Kevin Grossman, Managing Director in CIBC Innovation Banking’s Denver office. “The firm’s business is growing rapidly, and our team is pleased to support their continued expansion.” “We believe we have a significant opportunity to help capital markets participants further their digital transformation efforts by most effectively and efficiently making use of the vast amounts of data available today,” said Patrick Flannery, MayStreet’s CEO & Co-Founder. “The CIBC Innovation Banking team shares our vision, and we are pleased that they stand ready to help accelerate our growth plans should the need arise.” MayStreet’s existing equity investors include Credit Suisse Asset Management’s NEXT Investors. About CIBC Innovation BankingCIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About MayStreetMayStreet’s market data technology delivers the highest-quality, most complete global market data to enable data-driven decision-making. Combining ultra-low latency software with consolidated, top-of-book and full depth-of-book data, MayStreet empowers its clients – including the sell side, buy side, vendors, regulators and academics – to gain deeper insights to drive investing, trading, execution analytics and compliance. MayStreet’s services include: Bellport Feed Handler Solution – real-time and historical market data processing; MayStreet Market Data Lake – multi-asset, global exchange data to fuel pre-trade, trading and post-trade analytics; and MayStreet Analytics Workbench – flexible, cloud-based toolkit for analyzing, querying and visualizing normalized market data. For more information, please visit www.maystreet.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210324005086/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609"
https://venturebeat.com/2021/03/24/growjo-announces-500-fastest-growing-companies-in-new-york-city-for-2021/,Growjo Announces 500 Fastest Growing Companies in New York City for 2021,"BOULDER, Colo.–(BUSINESS WIRE)–March 24, 2021– Growjo is happy to announce the release of the 500 fastest growing companies in NYC for 2021! Some of the top big apple companies represented include: ExecOnline, Covera Health, Bizzabo, Rhino Labs, Even Financial, Blueground Co, Vestwell, H1, The Guarantors, Columbia Care, Backstage, Sevenrooms, Hyperscience, Socure, Attentive, LeafLink, Electric AI, Expert Institute, and Medly Pharmacy. The 500 companies awarded range from various industries including SaaS, FinTech, Healthcare and Real Estate. “There is no shortage of growth in New York City,” said Tom Blue, CEO of Growjo. “I think what is most impressive about the awarded companies is the variety of industries they represent. In the top 10 alone, there are companies representing 7 different industries.” To see the full list of fintech companies – Check Out the Growjo List Here Growjo only includes companies on a high growth path that are currently under 1000 employees. The list is inclusive of all growing companies based on a variety of data sources, and submission and/or payment is not required, giving companies a more accurate depiction of how they stand in their respective markets. For more information about Growjo, contact Jeremy Unruh, Head of Marketing – Jeremy@growjo.com About Growjo Growjo, the leader in awarding the fastest growing companies in the world, utilizes more than 20 unique growth indicators to assimilate the Growjo awards. Growjo recognizes the top growing companies for their accomplishments through the algorithm-based list ranking, and offers the list to anyone interested in an easily formatted and free downloadable format. If you are interested in learning more about Growjo and how you can subscribe to updates and download the free list or to make sure your company is listed, visit Growjo.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210324005116/en/ GrowjoJeremy Unruh, Head of MarketingJeremy@growjo.com"
https://venturebeat.com/2021/03/24/cloudflare-dlp-brings-zero-trust-to-corporate-network-data/,Cloudflare DLP brings zero trust to corporate network data,"Cloudflare’s new data loss prevention offering adds zero trust controls to an organization’s data, regardless of where that information is stored. Preventing data loss was hard enough when all of a company’s data was only stored on the corporate network, protected by a firewall. The challenge is even greater when so much of the application now lives outside the corporate network — whether that is in cloud infrastructure, software-as-a-service applications, or on devices used by employees working remotely. Defining rules for each application and configuring individual devices can be a time-consuming process that’s prone to error. The new Cloudflare Data Loss Prevention (DLP) looks at all the traffic passing through the network and applies security controls to protect sensitive information. Organizations are already using Cloudflare’s infrastructure and global network to accelerate user traffic to the internet, as well as to inspect traffic regardless of how it enters the network and filter out malicious activity. Cloudflare has been gradually taking over the corporate network: web traffic filtering with Cloudflare Gateway, zero trust access to cloud and local applications with Cloudflare Access, protection from distributed denial-of-service attacks with Magic Transit, and centralized controls over what is allowed in and out of the network with Magic Firewall. The new Magic WAN lets organizations connect branch offices, datacenters, virtual private clouds, and individual remote employees to Cloudflare’s network to create virtual networks. Almost all of the traditional data loss prevention products on the market ultimately force traffic to go through a central location, which impacts network performance, according to Cloudflare cofounder and CEO Matthew Prince. Cloudflare DLP takes advantage of the fact that an organization is already using Cloudflare’s infrastructure and applies network-wide data security policies to ensure sensitive information does not leave the network. “[Everyone] knows they need a DLP solution, but the only options are expensive, hard to manage, and haven’t seen innovation in years,” Prince said. “We’re doing something new by rethinking data loss prevention as an extension of our network, instead of adding yet another point solution for CISOs to manage.” DLP needs to do more than just look for specific types of data. The shift to remote work and software-as-a-service has meant administrators no longer have visibility into what kind of data they have and who is using it, making it harder to protect the data and prevent a data breach. The new tool takes advantage of the fact that all the traffic is passing through Cloudflare’s network and every DNS query, request, and file uploads/downloads are now logged. Cloudflare DLP builds on this increased visibility to identify specific types of personally identifiable information (such as credit card numbers and Social Security numbers) using prebuilt patterns, but that isn’t all it does. The new tool also gives administrators the ability to apply granular controls to applications to restrict access. Cloudflare DLP is part of Cloudflare One, the secure access secure edge (SASE) solution the company introduced last October. With Cloudflare One, enterprises can implement network security controls over the entire network instead of defining different sets of controls for traffic passing through the corporate firewall, cloud servers, software-as-a-service products, and remote employees connecting to corporate assets via virtual private networks. The growing popularity of SASE is a direct result of enterprises increasingly adopting cloud computing infrastructure and software-as-a-service applications, as well as the recent shift to a remote workforce. Cloudflare’s goal is to “help protect the application on the Internet, protect the infrastructure, and ensure that employees have access to the data they need to have to do their jobs,” Prince said. When so much of an organization’s data lives on infrastructure it doesn’t control, such as SaaS applications, administrators are often restricted when it comes to controlling who can access the data or how it is used. In many cases, the default setting is that anyone on the team with access to the application has access to all the data stored in that application. Some applications allow administrators to define roles and role-based access controls (RBAC), but these are specific to the application. Configuring rules for every application can be tedious and doesn’t address the fact that some applications don’t allow any rules to be created. “How do we extend the network when the threats come from all directions?” Prince asked. The first step was to give administrators visibility. The second was to give administrators the ability to build “need-to-know” rules for both internally-managed applications and SaaS applications in a single place. The rules can block users from accessing certain types of information, or allow users to view a record but prevent them from downloading the information. There are ways to add security controls to the application, such as requiring a hard key as a second factor authentication method. This way, enterprises aren’t restricted to using only the controls provided by the application. For example, the administrator can apply rules to the organization’s customer relationship management (CRM) system to restrict who has access to which kind of information. Legal and finance can look at revenue information stored in the CRM, but marketing teams may not need that same level of access. This kind of control can prevent disgruntled employees from deleting information from SaaS applications, as happened two years ago when an IT contractor for a California-based company deleted over 80% of employee Microsoft Office 365 accounts after his contract was terminated. Another step is to protect applications that may leak data through APIs. Administrators can now scan and block responses that contain data that was never intended to be sent out. When the application responds to an API query, Cloudflare will check to see if the response contains protected data such as credit card and Social Security numbers. There have been cases when certain types of data was being returned in response to an API call that was not part of the intended behavior. Another source of data leakage could be if the API wasn’t restricted to authenticated users. Cloudflare can now act as a “digital bouncer” and protect what data is being returned, Prince said, which is especially important for legacy APIs that can’t be changed to restrict what is returned in those results. Cloudflare’s “corporate network of the future” reflects the reality of the hybrid model, where applications can be inside or outside the corporate network and employees can be working in the office or remotely, Prince. Regardless of where the data resides, where the workers are, or who is hosting the application, enterprises need to reconsider how they manage and protect the network."
https://venturebeat.com/2021/03/24/how-anyroad-lets-brands-measure-customer-experiences-online-and-offline/,How AnyRoad lets brands measure customer experiences online and offline,"AnyRoad, an “experience relationship management” platform that helps brands create, manage, and measure customer experiences, has raised $10 million in a fresh round of funding. Experience relationship management (ERM) is a term AnyRoad cofounder and CEO Jonathan Yaffe coined in conjunction with Salesforce CEO Marc Benioff, who invested in AnyRoad as part of its seed round back in 2017. ERM could once be described as something akin to CRM for the real world, helping businesses measure how users are perceiving their brand and tracking shifts in buying behavior. However, in a year marked by a rapid shift away from physical world experiences, AnyRoad had to take a more holistic approach and quickly rolled out a new product last year to help marketers plan and monitor the impact of online and hybrid events. This shift was reflected in the broader events space, which also had to rapidly embrace the digital world, leading to a huge spike in demand for virtual events’ software. “We believe that as the world shifts from a things economy to an experience economy, the most category-defining brands are using experiential programs to build and deepen relationships with their customers,” Yaffe told VentureBeat. “These experiences are offline, online, or hybrid — we support everything from in-person events to online experiences, in-store brand activations, global field marketing, and more.” Founded out of San Francisco in 2014, AnyRoad today counts numerous high-profile customers, including Nestlé, Honda, Dick’s, Diageo, and Budweiser, with most of the company’s growth coming from “large Fortune 2000 consumer brands” that are striving to develop experiences to build a community and encourage brand loyalty. “Right now we have over 200 customers, which predominantly sit in three focus industries — CPG (consumer packaged goods) brands; retail brands; and automotive brands,” Yaffe said. In real terms, ERM is concerned with capturing data and helping companies manage customer experiences “at scale,” according to Yaffe, tracking every touchpoint a brand has with both existing and potential customers. The “experience” can be anything really, such as a real-world brand museum tour or an online masterclass in dog training operated by a major pet food brand. AnyRoad’s experience event manager can be used to manage bookings and garner post-event feedback. AnyRoad also offers a dedicated marketplace product that brands can use to connect with consumers to, for example, operate online classes that are aligned with the company’s products. Straddling all of this is Atlas, a data and insights platform brands can use to integrate and analyze all their experience-based data, allowing them to delve into customer sentiment and understand how people perceive the company. AnyRoad also integrates with myriad third-party enterprise tools, including Salesforce, SAP, Tableau, and Snowflake. According to Yaffe, Budweiser uses AnyRoad to engage with consumers through events and brewery tours. “They run these experiences as a form of marketing to create community and brand loyalty,” he said. “We enable them to not only power these experiences at scale, but also to understand exactly who is participating and how the experiences are changing consumers’ perspective and consumption habits.” AnyRoad intersects with several related technologies in the market, and it’s worth noting the similarities with existing customer experience management tools, such as Qualtrics and Medallia, both of which are billion-dollar publicly traded companies. However, AnyRoad is setting out to differentiate itself by offering an “integrated suite of products,” spanning data capture, event management, feedback/customer experience, and customer data. “We are building the next generation of customer experience software,” Yaffe said. “Experiences previously had a data problem — brands knew very little about who was participating in experiences. Disparate, siloed experiential data made analysis difficult, expensive, slow. There was limited ROI with no actionable insights. AnyRoad changes that.” AnyRoad had previously raised around $9 million, and with its latest round of funding — co-led by Andreessen Horowitz and Runa Capital — the company said it plans to invest heavily in research and development and go-to-market efforts."
https://venturebeat.com/2021/03/24/employees-attribute-ai-project-failure-to-poor-data-quality/,Employees attribute AI project failure to poor data quality,"A clear majority of employees (87%) peg data quality issues as the reason their organizations failed to successfully implement AI and machine learning. That’s according to Alation’s latest quarterly State of Data Culture Report, produced in partnership with Wakefield Research, which also found that only 8% of data professionals believe AI is being used across their organizations. For the report, Wakefield conducted a quantitative research study of 300 data and analytics leaders at enterprises with more than 2,500 employees in the U.S., U.K., Germany, Denmark, Sweden, and Norway. The enterprises were polled regarding their progress in establishing a culture of data-driven decision-making and the challenges they continue to face. According to Alation, 87% of professionals say inherent biases in the data being used in their AI systems produce discriminatory results that create compliance risks for their organizations. Survey-takers pointed to the need for curation and governance, data literacy and understanding, and data from more varied sources. A lack of executive buy-in was also cited as a top reason AI wasn’t being used effectively at organizations, with 55% of respondents citing this as more important than a lack of employees with the skills to create AI models. When it comes to data quality issues, data professionals said inconsistent standards across data collection, compliance and privacy issues, and a lack of democratization or access to data were the three most common blockers. As Broadridge VP of innovation and growth Neha Singh noted in a recent piece, many firms try to develop AI solutions without having clean, centralized data pools or a strategy for actively managing them. Without this critical building block for training AI solutions, the reliability, validity, and business value of any AI solution is likely to be limited. McKinsey estimates that companies may be squandering as much as 70% of their data-cleansing efforts. Of the enterprises that have deployed AI, respondents cited better modeling skills among analysts, cataloging data for visibility and access to data, and the ability to crowdsource info as ways to combat bias in AI. Roughly a third (31%) say incomplete data is a top issue that leads to AI failing. The findings agree with other surveys showing that, despite enthusiasm around AI, enterprises struggle to deploy AI-powered products. Business use of AI grew a whopping 270% over the past four years, according to Gartner, while Deloitte says 62% of respondents to its corporate October 2018 report adopted some form of AI, up from 53% in 2019. But adoption doesn’t always meet with success, as the roughly 25% of companies that have seen half their AI projects fail will tell you. “To assess readiness for AI, one must first look at the larger role of data within organizations — a language some companies struggle to learn and command,” the report reads. “There remains a large gap between the haves and the have-nots; successful deployment of AI among the haves and failure or a stop-and-start implementation among the rest will only widen that gap. Companies should be asking themselves if they have the right plans in place to become a more data-driven organization, and what that actually looks like in practice.”"
https://venturebeat.com/2021/03/24/qa-with-gismart-closing-the-loop-between-acquisition-and-monetization/,Q&A with Gismart: Closing the loop between acquisition and monetization,"This article is part of a Gaming Insights series paid for by Facebook. We recently spoke with Katerina Dudinskaya, VP of Performance Marketing at Gismart, to discuss how the award-winning European publisher is adapting its approach to face new challenges brought about by upcoming iOS 14 changes, and how the team is closing the gap between acquisition and monetization with campaign-level Return On Ad Spend (ROAS). Facebook: Let’s start off with a macro view of the industry: The pandemic is in full swing, and iOS 14 changes are coming up. How are these massive shifts impacting your business? Dudinskaya: The pandemic has driven more traffic to almost all of our entertainment products and much of the 2020 uplift has continued through 2021, although we are seeing more competition. And IDFA deprecation will force us to adapt our approach to marketing and business. For us, the best strategy is just to accept this new reality and try to be creative about how to adapt. Facebook: As limitations on targeting are becoming top-of-mind for publishers and developers, we’re seeing an evolution in the ads ecosystem. Can you talk about some of the new challenges you’re facing this year? Dudinskaya: At a high-level, our current challenges are firstly, how to deliver personalized ads and reach our most valuable customers and, secondly, how to provide our monetization partners with reliable user data to predict user-level value more accurately. Latency data and fraudulent data can affect our predictions, and using incorrect data can lead us to the wrong decisions. We’re also developing updates for our user acquisition structure, analytics, and creative production to help us deal with these challenges. Facebook: Let’s dive into what it means to build resilience in 2021. What approach are you taking when it comes to app monetization? Dudinskaya: Like most other publishers, we will be using a combination of SKAdNetwork and our own analytics systems to track all possible marketing funnel events (downloads, click-throughs, etc). This data will become the base for the decisions and predictions we make. We’re also working more closely with all our partners: publishers, traffic vendors, and monetization networks. In this new reality, more than ever, you need to work as a team. Facebook: User acquisition has been a key challenge for publishers and developers. How will you ensure that Gismart will continue to reach and acquire high-value users? Dudinskaya: We are using all open solutions that are currently available, including AppsFlyer’s proprietary user attribution solution and Unity’s machine learning solution. We also work closely with Facebook Audience Network and Facebook Instant Games teams who help us with user acquisition and monetization. We also continuously improve non-technical ways of personalization, such as growth activities on landing pages, creative production, and UA channel diversification. Facebook: Many publishers are concerned with revenue loss and the impact of ad targeting limitations and fluctuating CPMs. How do you plan to sustain revenue in this environment? Dudinskaya: We plan to apply a holistic approach, working on UA strategies and targeting creative production. We also use some CPM/bidding add-ons produced internally to be able to perform fast testing and to get a snapshot of user funnel metrics from the first impression to the deepest product metric. For games, this could be clickthrough rate, cost per install, cost per app event/acquisition, or average revenue per user. Facebook: How are you currently measuring return on ad spend (ROAS) at a campaign level? How do you maintain oversight of what’s working on which channels? Dudinskaya: Facebook Audience Network’s new campaign-level IAA ROAS has helped us to better understand our ROAS, offering accurate insights to make more profitable decisions. It enables us to acquire quality users, understand the ideal user experience, and ensure long-term engagement and revenue. The synergy of data we receive from different analytics systems, mediation, and other tools give us deeper insights into the channel performance. The more data we can bring into day-to-day operations, the better results we see in our marketing channels. Facebook: What advice would you share with other publishers and developers who are also preparing to face the headwinds of the upcoming industry changes? Dudinskaya: It’s worth noting that each advertising platform is quite unique; some creative approaches that work well on one platform, might not perform that well on another. So publishers need to have individual marketing strategies for each platform, taking into consideration their audience and content consumption habits, and the platforms’ technical specifics. Also, we’re always open to new solutions and we are constantly testing new Facebook features and tools available in an alpha/beta version. Testing new solutions is not only a great way to adapt your marketing strategy but also helps to identify errors and gaps in your current solution set. Anastasia Petrova is Strategic Partner Manager at Facebook Audience Network. VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/24/numares-announces-fda-510k-submission-of-axinon-system-a-nmr-platform-for-metabolomics-based-ai-driven-diagnostics/,"numares announces FDA 510(k) Submission of AXINON System®, a NMR platform for metabolomics-based, AI-driven Diagnostics","BOSTON, & REGENSBURG, Germany–(BUSINESS WIRE)–March 24, 2021– Leading NMR diagnostics company numares AG today announced that the company made a 510(k) submission to the U.S. Food and Drug Administration (FDA) for its AXINON® IVD System, a NMR-platform for AI-driven, metabolomics-based diagnostics. If cleared, AXINON® would become the first NMR-based clinical laboratory system using AI-evaluated metabolic data. Several multi-marker assays for AXINON® will cover numerous unmet medical needs, in order to prevent, diagnose and treat disease. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210324005057/en/ The clearance of AXINON® would bring a novel diagnostic approach to clinical laboratories, the combination of several metabolic biomarkers, so-called metabolite constellations, analyzed by advanced nuclear magnetic resonance (NMR) technology and evaluated by numares’ proprietary, AI-driven AXINON® diagnostic software. “This U.S. regulatory filing for its diagnostics platform is an important milestone for numares,” said Winton Gibbons, President – US and co-Chief Executive Officer of numares. “After entering into collaborations on joint diagnostics development with analytical instrumentation and solution provider Bruker Corporation in January 2021, and with Mayo Clinic Laboratories in 2019, this FDA submission is the first step in our common endeavor to bring NMR diagnostics into routine clinical use, and make metabolomics-based diagnostics accessible to the patients.” numares is currently developing multi-marker algorithms for several diagnostic tests on the AXINON® IVD System. These include AXINON® GFRNMR to reliably assess kidney function by improved determination of glomerular filtration rate (GFR). numares expects to submit this test to the FDA also in the first half 2021. The third assay AXINON® renalTX-SCORE®is intended to reliably identify early kidney rejection in post-transplant surveillance, and be submitted to the FDA in 2022. Further multi-marker tests are in development, e.g., for liver disease, cancer detection, and multiple sclerosis. Dr. Maulik Shah, numares’ Medical Director comments: “AXINON® provides clinical laboratories with a powerful technology that leverages the information-rich metabolome with artificial intelligence to give healthcare professionals new insights into disease diagnosis and management for improved patient outcomes. At the same time, automation capabilities, excellent precision, easy operability will allow easy adaption of the FDA-cleared system by clinical laboratories and will make AXINON® an indispensable routine diagnostic tool in precision medicine.” “The 510(k) submission of our AXINON® System is an important step and prerequisite to further strengthen commercialization of numares’ products in the United States”, concludes Dr. Volker Pfahlert, co-CEO of numares and chairman of the executive board. “Metabolic disorders are a rapidly increasing problem and major challenge to today’s healthcare systems. Patients will benefit from our approach to consider the patient’s metabolome with sophisticated algorithms. Once cleared by the FDA, we believe AXINON® has the potential to meaningfully improve health outcomes and quality of life for millions of patients.” The AXINON® System consists of AI-based AXINON® Software, ready-to-use AXINON® kits, and an advanced NMR technology, refined by numares’ proprietary Magnetic Group SignalingTM to ensure highly standardized and automated processing of serum or urine samples in high-throughput. About numares numares AG, based in Regensburg, Germany, is a fast-growing innovative diagnostics company that applies machine learning to metabolomics data to develop advanced analytical tests for high-throughput use in clinical diagnostics. The AXINON® System employs advanced nuclear magnetic resonance (NMR) spectroscopy to evaluate metabolic constellations. Magnetic Group Signaling (MGS®) is a proprietary technology that enables NMR for highly standardized and rapid throughput testing. Metabolic tests stand as an important pillar in precision medicine to address unmet needs in cardiovascular, kidney, liver, and neurological diseases. You will find more information at https://www.numares.com/  View source version on businesswire.com: https://www.businesswire.com/news/home/20210324005057/en/ numares Christiane ProllTel.: +49 941 280 949-14E-Mail: christiane.proll@numares.com"
https://venturebeat.com/2021/03/24/the-ran-revolution-good-news-for-5g-edge-iot-and-enterprise-private-networks/,"The RAN revolution: Good news for 5G, edge, IoT, and enterprise private networks ","Sponsored by Intel The growing torrent of data traffic and resulting push to build 5G and edge services are driving wider virtualization and cloudification of wireless networks. The new focus: Radio Access Networks (RANs), the key architecture that connects a host of devices to the core network, Internet, and cloud. Over the next three years, virtual RAN deployments will help create big new revenue opportunities for carriers and industry suppliers. For enterprises and consumers, this latest convergence of computing and communications will speed delivery of transformative new services for everything from AI and IoT to autonomous vehicles, private mobile networks, and more. It’s a key link in bringing cloud power to the mobile edge. “A decade ago, communications service providers came to us with a simple but bold challenge: Drive the same sort of transformation that we drove in the data center and cloud into the world of networking,” says Dan Rodriguez, corporate vice president and general manager of Intel’s Network Platforms Group. With an estimated 50% of mobile network cores virtualized, he says, attention has turned to RANs. Virtualized RAN (VRAN) is the fastest-growing area of cloud investment for operators, according to a new report by Analysys Mason. Spending is forecast to reach $11 billion by 2025, the firm says, a compound annual growth rate of 132%. Big-name carriers like Verizon, Rakuten and Dish are already fielding or building virtual RANs. So are a growing number of private enterprise networks. Many others are in trials. Nearly all early commercial deployments are running on Intel technology; the ecosystem is also using Intel’s FlexRAN software reference architecture to speed deployment of cloud native, fully virtualized 4G and 5G RAN. Here’s a brief look at what’s going on, why, and what it means to you. Like many things today, the massive transformation in network infrastructure is driven by data — current and future. Consider: An estimated 50 billion connected things are coming online in the next few years. The visual cloud will alter how we perceive reality, but requires massive new bandwidth.  Autonomous vehicles require-ultra low reliability and latency. The long list goes on. Growing data traffic means new network and radio capabilities are needed for scale and scope. To keep up, communications and cloud providers as well as enterprises are aggressively advancing new usage and deployment models. Many are modernizing existing LTE infrastructure while preparing for expansion of 5G and edge services. Operators are investing heavily to transform their networks into multi-cloud platforms that will maximize the benefits of rolling out 5G. Their overall spending on network cloud between 2019 and 2025 is expected to reach $114 billion (including network functions, cloud software, hardware, and professional services), according to Analysys Mason. Evolution to fully virtualized, cloud-native offers many benefits: greater flexibility and scale, new innovation and revenue streams, and the chance to bring data center economics to the mobile network. A well-designed software stack for the core and edge – supported by hardware that lets traffic be classified, sliced, and monitored – is seen as the best way to transform networks and make them ready for 5G. Crucially, it also allows for placing more compute, network and storage closer to the source of unrelenting volume of data – devices, applications, and end-users.  “Network function virtualization (NFV) has only partly enabled the software-ization and disaggregation of the network,” writes Gorkem Yiggit, principal analyst at Analysys Mason. “The telecoms industry is now entering a new phase of network cloudification. This will require radical changes to the way in which networks are designed, deployed and operated. The digital infrastructure used for 5G will be increasingly built as horizontal, open network platforms comprising multiple cloud domains such as mobile core cloud, vRAN cloud and network and enterprise edge clouds.” The new approach lets operators move away from the costly need to upgrade RANs by adding hardwired radio towers, base stations and antennas, with dedicated software. These hardware-based appliances limited both flexibility and the kinds of services that could be offered. While switching to an open, software-based infrastructure running on industry-standard computing hardware makes a lot of sense, one-size of RAN virtualization does not fit all. For starters,  5G is a heterogeneous network of many wireless technologies, so LTE, Wi-Fi, mmWave, and NB-IOT and 5G must work together seamlessly.  Usage is decentralizing, often in private networks and campuses. One of the biggest challenges is the need to support legacy devices and be back compatible. And any new solution needs to enable RAN deployment models, from access to edge. A wide variety of RAN solutions are available today, reflecting the diversity of form factors and models in use in the industry. But there’s near universal recognition that an open, multi-vendor approach is crucial for successful “cloudification” across cloud, core and edge networks. A new industry survey found that virtually all operators were considering OpenRAN (ORAN) for their edge and Radio Access Network; many seek lower operational costs.  Intel’s Flex RAN reference architecture has become the de facto standard for building 4G/5G RAN on industry-standard platforms. Its hardware and software enable operators to jump start vRAN development and rapidly deploy base stations at various edge and access locations.FlexRAN enables implementations for ORAN/OpenRAN/vRAN by providing optimized libraries for LTE and for 5G NR Layer 1 workload acceleration. Intel reports more than 100 licensees and is working to widen adoption across the industry and ecosystem. DISH and Intel, for example, are building out the nation’s first virtualized O-RAN 5G deployment. Intel’s infrastructure technology is creating the foundation for the groundbreaking new greenfield network. Similarly, in September Japanese e-commerce and communications giant Rakuten lit up the world’s first all-virtual, cloud-native 5G network, also employing FlexRAN.  It’s not just new carriers. Verizon, Samsung, Wind River and Intel collaborated on the industry’s first fully virtualized 5G sessions, including virtualized RAN. Telefonica, Vodafone and other top operators plan to virtualize RANs as a crucial part of modernization and expansion. Behind-the scenes transformations in RANs and mobile network infrastructure  won’t just help carriers improve operational costs and bring new services to market more quickly. Businesses and consumers also stand to benefit handsomely. “Network slicing” will allow carriers and enterprises to match 5G services to diverse delivery needs across a variety of verticals and use cases including but not limited to healthcare, IOT, smart retail and Industry 4.0.  One of the biggest benefits for enterprises will be the ability to buy or build private 5G networks and connectivity. Deployed in say a retail store, factory, or stadium, dedicated 5G mid-band mobile networks offer big benefits, says Caroline Chan, general manager of 5G Infrastructure Division within Intel’s Network Platform Group. Among them: greater control, flexibility, scalability, and data sovereignty. They’re especially useful in settings too large or demanding for WiFi. “Just think about new technologies like artificial intelligence and machine learning, AR/VR, advanced robotics, and the sheer amount of data being created that businesses could leverage with dedicated 5G connectivity,” says Chan.  “The 5G private mobile network and edge computing can help enterprises derive insights from massive amounts of data, go-to-market faster with new applications, deliver more real-time customized services, and enhance customer experiences with increased network performance.” They can be managed internally or by a third-party service provider. The American Dream, a massive retail and entertainment complex in New Jersey, is a good example of how dedicated private connectivity can help transform a business. One of the nation’s first deployments running on the newly available Citizen Broadband Radio Service (CBRS) 3.5 Ghz spectrum band, the complex has introduced a private, outdoor 4G LTE network and virtualized RAN for behind-the-scenes operations. Enabled by JMA Wireless XRAN and a 100% virtualized software baseband running on standard Intel Xeon processor-based servers, the  dedicated connectivity helps the retail tenants gather data and intelligence so they can run more efficient operations and increase sales. Enterprises looking to add their own private networks will continue to benefit from innovation in the nascent space. Corning and Intel, for example, have announced a strategic collaboration to speed deployment of 5G in-building network solutions that can scale from small to large venues and enterprises. The companies will deliver a virtual platform for Corning’s 5G network solutions powered by 2nd generation Intel Xeon Scalable processors; Intel Flex RAN 5G and 4G Reference Software; Intel FPGA Programmable Acceleration Card N3000; and 10/25/40Gb Intel Ethernet 700 Series Network Adapters to support diverse CommSP and enterprise 5G deployments. Industry initiatives such as the O-RAN Alliance and TIP OpenRAN made great strides in opening up RAN in 2020, says Yiggit of Analysys Mason. Continued, wide cooperation is crucial for continued cloudification and disaggregation of RAN and creation of a robust ecosystem. Last Fall, Intel, China Mobile and Reliance Jio along with participation from China Telecom, China Unicom, Radisys, Airspan, Baicells, CertusNet, Mavenir, Lenovo, Ruijie Network, Inspur, Samsung Electronics, Sylincom, WindRiver, ArrayComm, and Chengdu NTS launched the Open Test and Integration Center (OTIC) to collaborate on multi-vendor interoperability and validation activities for O-RAN compliancy. Meanwhile, marketplace innovation continues. VMware and Intel recently introduced a new collaboration on an integrated software platform for virtualized radio access networks (RAN) aimed at accelerating the rollout of existing LTE and future 5G networks. The platform aims to  simplify integration for communications service providers building on top of VRAN platforms, explains Rodriguez.  The goal is to build a repeatable platform that will foster a healthier ecosystem for open, virtualized RAN. And to encourage more mobile network operators to deploy these technologies at scale in large urban networks. Concludes Rodriguez: “Transformed networks transform industries. The radio access network is an area of tremendous innovation and creativity, today and in the coming years. The RAN revolution is just beginning.” Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com. "
https://venturebeat.com/2021/03/24/youdsalesforce-introduces-ai-powered-features-for-meetings-and-opportunity-scoring/,Salesforce introduces AI-powered features to improve meetings and identify sales,"Salesforce today announced new features designed to help companies transform their organizations for digital sales during the pandemic and in a post-pandemic world. Beginning this week, following new AI-powered account-based marketing tools, Sales Cloud 360 is gaining Einstein Conversation Insights, an AI-powered technology that analyzes video call transcripts, as well as global models for opportunity scoring and precall summaries with customer histories. According to Salesforce, 61% of salespeople believe their roles have changed permanently since the COVID-19 pandemic began. Even when salespeople are able to return to the road and in-person workplaces, 51% expect to travel less than they did before the pandemic — and fewer than half expect to go back to an office. To address this new normal, Salesforce is introducing Einstein Conversation Insights, which gathers insights on the frequency of certain keywords or types of interactions during video calls. The goal is to create customized training and one-on-one coaching that aligns with teams’ needs, according to Sales Cloud chief revenue officer Warren Wick. “Salesforce created the playbook for sales 22 years ago, and today we’re rewriting it for an all-digital world,” Wick said in a press release. “Over the past year, we held more than 6 million calls with customers to understand what they needed to be successful as they worked to transform their business with more urgency than ever before. We’ve reimagined Sales Cloud to guide every company as they rethink the digital sales experience, from leads to coaching to processing revenue.” Alongside Einstein Conversation Insights are new features in Salesforce Meetings, including post-call action items that Einstein automatically surfaces to keep deals moving. Meanwhile, global models for opportunity scoring use aggregated, anonymized trends across Salesforce customers to empower teams with AI before they have enough of their own data. Einstein shows the factors that have contributed most to the score, both positively and negatively. For example, when extra steps are added to an enterprise deal, it indicates that the deal is progressing. Forrester predicts that spend for marketing automation tools will grow “vigorously” over the next few years, from $11.4 billion annually in 2017 to $25.1 billion by 2023. It’s estimated that 55% of marketing decision-makers plan to increase their spending on marketing technology, including AI and machine learning, with one-fifth of the respondents expecting to increase by 10% or more. Another new capability in Sales Cloud — pipeline inspection — helps track changes week by week, using AI to focus on the deals that ostensibly matter most. Other additions include in-app learning for MyTrailhead, which surfaces relevant education materials like competitor analysis in sellers’ workspaces; Tableau Business Science, a set of AI-generated predictions, insights, and automated explanations from Tableau; and Mulesoft Composer, which allows sales operations teams to connect apps and systems to Salesforce, automate sales processes, and get end-to-end sales data visibility without have to write code or wait on development resources. Salesforce says that Einstein Call Insights, Mulesoft Composer, and the new Salesforce Meeting features will be generally available as of March 24. Tableau Business Science in the 2021.1 release will be available later this month. And in-app learning for MyTrailhead, the global models for opportunity scoring, and pipeline inspection will arrive in summer 2021."
https://venturebeat.com/2021/03/24/food-supply-traceability-startup-ifoodds-nabs-15m/,Food supply traceability startup iFoodDS nabs $15M,"iFoodDS, a Seattle, Washington-based provider of safety, traceability, and quality management data solutions for the fresh food supply chain, today announced the close of a $15 million series A funding round led by Insight Partners. iFoodDS CEO Scott Matthews says the capital will be used to support product development as well as market and category expansions. Until early 2020, consumer spending on food in the U.S. had been remarkably stable, growing by around 4% over the previous five years, according to McKinsey. Then came the pandemic. Physical distancing and lockdowns reversed the trend, upending distribution channels, along with the companies that produce, process, and deliver food. For example, throughout the pandemic, food service suppliers have faced abrupt order cancellations that have left many with excess stock they can’t redirect because of packaging-size mismatches. iFoodDS started as part of iDecision Sciences (iDS), a consulting practice whose principals came to the produce scene following an outbreak of E. coli associated with spinach in 2006. In the ensuing years, iDS worked closely with food growers and harvesters, packers and shippers, processors, and grocery and food service companies to develop safety practices and metrics for leafy greens and other produce commodities. From this work, iFoodDS was launched in 2013 to provide software solutions to manage the data requirements of food safety programs. iFoodDS offers field-level data capture solutions for environmental assessments, inspection records, shipping and receiving, inventory, corrective actions, and more. It also performs risk assessments and management for preseason, daily preplant, and preharvest, leveraging a science team that builds environmental monitoring models. Building on on-site sampling and workflow analyses of plants, iFoodDS develops systems that identify the areas of highest risk for pathogens such as listeria. The company also helps standardize safety programs across different physical locations.  For example, iFoodDS automates things like forms, checklists, and logs, storing data securely in the cloud. For distributors and grocery retailers, it provides tools for quality scoring, allowing customers to capture appearance or condition defects, temperature, and other metrics upon receipt from suppliers. Email alerts notify clients when freshness or quality thresholds fall below expectations, while role-based dashboards and reports help teams measure and identify the root cause of lost freshness and quality. By Matthews’ estimation, iFoodDS is addressing some of the core challenges that continue to plague the global food industry. It’s anticipated that 23% of future food recalls will cost suppliers $30 million and that 52% will cost suppliers $10 million. According to the U.S. Centers for Disease Control and Prevention, reducing foodborne illnesses by just 1% would prevent half a million Americans from getting sick each year. “The past 12 months have brought unprecedented change to the produce industry — from changing consumer shopping behaviors perpetuated by the pandemic to new regulation proposed by the U.S. Food and Drug Administration. These changes are accelerating a rapid transformation in the industry — one that is essential to delivering wholesome, high-quality produce,” Mathews said in a press release. “With the support of Insight Partners, we’re well-positioned to scale our efforts to help stakeholders in the supply chain improve their operations, provide transparency from farm to fork, deliver wholesome, high-quality produce, and connect their brand with consumers.” Matthews says that in 2020, iFoodDS experienced “growth and expansion” that included welcoming some of the world’s most prominent produce, food service, and grocery retail logos to its customer base. The company also acquired HarvestMark, known in the fresh foods industry as one of the leading providers of food traceability software and quality insight solutions, with more than 22 U.S. patents. Insights Partners led the funding round announced today."
https://venturebeat.com/2021/03/24/industry-ventures-raises-oversubscribed-850-million-secondary-fund/,Industry Ventures Raises Oversubscribed $850 Million Secondary Fund,"SAN FRANCISCO–(BUSINESS WIRE)–March 24, 2021– Industry Ventures, a leading investment firm focused on venture capital, announced today the final closing of Industry Ventures Secondary IX (“the Fund” or “Secondary IX”) at its $850 million hard cap. Consistent with the strategy of prior funds, Secondary IX will seek exposure to leading later-stage companies through flexible investment structures, including direct secondaries, secondary LP interests and special situations (direct portfolios, tail-end funds, etc.). The new fund brings the firm’s total committed capital under management to $4.5 billion. Secondary IX is Industry Ventures’ largest fund raised to date and will enable the firm to continue to provide a broad range of liquidity solutions for venture capital investors. As a pioneer in the venture secondary market, the firm has completed over 400 secondary investments in its 20-year history. “We are immensely grateful for the strong support the Fund received from our limited partners, both new and existing, particularly during this year’s uncertainty,” said Hans Swildens, CEO and Founder of Industry Ventures. “As we continue to see venture-backed companies remain private longer, the secondary venture market is playing an increasingly critical role in providing liquidity to venture capital stakeholders.” “The closing of our ninth secondary fund is an incredible milestone for our firm,” said Justin Burden, Senior Managing Director. “We have invested across multiple market cycles during our 20-year history, and this new fund enables our team to continue to identify attractive secondary opportunities for our investors.” The Fund’s investor base includes leading institutions representing public and corporate pension funds, endowments, foundations, financial institutions, and family offices, as well as a sizable commitment from its general partner. For more information on Industry Ventures and the firm’s secondary investment strategy, click here. About Industry Ventures LLC Founded in 2000, Industry Ventures is a leading venture capital platform with over $4.5 billion of committed capital under management. Industry Ventures invests across all stages of the venture capital lifecycle through complementary fund strategies. The firm is headquartered in San Francisco, with offices in Washington, DC, and London. For more information, please visit www.industryventures.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210324005073/en/ Alex AdamIndustry Ventures415-273-7229IR@industryventures.com"
https://venturebeat.com/2021/03/24/bevy-raises-40m-for-enterprise-events-in-deal-anchored-by-25-black-investors/,Bevy raises $40M for enterprise events in deal anchored by 25 Black investors,"Bevy has raised $40 million at a $325 million valuation to grow its platform for staging virtual events for enterprises. And it has also broken a diversity barrier by bringing 25 Black leaders on as investors. Palo Alto, California-based Bevy helps enterprises build, grow, and scale their global customer communities using its platform for virtual, in-person, and hybrid digital-physical events. The funding came from Accel Partners, LinkedIn, Qualtrics cofounder Ryan Smith, and Upfront Ventures. About 70% of the individuals participating in the round — representing 20% of total funds raised — are Black investors, including diversity pioneer James Lowry, Facebook board member Peggy Alford, and Beats by Dre marketing exec Omar Johnson. Bevy CEO Derek Andersen and cofounders Joel Fernandes and Alex Bendig started Bevy in 2017 after finding success with Startup Grind. With Bevy, they wanted to build a platform for enterprise communities, but the pandemic forced them to focus on virtual communities and digital events. A year ago, Bevy was looking at a different problem. It had grown its headcount to 27 without a single Black employee. After the killing of George Floyd and the Black Lives Matter unrest, Andersen had to grapple with uncomfortable truths. “A year ago, I’m embarrassed to say, of the 27 people on the team, not a single one was Black or brown,” Andersen said. “When George Floyd was murdered on May 25, we started to really look inside of ourselves. What could we do to bring about the change that needs to happen?” Andersen said he had conversations about racism with his children, who are between three and 11. He also looked inside his company to see what it could do. The more he thought about it, the more Andersen believed that improving racial inequities in the tech sector wasn’t only the right thing to do, it was the profitable thing to do, especially for a community-oriented company. “We set a goal internally to bring about the change, as we know about 14% of the U.S. population is Black,” Andersen said. “So we said we needed that to be at a minimum there. But we need to be well above that so we can really speed these things up. And so we set a goal internally to have 20% of our team be from the Black and brown communities.” Slowly, Andersen started hiring Black and Latinx employees. Today, the company has 14% Black and Latinx employees on its staff of 100. Andersen said his goal is to make that 20% of the workforce by September. Half of Andersen’s direct reports are women. Andersen didn’t do it alone. He got help from Black investors like Kobie Fuller, general partner at Upfront and founder of Valence. When Fuller and others learned he was serious, they referred more Black job candidates to Bevy. “I saw that this could be a very large company worth billions and billions of dollars,” Fuller said. “But what I’m most excited about [with] Derrick as a founder/CEO, is not what he’s building, but how he’s building it. And the type of angle he’s taking around prioritizing culture and team and thinking about it, not from only regarding his workforce, but all facets of the organization, including the board and also the cap table.” As the company has increased its diversity, it has also become more valuable. At its current $325 million valuation, Bevy is 4 times more valuable than it was a year ago. The company has raised $60 million to date. “I’m trying to build the future of my product,” Andersen said. “Why wouldn’t I try to build the future with my team? And why wouldn’t I try to build the future with my investor base? We are a community platform. Our message is that if Bevy can do it, anybody can do it. There’s no reason every company can’t do this.” What seemed obvious was that the company needed to reach out to the community to build a community platform. “With us being a community platform, it’s silly to think that we’re going to be the best community platform that’s out there if our organization doesn’t appropriately reflect the communities that we serve,” Fuller said. “Derek set KPIs against the diversity metrics, which a lot of people will talk about, but he drove action toward diversity. We have to think about bringing the Black community closer to the innovation economy, where wealth is created at a staggering rate.” Fuller believed enough in the mission that he recruited his friends. “I devoted my whole life to trying to deal with diversity and inclusion,” Lowry said. “It’s been a tough battle. I’ve had my successes with what I’ve done on Wall Street, major corporations, and the auto industry. But nothing has happened to shake up America more than George Floyd’s killing. In Silicon Valley, within the tech industry, you just haven’t seen progress with minorities and women. We’ve seen progress. But if you look at where we are in terms of the number of businesses of size, creating large revenues, and employing a lot of people, we haven’t been as successful as I had hoped,” Lowry said. Then Fuller came to him to talk about Bevy. Lowry saw a good opportunity in Andersen’s company, and they all agreed this was a chance to do good and do well at the same time. “I feel so energized and motivated to be part of this,” Lowry said. “Hopefully, we can serve the world. When you have a founder who cares, and you have investors who want to invest in the right thing, then we can make history.” Customers pay between $20,000 and $1 million annually to license Bevy to scale their communities and host virtual conferences. As hybrid events and eventually in-person events begin to emerge post-pandemic, Bevy wants to be the gold standard, enabling enterprise brands to not only host large virtual conferences and events, but also scale their global community events through a network of smaller, high-frequency, globally connected events. As a result, the company is poised to win the enterprise portion of the market and is on track to reach $30 million in annual revenue run rate by the end of 2021, a 15 times increase from 2019. Bevy powers enterprise event communities for brands such as Google, Snowflake, Facebook, HubSpot, Adobe, Salesforce, Slack, Twitch, Atlassian, Zendesk, Twilio, and others. In 2020, Bevy served as strategic advisor to 125 new and existing enterprise brands as they shifted from in-person communities to virtual and hybrid events. The collaborative approach enabled 30,000 corporate event planners and enterprise community leaders across Bevy’s ecosystem to host more than 100,000 events across 120 countries, empowering them to hit their key business growth metrics. The new funding will be used to grow Bevy’s team from 100 employees to 250 in 2021. Bevy’s completely remote workforce is located in 10 different countries on four continents. Lowry, who was the first Black employee at McKinsey in 1968, said in an interview that adding the Black investors was also a stroke of genius because now Andersen will be able to tap into the networks of these investors to stir up more minority candidates. That group wants the company to succeed and so they will only recommend the best candidates, Lowry said. “Derek has the strongest, most well-connected Black people in America to help him recruit and build his organization,” Lowry said. “I don’t think anybody’s ever taken advantage of that. I think it’s a tremendous thing.”"
https://venturebeat.com/2021/03/24/fourkites-raises-100m-for-supply-chain-visibility/,FourKites raises $100M for supply chain visibility,"Chicago, Illinois-based supply chain visibility platform FourKites today said that it raised $100 million in series D financing, bringing its total raised to over $200.5 million. The company says that the capital will be toward product development as FourKites expands its global reach. Supply chain challenges are myriad in the pandemic world. For retail, it’s estimated that inventory is accurate just 63% of the time, on average. But stakeholders with superior visibility into their supply lines consistently outperform the competition. Seventy-nine percent of companies with high-performing supply chains achieve revenue growth greater than the mean within their industries, according to Logistics Bureau. FourKites, whose customers include Coca-Cola, AB InBev, and Walmart, claims to track over 1 million shipments daily and 1 billion events monthly reaching 176 countries across road, rail, ocean, air, and parcel. More than 500 of the world’s largest brands including 9 of the top 10 consumer packaged goods companies use FourKites in their logistics and transportation workflows, according to the company, as well as a network of over 450,000 couriers. CEO Matt Elenjickal says that the idea for FourKites evolved from his time as a supply chain consultant, where he worked with Fortune 50 enterprises that often struggled with the need for basic supply chain visibility. Elenjickal founded the company in 2014 to address what he saw as a key gap in the logistics space: the lack of real-time visibility. In doing so, he made FourKites one of the first companies to deploy GPS-enabled electronic logging devices, ostensibly allowing shippers to improve on-time delivery and optimize their supply chains based on data and predictive intelligence. FourKites provides a real-time view of supply chain and carrier performance for shipments, with tools that enable companies to manage exceptions before they occur and deliver notifications to recipients. On the procurement and planning side, clients get insights for store operations, including product availability and labor planning, in addition to tracking for in-transit and in-year freight.  “FourKites primarily uses data science in service of improving supply chain performance. Predicting estimated time to arrival is the bread and butter of this industry because that drives a lot of decision-making,” Elenjickal told VentureBeat via email. “We monitor over 230 ports and use more than 150 data points associated with a single load — chiefly shipper, carrier, lane, terminal, route, journey, rest patterns, load, traffic, and point-in-time weather — to drive our models. Models are trained on roughly 2 million shipments spanning trucks, trains, air, rail and water shipments across the countries being tracked on our platform at any given time. They’re tuned to be sensitive to shipper, warehouse, lane and carrier behavior.” Elenjickal says that FourKites has also tracked over 3 million less-than-truckload shipments for customers around the world, where the freight is relatively small (usually less than 150 pounds). Leveraging this dataset and its machine learning capabilities, the company’s team analyzed over 3 million loads, 3.2 billion less-than-truckload miles, and more than 1.3 trillion data points to generate what FourKites claims is the industry’s first dynamic estimated time to arrival model for less-than-truckload shipments, applicable regardless of geography. In September 2019, FourKites, which recently grew beyond 500 employees, launched FourKites Community, a forum that gives customers the ability to connect with other shippers and third-party logistics providers in the FourKites network. It’s a part of FourKites’ plan to hit $100 million in revenue within the next two years. However, FourKites faces formidable challenges not only from competitors including Shippeo, Descartes, and Transporeon, but from headwinds arising from the pandemic. Early in the COVID-19 crisis, Amazon and other ecommerce retailers were forced to restrict the amount of inventory suppliers could send to its warehouses. Ecommerce order volume increased by 50% compared with 2019, and shipment times for products like furniture more than doubled in March 2020. Overall, U.S. digital sales have jumped by 30%, expediting the online shopping transition by as much as two years. To address this, in May 2020, FourKites announced new collaborative features designed to reduce human contact and decrease the risk of virus transmission to frontline workers. Among these were paperless document processing, customer notifications, enhanced instant messaging, and capabilities for retailers that enable them to track last-mile shipments from local warehouses to outlets. Just two weeks after the COVID outbreak in the U.S., FourKites introduced a publicly available live network congestion map that spotlights cross-border freight movements across North America, Mexico, and Europe; port delays; and interstate transit metrics, as well as free services to better support shippers, carriers, and drivers with data and transparency.  Elenjickal notes that New York City’s Economic Development Corporation (NYCEDC) used FourKites platform data to monitor the food supply chain across NYC at the beginning of the pandemic. By tracking stops that shippers were making to ZIP codes within the city, the NYCEDC was able to better understand where there may be geographic disparities or product-level trends over time so that it could more effectively keep food flowing throughout NYC. In addition, the City of New York was able to closely monitor interstate trucking and coordinate with regional and national trucking associations, port authorities, and the U.S. Federal Emergency Management Agency to ensure that truck drivers continued to be able to access the city and bring in product. NYC created temporary truck rest areas for drivers and worked with FourKites to push notifications and updates regarding new rest areas to all drivers within 50 miles of New York City via CarrierLink, FourKites’ mobile app for drivers. “The COVID-19 pandemic and ensuing economic shutdowns did not have the effect on freight that many thought they would. In fact, the initial disruptions caused by the shutdowns drove more interest in freight visibility solutions,” Elenjickal said. “Communication became absolutely critical during COVID, and that is a trend that’s here to stay. Many of us have dreamed of this happening. But this is what the next 5 to 10 years of supply chain is going to look like, and we’re excited to be creating the technology that can facilitate it.” Going forward, Elenjickal says the plan is to grow FourKites’ presence and customer success teams, particularly in Europe, Asia Pacific, and India. “If you look at the strategic investors in this growth financing, we are now evolving beyond transportation visibility into true end-to-end supply chain visibility so that you know exactly what is happening in your supply chain, anywhere and everywhere, at any given point in time,” he said. “We will be connecting the physical and digital worlds of warehouses, yards, stores and transportation with real-time data and machine learning.” Thomas H. Lee Partners, Zebra Technologies, Volvo Group Venture Capital AB, and Qualcomm Ventures participated in FourKites’ latest funding round. This brings the total capital raised to over $200 million."
https://venturebeat.com/2021/03/24/amazon-appoints-tableau-ceo-adam-selipsky-to-head-up-aws/,Amazon appoints Tableau CEO Adam Selipsky to head up AWS,"(Reuters) — Amazon on Tuesday appointed Salesforce executive Adam Selipsky to lead its high-margin cloud computing unit, Amazon Web Services. The move comes as AWS’ current lead Andy Jassy is vacating the role to become the CEO of Amazon, after Jeff Bezos announced his exit in February. AWS, a key part of Amazon’s growth strategy, has raked in record profits for the world’s largest online retailer and counts scores of startups, big corporations, and many government agencies among its clients. Seattle-based Amazon said Selipsky, who was one of the first VPs hired at AWS in 2005 and ran the cloud computing division’s sales, marketing, and support for 11 years, will return to AWS on May 17. Selipsky became the CEO of Salesforce’s Tableau Software unit in 2016, and under his leadership the value of the division quadrupled in just a few years, Amazon said."
https://venturebeat.com/2021/03/23/intel-will-invest-in-factories-and-manufacture-chips-for-other-companies/,Intel will invest in factories and manufacture chips for other companies,"Intel CEO Pat Gelsinger said in his first major announcement that the company will invest $20 billion in two new factories in Arizona and will start Intel Foundry Services to manufacture chips for external chip designers. Intel will remain a major manufacturer of its own silicon chips, as well as using factories owned by other companies as needed, and now it will also make chips for others. Intel is moving to a plan dubbed “IDM 2.0,” (for integrated device manufacturer), where it will make use of third-party foundry capacity across its portfolio of chip designs, according to need. In other words, Intel is adopting a flexible strategy of making or buying chips or selling its manufacturing capacity as required. “It’s a strategic use of our foundries,” Gelsinger said. Gelsinger was appointed CEO earlier this year as a replacement for Bob Swan, who oversaw Intel during a time when it lost manufacturing leadership to rival foundries like TSMC and chip design leadership to rival Advanced Micro Devices. Gelsinger said the new moves will help put Intel on a path back to manufacturing leadership. In after-hours trading, Intel’s stock is up 6% to $67.45 a share, with a market capitalization of $257.9 billion. Intel will work with Globalfoundries, TSMC, UMC, and Samsung and will also become a foundry itself. By 2025, this could be a $100 billion opportunity, Gelsinger said. Intel will grow its global operations, including in the U.S., to meet the demand for commercial customers, as well as government and security needs in the country. Gelsinger said that under this plan, it won’t be unusual for Intel to ask Qualcomm if it can make its chips, or have the same conversation with Apple. Intel Foundry Services will be a new business unit, headed by Randhir Thakur, who will report to Gelsinger. The two new factories to service the foundry business will be built in Arizona, creating an estimated 3,000 tech jobs, 3,000 construction jobs, and 15,000 other jobs. Intel will put as much as $20 billion into the two factories. The foundry will primarily serve customers in the U.S. and European markets. In response to a question from VentureBeat, Gelsinger said Intel isn’t yet saying which manufacturing node the Arizona plants will start with. From 2019 to 2021, Intel is spending $33.5 billion on capital expenditures and $27 billion in research and development, resulting in the creation of 25,000 jobs. Intel is competing now to become a domestic foundry for the U.S. government and has a new research collaboration with IBM. “We are setting a course for a new era of innovation and product leadership at Intel,” Gelsinger said in a livestream event. “Intel is the only company with the depth and breadth of software, silicon and platforms, packaging, and process with at-scale manufacturing customers can depend on for their next-generation innovations. IDM 2.0 is an elegant strategy that only Intel can deliver — and it’s a winning formula. We will use it to design the best products and manufacture them in the best way possible for every category we compete in.” Gelsinger said the company’s 7nm development (where the distance between circuits is seven billionths of a meter) is progressing well, driven by increased use of extreme ultraviolet lithography (EUV) in a rearchitected, simplified process flow. Intel expects to start making its first 7nm client central processing unit (CPU, code-named Meteor Lake) in the second quarter of this year. Gelsinger said the new moves will provide the increased flexibility and scale needed to optimize Intel’s roadmaps for cost, performance, schedule, and supply. Intel will try to move to a yearly cadence of manufacturing updates to be more competitive, Gelsinger said. On top of that, Intel will benefit from a longer life for its factories, which tend to become obsolete quickly. But with a foundry business, Intel can generate revenues by having foundry customers buy chips from Intel’s older factories, Gelsinger said. Intel will also bring back the Intel Developer Forum as a live event in San Francisco in October. The company will make chips based on a variety of architectures, including x86, Arm, and RISC-V designs. For the first quarter, Gelsinger said Intel expects to exceed its earnings and revenue targets. For the full year, Intel expects to earn $4.55 a share in net income on revenues of $75 billion for the year. Intel has tried to launch a foundry business before, but Gelsinger believes conditions are better now. “There are strong winds for expanding and accelerating the role of semiconductors,” Gelsinger said. Patrick Moorhead, an analyst at Moor Insights & Strategy, said in a message, “Overall, Intel CEO Pat Gelsinger’s disclosure gave me many reasons to believe Intel is ‘back’ if the company can execute its plans. First off, I appreciated the honest self-deprecation related to previously missing 10nm schedules and having to push out 7nm recently. I have become more confident with Intel’s 7nm plans, given its increased use of EUV, which simplifies manufacturing simplicity and efficiency when it is released. I especially like the use of Samsung and TSMC as what I consider gap fillers for some of the highest-performance CPU tiles, an area of intense competitive pressure, until the company gets its process house more density and power competitive.”"
https://venturebeat.com/2021/03/23/major-flaws-found-in-machine-learning-for-covid-19-diagnosis/,Major flaws found in machine learning for COVID-19 diagnosis,"A coalition of AI researchers and health care professionals in fields like infectious disease, radiology, and ontology have found several common but serious shortcomings with machine learning made for COVID-19 diagnosis or prognosis. After the start of the global pandemic, startups like DarwinAI, major companies like Nvidia, and groups like the American College of Radiology launched initiatives to detect COVID-19 from CT scans, X-rays, or other forms of medical imaging. The promise of such technology is that it could help health care professionals distinguish between pneumonia and COVID-19 or provide more options for patient diagnosis. Some models have even been developed to predict if a person will die or need a ventilator based on a CT scan. However, researchers say major changes are needed before this form of machine learning can be used in a clinical setting. Researchers assessed more than 2,200 papers and, through a process of removing duplicates and irrelevant titles, narrowed results down to 320 papers that underwent a full text review for quality. Finally, 62 papers were deemed fit to be part of what authors refer to as a systematic review of published research and preprints shared on open research paper repositories like arXiv, bioRxiv, and medRxiv. Of those 62 papers included in the analysis, roughly half made no attempt to perform external validation of training data, did not assess model sensitivity or robustness, and did not report the demographics of people represented in training data. “Frankenstein” datasets, the kind made with duplicate images obtained from other datasets, were also found to be a common problem, and only one in five COVID-19 diagnosis or prognosis models shared their code so others can reproduce results claimed in literature. “In their current reported form, none of the machine learning models included in this review are likely candidates for clinical translation for the diagnosis/prognosis of COVID-19,” the paper reads. “Despite the huge efforts of researchers to develop machine learning models for COVID-19 diagnosis and prognosis, we found methodological flaws and many biases throughout the literature, leading to highly optimistic reported performance.” The research was published last week as part of the March issue of Nature Machine Intelligence by researchers from the University of Cambridge and University of Manchester. Other common issues they found with machine learning models developed using medical imaging data was virtually no assessment for bias and generally being trained without enough images. Nearly every paper reviewed was found to be at high or uncertain risk of bias; only six were considered at low risk of bias. Publicly available datasets also commonly suffered from lower quality image formats and weren’t large enough to train reliable AI models. Researchers used the checklist for artificial intelligence in medical imaging (CLAIM) and radiomics quality score (RQS) to help assess the datasets and models. “The urgency of the pandemic led to many studies using datasets that contain obvious biases or are not representative of the target population, for example, pediatric patients. Before evaluating a model, it is crucial that authors report the demographic statistics for their datasets, including age and sex distributions,” the paper reads. “Higher-quality datasets, manuscripts with sufficient documentation to be reproducible and external validation are required to increase the likelihood of models being taken forward and integrated into future clinical trials to establish independent technical and clinical validation as well as cost-effectiveness.” Other recommendations suggested by the group of AI researchers and health care professionals include ensuring reproducibility of model performance results spelled out in research papers and considering how datasets are assembled and put together. In other news at the intersection of COVID-19 and machine learning, earlier this week the Food and Drug Administration (FDA) approved emergency use authorization of a machine learning-based screening device which the agency says is the first approved in the U.S."
https://venturebeat.com/2021/03/23/warchief-unveils-chris-metzens-fantasy-boardgame-world-auroboros/,Warchief unveils Chris Metzen’s fantasy board game world Auroboros,"Chris Metzen, the former Blizzard game designer that helped bring us StarCraft and Warcraft, has announced the first game for his tabletop gaming firm, Warchief Gaming. The fantasy title is Auroboros: Coils of the Serpent, and it’s based on Dungeons & Dragons campaigns that Metzen created as a teenager. Auroboros is a brand new role-playing game (RPG) series ksing the D&D 5th Edition rules, and the first release is dubbed Worldbook: Lawbrand. This is a sourcebook based on the campaign that Metzen ran in his childhood with friends in the 1980s and 1990s before he started working at Blizzard. The company is launching a Kickstarter campaign starting on April 20. Metzen started the Irvine, California-based Warchief Gaming last year with former Blizzard executive Mike Gilmartin. They’re working on the game with former Hearthstone designer Ryan Collins. Auroboros refers to a serpent consuming its own tail. Lawbrand is a confederation of trade cities, linked by commerce and a common religious order. The story focuses on the power of the Auroboros, and how it often leads to both enlightenment and madness and death. As your power grows, you can warp reality, but it has a high cost of madness and death. When you use the magic power, the serpent consumes you. “It’s our first big franchise, and it’s a fantasy setting that I developed with my buddies when we were kids playing a D&D,” said Metzen in an interview with GamesBeat. In order to bring Lawbrand into a modern ruleset (D&D in the ’80s and ’90s used the 1st and 2nd Edition rules), Metzen sought help from Collins, who’s the lead product developer for Warchief Gaming. Other contributors to the campaign are contractors. “We started it probably in the mid to late-’80s, and then it ran through the mid-’90s,” Metzen said. “We built this big world here to facilitate our D&D adventures. It is a multi-generational story. We played three distinct groups of characters in three distinct eras, all separated by hundreds of years.” Metzen was the primary visionary at Blizzard Entertainment on games such as Warcraft, Diablo, StarCraft, and Overwatch. But toiling on games took a toll, and after 22 years, he left it all behind in 2016. Four years later, he dove back into tabletop gaming with a couple of friends to make Auroboros. He thinks that others will enjoy the world. “Despite having my friends in the storyline, this is built such that any dungeon master can run an adventure party through whatever story they want to tell, in the same setting,” Metzen said. For the past couple of years, Metzen has been enjoying playing tabletop games with friends in a club, talking with them for hours at a time. That helped him heal his mind from a kind of burnout, and it made him hungry to create worlds again. Since Metzen had so much success creating worlds at Blizzard that turned into huge franchises, Gilmartin seized on the opportunity and encouraged Metzen to pursue his new dream through the studio in Irvine, California. While tabletop games are a much smaller market opportunity than video games, they are hot now. And Gilmartin thinks that if the tabletop games are successful, they could be licensed to become other things over time, much like comic books can spawn great movies or games. Metzen thinks less about that and more about the fact that tabletop games were his first love. “I knew I wanted to go back to this world that I built when I was young,” Metzen said. “It was the last world that was working on before I got hired at Blizzard.” But Metzen dropped it while at Blizzard, as he realized his new job was going to demand all of his time and creative output. Metzen is in his late 40s now, and he started this when he was maybe 13 years old. Over time, his group lost a few players and gained some, and picked it up again toward the end of high school. The record was surprisingly complete, and Metzen found there were some holes in the backstory and some things like names for places that he didn’t write down. On Kickstarter, one of the high-value items will be a kind of novelization of the storyline. “For the most part, the names and places and factions, that we had them all kind of one to one,” Metzen said. Most of us probably wouldn’t want to revisit the games we played back when we were teens. But Metzen appreciated the authenticity of what he and his friends tried to create. He kept his records and a lot of his hand-drawn art. And he has rekindled some old friendships in the past few years. Metzen thinks of D&D as a foundational wellspring of creative ideas. But he also acknowledged that he had “no idea what he was doing” back in the early days. As for designing the game at a small company, Metzen said, “It’s different in every way that it could be, except for my favorite part, which is just jamming with passionate, nerdy people. That’s exactly the same, which is an incredible comfort. At Blizzard, given its scale and audience, there was always a little bit of fear, or tension, about whether they were going to love it. Are they going to come back? Are they going to keep playing? Ultimately, you just need to do the best job you can do. It feels awesome not to feel the weight of all of those things.” He added, “This is awesome. I want to please my buddies who I played with back in the day. I want them to be proud of this. And I want them to see that the final product is authentic to what we developed. And I want Mike and Ryan to be proud of it as brothers in this thing. I want them to feel like they had a stake in it. I just I just love the fact that this old idea that means so much to me, this thing I was doing before Blizzard, felt so comfortable to slip back into. This has easily been one of the most rewarding things I’ve ever done.”"
https://venturebeat.com/2021/03/23/cloud-spend-management-startup-kubecost-raises-5-5m/,Cloud spend management provider Kubecost raises $5.5M to help enterprises cut costs,"San Francisco, California-based Kubecost, a platform that provides visibility into cloud usage, has raised $5.5 million. In announcing the news today, Kubecost said it would put the funds toward expanding the availability of its cloud management product. According to RightScale, in 2017 26% of enterprises with more than 1,000 employees spent over $6 million a year in the public cloud. But it’s estimated that a fair amount of that enterprise cloud spend is going to waste. The same report found that the average waste in cloud costs was 35%, netting out to $10 billion each year across Amazon Web Services, Microsoft Azure, and Google Cloud Platform. Kubecost claims to help solve this problem by maximizing the efficiency of cloud spend. Originally launched as an open source project by ex-Google Cloud Platform engineers in 2019, the startup offers free and paid services to handle infrastructure deployment and provide recommendations for reducing spend. Kubecost breaks down the costs of infrastructure, which it also monitors, as well as optimization insights that suggest where a company can reduce spend — ostensibly without sacrificing performance. It’s deployed on in-house infrastructure and doesn’t egress any data, according to CEO Webb Brown.  “Kubecost was initially formed by me and our CTO, Ajay Tripathy,” Brown told VentureBeat via email. “He and I felt inspired to create the Kubecost open source project in early 2019 as we saw engineering teams migrating to Kubernetes struggling with [fundamental] challenges. This open source project gave developers visibility into their Kubernetes spend by application, teams, project, microservice, and more. Demand was so strong for a cost monitoring tool purpose-built for Kubernetes that we ended up founding the company.” Kubecost directly integrates with cloud billing APIs to provide a window into how resources are being spent across clusters. It shows the cost of resources consumed by individual apps and teams and allows users to assign out of assets like databases and storage buckets to teams, products, services, and other native concepts. With Kubecost, managers can track capacity to avoid performance degradation and app outages. And they can analyze current configurations and resource utilization compared with others to improve reliability. Of course, there’s plenty of competition in the cloud spend management market. In January, Quali, a company developing sandbox software for cloud and DevOps automation, raised $54 million. Then there’s cloud management software provider Ivalua, which recently nabbed $60 million at a $1 billion valuation. But Brown says teams from over a thousand companies — including EA, Broadcom, Adobe, Equifax, and Siemens — are already using the Kubecost platform and that revenue grew over 7 times last year. “Unchecked growth in expenses can be catastrophic in scenarios where budgets need to be closely watched, and certainly companies are paying careful attention to spend as a result of the pandemic. At the same time, we see companies accelerating their transition to the cloud and Kubernetes,” he added. “This is happening across all industries, from hospitality to retail to government. As more teams empower developers across the company to dynamically provision the resources they need, we see more teams becoming aware of the risks of not having cost monitoring and governance in place.” First Round Capital led the seed round in Kubecost, with participation from Afore Capital, along with Chris Aniszczyk, Andrew Lee, David Lieb, and other angels."
https://venturebeat.com/2021/03/23/waylay-announces-new-organizational-structure-to-enter-its-next-phase-of-growth-and-global-expansion/,Waylay Announces New Organizational Structure to Enter its Next Phase of Growth and Global Expansion," Three distinct business units reflect Waylay’s evolvement into a global organization and create a growth roadmap in enterprise automation  GHENT, Belgium–(BUSINESS WIRE)–March 23, 2021– Waylay, a leading enterprise automation software company at the forefront of digital transformation has today announced its new organizational structure, designed to facilitate the company’s strategic growth plans. Waylay will be organized around three business units: Waylay IO for the developer community and SMBs, Waylay Enterprise, and Waylay Digital Twin for Salesforce. Waylay’s compelling product innovation together with numerous recent wins in high growth markets are strong indicators for a steep growth trajectory. The company is restructured to better address its different target markets with high growth opportunities and each business unit is well-positioned to fulfill its customers’ needs. The new structure integrates Waylay’s core competencies and builds on its winning automation, orchestration, and analytics software which makes the digital transformation painless and allows a seamless deployment from idea to working use case in just hours. Every Waylay automation solution is low-code or no-code enabled which is intrinsic to everything Waylay does and intercepts the new megatrend of global digitalization and IoT, across all industries. Waylay’s objective to provide a single automation tool is viewed as visionary. Its unique and trusted software platform is now ready to address the requirements of its three business unit customers. Waylay IO brings an entirely new low-code business model to the developer community. It allows developers to build automation flows and use cases in no time. Its developer-friendly environment, backed by an engaged community creates the perfect setting to bring ideas to life, experiment with data, and create insightful new applications and business models quicker than ever before. www.waylay.io/io Waylay Enterprise provides a proven cloud-agnostic solution to OT-IT unification. Its Citizen developer approach endorses innovation velocity and puts the value of data in the hands of domain experts, data scientists, and corporate tiger teams to initiate new revenue streams and guarantee high ROI for IoT data, without going through lengthy software development cycles. Waylay commits to supporting enterprises by simplifying their complex digital transformation journey when OT-IT finally converge. www.waylay.io/enterprise Waylay Digital Twin is a no-code Salesforce solution that connects IoT to Salesforce and boosts the visibility of data, health and performance metrics for connected assets. Salesforce users can configure asset monitoring based on the business context, create new condition-based and predictive maintenance use cases, optimize remote operations and leverage asset data for consumption-based business models. Waylay Digital Twin is available on Salesforce AppExchange. www.waylay.io/digitaltwin & AppExchange https://appexchange.salesforce.com/appxListingDetail?listingId=a0N3u00000OMaXKEA1 “Our motivation to create the new business units was fueled by our growth strategy to address the different market segments that can benefit from our automation technology,” said Leonard Donnelly, CEO of Waylay. “The three product offers have the same trailblazing Waylay data automation and orchestration engine under the hood. Adding diversified business units to optimize product development, sales and support eliminates a monolithic market approach in favor of our company goals to serve our customers in the most effective way. We offer our technology across all industries; our three new distinct business models each provide a satisfactory low-code or no-code solution for IoT data owners that do not have the time, nor the resources to create new automation flows in-house, in a traditional software development model.” Learn more about Waylay’s new structure at www.waylay.io and select the product offer that fits your needs. ABOUT WAYLAY Waylay is a leading enterprise IT-OT digital unification software company delivering low-code based orchestration, automation and analytics software solutions. Waylay has a passion for supporting citizen developer communities and ensuring it puts all valuable data to work for developers, data scientists and domain experts. Most importantly, Waylay guarantees its customers a significant reduction in cost and time to market new digital transformation projects to eventually make their enterprises become one. Find out more at www.waylay.io  View source version on businesswire.com: https://www.businesswire.com/news/home/20210323005806/en/ PRESS CONTACT Elly SchietseCMO, WaylayElly.schietse@waylay.io Tel +32 479 761825"
https://venturebeat.com/2021/03/23/feedback-loop-which-helps-enterprises-garner-real-time-market-research-raises-14m/,"Feedback Loop, which helps enterprises garner real-time market research, raises $14M","Feedback Loop, an “agile research” platform that enables businesses to rapidly collect user feedback, today announced it has raised $14 million in funding as part of a push to “automate and democratize” consumer research. Brands across the spectrum traditionally carry out consumer research over relatively long periods of time, and often at irregular intervals. Feedback Loop takes a different approach to make market research more of an ongoing process, one that’s aligned with “real-time” demands and accounts for changing attitudes and opinions, thanks to continuous consumer input. Founded out of New York as Alpha in 2014, the company rebranded to Feedback Loop six months ago and serves to automate many of the stages typically involved in conducting research surveys. It can be used to garner feedback on behaviors and preferences or to A/B test new ideas and concepts before, during, and after the main product development phase. For example, a business can carry out comparisons between different marketing messages, slogans, or new product features and figure out how the end user interprets a particular phrase or responds to a proposed design. One of the core selling points behind the platform is that it’s designed to allow non-researchers within a company to gather feedback by introducing what Feedback Loop CEO Rob Holland calls “research guardrails.” “Our platform eliminates common human errors that have historically made traditional researchers shy away from letting business teams conduct their own research,” Holland said. “For instance, we eliminate the need for manual analysis of insights by surfacing the most relevant findings.” Any company that’s looking to develop completely new services or products needs to understand the market they’re going after before they get too far down the product development path, and that’s where Feedback Loop comes in. “Getting feedback retroactively is a failed approach, as evidenced by the recent Quibi saga,” Holland said. “Foresight [is better than] hindsight, basically.” Feedback Loop claims a number of notable clients across multiple sectors, including Uber, Farmers Insurance Group, LendingTree, and Humana. To tap into user feedback, Feedback Loop uses APIs to channel into audiences that have been segmented from third-party panel providers through marketplace exchanges such as Lucid and CINT. “Our users select the demographic and behavioral criteria of their desired audiences, and from there the platform automatically screens and selects the right people,” Holland explained. “This process traditionally requires a number of time-consuming and manual steps — our technology automates all of these so that clients can launch their studies immediately.” Participants are prompted to start the study, with Feedback Loop collecting responses (either quantitative or qualitative) and checking and organizing the data into reports — replete with insights that are good to go. “This step is critical, as it eliminates the human error that has traditionally marred business teams’ attempts to conduct their own ‘shadow’ research,” Holland added. For now, Feedback Loop’s main data integrations are with the panel marketplace exchanges, though it can also connect with popular enterprise tools such as Slack to let users receive alerts when study results are ready, for example, or if one of their colleagues has any questions about a study. But there are plans to introduce further integrations that reach deeper into the research ecosystem to make “generating and sharing insights even more frictionless,” according to Holland. Other players in the space include Suzy, another New York-based startup that recently raised $34 million. These raises suggest real-time consumer feedback tools are in high demand. According to Holland, Feedback Loop’s core differentiator is that its technology infuses “research philosophy into a platform that automates 30-plus things” while introducing a framework that enables non-researchers to use it. The company’s $14 million investment closes off its series A round at $24 million. Feedback Loop actually opened the round more than three years ago with a $10 million cash injection, which was followed by two hitherto undisclosed follow-ups of $7.5 million and $6.5 million. The round was led by Crosslink Capital, which has previously invested in major cloud companies such as Equinix and Coupa, and included participation from Spider Capital."
https://venturebeat.com/2021/03/23/there-are-some-seriously-awesome-jobs-available-right-now-heres-5/,There are some seriously awesome jobs available right now (here’s 5),"Are you on the lookout for a new and exciting role this spring? Well, we have some pretty good news to share in that case. We’ve teamed up with Jobbio to create a job board where some of the best companies in the world can actively seek to hire the best talent (hey, that’s you!) Here’s a taste of what to expect, with five brilliant jobs open for applications right now. Time Doctor is looking for a driven Demand Generation Manager who will be responsible for the execution of marketing programs that capture and nurture leads across multiple channels. In this role, you’ll be responsible for managing programs to generate demand via SEM, SEO, targeted social advertising, retargeting, email, and co-marketing efforts. You’ll work closely with the sales teams to execute and monitor campaigns, partner with ad agencies on advertising, and partner with other members of the marketing team to execute integrated campaigns. You’ll play a crucial role in helping Time Doctor meet ambitious revenue goals in 2021 and beyond. For this role, you must be a goal-oriented, analytical thinker that is adaptable and capable of collaborating across multiple departments. You should be familiar with demand generation techniques and analysis and have a proven track record of success in a SaaS company. Do you believe entrepreneurs drive industry change? Are you interested in data-driven marketing? Are you excited about helping one of America’s top premium streaming video services expand globally? If so, there’s a unique opportunity available to work as a Growth Marketing Manager in-house at STARZ, where you will lead the team responsible for growing their international subscriber base. As a Growth Marketing Manager, you will help drive the strategic direction of their program of acquiring high-value customers for the STARZ OTT streaming service with high visibility to senior stakeholders. You will work cross functionally to ensure that acquisition efforts are operating at high efficiency and reaching performance goals. The ideal candidate is an analytical and strategic thinker with high attention to detail, and also has an ability to think creatively. Palo Alto Networks is seeking innovators — software engineers who want to design state-of-the-art products that do not exist today. These engineers love to code with a drive to build global products and bring new ideas to develop security disciplines to solve real-world problems. They’re looking for leaders who take ownership of their areas of focus and who are driven to pursue problems at every level. Collaboration is at the heart of this culture and they need engineers who can communicate at a high level and work well with multi-functional teams towards achieving a common goal. As a key member of their Consumer Group, you will be responsible for designing and developing customer-facing applications and backend services to support them. You will apply your knowledge to functional design, and utilize your programming skills for efficient and robust implementation throughout the entire software development cycle. Zwift is looking for a Program Manager for their Global Programs team that focuses on managing the end-to-end quality of Zwift as a whole. In this role, you will be responsible for activating, managing, coordinating, and driving strategy, implementation, and maintenance of Zwift’s end-to-end quality across hardware, software, business functions, and the game itself. Reporting to the Director of Global Programs, you will be a key leader for Zwift’s future success by ensuring that the customer experience and business operations meet the highest standards of quality. The successful candidate will work closely with quality professionals in hardware delivery, software delivery, and business processes to ensure that Zwift is delivering a high-quality customer experience and high-quality business operations. They will also create quality strategy and tactics for cross-functional and cross-workstream projects and programs. FanDuel is looking for a Marketing Director to drive their customer retention and marketing initiatives through automation and technology advancement. In this role, you will oversee the marketing technology stack and campaign automation projects across all customer marketing programs at the FanDuel Group. You will have expert knowledge of CRM systems and technologies, and foster a culture of Martech innovation. The ideal candidate will be an evangelist for best practices throughout the company to produce delightful customer experiences and drive business KPIs. You will use your background in CRM to change the way FanDuel thinks about marketing technology and automation, and enable different brand verticals to do more with new and existing tools. This is an exciting opportunity to drive marketing innovation at a premier gaming company."
https://venturebeat.com/2021/03/23/machine-translation-startup-language-i-o-raises-5m/,Machine translation startup Language I/O raises $5M,"Language I/O, a startup providing AI technologies for real-time, company-specific language translations, today announced that it raised $5 million. The company says it plans to put the funding toward customer acquisition as it expands the size of its workforce. In the digital era, translating information into different languages can have an impact on businesses. For example, there’s a risk of losing 40% or more of the total addressable market if online stores aren’t localized. In countries like Sweden, over 80% of online shoppers prefer to make a purchase in their own language. And around 75% of all online shoppers say that they’re more likely to purchase again if the after-sales care is in their language. Cheyenne, Wyoming-based Language I/O, which was founded in 2011, claims to perform more accurate, personalized translations via an engine that intelligently selects neural machine learning models for requests and adopts preferred translations for product names, misspellings, acronyms, industry jargon, and slang. Customers tell Language I/O which words they want in their dictionary, which enables the models to improve over time across more than 100 languages. “Our platform proactively detects new terms and phrases that require a translation [and] encrypts and pseudonymizes personal data,” CEO Heather Morgan Shoemaker told VentureBeat via email. “Language I/O uses natural language processing techniques to engineer unique features from the data, which power the machine learning models. We also use a special type of unsupervised neural network called a self-organizing map to automatically detect and flag anomalous content before a human even sees it. A second model uses this data to identify potential glossary terms and the external translation quality feedback allows it to adjust and improve over time.” Andrea Paragona, senior manager at Constant Contact, a Language I/O customer, has been using the platform to interact with multilingual clients. “Language I/O enables us to deliver our knowledge base content to our expanding international audience in their native language,” she said. “This is extremely meaningful to our customers, who can then focus on learning the tool without concern for translation.” Language I/O integrates with customer relationship management systems including Zendesk, Oracle, and Salesforce and offers an API that allows clients to access company-specific translations. These systems benefit from the aforementioned feedback provided by agents and the professional linguists that Language I/O works with to fine-tune its core technology.  While Language I/O’s platform is currently focused on translation in channels like email, articles, chat, and social messaging, Shoemaker says the company — whose competitors include Lilt — is poised to extend beyond basic support to “anywhere that businesses need conversational translation.” (Think Slack channels, gamer-to-gamer chats, virtual meeting tech, and learning management platforms.) It’s already testing new solutions with its roughly 60 customers including Shutterstock, PhotoBox, and Brave. “The pandemic caused our monthly recurring revenue to double in a matter of a couple of months during the pandemic as companies stopped traveling to staff up native-speaking agents globally,” Shoemaker continued. “Our technology offers a viable alternative and with advances in the quality of neural machine translation just in the past year, it’s even more attractive than it was just a year ago.” PBJ Capital, Gutbrain Ventures, and Omega Venture Partners led the series A raised today, with participation from individual investors Michael Wilens, Tom Axbey, and Eric Schnadig, along with early-stage investment firm Golden Seeds, which focuses on startups with female founders. Twenty-employee Language I/O claims to have been bootstrapped since 2015, with the exception of a $500,000 seed round in October 2020."
https://venturebeat.com/2021/03/23/database-consolidation-takes-flight-with-boeings-ramp-up-to-intel-optane-memory/,Database consolidation takes flight with Boeing’s ramp up to Intel Optane persistent memory,"This article is part of the Technology Insight series, made possible with funding from Intel. For the last two years, we’ve been writing about Intel Optane persistent memory, knowing that scores of evaluations and pilot projects are underway in enterprises around the world. But quotable case studies have been rare. That changed recently when Intel publicly offered examples of Optane’s real-world suitability and value. During a high-profile event, Boeing revealed its production data center deployment of Intel Optane persistent memory via Oracle’s Exadata X8M database servers. Not long after, VentureBeat snagged an exclusive meeting with Maruti Sharma, Boeing’s chief architect for digital common services and an associate technical fellow within the aviation giant. Sharma manages Boeing’s enterprise databases and other data management services. He’s the ideal person to take us inside this project and reveal the hands-on details of an enterprise-scale Optane deployment. Boeing manufactures on both U.S. coasts, which helps drive the company’s need for cloud-based data management. All told, Boeing runs over 21,000 databases, approximately 5,000 of which are Oracle. Given how tempting it is to deploy databases with dedicated server stacks, it’s easy to imagine sprawl and redundancy accumulating over time. So, as in many enterprises, reducing the data center server footprint is one of Sharma’s top objectives. Not surprisingly, given the huge number of databases, Boeing had an ocean of information it could barely sip. An example: “Every commercial flight is spitting out tons of data, most of which Boeing hadn’t even started processing,” Sharma explains. “We need a lot of infrastructure to process that data, so we can mine useful information to grow the business. So if a part is going bad, the plane can relay that information to the airport where the flight is going to land. Engineers and mechanics would be ready with that part. So time-to-service is lower. The mechanics spend less time maintaining that plane. And the airline wants the plane to be in the air as much as possible. Better data management means better value for our customers.” Whether addressing data from the air, factory, or supply chain, Boeing saw a growing need to handle database processing in real time. The company’s workloads often resembled online transaction processing (OLTP) datasets, similar to those used by financial institutions and decision support system (DSS) datasets, which are typically read-only.  Boeing often handled both types concurrently in mixed-workload scenarios, especially when combining with other datasets coming in from supply chain partners. One of Boeing’s database projects involved an operational data store (ODS) running Oracle real application clusters (RACs) on commodity systems. Including dev and test systems and the production environment, the ODS infrastructure consumed roughly 100 servers. Each cluster server ran OLTP and DSS workloads with 1TB of memory and more than 1PB of aggregate storage. As Sharma describes it, cluster performance was “not at par”, and the hardware had reached its end of life. Boeing sent out several RFPs and ultimately examined  three options: an HPE solution, commodity hardware based on the latest-generation Intel processors, and Oracle’s Exadata Database Machine X8M. One of the latter’s primary advantages, Sharma says, was its incorporation of 1.5TB of Optane persistent memory for each server. Optane’s DRAM-class latency and higher-than-DRAM capacity points made it an ideal fit for Oracle’s OLTP-type workloads, which benefit from very fast access to small chunks of data from cached storage. (In the X8M’s architecture, persistent memory occupies the first of three automatically managed storage tiers, followed by NVMe-attached NAND and finally hard disk storage.) Boeing’s commercial operations gathers all the company’s manufacturing and supply chain data. It’s then aggregated into a data store, where factory floor transactions are integrated with supply chain data. Boeing uses the results of those integrations to make informed decisions on what inventory is required for each specific plane. “We need to get data from storage faster and keep it closer to compute,” explains Sharma. “How much needed data can reside in memory? When data is closer to compute, how fast can I process it? With the introduction of persistent memory and 100 gigabit-per-second RDMA over converged Ethernet (RoCE) network fabric — which lets nodes request data directly from PMem rather than going through the entire stack —  we saw an opportunity to eliminate most of the latency.” To be clear, Optane “PMem” does not replace DRAM. Rather, it serves as high-capacity volatile system memory while bumping DRAM into a high-speed caching role. Or it can act as a super-fast, non-volatile (persistent) cache for storage. Boeing mainly employs the latter, via Oracle’s data accelerator functionality. However, Boeing has a second use in “persisting” (saving into non-volatile storage) Oracle redo logs —  a necessary step before a transaction can be committed. Redo logs normally get persisted to conventional SAN storage, which introduces substantial latency. That step previously accounted for a lot of Boeing’s lag, especially since the group’s redo lot sizes average around 24GB. Trying to persist that amount of data frequently adds considerable process delays. Thanks to Intel’s App Direct Mode for Optane persistent memory, and Oracle’s existing support for App Direct in its platform, Boeing could address both volatile and persistent models simultaneously, says Sharma. After months of extensive testing, Boeing deployed its new Exadata servers into production in June, 2020. Teams consolidated nearly 100 commodity servers down to only two Exadata racks with eight servers each, and a pair of half racks divided across two data centers. “Overall, based on how much value we could get from any of the options, the Exadata with persistent memory stood out most,” says Sharma. “It was integrated with other Oracle internals, like Oracle Linux and GoldenGate, that we use heavily to bring in data from our OLTP environment.” According to Sharma, Boeing encountered no issues during deployment or even a need to adapt its software to accommodate persistent memory, as Oracle had already done this work with Intel. The only extra labor arose from Boeing’s policy against third-party racks into its data centers. As a result, Oracle had to re-rack its Exadata systems into Boeing’s own racks over the course of several days. In the months since initial deployment, Boeing reports 2x to 10x productivity gains by switching to the Exadata X8M platform. The biggest database operations improvements over the previous  commodity infrastructure came from bolstered redo log performance, Sharma says, adding that other examples abound. “When we run our batch processing, multiple jobs run overnight. Workloads that consistently used to take 14 hours now take about two hours. This really matters because of various work shifts coordinating across time zones. It becomes very challenging when a shift starts and needs results from another group that hasn’t finished. With jobs finishing faster, our shifts can make better decisions.” Despite its considerable server consolidation, Boeing says it still has ample capacity left in its new Exadata solution. This opens the door to taking on more workloads from other tasks or groups. Sharma expects container technology to play a role in further consolidation, allowing engineers to cleanly separate, say, manufacturing workloads from the supply chain, engineering, or analytics. Containers could also help with compliance in Boeing’s government operations, he adds. Beyond consolidation and data isolation, the company says it can now maintain and manage workloads with fewer resources. For example, rather than needing separate administrators  to manage different layers of the solution (storage networking, etc.), one consolidated team now can manage the entire stack. Sharma says this becomes doubly important because Boeing’s Oracle databases run on a mix of Linux, IBM AIX, HP-UX, and other operating systems. Having one standard platform reduces  spending on resources and infrastructure. Again, it’s about the efficiencies of consolidation. “There has always been a race between the different components of the infrastructure stack,” Sharma notes. “These advances in compute and persistent memory allow customers like us to process more data in a timely fashion. Data is exploding, and so is the demand for storing, retrieving, and processing the data set. These innovations will give us more leverage in consolidating workloads and doing more data analytics locally. Especially with container technology onboard, we can bring in petabytes of data for processing in one location and help drive the business.”"
https://venturebeat.com/2021/03/23/google-cloud-announces-network-connectivity-center-to-simplify-hybrid-cloud-management/,Google Cloud announces Network Connectivity Center to simplify hybrid cloud management,"While the cloud offers the potential for greater speed and agility, the transition poses new forms of complexity as enterprises juggle legacy technology and their expanding online presence. To ease that pain, Google Cloud today announced the Network Connectivity Center, which is designed to integrate management of on-premises IT, as well as cloud-based services. By centralizing management, Google hopes to encourage companies to embrace digital transformation. In a blog post describing the new connectivity center, Google Cloud Networking product manager Rohith Ramkumar explained that enterprises should be able to eliminate steps like configuring multiple networks to operate in sync that have been necessary to avoid hiccups across services. “Network Connectivity Center provides a single management experience to easily create, connect, and manage heterogeneous on-prem and cloud networks leveraging Google’s global infrastructure,” he wrote. “Network Connectivity Center serves as a vantage point to seamlessly connect VPNs, partner, and dedicated interconnects, as well as third-party routers and Software-Defined WANs, helping you optimize connectivity, reduce the operational burden, and lower costs — wherever your applications or users may be.”  Among the promised benefits, Google said the Network Connectivity Center will include a single connectivity model, flexible cloud connectivity, VPN-based multi-cloud connectivity, and real-time visibility into a company’s global network. In addition, the center will make it easier to connect to Cisco SD-WAN. Cisco and Google announced a partnership in April 2020 to connect Cisco SD-WAN Cloud Hub with Google Cloud to enable more secure and faster on-demand connectivity. The companies said they are expanding that partnership via the Network Connectivity Center to include on-prem datacenters. “Users can now enjoy a seamless experience across major cloud providers when connecting their on-premises environments to the cloud via Cisco SD-WAN,” wrote Cisco Enterprise Routing vice president JL Valente in a separate blog post. “Through a unified UI and consistent workflow, Cisco Cloud OnRamp automates the backend processes so that you, the enterprise IT team has full control, and can focus on business-impacting tasks.”"
https://venturebeat.com/2021/03/23/75f-an-ai-powered-hvac-management-startup-nabs-4-75m/,"75F, an AI-powered HVAC management startup, nabs $4.75M","75F, a Burnsville, Minnesota-based startup providing AI-powered building management technologies, today announced that it raised a $4.75 million extension to its $18 million series A, bringing its total raised to $29.2 million. The company plans to use the funds to further expand innovations in energy savings and market reach. Commercial buildings are the fourth-leading contributor to greenhouse gas emissions globally, at 20% of emissions around the world. Roughly half of their emissions is from heating and cooling, with HVAC and lighting accounting for 65%. Building inefficiency will remain an important emissions target — the world’s building footprint is expected to grow by 2.5 trillion square feet by 2060, which equates to building another New York City every month for the next 40 years. That’s why at least 31 U.S. metros have passed laws establishing power benchmarks or reporting mandates, with 15 requiring energy performance targets. 75F, which was founded in 2012, offers an AI-powered, internet of things-based building platform that includes wireless sensors, equipment controllers, and cloud-based mobile and web apps that deliver analytics and predictive, proactive building automation. The company’s system aims to reduce energy costs by taking advantage of outside air to provide free cooling, tracking factors like weather and room occupancy to learn the behavior of building occupants. According to 75F, its system supports both old and new buildings, with autonomous controls that work with a range of heating and cooling equipment like rooftop units, boilers, and single-stage equipment. When active, 75F says the platform can anticipate things like restaurant rush hour and start cooling the kitchen down, or dim the office lights when the sun is out. Moreover, it lets managers remotely adjust, monitor, or schedule temperature changes and add zones to existing HVAC equipment, which tenants and employees can control via their phones. “75F’s full-stack, simple, yet low-cost building automation solution is the next-generation of building automation systems. Today, most building automation solutions in existence are cost-effective for only the largest, most sophisticated buildings, which represent a very small percentage of all buildings globally,” Brian Walsh, head of Wind Ventures, the lead investor in 75F’s funding extension, said in a press release. “For emerging regions like Latin America, the 75F product-market fit represents a leapfrog opportunity for most of the market from small to very large, so we are excited to work with the team to bring the 75F solution into the region.” Utility-optimizing AI is a burgeoning business, with plenty of competition to go around. There’s BrainBox and Aquicore, whose algorithms make fine-grained adjustments to HVAC systems on the fly. Augury, a startup developing sensors that attach to machines and record data that’s then analyzed in the cloud, works with service companies to diagnose and optimize systems like industrial HVAC. GE Digital’s Predix and startup Petasense offer similar Wi-Fi-enabled, cloud- and AI-driven monitoring sensors. And Sidewalk Labs and Carbon Relay boast products that leverage sensor data to make predictions about datacenters’ cooling usage. But 117-employee 75F, which previously raised $2 million in a June 2018 seed round, uniquely claims its system can reduce energy costs by 50%. Lower installation costs, “digital twin” technology that leads to faster data ingestion, and sensors designed in-house result in a payback period of less than 2 years for most customers, according to 75F. “Since the pandemic began last year, occupancy in commercial buildings has been low or nonexistent. While the commercial buildings industry struggles with the pandemic in obvious ways, COVID-19 has highlighted how existing building technology can help make spaces healthier and save energy that would have otherwise been wasted,” a spokesperson told VentureBeat via email. “Because 75F’s control capabilities are so agile, we were able to develop and deploy a custom control sequence called Epidemic Mode that automates expert ventilation recommendations.” While 75F says that only 10% of revenue from its customers is recurring today, it hopes to fill 45 positions in the coming months across its offices in Minneapolis, Singapore, and India. The company says that some of the funding will be set aside for an OEM relationship not yet publicly disclosed."
https://venturebeat.com/2021/03/23/spatial-launches-immersive-audio-for-homes-offices-and-theme-parks/,"Spatial launches immersive audio for homes, offices, and theme parks","Spatial Inc. is unveiling a new kind of immersive audio experience today that will enable the company to create soundscapes a variety of uses: homes or offices or even theme parks. The startup has also raised an undisclosed amount of funding. The company is unveiling a real-time product platform as a service for building soundscapes. Spatial (which is registered in Delaware as SpatialX, not to be confused with the AR/VR company Spatial) is positioning itself to lead interactive soundscape design for creators. Spatial is targeting everyone who would like to have the audio experiences of home theaters wherever they go, even to outdoor venues, said Calin Pacurariu, cofounder and CEO of Spatial, in an interview with GamesBeat. In contrast to home theaters that are contained by walls, Spatial plans to create sound experiences that adapt to the audio environment. “We really looked at reinventing sound for immersion,” Pacurariu said. “If you look at kind of traditional audio, everything from stereo to surround sound, the Dolby Atmos, it’s all built around structured environments, structured room layouts, and listener position. It’s well understood. For immersive audio, it is a different set of challenges. It’s just how you trick people’s minds into believing that something is actually real, and how you make it real-time and interactive and flexible.” Scott Rupp, the founding general partner of investor Bitkraft, said in a statement that Spatial connects real-time virtual worlds of modern game engines to deeply immersive and interactive real-world locales. He said he sees big opportunity for Spatial as these worlds blend and scale from home gaming rigs to esports stadiums and full theme parks, where there could be hundreds of speakers. In 2017, Pacurariu and Michael Plitkins cofounded Spatial with the goal of bringing a modern approach to real-time interactive and deeply immersive experiences to the world. Their headquarters is in Emeryville, California, just across the street from Pixar Animation. Part of the mission is to look at the intersection of gaming and creativity, Pacurariu said. Prior to founding Spatial, Pacurariu founded and led teams at Fitview and Podtek, which broke new ground in software applications and platforms. He also spent years at Apple leading product management from inception for many of the products that the world loves today, including iPhone software, AppleTV, and more. Prior to Apple, he led the core teams at Handspring that developed the Treo product line, which became the first modern smartphones. Pacurariu has more than 45 issued and pending patents in intelligent audio, methods for providing audio and visual cues in spaces, camera-based tracking, among other novel inventions. Plitkins was a founding engineer at Nest, which was ultimately acquired by Google. He helped develop a set of groundbreaking smart consumer products for the home such as the Nest Thermostat and helped define the category we know as the home internet of things (IoT) today. Prior to Nest, he was a founding engineer at Tellme Networks, which was acquired by Microsoft. He also has experience in developing tools and technologies for 3D modeling, animation, VR, and graphics. Plitkins has over 35 patents in UI design, streaming audio, smart home automation, and more. Former Apple spokesperson Jennifer Bowcock joined Spatial to lead marketing and communications. She previously ran global communications for Dolby, where she launched Dolby Cinema, Dolby Vision, Dolby Dimension, Dolby Music, and led PR events for the Oscars, Sundance, and supporting premieres at The Dolby Theatre. She was also part of the original iPhone team at Apple and was instrumental in the PR launch and global expansion of iPhone, iPad and the world’s first App Store. Prior to Apple, she led communications for AT&T, helping launch up to 40 devices per year. Leading operations, Darrell Rodriguez joined the founding team after a storied career in entertainment and tech. He has been a chief operating officer for Electronic Arts, the president of LucasArts, and a leader of innovation and worldwide studios for IGT. In an interview, he said that his time as a former Disney Imagineer, where he worked on creative and immersive experiences for theme parks, helped prepare him for Spatial. Karen Carte, formerly of Pearl Automation and Lyve, joined in 2017 as head of finance. “I really fell in love with the idea of reinventing sound and to provide another brush in the palette of creators that they typically haven’t had,” Rodriguez said. “Audio is typically the last thing that is considered in the last thing that’s built in games or in movies, or in a lot of immersive experiences.” The investors include DBL Partners and Bitkraft Ventures, as well as strategic investor National Geographic Society. Other investors include  Valia, Incite, Monta Vista and Transcend. The Spatial Reality engine is built on an object-driven platform that renders realistic, believable soundscapes in real time, with natural physics and complex object behaviors that allow for all day dynamic experiences. Spatial experiences are completely interactive, encouraging guests to explore and experiment throughout the space. With comprehensive and immediate support for both inputs and outputs, Spatial experiences can react to anything in real time, from sensors and computer vision systems in the space to data feeds and live audio streams. The same holds true for outputs, with the action in a Spatial experience driving lighting, control systems or whatever you can imagine for a completely immersive engagement. Spatial said its sound scenes are flexible and scale and adapt to each location. From a small space with a handful of speakers to theme park scale with thousands, Spatial is designed with flexibility which allows for indoor and outdoor installations that are unrestricted by traditional channel-based audio. Among the things it can do is mask the sound of your neighbors. Spatial Reality runs on inexpensive high performance macOS and Linux computers and works with industry-standard audio hardware. The company has also created audio tools, dubbed Spatial Studio. The Spatial applications and services are crafted to combine the simplicity and ease-of-use necessary for widespread adoption that experiential designers demand. “It’s a set of applications and a service that just allows us to do this. And the core of it is what we call spatial reality,” Pacurariu said. “Our Spatial Studio allows you to build these environments in a fully 3D visualized world.” With the tool, creators can see the whole scene take place on the 3D canvas and fine tune object position, size, motion, and behaviors while listening to real-time preview. Scaling controls and adaptive physics for position, speed and distance give designers the confidence the scene will be incredible in any space. A few quick clicks publishes the scene to the Spatial service. Spatial Control runs on your iOS device, for real-time control of and interaction with all of your spaces. User-friendly scene and space controls are woven in with detailed user roles and permissions, providing the access and management expected of a business-critical service experience. Spatial Control also provides tools for easy setup, tuning and customization of your spaces, using the portability and power of the mobile device to get everything just right without ever leaving the space. The Spatial service runs in the cloud and communicates with the platform for administration and content updates. Spatial shipped to early customers last year enabling installations from large scale professional indoor and outdoor experiences to hobbyists immersive studios for less than $1,000. One of Spatial’s partners is Zeitgeist, which does work on theme parks. Another partner is Eight, which helped build Apple retail stores. Spatial has 26 employees. “We’ve already hinted that and we’ve had customers over the last year that we’ve already been working with,” Pacurariu said. “It cuts across different segments.”"
https://venturebeat.com/2021/03/23/appliedvr-secures-29-million-in-series-a-funding-to-make-virtual-reality-the-standard-of-care-for-chronic-pain/,AppliedVR Secures $29 Million in Series A Funding to Make Virtual Reality the Standard of Care for Chronic Pain," New funding enables AppliedVR to pursue FDA approval, prep for market launch and continue building body of clinical trial evidence  LOS ANGELES–(BUSINESS WIRE)–March 23, 2021– AppliedVR, a pioneer advancing the next generation of digital medicine, today announced $29 million in series A funding, bringing its total funds raised to date to $35 million. The round, which includes key investors F-Prime Capital, JAZZ Venture Partners, Sway Ventures, GSR Ventures, Magnetic Ventures and Cedars-Sinai, will fuel the company’s growth as it pursues full FDA approval over the next year. AppliedVR provides virtual reality-based treatments aimed at comprehensively treating chronic pain. Combining well-established cognitive behavioral therapies with mindfulness exercises, the company’s EaseVRx solution recently became the first virtual reality (VR) prescription therapeutic to receive Breakthrough Device Designation from the FDA for treatment-resistant fibromyalgia and chronic intractable lower back pain. The company also just released results from its pivotal eight-week randomized clinical trial, finding that the EaseVRx device produced “clinically meaningful” improvement in multiple pain outcomes, and had high participant satisfaction and engagement. “Chronic pain is one of the most common medical conditions in the world, yet it still is incredibly debilitating to patients, costly to the system and complex to treat. While our mission has always been to demonstrate that VR can be a powerful analgesic in any setting, the COVID-19 pandemic has created a surge in demand for digital medicines like VR that can be delivered safely to patients in their own homes,” said Matthew Stoudt, co-founder and CEO of AppliedVR. “As a leading evidence-backed VR therapeutics provider in healthcare, we’re committed to meaningfully improving the lives of people suffering from chronic pain by making VR the standard of care for treating chronic pain in a provider-prescribed, payer-reimbursed model.” Affecting approximately one-third of all Americans1, chronic pain is estimated2 to cost as much as $635 billion each year, making it more expensive than cancer, heart disease and diabetes combined. While most people, especially seniors, have relied on pharmacological interventions to treat their pain, digital therapeutics like VR have emerged as an effective, safe, and potentially cost-saving solution. Furthermore, the COVID-19 pandemic has demonstrated the need for more digital therapeutic treatments that can be delivered safely in patients’ homes. “We’re big believers in the potential for digital therapeutics to transform outcomes for patients with challenging conditions, and we have been closely evaluating the market for solutions for some time. AppliedVR stood out as a great choice for our first prescription digital therapeutics investment,” said Jon Lim, partner at F-Prime. “With its market potential, solid executive team, and commitment to providing evidence-based therapies, we’re confident that AppliedVR will become a leader in digital medicine.” AppliedVR’s technology is already trusted by more than 200 of the top healthcare provider organizations in the world, including Geisinger and Cleveland Clinic, who are advancing separate NIDA-funding clinical trials to study VR as an opioid-sparing tool for acute and chronic pain. The company also has partnered with University of California at San Francisco (UCSF) to study how digital therapeutic platforms, including virtual and augmented reality, can be used to improve care access for underserved populations. AppliedVR also is engaged with multiple payers, testing VR as a cost-effective solution for treating chronic pain. AppliedVR’s program has been used to help more than 60,000 patients to manage their expectations of pain. With today’s investment, the company plans to continue EaseVRx’s FDA approval pathway, conduct more payer pilots, develop its product pipeline, and build out its clinical, marketing and sales teams. AppliedVR also was just named as a 2021 Fast Company Most Innovative Company in the augmented and virtual reality category. About AppliedVR AppliedVR is a leader in digital therapeutics, pioneering virtual reality-based treatments that address the complexity of chronic pain. Our mission is to empower patients with the tools to live life, beyond chronic pain. Rooted in cognitive behavioral therapy and mindfulness, AppliedVR’s EaseVRx is the first VR-based prescription therapeutic to receive Breakthrough Device Designation by the FDA. Offering a comprehensive approach that encompasses the biological, psychological and social factors that influence how people experience chronic pain, EaseVRx enables patients to change the way they process pain and develop new, positive habits and coping skills that improve quality of life. Patients can easily self-administer EaseVRx3 in the comfort of their own homes, at any time, without restrictions tied to a healthcare professional’s schedule – advancing remote care as well as quality, equity and efficiency in chronic pain management. About F-Prime Capital F-Prime Capital is a global venture capital firm investing in healthcare and technology. For the past 50 years, our independent venture capital group has had the privilege of backing great entrepreneurs building groundbreaking companies. With over two billion dollars under management and a global portfolio of more than 200 companies, we champion those dedicated to creating positive change in the world. F-Prime is headquartered in Cambridge, MA, with offices in London, UK and San Francisco, CA. For more information, please visit fprimecapital.com ​and follow us on Twitter and LinkedIn. 1 Institute of Medicine. Relieving Pain in America: A Blueprint for Transforming Prevention, Care, Education, and Research. Washington, DC: The National Academies Press; 2011.2 Darrell J. Gaskin, Patrick Richard. The Economic Costs of Pain in the United States. The Journal of Pain, 2012; 13 (8): 715 DOI: 10.1016/j.jpain.2012.03.0093 EaseVRx is an investigational device, limited by federal law for investigational use. AppliedVR is seeking market authorization from the FDA.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210323005584/en/ Sam Moorepress@appliedvr.io (225) 931-4081"
https://venturebeat.com/2021/03/23/orca-security-scores-210m-to-reduce-cloud-security-complexity-for-enterprises/,Orca Security picks up $210M to simplify enterprise security with cloud-native tools,"As companies expand their cloud footprint, they have been forced to install and maintain complex workarounds to secure their networks and data. While these band-aids can offer some protection, the cost of managing and deploying such fixes can be a drain on resources. Orca Security has developed a cloud-native solution that integrates these security features to provide many of the same benefits of tools like agents and sidecars without the complexity that comes with managing them. Solving this problem has propelled the young startup to extraordinary growth: It reports annual revenues rose 1000% in 2020. Today, Orca announced a $210 million venture capital round led by CapitalG, Alphabet’s independent growth fund, and Redpoint Ventures. The funding comes just four months after Orca, which was only founded two years ago, raised its previous round of funding and brings its total raised to about $300 million at a $1.2 billion valuation. Orca CEO and cofounder Avi Shua said the ability to simplify cloud-native security and visibility into security threats has been a powerful combination for enterprises struggling to manage all the facets of their cloud journey. “What we brought to the market, using a unique technology is an ability to literally connect touchlessly with the environment and have this holistic view of the environment in minutes,” he said. Orca’s SideScanning technology is at the heart of the platform and automatically reads a customer’s cloud configuration to detect everything from vulnerabilities to malware to misconfiguration and authentication risk. Because it eliminates the need to install numerous security agents, it’s able to be launched and perform the first scans almost instantly. The platform also helps with compliance across all major cloud providers, including Google Cloud, Amazon Web Services (AWS), and Microsoft Azure. This fluid system makes it easier for developers and security teams to collaborate. In the past, the need to stop and check with each team has slowed the application development process and led to tension between the desire for speed and the need for security. “We’re helping to reduce and eliminate the friction by providing this visibility into what’s going on, which allows the discussion between these two teams to be focused on the problems that need to be fixed,” he said. The company’s solution caught the eye of Google, which led CapitalG to seek Orca out for a possible investment, according to CapitalG general partner Gene Frantz. The VC firm, which has extensive cybersecurity investments, was impressed by Orca’s potential to help companies rapidly scale applications deployment without sacrificing security. “Developers kind of care about security,” Frantz said. “But what they really care about is getting their app live. And so relying on developers to place agents and to worry about security and the products that they’re developing isn’t realistic. You’re going to end up with huge amounts of your real estate out in the cloud that are basically uncovered. And not only are you not monitoring them, but you also don’t even know that they’re there.” One of the biggest challenges Orca faces is managing its own ferocious growth. This year, the company is on track to triple its R&D and sales teams. And it’s also scaling its own infrastructure to keep up with demand. “Our vision for the company is to become the world’s best cloud security platform in terms of seamless coverage and comprehensiveness, the one platform that you go to understand your cloud security posture,” Shua said. “We are going from a small to medium-sized company that supported dozens of customers to a company that supports thousands of customers.”"
https://venturebeat.com/2021/03/23/big-data-analytics-firm-dataminr-raises-475m-to-fund-platform-expansion/,Big data analytics firm Dataminr raises $475M to fund platform expansion,"Dataminr, a New York-based company specializing in AI that provides real-time information to customers, today announced it has raised $475 million at a post-money valuation of $4.1 billion. The company says the capital will be used to accelerate the growth of its corporate business line, which spans physical safety and security, reputation risk and crisis management, business intelligence, and cyber threat detection. Dataminr also plans to make investments in internationalization, expanding its private and public sector sales footprint across the Europe and Asia Pacific regions. Data analytics is the science of analyzing raw data to extract meaningful insights. Market Research Future predicts that the global data analytics market will be valued at over $132 billion by 2026. A range of organizations can use data to boost their marketing strategies, increase their bottom line, personalize their content, and better understand their customers. Businesses that use big data increase their profits by an average of 8%, according to a survey conducted by BARC research. Dataminr, which was founded in 2009 by Yale graduates Ted Bailey, Sam Hendel, and Jeff Kinsey, offers an information discovery platform that detects patterns of emerging events and information from public data signals. While in college, Bailey studied the impact of real-time data on society and was specifically interested in how technologies change the way humans become aware of world events. In 2008, Bailey became fascinated by the new wave of platforms like YouTube and Twitter and realized these tools gave people the ability to capture events as they’re happening. Bailey also realized these platforms might be able to fill in information gaps during events if scaled. Today Dataminr claims to perform trillions of daily computations across billions of public data inputs in more than 150 languages, drawing on text, images, videos, logs from sensors, and multimodal combinations of these formats from over 100,000 sources. These sources span blogs, global and regional social media sites, web forums, local digital media, radio and audio transmissions, the deep and dark web, cyber signals, and internet of things devices.  Dataminr develops products targeting businesses, the public sector, and newsrooms, all of which use a combination of AI to parse the public data it regularly analyzes. Among the techniques the company employs are natural language processing, computer vision, audio processing and classification, and anomaly detection, all of which help surface “high-impact” events and emerging risks in real time. For example, one of the world’s largest airlines uses the platform to detect events that require adjustments in flight schedules, Dataminr says. “Dataminr has invested highly in deep learning in the last few years, which has enabled the company to pioneer new AI signal detection fields, like multimodal fusion AI, which synthesizes real-time inputs in different data formats into multi-variable event detection models,” a spokesperson told VentureBeat via email. “Dataminr can take advantage of its now over 11-year proprietary data archive, which holds the patterns of how all events were recorded in digital data and serves as the essential foundation upon which Dataminr can continue to train and update its AI models. On average, Dataminr signals on breaking events are delivered to our clients nearly four hours ahead of a wire service like the Associated Press.” Dataminr first came into the public eye in 2011, when it issued an alert that Osama bin Laden had been killed 23 minutes before major news organizations did. In 2019, Dataminr claimed to have detected the first signs of the COVID-19 outbreak in Wuhan on local Chinese social media platforms like Weibo and went on to identify clusters indicating future spikes in 14 U.S. states. But Dataminr has often flirted with controversy. In 2020, the Intercept released a report showing that police departments used the company’s services for surveillance during the George Floyd protests, including accessing social media posts about protest locations and actions. The piece noted that the monitoring seemed at odds with claims from Dataminr that the company would neither engage in nor facilitate surveillance. This followed a string of bad press in 2016, when Twitter cut off geospatial data access for police intelligence centers.  Dataminr’s public image problems haven’t impacted business, though, with the roughly 650-person company reporting a doubling in revenue three years in a row from its corporate enterprise business line. The company’s clients include CNN, USA Today, the United Nations, Airbus, Shell, and the New York City Office of Emergency Management, among others. “Large corporate clients are always discovering new use cases for our signals as they adopt Dataminr’s platform more broadly across their organization,” Bailey told VentureBeat via email. “As you can imagine, knowing about what is happening in the world faster than ever before possible, and at a scope unparalleled in human history, has a wide range of multi-dimensional use cases for corporate enterprises.” Existing investors Valor Equity Partners, Morgan Stanley Tactical Value Fund, MSD Capital, The Pritzker Organization, DNS Capital, Moore Capital Management, Eldridge, ReInvent Capital, and Arrow Mark Partners participated in Dataminr’s latest funding round. It brings the company’s total raised to date to over $1.05 billion, following a $391.6 million series E round in June 2018."
https://venturebeat.com/2021/03/23/service-now-acquires-rpa-startup-intellibot-to-bring-automation-to-legacy-systems/,ServiceNow acquires RPA startup Intellibot to bring automation to legacy systems,"Cloud computing firm ServiceNow today announced it has signed an agreement to acquire Intellibot, a robotic process automation company based in Hyderabad, India, for an undisclosed amount. ServiceNow, which expects to complete the purchase in Q2 2011, says it plans to fold Intellibot’s capabilities natively into its platform, enabling ServiceNow customers to bring AI and machine learning to legacy systems. RPA — technology that automates monotonous, repetitive chores traditionally performed by human workers — is big business. Forrester estimates that RPA and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce found that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than four hours savings per employee each week. Intellibot, which was cofounded in 2015 by Alekh Barli, Srikanth Vemulapalli, and Kushang Moorthy, offers RPA solutions designed specifically for enterprise corporations. Its design studio helps with the modeling and development of RPA projects, while its orchestrator assists with cloud compatibility, multitenancy, and compliance. Beyond this, Intellibot provides features including role-based access control, version control, business and IT reports, audit logging, and 256-bit encryption. “We are proud to join forces with ServiceNow as it continues to invest in powerful end-to-end automation capabilities to make the world of work, work better for people,” Vemulapalli said in a statement. “Our RPA combined with ServiceNow workflows will help businesses better connect disparate systems to accelerate innovation and thrive in a new world of work.” ServiceNow SVP Josh Kahn notes that the acquisition supports ServiceNow’s broader commitment to the Indian market. The company plans to bring two datacenters in India online by the first quarter of 2022, which will form part of ServiceNow’s high-availability architecture as the company doubles its staff in the country within the next three years. ServiceNow recently released Now Platform Quebec, which brought several AI-powered and low-code capabilities to the Now Platform. Kahn says Intellibot will complement these technologies, extending ServiceNow’s existing integrations, process mining, process automation, chatbot, and virtual agent capabilities — including a tie-in with RPA startup UiPath that launched last April. “ServiceNow is the platform of platforms for the workflow revolution, offering powerful end-to-end automation capabilities that allow customers to streamline business decisions and unlock new levels of productivity,” Kahn said in a press release. “Our customers represent nearly 80% of the Fortune 500, and the vast majority are trying to drive automation across a mix of legacy and modern applications. With Intellibot, we will extend ServiceNow’s ability to help customers connect systems so they can easily automate workflows and drive productivity.” ServiceNow’s buyout — its first this year — comes after Google said it would partner with Automation Anywhere to develop RPA capabilities and Microsoft and IBM snatched up Softomotive and WDG Automation, respectively. With a market opportunity anticipated to be worth $3.97 billion by 2025, according to Grand View Research, RPA is fast becoming too large to ignore. Automation Anywhere rival Blue Prism has raised over $120 million, Kryon $40 million, and FortressIQ $30 million. And in July, UiPath nabbed $750 million, bringing its total raised to $2 billion at a post-money valuation of $35 billion."
https://venturebeat.com/2021/03/23/onetrust-acquires-convercent-to-drive-ethics-and-compliance-in-the-enterprise/,OneTrust acquires Convercent to drive ethics and compliance in the enterprise,"Data privacy, governance, and compliance company OneTrust has announced plans to acquire Convercent, an ethics and compliance software platform with major enterprise clients that include Hain Celestial, Kimberly-Clark, and Under Armour. The deal comes amid a flurry of activity across the digital privacy space — spanning product launches, VC investments, and acquisitions — as businesses have come under pressure to comply with not only an array of data privacy regulations, such as GDPR in Europe and CCPA in California, but also growing privacy expectations from their users. Founded in 2016, Atlanta, Georgia-based OneTrust, which recently raised $300 million at a $5 billion valuation, is one of numerous startups striving to capitalize on the burgeoning $80 billion data privacy market. The company offers various data privacy tools that help businesses such as Oracle, Marketo, and Akamai determine how closely they are complying with legal frameworks such as GDPR and how data is flowing through their organization and across borders. OneTrust also offers tools spanning cookie compliance, consent management, breach response, and more. Based in Denver, Colorado, Convercent is an eight-year-old VC-backed startup specializing in cloud-based ethics and compliance software and includes a suite of apps spanning whistleblowing, policy management, conflict, and disclosure. The company’s main ethics and compliance portal serves as a single channel through which employees can access company policies and programs related to data privacy, with built-in features that support online chat, videos, and more. The acquisition is expected to close in April, after which OneTrust said it will integrate Convercent into its main platform for “enhanced ethics and compliance capabilities,” including centralized policy management through which its customers will be able to “orchestrate all aspects of trust.” The momentum behind the acquisition appears to stem from OneTrust’s recent launch of its own ethics product, which includes tools to help whistleblowers securely and confidentially report incidents they witness internally. “Based on the significant demand we saw for this solution, Convercent provided an opportunity to accelerate the availability of both that solution and the broader ethics and compliance platform,” OneTrust CEO Kabir Barday told VentureBeat. “After the acquisition, OneTrust and Convercent together have the market-leading whistleblowing solution used by 750 organizations worldwide, as well as an ethics and compliance portal, policy management, disclosure management, analytics and benchmarking, and learning.” The acquisition follows a degree of consolidation in the data privacy and compliance sphere. In the past few months alone, LiveRamp acquired privacy-focused data unification engine Datafleets; HelpSystems snapped up data security platform Vera; and OneTrust itself this month bought DocuVision, the company behind an AI-powered platform that companies use to find and redact sensitive data in large volumes of documents. So what is driving all this consolidation? Barday thinks it’s partly due to companies wanting unified solutions rather than having to select from different software suppliers. “Organizations are finding that their bigger-picture goal is to be a trusted brand for their customers, vendors, and employees,” Barday explained. “Investing in separate privacy, ethics and compliance, or third-party risk tools in silos isn’t as efficient as a platform [with] embedded, integrated capabilities. This, plus the pace organizations need to move at and also the desire to reduce the number of vendors they work with — and therefore reduce risk surface — is driving the consolidation.” Ultimately, as enterprises continue to embrace the cloud and applications that harness large swathes of data, the issues of data privacy, governance, ethics, and security are only going to grow. Moreover, continued investments in AI and machine learning will underscore the need to invest in robust ethics and compliance tooling. “The challenge is, AI and machine learning depends on really large datasets of information — sometimes produced internally, sometimes produced externally — that can have inherent bias and other issues,” Barday said. “These can cause massive problems, like reinforcing social issues, divides, and biases. Solving these issues is an ethical problem, and the combination of an ethics platform with a privacy, data governance, and security platform allows this more ethical view of how data powers AI and machine learning.” Barday confirmed that when the acquisition closes next month, Convercent will continue to be available as it currently is, through its own platform and website. “We’re accelerating investment in the platform on its own, and in parallel integrating and merging into the OneTrust platform,” he said."
https://venturebeat.com/2021/03/23/haystack-wants-to-be-the-modern-enterprise-intranet-for-remote-work/,Haystack wants to be the modern enterprise intranet for remote work,"As businesses grow and branch into multiple divisions and units spanning cities and time zones, it can be easy for workers to feel disconnected — both from each other and the information they need to fulfill their jobs. Remote work now being on the permanent menu for many companies may only compound matters. And that is why Los Angeles-based startup Haystack is launching out of stealth today with $8.2 million in funding from a slew of top investors, to serve as a unified hub for employees to “receive important announcements, discover knowledge, and learn about your coworkers,” Haystack cofounder and CEO Cameron Lindsay told VentureBeat. Haystack, which is targeting larger companies with at least 300 employees, is entering a space consumed by various incumbents. This spans siloed internal company intranets through to third-party platforms such as Jive (acquired by Aurea four years ago), Simpplr, and LumApps, the latter two of which have raised sizable investments in the past year. As the new kid on the block, Haystack is betting that a modern, easy-to-configure platform with deep enterprise integrations and universal search will be enough to garner attention. “Almost every company after a certain number of employees will have some form of intranet,” Lindsay said. “These are either built internally — expensive and often siloed from other systems — or purchased from ‘modern’’ intranets that end up looking more like vintage social networks than tools to help people get their jobs done. These ‘modern’ intranet products take months to configure, and prefer companies to start from scratch rather than integrate with existing systems.” Haystack was founded in 2019 by Lindsay and CTO Haibo Zhao, a former senior software engineer at Google and, more recently, tech lead at Snap. Haystack kicked off a pilot program in early 2020 with a high-profile ecommerce platform as its inaugural customer, followed later in the year by other pilot customers including Nerdwallet and Chime. During that period, Haystack also secured $8.2 million in seed funding from notable names from the VC and angel investor realm, such as Twitter cofounder Biz Stone, Greycroft, Coatue Management, and Day One Ventures. Enterprises use Haystack to connect all their employees, systems, and communication channels in a centralized hub. The company offers a bunch of pre-built connectors for many of the popular SaaS business tools such as Slack, Google Workspace, Microsoft 365, Workday, Okta, and Atlassian’s Confluence, while it also offers an API for businesses that want to develop more customized integrations. All this, ultimately, is designed to save personnel from having to switch between multiple applications to find key information they need for their jobs. Across all these connections, Haystack offers universal search capabilities that allows anyone to search for any content across all their conduits — this is perhaps one of the core components of the Haystack platform. “We lead with search as much as we can, and make sure search is the focal point of our product,” Lindsay said. “We also provide advanced search analytics that give admins insight into how search is performing and provide actionable insights to improve information accessibility.” Haystack has all the usual features you would expect from a modern intranet, including employee directories and event management portals, but some of the specific features within the platform are worth focusing on. Through Haystack, managers or HR personnel can draft an announcement and distribute it to everyone in the company through email, Microsoft Teams, Slack, SMS, or a self-branded company app. Haystack offers no-code customization so businesses can make the interface reflect their own branding. Related to this, businesses are given granular controls in terms of how users can engage with the content. It’s possible to disable emoji reactions, for example, if a message is deemed to be of such importance, while comments can also be turned off on a message-by-message basis. Haystack’s analytics, meanwhile, can surface insights into who has opened the message, and in the future it will be able to apply sentiment analysis to show how it is being perceived. Digging down into the security and privacy side of things reveals some pretty intriguing features, such as disabling copy/paste and screenshotting, and general functionality that prevents leaked information — this includes requiring each user to sign in using SSO (single sign-on) authentication to view content, meaning that the user won’t be able to see the entirety of the message on email or Slack. Additionally, management can stipulate that a message is uniquely watermarked with each employee’s name, to further dissuade information leaks. Haystack is also planning to introduce some machine learning smarts in the coming months, tapping the collective experience of a founding engineering team spanning Google, Snap, and PayPal. “Many of our future add-on products will use machine learning algorithms to connect employees, improve search, and help admins better understand their organizations,” Lindsay said. In terms of costs, Haystack said that it offers tailored pricing for each customer spread across two broad “professional” and “enterprise” tiers, with features like white-labeled mobile apps and custom URLs available across both, while custom integrations is limited to the enterprise incarnation. Haystack is officially accepting new customers from today."
https://venturebeat.com/2021/03/23/aiven-raises-150m-to-manage-cloud-data-infrastructure-for-open-source-technologies/,Aiven raises $100M to manage cloud data infrastructure for open source technologies,"Aiven, a Finnish company that provides managed open source data technologies on most of the major public clouds, today announced it has raised $100 million in a series C round of funding. Open source software, by many estimations, has eaten the world, and it now intersects with just about every piece of software from scripts that help servers run faster to systems architecture and APIs. Enterprises are adopting open source software more than ever, a shift accelerated by factors such as the push toward cloud computing and the ongoing pandemic. However, despite the well-established benefits that open source software brings to the table such as lower costs and more flexibility, there remains a lot of friction in using open source software — it can be time-consuming to set up, and it requires specific expertise and resources in related domains such as cybersecurity and maintenance. This is where Aiven comes into play. Founded in 2016, Aiven manages companies’ open source data infrastructure in the cloud, freeing developers to focus on building applications. With support for AWS, Google Cloud Platform, Azure, Digital Ocean, and UpCloud, users are free to shift data between clouds or even adopt a multi-cloud approach. “We give our customers the freedom of choice — their data is not locked to one vendor or system, but they can migrate from one cloud to another with a click of a button,” Aiven cofounder and CEO Oskari Saarenmaa told VentureBeat. Aiven offers fully managed services across nine core open source projects, including Apache Kafka, M3, MySQL, Redis, InfluxDB, Apache Cassandra, Elasticsearch, PostgreSQL, and Grafana, and includes all the necessary support such as end-to-end security, maintenance, and 24/7 monitoring. Additionally, Aiven maintains other open source projects such as Pghoard, a backup and restore service for PostgreSQL. The Helsinki-based startup works with companies across sectors and claims a number of high-profile enterprise clients, such as Atlassian, Comcast, and Toyota. “Many of our customers are medium to large companies that generate massive amounts of data and need a modern infrastructure to iterate on this data faster,” Saarenmaa said. “For example, we work with a few large retail companies who are moving data between their physical stores, ecommerce site, warehouse, and more. On the Aiven platform, these companies can build a customized data infrastructure in the public cloud that allows them to move this data quicker and gain insights from their data pipelines.” Prior to now, Aiven had raised around $50 million across several funding rounds. Its latest funding round was spearheaded by European VC juggernaut Atomico, with participation from Salesforce Ventures, World Innovation Lab, Earlybird Venture Capital, and IVP. With another $100 million in the bank (and an $800 million valuation), the company said that it plans to allocate a “significant part” of its financing to growing its contributions to pure open source projects, as well as bring new products to market and expand internationally. Moreover, the company said that it plans to double its headcount over the next 12 months, and will open a dedicated open source program office to drive its own contributions to open source projects. “We are recruiting Elasticsearch, Kafka, and PostgreSQL open source developers,” Saarenmaa said, adding that it plans to hire at least 10 full-time open source developers in the next year. “Our customers come to Aiven to build their data infrastructure on open source, and Aiven will continue to only support software licenses backed by the community,” Saarenmaa said. Other notable players in this space include Confluent and Redis Labs, which have raised $350 million between them in the past year to commercialize open source projects, while publicly traded MongoDB and Salesforce-owned Heroku are also comparable. While Aiven is setting out to differentiate in several ways, Saarenmaa acknowledged that in the open source sphere, there is a fine line between friends and rivalry in the open source realm. “We stand out with a fully managed, open-source, multi-cloud offering,” Saarenmaa said. “However, in the open source world, your competitors are often also your collaborators, as many developers must work together to build on open source projects and contribute to the community.” This echoes sentiments recently conveyed by Microsoft, which noted that “open source is now the accepted model” for cross-company collaboration. It’s worth noting that a lot of commotion arose in the commercial open source realm in recent years, when companies that have monetized open source projects they maintain decide to impose stricter limits on their licenses. For example, Elastic, the private company that develops the open source NoSQL database Elasticsearch and the associated data visualization dashboard Kibana, recently confirmed that it was switching its licensing arrangements to restrict cloud service providers from offering these tools “as-a-service”. As a result, Amazon’s AWS revealed it was forking both Elasticsearch and Kibana, a project that Aiven itself is helping to support moving forward. “We only support community-backed open source products and community licenses, rather than closing off our products with more restrictive licenses, as some of our competitors have done,” Saarenmaa added. “We have always been committed to open source and will continue to support the open source community by contributing to new and existing projects.” *This article was corrected to say that Aiven’s series C investment was $100 million, not $150 million."
https://venturebeat.com/2021/03/23/virtual-events-platform-hopin-acquires-streamable-and-jamm-to-advance-video-ambitions/,Virtual events platform Hopin acquires Streamable and Jamm to advance video ambitions,"Virtual events juggernaut Hopin continues its rapid acquisition spree today with the news that it has bought U.S.-based video platforms Streamable and Jamm. The announcement comes weeks after Hopin raised $400 million at a staggering $5.65 billion valuation just 20 months after the company was founded. Over that period, Hopin has grown its headcount from six people at the start of 2020 to more than 400 today, with thousands of enterprise customers that include Hewlett Packard, American Express, Adobe, and GitLab. While Hopin has always provided its own native video streaming technology, the company also allows users to ingest streams from third-party sources, including YouTube, Vimeo, Wistia, and Streamable. In fact, Hopin elected to buy Streamyard outright back in January after noting its popularity among events organizers. This came just weeks after Hopin’s first acquisition, when it bought event app development company Topi. Streamable, for the uninitiated, is a video hosting platform that makes it easy for anyone to upload and share videos without having to create an account, while Jamm is a video collaboration app for the workplace — “it’s like Slack and Zoom had a baby,” the company proudly proclaims. Hopin is now strongly positioned to hold its upward trajectory in the fast-growing virtual events space, a trend that is likely to continue even when in-person meetings become feasible again. The pandemic has highlighted many of the benefits offered by online events, which enable companies to scale their meetups with fewer resources and access valuable data that can help sales and marketing teams correlate digital interactions with business objectives. By most accounts, online events are very much here to stay, either as standalone entities or as part of a hybrid format. Streamable and Jamm’s CEOs will continue to spearhead their respective platforms as part of Hopin, which said it plans to integrate both technologies into its existing platform to serve as the “basis for the development of future video products and features.” Stopping short of confirming specific plans, Hopin said the acquisitions would help it expand into a “multi-product suite of video-centric collaboration products.” Adding integrated YouTube-like video hosting smarts from Streamable opens the door to any number of potential use cases. It makes it easier for marketers and content producers to upload and share video in real time — either from a virtual location or on-location from a conference as part of a hybrid setup. Similarly, Jamm’s built-in features spanning video chat, screen-sharing, and Slack integration could power Hopin’s ambitions beyond events and catapult it into the broader business collaboration sphere. Terms of the two deals were not disclosed, though it’s worth noting that Streamable and Jamm had only raised a small amount of seed funding, meaning that their sale prices are unlikely to have broken the bank for Hopin."
https://venturebeat.com/2021/03/23/brii-biosciences-closes-us155-million-series-c-financing/,Brii Biosciences Closes US$155 Million Series C Financing," – Financing led by Invesco Developing Markets Fund, joined by a syndicate of new and current investors – Proceeds will support accelerating clinical development programs in the United States and China  DURHAM, N.C. & BEIJING–(BUSINESS WIRE)–March 23, 2021– Brii Biosciences (“Brii Bio”), a multi-national company developing innovative therapies for diseases with significant unmet medical needs and large public health burden, today announced completion of a Series C financing of US$155 million. The financing, participated by existing and new investors, was led by Invesco Developing Markets Fund, with significant additional funding provided by GIC and another global leading investment management organization, followed by Lake Bleu Capital and an Asia-based leading investment organization, as well as three current investors. Proceeds from the financing will be used to advance Brii Bio’s broad infectious diseases pipeline as well as the company’s CNS program. “Since our founding in 2018, Brii Bio has made tremendous progress toward our mission to accelerate the development and delivery of breakthrough medicines through partnerships and our own insight in highly differentiated medicine discovery,” said Zhi Hong, Ph.D., CEO of Brii Bio. “As we work to address some of the world’s biggest public health issues and disease burdens, we are pleased and honored to have the support of investors who share our mission to discover, develop and deliver innovative treatments for patients not only in China, but throughout the world.” Brii Bio is currently conducting clinical studies in multiple infectious diseases, and is preparing to enter clinical-stage research in diseases of the central nervous system. The company’s programs include: About Brii Biosciences Brii Biosciences (Brii Bio) is a multi-national company committed to serving patients’ needs and improving public health by accelerating the development and delivery of breakthrough medicines through partnerships, best-in-class research and development, and the disruptive application of digital and data insight. With operations in the People’s Republic of China and the United States, Brii Bio is poised to serve as a bridge to carry transformative medicines to patients, help create significant growth for our partners and establish an innovation engine to help improve the public health and wellbeing of patients around the world. Brii Bio is developing treatments for illnesses with significant public health burdens, including infectious diseases, liver diseases, and CNS diseases. For more information, visit www.briibio.com. About Invesco Developing Markets Fund Invesco Developing Markets Fund has approximately US$50 billion asset under management. The fund is recognized as one of the market leaders in emerging market equities and has successful track record of investing since 1996. We are long-term investors with disciplined approach to identify extraordinary companies, which deliver strong long-term performance and have innovative products or unique assets that capture demands domestically and/or outside their home economy. About GIC GIC is a leading global investment firm established in 1981 to manage Singapore’s foreign reserves. As a disciplined long-term value investor, GIC is uniquely positioned for investments across a wide range of asset classes, including equities, fixed income, private equity, real estate and infrastructure. Headquartered in Singapore, GIC has investments in over 40 countries and employs over 1,700 people across 10 offices in key financial cities worldwide. For more information on GIC, please visit https://www.gic.com.sg/ or LinkedIn. About Lake Bleu Capital Based in Hong Kong and Shanghai, Lake Bleu Capital (www.lakebleu-cap.com) is a multi-billion investment platform focused on healthcare space. Lake Bleu Capital’s USD public equity fund is one of the largest healthcare specialist funds in Asia. Thanks to its outstanding performance since inception, the fund has been awarded as the sole winner of the “5 Year Award: Over 500m category” by AsiaHedge in 2020 and the “Best Asia ex-Japan Fund” by industry authorities multiple times in recent years. Lake Bleu Capital is also active in mid-to late-stage private equity investment in China’s healthcare sector. It has successfully invested in over 40 high-quality private companies and provided the invested companies with strategic value- add services while growing with the companies in the long run.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210323005250/en/ Lisa Becklisa.beck@briibio.com orMedia@briibio.com"
https://venturebeat.com/2021/03/23/box-explores-sale-amid-pressure-from-activist-investor-starboard/,Box explores sale amid pressure from activist investor Starboard,"(Reuters) — U.S. cloud services provider Box is exploring a sale amid pressure from hedge fund Starboard Value LP over its stock performance, according to people familiar with the matter. Redwood City, California-based Box has discussed a potential deal with interested buyers, including other companies and private equity firms, the sources said, cautioning that no sale of the company is certain. The sources requested anonymity because the matter is confidential. Box declined to comment. Box shares jumped as much as 17% on the news to $26.47, giving the company a market value of about $4.3 billion. DA Davidson analyst Rishi Jaluria wrote in research note on Monday afternoon that Box could fetch more than $34 per share in a sale. Reuters reported last month that Starboard was preparing to launch a board challenge against Box unless it took steps to boost value for shareholders. It has privately expressed disappointment that the company has failed to capitalize on the work-from-home trend during the COVID-19 pandemic, as many of its cloud computing peers have done. Its shares currently trade at around $22 apiece, only marginally higher than the price at which it debuted on the New York Stock Exchange in January 2016. Box said last week it would extend the deadline to nominate directors to its board from mid-April to May 11. Founded in 2005, Box offers file sharing, cloud storage, and cloud backup, among other services. Demand for secure file-sharing and other workplace collaboration services has risen since the onset of COVID-19, driven by the information technology needs of companies whose employees are working from home. While Box has benefited from this trend, it has struggled to fully capitalize on it, as some of its services and products are offered by competitors such as Microsoft either for free or at a lower cost. Box earlier this month reported fourth-quarter earnings that beat analyst expectations."
https://venturebeat.com/2021/03/23/lgn-which-helps-companies-deploy-ai-at-scale-raises-2m/,"LGN, which helps companies deploy AI at scale, raises $2M","LGN, a company specializing in distributed machine learning, deep learning, and AI technologies, today announced that it raised $2 million. The company says it’ll use the investment to bolster its product development and hiring efforts as it expands its market reach. In particular, LGN intends to pursue low-latency inference technology that can process optical data on-chip orders of magnitude faster than current-gen tech. Businesses are in the midst of a shift in where and how they analyze data and derive actionable insights from it. Spending on AI is anticipated to break the $500 billion mark by 2024, according to IDC, and Gartner forecasts that over 50% of enterprise data will be processed outside the cloud by 2022.  But AI projects remain highly susceptible to failure. According to one study, only 25% of companies have successfully developed an “enterprise-wide” AI strategy. LGN aims to help enterprises address data science challenges by scaling out AI deployments, improving resiliency in machine learning models, and optimizing the hardware and devices that make up edge endpoints. Borne in 2018 out of a collaboration between former Apple and BMW executive Daniel Warner, Oxbridge research fellow Luke Robinson, and Vladimir Čeperić of MIT and the University of Zagreb, LGN designs solutions for customers with the goal of minimizing transfer costs and improving training dataset quality while reducing storage and processing requirements. “We previously worked on defense projects improving laser vision systems in less-than-ideal environments with cloud and fog. We were generating extreme amounts of data at the edge and needing to constantly retrain and improve a model,” Warner told VentureBeat via email. “These moments were the early days in us seeing how the future would unfold and plan to address them. Ceperic was at MIT making the world’s first optical convolutional neural network chip. Dan and Luke were at Harwell using AI to piece back together optical beams from quantum lasers. We immediately realized the synergy around on-chip optical signal processing and realised we shared a vision around the future of AI.” Data scientists spend the bulk of their time cleaning and organizing data. Much of the remaining time is spent on feature engineering, or the process of using domain knowledge to extract features from input data. It’s essential to tuning AI and machine learning performance, but it’s also typically arduous and involves rewriting features before they’re deployed. Often, a missing piece is infrastructure that bridges the gap between training models and serving AI results in production environments. LGN’s solution to this is what Warner calls “networked AI.” As Warner explains, currently, much of what AI does needs to be translated so that humans can understand it and make decisions. By contrast, networked AI is “AI-to-AI” communication, which removes the need for the human element and speeds up the action-taking process. “AI has huge implications for the way businesses operate, yet so much of the modelling is done in carefully controlled test environments. When deployed in real world situations, anomalies always occur, which disrupts lab-grown models and undermines companies’ efforts to revolutionise how they use autonomous systems effectively,” Warner said. “By scaling edge AI, optimising the endpoints collecting the data, and making models more resilient, we are radically accelerating learning and, in doing so, giving our customers a competitive advantage in a crowded marketplace.” To this end, LGN offers a product that automatically calibrates AI-powered sensor arrays as well as a platform — Intelligent Select — that reduces data capture and compute needs by filtering data to only record and process edge cases while transferring and storing data in a compressed form. LGN claims that Intelligent Select can boost model performance on hard classes by 180% while using just 2.5% of the original training data, and moreover cut down on data, labeling, and processing expenses. “Networked AI will change the way the world works. All the potential benefits of AI are limited while we need to involve humans in the majority of the decisions,” Warner continued. “Edge AI is the first step towards speeding up that process, by having AI at the endpoint to rapidly collect, analyse and determine action based on data. Networked AI will take that and apply it across all aspects of an organisation, freeing up workers to focus on more valued-adding activities.” Eight-employee LGN supports five companies operating in a range of sectors including automotive, agriculture, and manufacturing. One is investor Jaguar Land Rover, which plans to retain LGN’s services to expand its data capture efforts and analyze road travel data toward the development of autonomous cars. “Our product[s] allow companies to operate edge AI at scale in challenging, diverse, real- world environments. For example, we’re working with agriculture companies to scale out the commercial deployment of machine vision systems on farms and in industrial chicken sheds,” Warner said. “Our model optimisation solution allows enterprises to deploy AI and machine learning models on low-cost hardware. For example, we can deploy computer vision models to run object detection or semantic segmentation on high-definition camera feeds, with a full supervision and monitoring stack on a $6 AI camera chip. This keeps the cost of the bill-of-materials down, which increases margins and unlocks commercial sales.” The funding round announced today is London-based LGN’s first public raise. Trucks Venture Capital, Luminous Ventures, and Jaguar Land Rover participated, among others."
https://venturebeat.com/2021/03/23/marketing-campaign-analytics-startup-sellforte-raises-4-78m/,Marketing campaign analytics startup Sellforte raises $4.78M,"Helsinki-based software-as-a-service startup Sellforte today announced a €4 million ($4.78 million) funding round. The company says it’ll use the capital to scale R&D on its products and expand its operations internationally, soon opening a new office in Germany. According to Gartner, big data analytics influences only 54% of marketing decisions at enterprises. Among the top reasons marketers haven’t been able to use analytics in decision making are poor data quality and unclear recommendations. Traditionally, companies have resorted to one-off projects, trying to access marketing campaigns’ results on an ad-hoc basis. For businesses that try to go it alone, marketing data is often scattered around different internal and external platforms, making it difficult to get a clear picture of investment performance. Sellforte attempts to solve this with AI that measures the effectiveness of marketing investments, generating continuous recommendations for growth. The company, which was founded in 2017, claims its data science models can calculate comparable return on investment for marketing investments across different channels and campaigns by learning from millions of rows of sales and marketing data. “When you do one-time projects year after year, the models don’t learn from the previous results. For example, TV return on investment could be 5.1 times in 2019, then 2.4 times in 2020, and 6.8 times in 2021, making it really hard to say whether to increase or decrease TV investments as nobody believes in these results because they vary so much,” cofounder and CEO Juha Nuutinen told VentureBeat via email. “[With Sellforte’s technology,] a marketing director can use the platform to compare marketing campaign investments to profitability and uplifts against reference periods in time, as well as measure how marketing affects the key brand KPIs … Additionally, they can get country-specific marketing effectiveness metrics in the same currency, and analyze which media groups perform the best in each country and why.” Using Sellforte, enterprises can set up pipelines for commercial, media, and secondary data sources, including things like receipt line sales from data warehouses, competitor activities, business KPIs, and online display ad and email campaigns. The customers then identify relevant secondary datasets and apply business priors based on domain expertise before evaluating the results’ reliability. Post-setup, Sellforte updates the results on a monthly or quarterly basis and provides actionable suggestions.  Sellforte typically needs sales and marketing data from the past two to three years to create a reliable predictive model. In refining the model, the company leverages a technique called Bayesian learning that allows inputting prior knowledge in the form of informative priors. Nuutinen says this makes it easier to leverage past analyses and business intuition as well as develop granular models without breaking their consistency. For example, instead of simply showing weekly sales totals, the models can expose what’s going on at each specific date, product group, brand, and location. “We typically utilize 10 million to 500 million rows of data rather than hundreds or thousands of data points that competitors would typically utilize in projects. We have also been doing sales forecasting and optimizing future campaigns and media plans,” Nuutinen explained. “Other examples include text classification to prepare media data, training machine learning models to explain marketing effectiveness by geographic and postal code area by population structure, life stages, activities, and other similar statistics.” The marketing analytics sector is predicted to grow from $3.01 billion in value to $6.92 billion from 2019 to 2027, according to Verified Market Research. Sellforte has competition in media agencies and management consultancies like Ebiquity, GroupM, and Accenture, but Nuutinen says that what differentiates the company is its ability to learn and improve with model updates, recommend actions, and track the realized business impact of those recommendations. “Sellforte’s platform is a home for previously scattered internal and external marketing data … This creates a common currency across the previously incomparable Facebook impressions, Google clicks, and many other media channel-specific metrics. Continuously updating ROMI [return on marketing investment] results enable our customers to run controlled tests with all of their marketing campaigns — in digital and traditional media, both in offline and online sales channels,” Nuutinen said. “Typically our customers achieve 20-30% improvements in their marketing effectiveness by optimizing their media mix, campaign mix, brand mix, geographic mix, and timing of media investments.” Sellforte, which has 33 employees, claims to serve 20 “industry-leading” businesses such as C&A, Elgiganten, and Cloetta in 14 countries from its offices in Finland, Sweden, and the Netherlands. Annual recurring revenue stands at €1.5 million ($1.79 million) and is expected to double annually for the next two years. Sonae IM led Sellforte’s latest funding round with participation from Bonnier Ventures and Icebreaker.vc. It brings the company’s total raised to date to €4.5 million ($5.37 million) at a post-money valuation of €15 million ($17.9 million)."
https://venturebeat.com/2021/03/23/jumio-raises-150m-to-secure-onboarding-with-image-verification-ai/,Jumio raises $150M to secure onboarding with image verification AI,"Jumio, an AI platform that offers identity verification as a service, today announced the closure of an over $150 million fundraising round, bringing its total raised to $163 million. The company, which claims the tranche is among the largest ever for an identity verification startup, plans to put the capital toward hiring and expanding its customer base. Javelin Strategy reported that 6.64% of consumers, or about 16.7 million people, fell victim to identity fraud in 2017, up 1 million from 2016. In 2018, over 2.6 billion records were stolen or exposed in more than 1,100 data breaches around the world. That’s perhaps why by 2022, 80% of organizations will be using document-centric identity proofing as part of their onboarding workflows, which is an increase from approximately 30% today, according to Gartner. Palo Alto, California-based Jumio was relaunched in 2016, when the company emerged from bankruptcy as it was being investigated by the U.S. Securities and Exchange Commission after restating its 2013 and 2014 financials. In 2019, the agency fined Jumio’s original founder and former CEO Daniel Mattes more than $17 million to settle the charges that he defrauded investors, including Facebook cofounder Eduardo Saverin. Jumio is now owned by private equity firm Centana Growth Partners and says its products have helped to verify over 300 million identities for hundreds of customers including Airbnb, Coinbase, United Airlines, and Instacart. In 2018, Montreal-based Jumio AI Labs was founded, dedicated to the creation, experimentation, and at-scale deployment of machine learning and deep learning technologies related to online identity verification, data extraction, fraud detection, and risk scoring. And in 2020, Jumio claims to have achieved record revenues, culminating in the acquisition of San Francisco-based Beam Solutions’ anti-money laundering platform, which focuses on mobile transaction monitoring and case management. Jumio integrates with websites, iOS and Android apps, and more to enable businesses to confirm users are who they say they are for new account onboarding, fraud detection, age verification, and transaction monitoring. The platform combines biometrics for identity proofing and “ongoing” 3D face authentication, powered in part by technology from Nevada-based FaceTec. Users first snap a photo of their driver’s license, passport, or ID card and then use a mobile device camera or webcam to capture their faces. In the course of authentication, Jumio compares the “selfie” — a 3D face map containing 100 times the liveness data of a 2D photo — with the picture on the aforementioned ID and retakes the selfie for good measure.  Jumio claims to leverage 10 years of real-world data, hundreds of millions of domain-specific data points from 3,500 ID types across 200 regions, AI for liveness detection, and face-based biometrics to power its authentication products. Beyond verifying identities, Jumio offers services that monitor transactions and customers to identify suspicious or unusual activity. The company also provides solutions that automatically determine proof of address from pictures of utility bills, credit cards, and bank statements to validate and corroborate addresses with independent third-party sources. “Jumio leverages AI in every identity verification that we process,” CEO Robert Prigge explained to VentureBeat via email. “Jumio uses AI to align pictures of ID documents so that we can reliably optical character recognition (OCR) the data. Jumio uses AI to inspect an ID for signs of fraud or tampering. Jumio uses AI as part of its liveness detection to ensure that the person is physically present and to compare the picture in the selfie with the picture on the ID document. We have dozens of AI and machine learning models in place to improve the user experience, sniff out fraud, and to expedite the customer experience. Powered by informed AI, Jumio’s OCR engine overcomes many of the limitations of traditional OCR and is capable of highly accurate data extraction — data that can be used to ping third-party databases or to verify a person’s age (based on the date of birth).” The pandemic has led to an uptick in the types of fraud that Jumio claims its platform can actively prevent. A recent report from Socure found that fraudulent credit card applications jumped 93% from March to April 2020. Between March and late June, attempted money transfer fraud increased 43% as challenger banks experienced a 200% climb in attempted demand deposit account fraud. “The pandemic expedited digital transformation efforts as physical locations and face-to-face verifications became impossible. When consumers turned to digital transactions during the pandemic, companies’ need to verify the identity of users skyrocketed. Jumio processed its single-highest volume of identity verifications in 2020, compared to any other quarter in the company’s history,” Prigge said. “The growth has also been driven by enterprises switching from data-centric approaches to document-centric approaches to identity proofing. This exodus was caused, in large part, by large-scale data breaches of personally identifiable information which have rendered data-centric approaches less reliable.” For example, Jumio recently partnered with delivery startup Rappi to help verify the identities of its new customers and couriers. Since partnering with Jumio, more than 750,000 users have opened new Rappi payment accounts, according to Prigge. “At a time where remote operations have made it challenging to verify identity online, we’re proud to provide a secure solution for enterprises to detect fraud and confirm their users are who they claim to be,” he said. Great Hill Partners led this latest investment in Jumio — one of the single largest funding rounds in the digital identity space. Jumio, who counts among its previous backers Millennium Technology Value Partners, now employs over 1,500 people across its offices in North America, Latin America, Europe, and Asia Pacific."
https://venturebeat.com/2021/03/22/synnex-and-tech-datas-7-2b-merger-spawns-an-it-solutions-behemoth/,Synnex and Tech Data’s $7.2B merger spawns an IT solutions behemoth,"(Reuters) — IT solutions firm Synnex said on Monday that it will merge with peer Tech Data, owned by funds affiliated to Apollo Global Management, in a deal worth about $7.2 billion, including debt. Shares of Synnex jumped over 11% in premarket trade. Apollo Funds will receive 44 million shares of Synnex common stock and the refinancing of existing Tech Data net debt and redeemable preferred shares of about $2.7 billion. Upon closing of the deal, expected in the second half of 2021, Synnex shareholders will own about 55% of the combined company, while Apollo Funds will own about 45%. Tech Data was taken private by Apollo Global Management in 2020 in a deal valued at about $6 billion. Synnex earlier separated into two publicly listed companies, Synnex Technology Solutions and Concentrix."
https://venturebeat.com/2021/03/22/camunda-nabs-100m-for-cloud-native-software-that-drives-hyperautomation/,Camunda nabs $100M for cloud-native software that drives hyperautomation,"With production accelerating across markets, the task of automating workflows has taken on greater urgency. This has led to a surge in chatter around hyperautomation, the concept of integrating several process automation technologies to achieve even greater digital transformation velocity. Berlin-based Camunda was founded in 2008 and is using open source software to help enterprises achieve that hyperautomation by combining areas like process design, modeling, and analyzing large datasets through the use of visual interfaces so developers and business-side employees can contribute. Today Camunda announced it has raised $100 million in a round led by Insight Partners. The raise included money from Highland Europe, which led the company’s $28 million round in 2018. “Camunda was founded on the premise that making it easy for business stakeholders and developers to automate and improve business processes would have a transformational impact on organizations,” company CEO and cofounder Jakob Freund said. “It also shows the huge global market opportunity that is in front of Camunda as demand for open, cloud-native process automation solutions that can orchestrate processes end-to-end is quickly reshaping the market.” In its press release, the company said the investment validates its progress on hyperautomation. Clients include Allianz, ING, Intuit, and Vodafone. Those companies are facing the challenge of keeping up with a vast number of new technologies that hold the promise of greater automation. But integrating them all and optimizing to achieve maximum efficiency remains a big hurdle. Companies that fail to do so risk falling behind competitors while disappointing customers and clients who are demanding greater speed. Camunda promises to help reduce this technical backlog by making it easier to orchestrate automation across a wider range of processes. That effort has been boosted by the company’s process-automation-as-a-service platform — Camunda Cloud — that was released last year. The company plans to use the latest round of funding to expand marketing and sales, as well as continuing its focus on product development."
https://venturebeat.com/2021/03/22/optimizely-adds-real-time-customer-data-tracking-with-zaius-acquisition/,Optimizely adds real-time customer data tracking with Zaius acquisition,"Optimizely, a provider of tools for building websites and applications that drive digital experiences, today announced it has acquired Zaius to add a customer data platform (CDP) to its portfolio. Terms of the acquisition were not disclosed. The addition of a CDP that tracks user behavior in real time will enable organizations to create digital experiences that are personalized down to the individual level, Optimizely CEO Alex Atzberger told VentureBeat. Zaius is able to accomplish that by creating a predictive model using a neural network that analyzes the data an organization collects about each user to determine, for example, what products they are likely to purchase next. Atzberger said this insight makes it possible to surface a highly personalized online experience in real time. That specific capability is what attracted Optimizely to Zaius, Atzberger said, adding, “Not all CDPs are structured the same.” Vendors offering CDPs range from Salesforce, Adobe, Microsoft, Teradata Oracle, and SAP to a host of smaller rivals, such as Segment and Emarsys. However, Atzberger said a CDP platform that is integrated with the web platform organizations are employing to build digital experiences offers a strategic advantage over a CDP that might be integrated with a customer relationship management (CRM) application. Zaius also comes with 50 prebuilt connectors for pulling data into the platform, in addition to providing over 120 predictive models for analyzing user behaviors, Atzberger noted. Optimizely has gained traction via a platform for building websites and applications that incorporates feature flagging, also known as feature toggles. This software development technique enables specific functionality to be turned on and off during runtime without deploying new code. That capability makes it easier to either test certain capabilities on a segment of end users or only make specific features of an application available to a subset of those end users. By acquiring Zaius, the company can extend that capability all the way down to a specific end user as part of a larger digital experience management (DEM) strategy, Atzberger said. As organizations continue to invest heavily in digital business transformation initiatives to engage customers directly online, interest in CDPs has risen. Most recently, digital experience management software provider Sitecore acquired Boxever to gain a CDP. It’s not clear whether we can expect a wave of CDP mergers and acquisitions, but there are a host of standalone CDP platform providers. Many organizations are looking to consolidate the number of vendors they need to engage to drive their digital business transformation initiatives as part of an effort to reduce integration costs. In the meantime, a report MarketDigits published last week predicts the CDP market will grow from $2.4 billion in 2020 to $10.3 billion by 2026, representing a compound annual growth rate of 34%. In general, CDP is considered a type of data warehouse that includes tools to address everything from personalized recommendations using predictive analytics to marketing data segmentation, customer retention and engagement, data monetization, and data enrichment. Most organizations are data-rich today because of the investments they have made in platforms such as data warehouses, Atzberger noted. However, those same organizations are often “insight poor” because they lack the tools and expertise required to analyze that data within the context of real-time customer engagement, Atzberger added. It’s too early to say how many organizations will need to invest in a CDP. However, there is a pressing need to harmonize customer data that today resides in multiple application silos, especially as those customers begin losing patience with vendors that don’t understand their preferences."
https://venturebeat.com/2021/03/22/verisim-life-launches-pulmosim-therapeutics-to-develop-treatments-for-rare-respiratory-diseases/,VeriSIM Life launches PulmoSIM Therapeutics to Develop Treatments for Rare Respiratory Diseases,"SAN FRANCISCO–(BUSINESS WIRE)–March 22, 2021– Today, VeriSIM Life (VeriSIM) announces the launch of its pharmaceutical subsidiary, PulmoSIM Therapeutics (PulmoSIM) with the mission to address the unmet needs in the treatment of rare respiratory diseases, with an immediate emphasis on pulmonary arterial hypertension (PAH) and idiopathic pulmonary fibrosis (IPF). PulmoSIM is developing breakthrough therapies for rare and progressive respiratory diseases to improve patient outcomes and quality of life. PulmoSIM’s approach modulates multiple biological pathways with a single agent, thus alleviating the need of combination therapies. With repurposing and repositioning already approved FDA drugs, PulmoSIM is targeting the underlying biology to effectively treat PAH and IPF. “We are thrilled to announce the launch of PulmoSIM as it demonstrates the indefinite capabilities of the VeriSIM’s platform, BIOiSIM, in accelerating drug development by bridging the translational gap between preclinical and clinical phases. In contrast to working with pharmaceutical and biotech companies in developing their drug portfolios, the PulmoSIM launch will be the first of many assets we will be developing to solve the most challenging and unaddressed diseases impacting human-kind”, said Dr. Jo Varshney. “Out of >7,000 rare diseases, many exist in hard-to-reach peripheral regions of the respiratory system. PulmoSIM aims to leverage the immense potential drug repurposing has to offer, to tackle at least a few of these rare diseases. Development of affordable, and commercially feasible therapies is one of the many ways to improve quality of life and compliance among the patients suffering from these indications”, said the Scientific Founder, Dr. Vivek Gupta. PulmoSIM is also pleased to announce the establishment of its Scientific Advisory Board: Dr. Eva S. Nozik, MD (Professor, Pediatrics-Critical Care Medicine, School of Medicine, University of Colorado, Denver, CO), Dr. Samir Mitragotri, PhD (Hiller Professor of Bioengineering and Wyss Professor of Biologically Inspired Engineering, Harvard University, Boston, MA), and Dr. Hugh Smyth, PhD (Alcon Centennial Professor of Molecular Pharmaceutics and Drug Delivery, University of Texas, Austin, TX). “Unlike current approaches that seek symptomatic relief, PulmoSIM’s approach is novel as it targets the biology of the disease. Targeting the underlying disease mechanisms opens the path to effective and life-saving therapies”, said Dr. Samir Mitragotri. The launch of PulmoSIM is an exciting venture for VeriSIM, and is the next phase in leveraging their first-in-class artificial intelligence platform to predict the clinical efficacy of drugs and propel drug development innovation forward. The PulmoSIM team is well-versed in respiratory diseases, product development, and has the experience, expertise, and relationships to efficiently address further operational, clinical, regulatory, competitive, and financial risks as we propel to clinical proof-of-concept. To learn more about our pipeline, approach, and how we can partner – contact us at: press@verisimlife.com About VeriSIM Life: VeriSIM is redefining the approach to preclinical and clinical drug development with an AI enabled platform that delivers bio-simulations and predicts the clinical outcomes of drugs before human trials. Our approach model is built to accurately translate, scale, and accelerate development; derisking R&D decisions and enabling pharmaceutical and biotech organizations with the insights to propel innovation forward. With our first-in-class, proprietary AI-enabled platform our team of scientists, modelers, chemists, and software engineers are your partners on a mission to advance human health. About Dr. Jyotika Varshney: Dr. Jo Varshney, DVM, PhD in comparative oncology/genomics, is the founder and CEO of VeriSIM and its wholly-owned subsidiary, PulmoSIM. She is a visionary in the field of personalized disease models and artificial intelligence/machine learning led drug development. She is the inventor of the core technology that utilizes a unique approach of integrating AI/ML techniques into the modeling paradigm to enhance the scalability and security to advanced software systems and can capitalize or capture the early drug development market. Dr. Varshney is a dynamic and celebrated leader and additionally, she serves on several advisory boards to help other deep-tech companies commercialize their science and technology. About Dr. Vivek Gupta: Dr. Vivek Gupta, PhD, is the scientific founder of PulmoSIM. Dr Gupta has 14+ years of experience in developing localized therapies for rare respiratory diseases including PAH and IPF. He also serves as an Assistant Professor of Pharmaceutical Sciences at St. John’s University, NY, where his research group focuses on novel drug discovery and delivery for respiratory disorders.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210322005089/en/ PulmoSIM Therapeutics // VeriSIM Life Contact Information: Alexandra Flecha-Hirsch, Senior Product Marketing Managerinfo@verisimlife.com 415-991-3783"
https://venturebeat.com/2021/03/22/flex-logix-raises-55m-to-design-ai-chips-for-edge-enterprise-applications/,Flex Logix raises $55M to design AI chips for edge enterprise applications,"Flex Logix, a startup designing reconfigurable AI accelerator chips, today announced that it closed a $55 million funding round led by Mithril Capital Management. CEO Geoff Tate says the funding will enable the company to build out its software, engineering, and customer support teams to accelerate the availability of its hardware and software for edge enterprise applications. AI accelerators are a type of specialized hardware designed to speed up AI applications, particularly neural networks, deep learning, and various form of machine learning. They focus on low-precision arithmetic or in-memory computing, which can boost the performance of large AI algorithms and lead to state-of-the-art results in natural language processing, computer vision, and other domains. That’s perhaps why they’re forecast to have a growing share of edge computing processing power, making up a projected 70% of it by 2025, according to a recent survey by Statista. Mountain View, California-based Flex Logix, which was founded in 2014, claims its AI inference chip — InferX X1 — is among the fastest and most efficient. The InferX1 outperforms Nvidia’s Xavier NX on the popular computer vision benchmark YOLOv3 and “real customer models,” according to Flex Logix, and the company says it’s targeting a price-to-performance ratio 10 to 100 times better than existing edge inference solutions. “Flex Logix set out to be for FPGA what Arm is for processors,” Tate told VentureBeat via email. “We believe this original eFPGA business can grow to be as big as Arm’s over time, while our second line of business is driving edge AI Inference capabilities into high volume applications, thus growing the market to the billions of dollars that market forecasters predict.” The InferX X1 also features what Flex Logix calls a reconfigurable tensor processor, nnMax, containing 64 processors coupled with SRAM that can be reprogrammed in 4 millionths of a second. In machine learning, a tensor is a generalization of vectors and matrices — representations of the data inputs, outputs, and transformations within neural networks. Flex Logix asserts that the nnMax is 3 to 18 times more efficient in terms of throughput per millimeter squared than the average Nvidia GPU.  “[The nnMax] reconfigures the 64 [processors] and RAM resources to efficiently implement a layer with a full bandwidth, dedicated data path, like an ASIC, then repeats this layer by layer,” Flex Logix explains on its website. “[We use] a new breakthrough interconnect architecture with less than half the silicon area of traditional mesh interconnect, fewer metal layers, higher utilization, and higher performance … We can easily scale up our architectures to deliver compute capacity of any size … using a patented tiling architecture with interconnects at the edge of the tiles that automatically form a larger array of any size.” On the software side, Flex Logix’s compiler takes models from machine learning frameworks including Google’s TensorFlow and ONNX and optimizes them for its nnMax and InferX1 architectures. A performance modeler is available now and in use by “dozens” of customers, and Flex Logix eventually plans to make available software drivers for operating systems commonly used in server and real-time scenarios. Flex Logix’s products have yet to come to market, but when they do, the company says they’ll be available in PCIe card and M.2 format for edge servers and gateways. A PCIe board containing the InferX1, X1P1, is expected to kick off production in Q2 2021 priced between $399 and $499, depending on the processor speed. A less powerful variant of the chip, InferX1 1KU, will cost between $99 and $199, with volume pricing reaching as low as $34 to $69.  Flex Logix has competition in a market that’s anticipated to reach $91.18 billion by 2025. In March 2020, Hailo, a startup developing hardware designed to speed up AI inferencing at the edge, nabbed $60 million in venture capital. California-based Mythic has raised $85.2 million to develop custom in-memory compute architecture. Graphcore, a Bristol, U.K.-based startup creating chips and systems to accelerate AI workloads, has a war chest in the hundreds of millions of dollars. And Baidu’s growing AI chip unit was recently valued at $2 billion after funding. But Flex Logix investor Ajay Royan points to Tate’s pedigree as one reason for his continued confidence. Tate previously managed AMD’s microprocessor and logic group, and he took his first startup, chip licensing firm Rambus, from four people and $2 million in equity to a Nasdaq IPO and multibillion-dollar market cap. Flex Logix says its revenue in 2020 was in the “double digits” millions and is expected to grow 50% to 100% this year. “We are impressed with the … architecture that Flex Logix has developed based on unique intellectual property that gives it a sustainable competitive advantage in a very high growth market,” Royan said in a press release. “This technology advantage positions Flex Logix for rapid growth in edge enterprise inference in applications such as medical, retail, industrial, robotics and more. It is even more impressive that they have done this with so little capital and at the same time built a cash-flow positive … business with large growth potential as system-on-chip designers look to incorporate reconfigurability into their communications and data centers.” Lux Capital, Eclipse Ventures, and the Tate Family Trust also participated in Flex Logix’s latest fundraising round, a series D. It brings the company’s total raised to date to $82 million."
https://venturebeat.com/2021/03/21/celonis-claims-the-low-code-movement-will-push-process-mining-past-rpa/,Celonis claims the low-code movement will push process mining past RPA,"Process mining technology provider Celonis says its revenues have more than doubled over the last year, and CEO Alexander Rinke says the low-code movement is set to help process mining overtake the more hyped robotic process automation (RPA) market. Process mining helps companies identify processes they can automate and so comes before RPA, which is where automation is implemented. The momentum in process mining is a sign that enterprises increasingly want to better understand and optimize business processes before they rush to digitize and automate them. Rinke said in an interview that the combination of process mining and low-code automation could overtake technologies like robotic process automation as the gateway for AI and automation. Rinke says this combination is more efficient and scalable than traditional RPA development techniques because it has less overhead associated with the UI. Rinke observed that process mining can also be used outside of automation to combine processes or streamline them in other ways. In another indication of his confidence in the technology, Rinke last month launched an automation challenge, promising companies with over $2 billion in revenues that Celonis will find $10 million in savings for them. If it doesn’t, Celonis has pledged to donate $100,000 to a nonprofit of the company’s choice. In the first month, more than 10,000 people have visited the challenge webpage and over 80 companies have expressed interest in taking part, Rinke said. The company also recently purchased low-code platform Integromat. Celonis is combining its process mining tool with Integromat’s technology to create an Execution Management System for automatically discovering, prioritizing, and implementing enterprise automation. All of the major RPA vendors, including UiPath, Automation Anywhere, and Blue Prism, are getting involved in process mining in various ways. Rinke argues that low-code approaches like Integromat’s promise better scalability than RPA for automating the opportunities process mining tools surface. Apps built to take advantage of native APIs have less overhead than those that emulate typing and mouse clicks. A 2020 report from the Everest Group technology advisory firm estimated that Celonis currently has about 60% of the process mining market and has seen a 400% growth in its year-over-year customer base. To use the mining analogy, process mining can automate the creation of enterprise treasure maps pointing to modern-day gold buried across various business processes. This is where Celonis has excelled. For example, Celonis has helped Comcast save $85 million, enabled an 800% improvement in touchless orders for L’Oréal, and reduced British Telecom’s customer service cycle times by 60% — according to Celonis’ statements over the past two years. It’s one thing to simply model how different kinds of sales, procurement, and payment processes work and another to correlate these models with specific dollar amounts. “It’s like an X-ray for your business,” Rinke said. Building a treasure map starts with plugging the process mining engine into core enterprise apps like SAP, Salesforce, and Oracle. This can be completed in a few hours if these are standard implementations. It can take a few weeks to fine-tune the process mining engine if the enterprise apps have been customized. Then the search for buried treasure can begin. Finance and business teams can run hundreds of customized analytics to help companies understand why they might be struggling to fill orders in a timely manner, what might be slowing down payment, and how different kinds of exceptions or delays may be inflicting direct costs on the business. “There are a lot of disgruntled paths outside of the happy path,” Rinke explained. For example, a process mining analysis might quantify the percentage of orders delayed due to factors like excess credit checks, improper inventory levels, or missing information. As teams begin exploring more exceptions, a seemingly simple business process begins to look like a bowl of spaghetti. “I always tell the CFOs we talk to that if your company does not look like this [bowl of spaghetti], you will get your money back, but that never happens,” Rinke said. “And then we can really walk to the punch line and quantify how much this is them, and that’s how we came up with the $10 million.” Common examples of savings include quantifying how many orders get canceled, quantifying the value of the net promoter score, and assessing the impact of a reduced on time delivery rate. The next step lies in fixing things, and this is where the combination of process mining and low-code capabilities begins to directly compete with RPA for enterprise automation mindshare. Celonis is betting it can capture this mindshare by mining enterprise data to prioritize opportunities for cost savings through new automations. Perhaps more significantly, the company is also growing an enterprise app market that allows accounting, systems integration, and consulting partners to package their expertise for solving specific types of industry problems. Partners include companies like Protiviti, EY, and Deloitte. Rinke predicted, “There will be an explosion of creation of third-party apps on top of process mining because when you unleash the data, you can put a lot of things on top of it.” Dell Technologies has developed an internal app called Predictive Case Intelligence (PCI) that combines Celonis’ process mining tools, AI, and analytics to identify and predict potential issues in customer IT support. This helped reduce the time it takes to resolve customer support cases by up to 10% and improved customer satisfaction ratings. “Our customers are happier, as they are proactively informed of any potential delays, feel more confident in their support agent and the process, and benefit from faster issue resolution,” said Amit Sawhney, vice president of services operations at Dell Technologies. He said it is important to clearly identify objectives when considering the use of process mining, especially when processing data from various sources. A clear objective of reducing resolution time improved the use of process mining to identify important patterns captured during the customer journey across multiple systems. Additionally, it’s important to develop an environment that supports scalability and connectivity for process mining. For example, Dell connected multiple cloud platforms, including its CRM systems, with on-premises systems to scale this application and improve its accuracy. Rinke said the idea for the company came after he read a paper by Wil van der Aalst about the new science of process mining. A few months later, Rinke got a job at a German media company that was trying to reduce the time it took to resolve customer complaints. The company started off using a manual and slow approach to process mapping that involved a lot of interviews. “I wondered if we could just use process mining to get all the data automatically without having to do all of these interviews,” he said. “We tried the open source process mining tools, but you needed advanced math skills to make it work.” Eventually, he helped the company reduce ticket resolution times from an average of five days to resolving 80% on the first day. “We never had the plan to start a company, but that first company got us so excited we decided to build a software product.” In the early days, the team explored various strategies for marketing with almost no budget. “The first thing we did was write letters by hand,” Rinke said, “When an assistant got a handwritten letter, it could be their boss’ grandmother and so they did not want to throw it in the garbage. That’s how we signed Bayer Pharmaceuticals.” The company also faced challenges hiring its first sales team. Rinke said, “We always had great engineers, but we did not have a good sales team because we had no idea how to do it. So I was driving around in a VW bus selling myself because we had to get some revenue. In the early days, no VC wanted to invest in us.” But once they worked out the bugs with sales, things took off. The company has raised $370 million from Accel, 83North, and other VC firms since it launched in 2011 and has a valuation of over $2.5 billion"
https://venturebeat.com/2021/03/20/how-synthetic-data-could-save-ai/,How synthetic data could save AI,"AI is facing several critical challenges. Not only does it need huge amounts of data to deliver accurate results, but it also needs to be able to ensure that data isn’t biased, and it needs to comply with increasingly restrictive data privacy regulations. We have seen several solutions proposed over the last couple of years to address these challenges — including various tools designed to identify and reduce bias, tools that anonymize user data, and programs to ensure that data is only collected with user consent. But each of these solutions is facing challenges of its own. Now we’re seeing a new industry emerge that promises to be a saving grace: synthetic data. Synthetic data is artificial computer-generated data that can stand-in for data obtained from the real world. A synthetic dataset must have the same mathematical and statistical properties as the real-world dataset it is replacing but does not explicitly represent real individuals. Think of this as a digital mirror of real-world data that is statistically reflective of that world. This enables training AI systems in a completely virtual realm. And it can be readily customized for a variety of use cases ranging from healthcare to retail, finance, transportation, and agriculture. There’s significant movement happening on this front. More than 50 vendors have already developed synthetic data solutions, according to research last June by StartUs Insights. I will outline some of the leading players in a moment. First, though, let’s take a closer look at the problems they’re promising to solve. Over the last few years, there has been increasing concern about how inherent biases in datasets can unwittingly lead to AI algorithms that perpetuate systemic discrimination. In fact, Gartner predicts that through 2022, 85% of AI projects will deliver erroneous outcomes due to bias in data, algorithms, or the teams responsible for managing them. The proliferation of AI algorithms has also led to growing concerns over data privacy. In turn, this has led to stronger consumer data privacy and protection laws in the EU with GDPR, as well as U.S. jurisdictions including California and most recently Virginia. These laws give consumers more control over their personal data. For example, the Virginia law grants consumers the right to access, correct, delete, and obtain a copy of personal data as well as to opt out of the sale of personal data and to deny algorithmic access to personal data for the purposes of targeted advertising or profiling of the consumer. By restricting access to this information, a certain amount of individual protection is gained but at the cost of the algorithm’s effectiveness. The more data an AI algorithm can train on, the more accurate and effective the results will be. Without access to ample data, the upsides of AI, such as assisting with medical diagnoses and drug research, could also be limited. One alternative often used to offset privacy concerns is anonymization. Personal data, for example, can be anonymized by masking or eliminating identifying characteristics such as removing names and credit card numbers from ecommerce transactions or removing identifying content from healthcare records. But there is growing evidence that even if data has been anonymized from one source, it can be correlated with consumer datasets exposed from security breaches. In fact, by combining data from multiple sources, it is possible to form a surprisingly clear picture of our identities even if there has been a degree of anonymization. In some instances, this can even be done by correlating data from public sources, without a nefarious security hack. Synthetic data promises to deliver the advantages of AI without the downsides. Not only does it take our real personal data out of the equation, but a general goal for synthetic data is to perform better than real-world data by correcting bias that is often engrained in the real world. Although ideal for applications that use personal data, synthetic information has other use cases, too. One example is complex computer vision modeling where many factors interact in real time. Synthetic video datasets leveraging advanced gaming engines can be created with hyper-realistic imagery to portray all the possible eventualities in an autonomous driving scenario, whereas trying to shoot photos or videos of the real world to capture all these events would be impractical, maybe impossible, and likely dangerous. These synthetic datasets can dramatically speed up and improve training of autonomous driving systems. (Above image: Synthetic images are used to train autonomous vehicle algorithms. Source: synthetic data provider Parallel Domain.) Perhaps ironically, one of the primary tools for building synthetic data is the same one used to create deepfake videos. Both make use of generative adversarial networks (GAN), a pair of neural networks. One network generates the synthetic data and the second tries to detect if it is real. This is operated in a loop, with the generator network improving the quality of the data until the discriminator cannot tell the difference between real and synthetic. Forrester Research recently identified several critical technologies, including synthetic data, that will comprise what they deem “AI 2.0,” advances that radically expand AI possibilities. By more completely anonymizing data and correcting for inherent biases, as well as creating data that would otherwise be difficult to obtain, synthetic data could become the saving grace for many big data applications. Synthetic data also comes with some other big benefits: You can create datasets quickly and often with the data labeled for supervised learning. And it does not need to be cleaned and maintained the way real data does. So, theoretically at least, it comes with some large time and cost savings. Several well-established companies are among those that generate synthetic data. IBM describes this as data fabrication, creating synthetic test data to eliminate the risk of confidential information leakage and address GDPR and regulatory issues. AWS has developed in-house synthetic data tools to generate datasets for training Alexa on new languages. And Microsoft has developed a tool in collaboration with Harvard with a synthetic data capability that allows for increased collaboration between research parties. Notwithstanding these examples, it is still early days for synthetic data and the developing market is being led by the startups. To wrap up, let’s take a look at some of the early leaders in this emerging industry. The list is constructed based on my own research and industry research organizations including G2 and StartUs Insights. Gary Grossman is the Senior VP of Technology Practice at Edelman and Global Lead of the Edelman AI Center of Excellence."
https://venturebeat.com/2021/03/20/how-many-robot-helpers-are-too-many/,How many robot helpers are too many?,"AI that can follow a person seems like a simple enough task. It’s certainly a simple thing to ask a human to do, but what if people or objects get in the way of the robot following behind a person? How do you navigate an environment that’s in a constant state of change? About a year ago, at a robotics conference TechCrunch held at UC Berkeley, AI startup founders explored solutions for common problems encountered when trying to automate construction projects. Dusty Robotics CEO Tessa Lau called attention to the challenge of moving machines in an unstructured environment filled with people. “[The] typical construction site — it’s chaos, and anyone with a robotics background who knows anything about robotics knows it’s really hard to make robots work in that kind of unstructured environment,” she said. That’s why this week Piaggio Fast Forward, maker of the Gita personal robot, shared details about work it’s undertaking with industrial technology services provider Trimble using the Boston Dynamics API to create robots that follow construction workers. Pilot tests took place at an office building under construction in Colorado. Robots can also travel in groups with one Spot Mini and two Gita robots in what Piaggio Fast Forward calls platooning. As part of the construction pilot, Piaggio is assessing human attitudes about how many robots following a human is too many. Optimizing the number of robots following people on a construction site concerns not only whether the robot can make it safely out of the path of a dozer driven by a human, but also the question of how many robots can be involved before things get weird. Like, if you’re a construction worker watching the site manager approach with a robot entourage, is five robots the limit? Six? Piaggio Fast Forward CEO Greg Lynn told VentureBeat the company could support a convoy of 50 to 100 robots but that this would be impractical. “How long a platoon of robots, just as a dimension, would be acceptable? It’s probably like 15 or 20 feet. To be totally honest, we don’t know yet, but it’s not an infinite length,” he said.  The approach uses Trimble lasers and geospatial sensors to map the environment and establish location. One of the tests of Spot Mini involved training a robot to walk a path once and then having it try to repeat that route automatically. The approach follows how people navigate the landscape, understanding human movement instead of relying only on mapping. In 2019, Piaggio Fast Forward introduced its Gita robot to follow behind people and carry up to 40 pounds of weight. The device can reach speeds of 22 mph. The Trimble collaboration is the first industrial deployment of Piaggio tracking technology, Lynn told VentureBeat. Piaggio Fast Forward is a part of the Piaggio Group, one of the largest scooter and motorcycle manufacturers in Europe, best known for its Vespa brand. Autonomous trucks have worked in convoys before, but as part of a drive into more business applications, a spokesperson told VentureBeat that next month a company will test platooning for grocery delivery in a planned community with several thousand households. As part of trials announced in late 2020 without platooning, Gita robots are being used in Cincinnati and San Diego airports and, with Doğan Group, a shopping mall in Turkey. Learning to navigate unstructured environments like construction zones can lead to more robust AI that can help farmers in fields, delivery robots in cities, or people in wheelchairs on sidewalks. Should such systems become trusted and reliable, they could become commonplace in industrial environments for surveillance — but going further, why push a wheelbarrow if you can just have it follow you? Ideally, a robot companion on a worksite could augment human workers by carrying tools for an electrician, as the company’s video demonstrates. They could also use computer vision to bring attention to safety hazards or survey the progress of construction projects. One of the earliest applications of Spot Mini robots on construction sites was to survey construction projects. Spot Mini is also being used in law enforcement by the New York Police Department, which posits the scenario of a Spot Mini robot following a police officer walking a beat. Piaggio is not alone in its ambition to implement people-tracking tech in the industrial workplace. In 2018, VentureBeat covered a $10 million funding round for ForwardX, which at the time was known for luggage that followers travelers. A company spokesperson told VentureBeat that the company, which is based in China, pivoted away from luggage during the pandemic and now focuses on people-tracking solutions for robots that work alongside humans in warehouses and manufacturing facilities. Thus far, ForwardX has run test cases in factories for companies like DHL and Toyota, among other customers.  As AI companies like Piaggio Fast Forward were exploring opportunities in the construction industry, traditional players like Caterpillar and Komatsu boosted sales of autonomous software last year for industrial environments like construction, mining, and space. Gita robots aren’t designed for roving around in a landscape of loose dirt where dozers and earthmovers operate, but Lynn said once slabs of concrete are laid for a building project, initial tests showed few areas the robots were unable to travel. “One of the things about the Trimble announcement is we’re kind of announcing we don’t have to build every robot on Earth. We want to get involved with people that have problems with this because we’re a software company as much as we are a hardware robotics company,” Lynn said. “We don’t want to do it all ourselves. And we don’t think that it’s 100% of the solution, either.” There are a number of AI startups in the business of automating the construction zone. In 2019, Built Robotics spoke to VentureBeat about efforts to automate dozers and create predictive systems for project management and other purposes. Last fall, Canvas emerged from stealth to bring AI and robotics into construction zones in the San Francisco Bay Area to install drywall. And in January Swapp raised $7 million to compete among a fleet of businesses working to automate the mapping of construction project planning. Robotic deployments in construction environments have led to some accidents, however. According to the safety and compliance website BLR, in 2019 two construction workers were injured in accidents involving demolition robots on construction sites in the state of Washington, with robots pinning one worker against a wall and crushing the foot of another person. In other news about robots navigating unstructured or industrial settings, in May 2020 Burro, which is developing robots that follow farmworkers, assisted the grape harvest in Coachella, California. And last fall UC Berkeley AI researchers introduced LaND, AI that learns from disengagement episodes to improve delivery robots’ navigation on sidewalks."
https://venturebeat.com/2021/03/19/why-machine-learning-struggles-with-causality/,Why machine learning struggles with causality,"When you look at a baseball player hitting the ball, you can make inferences about causal relations between different elements. For instance, you can see the bat and the baseball player’s arm moving in unison, but you also know that it is the player’s arm causing the bat’s movement and not the other way around. You also don’t need to be told that the bat is causing the sudden change in the ball’s direction. Likewise, you can think about counterfactuals, such as what would happen if the ball flew a bit higher and didn’t hit the bat.  Such inferences come to us humans intuitively. We learn them at a very early age, without being explicitly instructed by anyone and just by observing the world. But for machine learning algorithms, which have managed to outperform humans in complicated tasks such as Go and chess, causality remains a challenge. Machine learning algorithms, especially deep neural networks, are especially good at ferreting out subtle patterns in huge sets of data. They can transcribe audio in real time, label thousands of images and video frames per second, and examine X-ray and MRI scans for cancerous patterns. But they struggle to make simple causal inferences like the ones we just saw in the baseball example above. In a paper titled “Towards Causal Representation Learning,” researchers at the Max Planck Institute for Intelligent Systems, the Montreal Institute for Learning Algorithms (Mila), and Google Research discuss the challenges arising from the lack of causal representations in machine learning models and provide directions for creating artificial intelligence systems that can learn causal representations. This is one of several efforts that aim to explore and solve machine learning’s lack of causality, which can be key to overcoming some of the major challenges the field faces today. Why do machine learning models fail at generalizing beyond their narrow domains and training data? “Machine learning often disregards information that animals use heavily: interventions in the world, domain shifts, temporal structure — by and large, we consider these factors a nuisance and try to engineer them away,” write the authors of the causal representation learning paper. “In accordance with this, the majority of current successes of machine learning boil down to large scale pattern recognition on suitably collected independent and identically distributed (i.i.d.) data.” A common machine learning term, “i.i.d.” supposes that random observations in a problem space are not dependent on each other and have a constant probability of occurring. The simplest example of i.i.d. is flipping a coin or tossing a die. The result of each new flip or toss is independent of previous ones, and the probability of each outcome remains constant. When it comes to more complicated areas such as computer vision, machine learning engineers try to turn the problem into an i.i.d. domain by training the model on very large corpora of examples. The assumption is that with enough examples the machine learning model will be able to encode the general distribution of the problem into its parameters. But in the real world, distributions often change due to factors that cannot be considered and controlled in the training data. For instance, convolutional neural networks trained on millions of images can fail when they see objects under new lighting conditions or from slightly different angles or against new backgrounds. Efforts to address these problems mostly include training machine learning models on more examples. But as the environment grows in complexity, it becomes impossible to cover the entire distribution by adding more training examples. This is especially true in domains where AI agents must interact with the world, such as robotics and self-driving cars. Lack of causal understanding makes it very hard to make predictions and deal with novel situations. This is why you see self-driving cars make weird and dangerous mistakes even after having trained for millions of miles. “Generalizing well outside the i.i.d. setting requires learning not mere statistical associations between variables, but an underlying causal model,” the AI researchers write. Causal models also allow humans to repurpose previously gained knowledge for new domains. For instance, when you learn a real-time strategy game such as Warcraft, you can quickly apply your knowledge to other similar games StarCraft and Age of Empires. Transfer learning in machine learning algorithms, however, is limited to very superficial uses, such as fine-tuning an image classifier to detect new types of objects. In more complex tasks, such as learning video games, machine learning models need huge amounts of training (thousands of years’ worth of play) and respond poorly to minor changes in the environment (e.g., playing on a new map or with a slight change to the rules). “When learning a causal model, one should thus require fewer examples to adapt as most knowledge, i.e., modules, can be reused without further training,” the authors of the causal machine learning paper write.  So, why has i.i.d. remained the dominant form of machine learning despite its known weaknesses? Pure observation-based approaches are scalable. You can continue to achieve incremental gains in accuracy by adding more training data, and you can speed up the training process by adding more compute power. In fact, one of the key factors behind the recent success of deep learning is the availability of more data and stronger processors. In addition, i.i.d.-based models are easy to evaluate. Take a large dataset, split it into training and test sets, tune the model on the training data, and validate its performance by measuring the accuracy of its predictions on the test set. Continue the training until you reach the accuracy you require. There are already many public datasets that provide such benchmarks, such as ImageNet, CIFAR-10, and MNIST. There are also task-specific datasets such as the COVIDx dataset for COVID-19 diagnosis and the Wisconsin Breast Cancer Diagnosis dataset. In all cases, the challenge is the same: Develop a machine learning model that can predict outcomes based on statistical regularities. But as the AI researchers observe in their paper, accurate predictions are often not sufficient to inform decision-making. For instance, during the coronavirus pandemic, many machine learning systems began to fail because they had been trained on statistical regularities instead of causal relations. As life patterns changed, the accuracy of the models dropped. Causal models remain robust when interventions change the statistical distributions of a problem. For instance, when you see an object for the first time, your mind will subconsciously factor out lighting from its appearance. That’s why, in general, you can recognize the object when you see it under new lighting conditions. Causal models also allow us to respond to situations we haven’t seen before and think about counterfactuals. We don’t need to drive a car off a cliff to know what will happen. Counterfactuals play an important role in cutting down the number of training examples a machine learning model needs. Causality can also be crucial to dealing with adversarial attacks, subtle manipulations that force machine learning systems to fail in unexpected ways. “These attacks clearly constitute violations of the i.i.d. assumption that underlies statistical machine learning,” the authors of the paper write, adding that adversarial vulnerabilities are proof of the differences in the robustness mechanisms of human intelligence and machine learning algorithms. The researchers also suggest that causality can be a possible defense against adversarial attacks. In a broad sense, causality can address machine learning’s lack of generalization. “It is fair to say that much of the current practice (of solving i.i.d. benchmark problems) and most theoretical results (about generalization in i.i.d. settings) fail to tackle the hard open challenge of generalization across problems,” the researchers write. In their paper, the AI researchers bring together several concepts and principles that can be essential to creating causal machine learning models. Two of these concepts include “structural causal models” and “independent causal mechanisms.” In general, the principles state that instead of looking for superficial statistical correlations, an AI system should be able to identify causal variables and separate their effects on the environment. This is the mechanism that enables you to detect different objects regardless of the view angle, background, lighting, and other noise. Disentangling these causal variables will make AI systems more robust against unpredictable changes and interventions. As a result, causal AI models won’t need huge training datasets. “Once a causal model is available, either by external human knowledge or a learning process, causal reasoning allows [it] to draw conclusions on the effect of interventions, counterfactuals, and potential outcomes,” the authors of the causal machine learning paper write. The authors also explore how these concepts can be applied to different branches of machine learning, including reinforcement learning, which is crucial to problems where an intelligent agent relies heavily on exploring environments and discovering solutions through trial and error. Causal structures can help make the training of reinforcement learning more efficient by allowing enabling agents to make informed decisions from the start of their training instead of taking random and irrational actions. The researchers provide ideas for AI systems that combine machine learning mechanisms and structural causal models: “To combine structural causal modeling and representation learning, we should strive to embed an SCM into larger machine learning models whose inputs and outputs may be high-dimensional and unstructured, but whose inner workings are at least partly governed by an SCM (that can be parameterized with a neural network). The result may be a modular architecture, where the different modules can be individually fine-tuned and repurposed for new tasks.” Such concepts bring us closer to the modular approach the human mind uses (at least as far as we know) to link and reuse knowledge and skills across different domains and areas of the brain.  It is worth noting, however, that the ideas presented in the paper are at the conceptual level. As the authors acknowledge, implementing these concepts requires facing several challenges: “(a) in many cases, we need to infer abstract causal variables from the available low-level input features; (b) there is no consensus on which aspects of the data reveal causal relations; (c) the usual experimental protocol of training and test set may not be sufficient for inferring and evaluating causal relations on existing data sets, and we may need to create new benchmarks, for example with access to environment information and interventions; (d) even in the limited cases we understand, we often lack scalable and numerically sound algorithms.” But what’s interesting is that the researchers draw inspiration from much of the parallel work being done in the field. The paper contains references to the work done by Judea Pearl, a Turing Award–winning scientist best known for his work on causal inference. Pearl is a vocal critic of pure deep learning methods. Meanwhile, Yoshua Bengio, one of the co-authors of the paper and another Turing Award winner, is one of the pioneers of deep learning. The paper also contains several ideas that overlap with the idea of hybrid AI models proposed by Gary Marcus that combines the reasoning power of symbolic systems with the pattern recognition power of neural networks. The paper does not, however, make any direct reference to hybrid systems. The paper is also in line with system 2 deep learning, a concept first proposed by Bengio in a talk at the NeurIPS 2019 AI conference. The idea behind system 2 deep learning is to create a type of neural network architecture that can learn higher representations from data. Higher representations are crucial to causality, reasoning, and transfer learning. While it’s not clear which of the several proposed approaches will help solve machine learning’s causality problem, the fact that ideas from different — and often conflicting — schools of thought are coming together is guaranteed to produce interesting results. “At its core, i.i.d. pattern recognition is but a mathematical abstraction, and causality may be essential to most forms of animate learning,” the authors write. “Until now, machine learning has neglected a full integration of causality, and this paper argues that it would indeed benefit from integrating causal concepts.” Ben Dickson is a software engineer and the founder of TechTalks. He writes about technology, business, and politics. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/03/19/with-rpa-on-the-rise-security-challenges-remain/,"AI Weekly: With RPA on the rise, security challenges remain",
https://venturebeat.com/2021/03/19/how-adaptive-computing-solves-ai-productization-challenges/,How adaptive computing solves AI productization challenges,"The field of artificial intelligence moves swiftly, with the pace of innovation only accelerating. While the software industry has been successful in deploying AI in production, the hardware industry — including automotive, industrial, and smart retail — is still in its infancy in terms of AI productization. Major gaps still exist that hinder AI algorithm proofs-of-concept (PoC) from becoming real hardware deployments. These drawbacks are largely due to small data problems, “non-perfect” inputs, and ever-changing “state-of-the-art” models. How can software developers and AI scientists overcome these challenges? The answer lies in adaptable hardware. Internet giants, such as Google and Facebook, routinely collect and analyze massive amounts of data every day. They then use this data to create AI models that have acceptable performance off the bat. In such cases, the hardware used to train the models is very different from the hardware used to run the models. On the other hand, in the hardware industry, the availability of big data is much more limited, resulting in less mature AI models. Therefore, there is a major push to collect more data and run “online models,” where training and inference are performed on the same deployed hardware to continuously improve the accuracy. To address this, adaptive computing — such as field-programmable gate arrays (FPGA) and adaptable system-on-chip (SoC) devices that are proven on the edge — can run both inference and training to constantly update themselves to the newly captured data. Traditional AI training requires the cloud or large on-premise data centers and takes days and weeks to perform. The real data, on the other hand, is generated mostly at the edge. Running both AI inference and training in the same edge device not only improves the total cost of ownership (TCO) but also reduces latency and security bleaches. While it’s becoming easier to publish an AI model PoC to show something like better accuracy of COVID-19 detection using X-ray images as an example, these PoCs are almost always based on well-cleaned-up input pictures. In real life, camera and sensor inputs from medical devices, robots, and moving cars will have random distortion such as dark images and various angled objects. These inputs first need to be processed by sophisticated preprocessing to clean up and reformat before they can be fed into AI models. Postprocessing is very important to make sense of the AI model outputs and calculate the proper decision making. Indeed, some chips may be very good at AI inference acceleration, but they almost always accelerate only a portion of a full application. Using smart retail as an example, pre-process includes many-stream video decode followed by conventional computer vision algorithms to resize, reshape and format convert the videos. Post-processing also includes object tracking and database look-up. End customers care less about the speed the AI inference runs at but whether they can meet the video stream performance and/or real-time responsiveness of the full application pipeline. FPGAs and adaptable SoCs have a proven track record of accelerating these pre- and post-processing algorithms using domain-specific architectures (DSAs). Plus, adding an AI inference DSA will allow the whole system to be optimized to meet the product requirements from end-to-end. The AI research community is arguably the most active with new AI models being invented daily by top researchers around the world. These models improve accuracy, reduce computational requirements, and address new types of AI applications. This fast innovation continues to put pressure on existing semiconductor hardware devices, demanding newer architecture to efficiently support the modern algorithms. Standard benchmarks, such as MLPerf, prove that state-of-the-art CPUs, GPUs, and AI ASIC chips fall well below 30 percent of the vendor advertised performance when running real-life AI workloads. This is constantly pushing the need for new DSA to keep up with the innovation. There are several recent trends that are pushing the need for new DSAs. Depthwise convolution is an emerging layer that requires large memory bandwidth and specialized internal memory caching to be efficient. Typical AI chips and GPUs have fixed L1/L2/L3 cache architecture and limited internal memory bandwidth resulting in very low efficiency. Researchers are constantly inventing new custom layers that today’s chips do not support natively. Because of this, they need to be run on host CPUs without acceleration, often becoming the performance bottleneck. Sparse Neural Network is another promising optimization where networks are heavily pruned, sometimes up to 99 percent, by trimming network edges, removing fine-grained matrix values in convolution, etc. However, to run this efficiently in hardware, you need specialized sparse architecture, plus an encoder and decoder for these operations which most chips simply do not have. Binary / Ternary are the extreme optimizations, transforming all math operations to bit-manipulation operations. Most AI chips and GPUs only have 8 bit, 16 bit, or floating-point calculation units so you will not gain any performance or power efficiency by doing extreme low precisions. FPGAs and adaptable SoCs are perfect as a developer can develop the perfect DSA and reprogram the existing device for the very workload for the product. As a proof point, the latest MLPerf included a submission by Xilinx, collaborating with Mipsology, that achieved 100 percent of the hardware datasheet performance using the ResNet-50 standard benchmark. Historically, the biggest challenge for FPGAs and adaptable SoCs has been the need for hardware expertise to implement and deploy DSAs. The good news is that now there are tools — like the Vitis unified software platform — that support C++, Python, and popular AI frameworks like TensorFlow and PyTorch, closing the gap for software and AI developers. In addition to more development in software abstraction tools, open-source libraries, such as the Vitis hardware-accelerated libraries, are significantly boosting adoption within the developer community. In the most recent design contest, Xilinx was able to attract more than 1,000 developers and published many innovative projects, from a hand-gesture-controlled drone to reinforcement learning using a binarized neural network. Importantly, most of the projects submitted were by software and AI developers who had no previous experience with FPGAs. This is proof that the FPGA industry is taking the right steps to enable software and AI developers to solve real-world AI productization challenges. Up until recently, unlocking the power of hardware adaptability was unattainable for the average software developer and AI scientist. Specific hardware expertise was previously required but thanks to new open-source tools, software developers are now empowered with adaptable hardware. With this new ease of programming, FPGAs and adaptable SoCs will continue to become more accessible to hundreds of thousands of software developers and AI scientists, making these devices the hardware solution of choice for next-generation applications. Indeed, DSAs will represent the future of AI inference with software developers and AI scientists harnessing hardware adaptability for their next-generation applications. This post was written by Nick Ni, the Director of AI Products — software and ecosystem at Xilinx, for TechTalks. Ni has a master’s degree in Computer Engineering from the University of Toronto and holds over 10 patents and publications. This story originally appeared on Bdtechtalks.com. Copyright 2021"
https://venturebeat.com/2021/03/19/the-deanbeat-stopasianhate-by-making-us-more-visible/,The DeanBeat: #StopAsianHate by making us more visible,"Bear with me on this column, because I’m angry, and I’d like to make a reasoned argument that the game industry can do something about the cause of the moment — #StopAsianHate. Moments like these are opportunities, because someone is probably listening. It has always been easy to pick on Asian Americans and Pacific Islanders because we are such a small part of the population in the U.S. Haters can think of us as “others,” or people who are out of sight, not easily understood, easily stereotyped, and less than fully human. (For more on that last claim, check out the book War Without Mercy: Race and Power in the Pacific War by John Dower.) When former U.S. President Donald Trump repeatedly talked about the “China Virus” or “kung flu,” he did so in a calculated way. Asian Americans are just 5.6% of the U.S. population. He could afford to offend us because we weren’t big enough to vote him out of office. He treated other minority groups no better. He was confident his base would reward him for being openly racist. No one stopped the bully. Fox News reveled in his clever wordplay. The predictable downstream effect of this is blood running in the streets. The thing that all minority groups share is that we never have enough people to protect us when things get bad. We couldn’t stop the internment of 110,000 Japanese Americans — including my father and many members of my extended family — in incarceration camps in World War II. Last week others protested the “cancel culture” that got six books by the beloved Dr. Seuss pushed out of publication. I tried to remind people on Facebook that Seuss was initially for the internment, and he created some racist images that stirred hate against Japanese Americans. Later on, he apologized. In the children’s book, Horton Hears a Who, he wrote a dedication to a Japanese friend. The book’s refrain is, “A person is a person no matter how small.” This week, things got bad again. A man in Georgia killed eight people at three spas, where six of the victims were Asian women. While the true motivation behind the killings isn’t known, it was part of a disturbing trend. Activist group Stop AAPI Hate reported this week that it has received 3,795 reports of hate incidents against Asian Americans and Pacific Islanders during the past year. I recall in the 1990s, when I worked at the Los Angeles Times, we were about 3% of the population — too small to be included in polls that broke out the opinions of populations that included Latinx, Blacks, and whites. I complained about this, and the pollster and editor explained that not enough Asian Americans responded to be statistically significant. I felt that they could find the Asian American responses if only they were willing to spend more money on their polls. But the net result was that we were invisible. It felt like nobody really wanted to know our opinions because they assumed that we were more like white people than other minority groups. Asian Americans have been cast as “model minorities,” an attitude that surfaced in the 1960s. Proponents of the myth used it to show that America wasn’t racist. If Asian Americans could succeed, then the American system was fair, and so Black and LatinX people should be able to succeed too. It was a myth that left behind many Asian Americans who weren’t successful, and it was a way to pit minority groups against each other. The question arises. Is it better to be invisible or better to be viewed as a model minority? The answer, of course, is that it is better to be viewed as individual people. I grew up stereotypically quiet. I would physically shake and have to steady myself before I spoke up in class. It took me years to find my voice, and to realize that I was more at home in an English literature or journalism class than I was in business, engineering, or science classes. Years later, I found some solace in the book Quiet: The Power of Introverts in a World That Can’t Stop Talking.  And so I understood the urge older generations of Asian Americans had to be quiet. To comply. To not talk about it when they were victimized, harassed, bullied, or looked over. The Japanese have a phrase, shikata ga nai, or “it cannot be helped.” And deru kui wa utareru, or “the nail that sticks up gets hammered down.” But newer generations said to hell with that. They fought for our rights, vetoed victimhood, and got us an apology from the U.S. government for the wartime incarceration. At this time, I’m heartened to see people speaking out. Jeremy Lin, the NBA basketball player, and actress Olivia Munn have done admirable jobs speaking out against the hate and giving the mainstream faces and voices to listen to. And I was inspired by Ashlyn So, a 13-year-old middle school student who organized a march against hate in San Mateo, California, a few weeks ago. My wife and I attended that march, and it was good to see hundreds of people march together — including non-Asian people — against anti-Asian hate.  I was also delighted to see the game industry speak out on our behalf. Bandai Namco US tweeted, “As a company based in Japan, the recent rise in anti-Asian hate has truly hit home for our employees. We always appreciate support from our fans and we now ask for your support to #StopAsianHate and fight the rise in hate crimes committed against people of Asian descent.” Xbox chief Phil Spencer tweeted, “Hate has no home on Xbox. Team Xbox is appalled by the violence and racism against Asian communities. For all Asian players and peers, I want you to know that we see you, we hear you, and we will and must do more together to #StopAsianHate.”  Bethesda tweeted that it “rejects racism in all forms and stands in solidarity with our Asian employees and the AAPI community.” Sony Interactive Entertainment said it was “troubled by a recent pattern of violence targeting the Asian American and Pacific Islander communities.” Ubisoft said it “condemns the horrific racist and xenophobic attacks against the Asian American and Pacific Islander communities.” But here’s the next step, and it’s similar to the next step people can take to follow up on last year’s commitments in the wake of the Black Lives Matter protests. Make us visible. Give us representation. Show us in your games. I remembered when I played Prey, the triple-A shooter game from Bethesda’s Arkane Studios. It struck me because the main character was an Asian American male. I had never seen that before, and so I became lost in thought while I played the game. I was struck again when a few major characters in The Last of Us Part II — including the transgender Lev, the protective Yara, and the macho Jesse — were given memorable roles in a mainstream video game. And I was overjoyed to see yesterday that Square Enix and Deck Nine Games included an Asian American woman, Alex Chen, as the main character of the upcoming Life is Strange: True Colors game coming on September 10. I don’t want us to be invisible anymore. It’s too dangerous. It allows ignorant people like Trump to define us. Represent us as human and normal so that it becomes a little harder to dehumanize us. And thank you for the diversity that you’re designing into your games."
https://venturebeat.com/2021/03/19/ibms-rob-thomas-details-key-ai-trends-in-shift-to-hybrid-cloud/,IBM’s Rob Thomas details key AI trends in shift to hybrid cloud,"The last year has seen a major spike in the adoption of AI models in production environments, in part driven by the need to drive digital business transformation initiatives. While it’s still early days as far as AI is concerned, it’s also clear AI in the enterprise is entering a new phase. Rob Thomas, senior vice president for software, cloud, and data platform at IBM, explains to VentureBeat how this next era of AI will evolve as hybrid cloud computing becomes the new norm in the enterprise. As part of that effort, Thomas reveals IBM has formed a software-defined networking group to extend AI all the way out to edge computing platforms. This interview has been edited for brevity and clarity. VentureBeat: Before the COVID-19 pandemic hit, there was a concern AI adoption was occurring slowly. How much has that changed in the past year? Rob Thomas: We’ve certainly got massive acceleration for things like Watson Assistant for customer service. That absolutely exploded. We had nearly 100 customers that started and then went live in the first 90 days after COVID hit. When you broaden it out, there are five big use cases that have come up over the last year. One is customer service. Second is around financial planning and budgeting. Thirdly are things such as data science. There’s such a shortage of data science skills, but that is slowly changing. Fourth is around compliance. Regulatory compliance is only increasing, not decreasing. And then fifth is AI Ops. We launched our first AI ops product last June and that’s exploded as well, which is related to COVID in that everybody was forced remote. How do we better manage our IT systems? It can’t be all through humans because we’re not on site. We’ve got to use software to do that. I think that was 18 months ago, I wouldn’t have given you those five. I would have said “There’s a bunch of experimentations.” Now we see pretty clearly there are five things people are doing that represent 80% of the activity. VentureBeat: Should organizations be in the business of building AI or should they buy it in one form or another? Thomas: I hate to be too dramatic, but we’re probably in a permanent and a secular change where people want to build. Trying to fight that is a tough discussion because people really want to build. When we first started with Watson, the idea was this is a big platform. It does everything you need. I think what we’ve discovered along the way is if you componentize to focus where we think we’re really good, people will pick up those pieces and use them. We focused on three areas for AI. One is natural language processing (NLP). I think if you look at things like external benchmarks, we had the best NLP from a business context. In terms of document understanding, semantic parsing of text, we do that really well. The second is automation. We’ve got really good models for how you automate business processes. Third is trust. I don’t really think anybody is going to invest to build a data lineage model, explainability model, or bias detection. Why would a company build that? That’s a component we can provide. If you want them to be regulatory compliant, you want them to have explainability, then we provide a good answer for that. VentureBeat: Do you think people understand explainability and the importance of the provenance of AI models and the importance of that yet? Are they just kind of blowing by that issue in the wake of the pandemic? Thomas: We launched the first version of what we built to address that around that two years ago. I would say that for the first year we got a lot of social credit. This changed dramatically in the second half of last year. We won some significant deals that were specifically for model management explainability and lifecycle management of AI because companies have grown to the point where they have thousands of AI models. It’s pretty clear, once you get to that scale, you have no choice but to do this, so I actually think this is about to explode. I think the tipping point is once you get north of a thousandish models in production. At that point, it’s kind of like nobody’s minding the store. Somebody has to be in charge when you have that much machine learning making decisions. I think the second half of last year will prove to be a tipping point. VentureBeat: Historically, AI models have been trained mainly in the cloud, and then inference engines are employed to push AI out to where it’d be consumed. As edge computing evolves, there will be a need to push the training of AI models out to the edge where data is being analyzed at the point of creation and consumption. Is that the next AI frontier? Thomas: I think it’s inevitable AI is gonna happen where the data is because it’s not economical to do the opposite, which is to start everything with a Big Data movement. Now, we haven’t really launched this formally, but two months ago I started a unit in IBM software focused on software-defined networking (SDN) and the edge. I think it’s going to be a long-term trend where we need to be able to do analytics, AI, and machine learning (ML) at the edge. We’ve actually created a unit to go after that specifically. VentureBeat: Didn’t IBM sell an SDN group to Cisco a long time ago now? Thomas: Everything that we sold in the ’90s was hardware-based networking. My view is everything that’s done in hardware from a networking at the edge perspective is going to be done in software in the next five to seven years. That’s what’s different now. VentureBeat: What differentiates IBM when it comes to AI most these days? Thomas: There are three major trends that we see happening in the market. One is around decentralization of IT. We went from mainframes that are centralized to client/server and mobile. The initial chapter of public cloud was very much a return to a centralized architecture that brings everything to one place. We are now riding the trend that says that we will decentralize again in the world that will become much more about multicloud and hybrid cloud. The second is around automation. How do you automate feature engineering and data science? We’ve done a lot in the realm of automation. The third is just around getting more value out of data. There was this IDC study last year that 90% of the data in businesses is still unutilized or underutilized. Let’s be honest. We haven’t really cracked that problem yet. I’d say those are the three megatrends that we’re investing against. How does that manifest in the IBM strategy? In three ways. One is we are building all of our software on open source. That was not the case two years ago. Now, in conjunction with the Red Hat acquisition, we think there’s room in the market for innovation in and around open source. You see the cloud providers trying to effectively pirate open source rather than contribute. Everything we’re doing from a software perspective is now either open source itself or it’s built on open source. The second is around ecosystem. For many years we thought we could do it ourselves. One of the biggest changes we’ve made in conjunction with the move to open source is we’re going to do half of our business by making partners successful. That’s a big change. That why you see things like the announcement with Palantir. I think most people were surprised. That’s probably not something we would have done two years ago. It’s kind of an acknowledgment that all the best innovation doesn’t have to come from IBM. If we can work with partners that have a similar philosophy in terms of open source, that’s what we’re doing. The third is a little bit more tactical. We announced earlier this year that we’ve completely changed our go-to-market strategy, which is to be much more technical. That’s what we’ve heard customers want. They don’t want a salesperson to come in and read them the website. They want somebody to roll up their sleeves and actually build something and co-create. VentureBeat: How do you size up the competitive landscape? Thomas: Watson components can run anywhere. The real question is why is nobody else enabling their AI to run anywhere? IBM is the only company doing that. My thesis is that most of the other big AI players have a strategy tax. If your whole strategy is to bring everything to our cloud, the last thing you want to do is enable your AI to run other places because then you’re acknowledging that other places exist. That’s a strategy advantage for us. We’re the only ones that can truly say you can bring the AI to where the data is. I think that’s going to give us a lot of momentum. We don’t have to be the biggest compute provider, but we do have to make it incredibly easy for companies to work across cloud environments. I think that’s a pretty good bet. VentureBeat. Today there is a lot of talk about MLOps, and we already have DevOps and traditional IT operations. Will all that converge one day or will we continue to need a small army of specialists? Thomas: That’s a little tough to predict. I think the reason we’ve gotten a lot of momentum with AI Ops is because we took the stuff that was really hard in terms of data virtualization, model management, model creation, and automated 60-70% of that. That’s hard. I think it’s going to be harder than ever to automate 100%. I do think people will get a lot more efficient as they get more models in production. You need to manage those in an automated fashion versus a manual fashion, but I think it’s a little tough to predict that at this stage. VentureBeat: There’re a lot of different AI engines. IBM has partnered with Salesforce. Will we see more of that type of collaboration? Will the AI experience become more federated? Thomas: I think that’s right. Let’s look at what we did with Palantir. Most people thought of Palantir as an AI company. Obviously, they associate Watson with AI. Palantir does something really good, which is a low-code, no-code environment so that the data science team doesn’t have to be an expert. What they don’t have is an environment for the data scientist that does want to go build models. They don’t have a data catalog. If you put those two together, suddenly you’ve got an AI system that’s really designed for a business. It’s got low code, no code, it’s got Python, it’s got data virtualization, a data catalog. Customers can use that joint stack from us and will be better off than had they chosen one or the other and then tried to fix the things themselves. I think you’ll probably see more partnerships over time. We’re really looking for partnerships that are complementary to what we’re doing. VentureBeat: If organizations are each building AI models to optimize specific processes in their favor, will this devolve into competing AI models simply warring with one another? Thomas: I don’t know if it’ll be that straightforward. Two companies are typically using very different datasets. Now maybe they’re both joining with an external dataset that’s common, but whatever they have is first-party data or third-party data that is probably unique to them. I think you get different flavors, as opposed to two things that are conflicting or head to head. I think there’s a little bit more nuance there. VentureBeat: Do you think we’ll keep calling it AI? Or will we get to a point where we just kind of realize that it’s a combination of algorithms and statistics and math [but we] don’t have to necessarily call it AI? Thomas: I think the term will continue for a while because there is a difference between a rules-based system and a true learning machine that gets better over time as you feed it more data. There is a real distinction."
https://venturebeat.com/2021/03/19/openlogics-stack-builder-helps-enterprises-choose-the-right-open-source-software/,OpenLogic’s Stack Builder helps enterprises choose the right open source software,"Enterprises are adopting open source software now more than ever, according to most estimations, a shift accelerated by factors such as the push toward cloud infrastructure and the ongoing pandemic. Many of the major tech companies not only use open source software but contribute code and even open-source their own tools when it makes sense. Open source, it seems, has eaten the world. However, the sheer number and variety of open source software packages can make it difficult for even the largest of businesses to determine what’s best for their needs, not to mention which ones will work well together as part of a broader open source software stack. This is the problem OpenLogic is targeting with Stack Builder, a free tool designed to help enterprises build a customized open source stack. OpenLogic, for the uninitiated, delivers open source services, support, design guidance, training, and more. The company was founded as EJB Solutions back in 1998 and rebranded as OpenLogic in 2004. In 2013, it was acquired by Rogue Wave Software, which was in turn acquired by Perforce six years later. OpenLogic claims a number of notable enterprise clients, such as Fannie Mae, which used OpenLogic to migrate from Oracle Java to OpenJDK, and Moody’s, which used OpenLogic to migrate from RHEL to CentOS. Traditionally, enterprises have been inclined to use commercial off-the-shelf software solutions that are vertically integrated, something viewed as “anti-ethical” by the open source software world, OpenLogic chief architect Justin Reock told VentureBeat. “So enterprises seek to recreate that experience by curating a full stack of open source packages and treating it as a single solution,” he said. “There is no place to go to get this curated stack and to understand the best choices for their use cases.” That, in a nutshell, is what Stack Builder attempts to solve. OpenLogic first debuted Stack Builder last year, though it was a much more primitive, static incarnation based on a Q&A format. Version 2.0, which launched this week, takes a more dynamic template-based approach, replete with a drag-and-drop interface. After submitting an email address, to which OpenLogic will later send a personalized report, the user is faced with a display divided by various categories spanning application delivery, data layer, front end, monitoring, operating system, VM/containers/cloud, and workflow. The user then selects their package category from the menu and populates the “stack” by dragging and dropping packages from the options available. Alternatively, OpenLogic provides a bunch of prebuilt templates, such as the lightweight Java or PHP stack, and automatically selects what it deems the best open source software packages. Afterward, OpenLogic sends the user a report that outlines the purpose of each of the packages, what they are typically used for, and — as you might expect — how to put it all together by employing OpenLogic’s services. The problem Stack Builder is trying to solve is not new by any stretch. Open source software intersects with just about every piece of software these days, from scripts that help servers run faster to code that contributes to systems architecture and APIs. But estimates suggest there are at least 1.5 million JavaScript packages alone. There are other tools and platforms designed to help developers dig through the weeds, such as Openbase, which provides data on the millions of open source packages — including figures on weekly downloads, monthly commits, and even user reviews. With Stack Builder, OpenLogic is bringing curation to the table and has narrowed down the options to what it believes are the best open source packages. “We have curated this selection because these are open source technologies proven to work for enterprise requirements, at enterprise scale,” Reock explained. “They have all passed OpenLogic certification — which includes a 72-point checklist that gauges aspects of community behavior, enterprise adoption, responsiveness to security vulnerabilities, and sponsorship by broader industry organizations, such as the Linux foundation.” Moreover, not all open source software plays nicely together, perhaps due to incompatible protocol layers or standards that have not been implemented correctly. As such, Stack Builder not only serves as a curator, but as a compatibility tester and evaluator as well. “Open source projects are developed by completely different communities — which may or may not adhere to the open standards, or may have different interpretations,” Reock said. “Critical elements that determine interoperability include following standards, wide adoption and testing, [and] interaction with other products and packages.” Stack Builder is designed to give users the best options for their use cases through a “living tool” that is constantly updated with new or better technologies as they evolve. If nothing else, it should save companies a little time and help them better understand the open source software landscape from a security, stability, and interoperability perspective. So what’s the alternative? “The alternative is to do a lot of research on your own, search community threads, and ultimately test and try to really understand if the stack’s components will work together for your use cases at enterprise scale,” Reock said.”[That involves] lots of time and trial and error.”"
https://venturebeat.com/2021/03/18/the-great-data-decentralization-is-coming-are-you-ready/,The great data decentralization is coming — are you ready?,"The move to cloud computing is one of the most important technology shifts of our generation. Along with it, the decades-long push to centralize data storage in a single warehouse is coming to an end, as dumping everything into a “data lake” has caused more harm than good. For some applications, centralizing data via cloud storage solutions such as Amazon S3 and Snowflake works to an extent (read: Snowflake’s IPO). At the same time, several major factors are creating greater data decentralization. Here are three of the biggest. The major ad platforms have shared less and less detailed customer and performance data over the years. I was on the phone with a Procter & Gamble executive recently and they complained that even at their level of spend, Google and Facebook expect them to “rent” data instead of owning it. So a marketer can’t just build a giant data warehouse, like in the good ol’ days, dump everything in it, and run analytics on it. Because as Google and Facebook have taken over the ad market, marketers are getting only “cohort-level” data, which makes analysis in one central data store impossible (this has interesting effects on marketing spend and prices, but that’s another post). It looks like this trend won’t stop anytime soon since moves like the banning of third-party cookies and Apple’s move to ban IDFA are only consolidating more power in Google, Facebook, and somewhat in Apple. Data scarcity has existed for a long time, particularly in the retail/consumer sector. Retailers sharing as little as possible with the brands they work with has been a persistent source of friction. For example, pharmaceutical companies don’t know anything about where their drugs are prescribed and sold; meanwhile, vendors like CVS and Walgreens sell their prescription data to IQVIA, who in turn rents it back to the pharma companies. They literally rent their own data! It’s gotten worse with e-commerce as Amazon doesn’t share any data with its sellers, so these brands are receiving even less data now than they used to from brick-and-mortar stores. That is why traditional brands have been eager to acquire direct-to-consumer brands (think Walmart and Jet.com, for example): They have direct data on end-consumers. As international privacy measures like GDPR and CCPA, as well as other regulations, get more strict, the laws about how you move data around both internally and between companies are undergoing more scrutiny. So far, companies have “solved” this by requiring their software vendors to take on the entirety of the monetary risk of violating regulations. My firm has walked away from some investment opportunities in software startups because they were too exposed on this front, and I expect there will be technology companies that fail due to regulatory exposure down the line. Eventually, I think this will lead to changes in how these companies operate. The first change that we’ll likely see is a move from SaaS data tools to self-hosted tools in virtual private clouds, something that’s already happening in finance. Every large company is one security breach away from freaking out and bringing everything in-house. This has created the need for a new generation of tools like OneTrust and BigID, among others. Let’s say a retailer like Macy’s wants a SaaS vendor to run its application and store its data in Azure or Google’s VPC. In that case, the SaaS vendor needs to think about data partitioning, running software in multiple clouds and sometimes even multiple zones of the same cloud. Retailers also frequently don’t want their data in AWS because they all compete with Amazon. So a SaaS company that’s running on AWS needs to figure out a way to store that retailer’s data in Microsoft or Google Cloud, or have to rewrite its entire software to run on multiple clouds. Sometimes that data needs to reside in a specific geographic or availability zone of a cloud provider in order to integrate with other data or services in that same zone. Outside of retail this is affecting advertising, though to a much lesser degree. But at a minimum, when a customer is on a given cloud they expect the vendor to at least integrate with it. So if the customer runs its databases in Azure or GCP or AWS, the vendor must follow suit when it’s time to export data. Another issue is data location. Say some big company keeps its data in the West Coast region of AWS or Snowflake and uses software from Salesforce or another SaaS provider. When that SaaS provider shares data with another partner/SaaS provider, both SaaS companies have to figure out ways to move or replicate data from region to region. The growing orthogonal movements towards data decentralization and cloud data migration will continue to spawn useful and profitable data tools, especially with the rise of the data science profession. Moving forward, companies that provide applications, data transport tools, and data itself will see real demand. Data diversity is now greater than data scale. No one has a data decoder ring. Other than building one of those, the transition to decentralized data will result in the need for data management solutions in areas like data survivability, data residency, access control, data masking, encryption, identity management, and much more. Alex Rosen is a Co-founder and Managing Partner at Ridge Ventures. He has worked in venture capital for more than 20 years, tallying 15 exits exceeding $20 billion in value."
https://venturebeat.com/2021/03/18/amazon-launches-s3-object-lambda-to-transform-data-across-applications/,Amazon launches S3 Object Lambda to transform data across applications,"Apps in the cloud each have their own requirements and sometimes need a different view of data in a dataset. For example, an ecommerce database might include personally identifiable information that isn’t required when the data is processed for analytics. On the other side, if the ecommerce dataset is used for a marketing campaign, it might need to be enriched with additional details like information from a customer loyalty database. To address this modality challenge, Amazon today launched S3 Object Lambda, a new capability that allows developers to add their own code to process data retrieved from Amazon Web Services (AWS) Simple Storage Service (S3) before returning it to an app. Amazon says that S3 Object Lambda works with existing apps, leveraging AWS Lambda functions to automatically transform data as it’s being retrieved from an S3 bucket. Prior to the rollout of S3 Object Lambda, AWS offered two primary ways to provide different views of data to multiple applications. Enterprises could create, store, and maintain additional derivative copies of the data, so that each app had its own custom dataset tailored to its needs. Alternatively, they could build and manage infrastructure as a proxy layer in front of S3 to intercept and process data as it was requested. But both options added complexity and costs.  With S3 Object Lambda, Amazon says that companies can more easily automate tasks like converting across data formats, compressing or decompressing files as they’re being downloaded, resizing and watermarking images, and implementing custom authorization rules to access databases. Using S3 Object Lambda, developers can present multiple views from the same dataset and update the necessary Lambda functions to modify these views at any time. “When retrieving an object using S3 Object Lambda, there is no need for an object with the same name to exist in the S3 bucket. The Lambda function can use information in the name of the file or in the HTTP headers to generate a custom object,” AWS chief evangelist Danilo Poccia explained in a blog post. “This new capability makes it much easier to share and convert data across multiple applications.” S3 Object Lambda is available today in all AWS Regions excepting Asia Pacific (Osaka), AWS GovCloud (US-East), AWS GovCloud (US-West), China (Beijing), and China (Ningxia) Regions. Customers can use S3 Object Lambda with the AWS Management Console, AWS Command Line Interface, and AWS SDKs. They pay for the AWS Lambda compute and request charges required to process the data, the data S3 Object Lambda returns to their applications, and S3 requests invoked by Lambda functions. To begin using S3 Object Lambda, follow these steps suggested by Amazon:"
https://venturebeat.com/2021/03/18/these-awesome-companies-are-actively-hiring-right-now-so-check-them-out/,"These awesome companies are actively hiring right now, so check them out","Presented by Optum None of us will deny the fact that looking for a new job is exhausting. It takes time, patience, and a whole lot of scrolling online. As the saying goes, looking for a job is a full time job in itself. Given how tricky it can be, we wanted to help in some way, so that you can all find roles you genuinely love (the dream, right?) We have been collaborating with Jobbio to bring you VentureBeat Jobs, a fantastic hub where some of the coolest companies in the world can advertise their roles, and land talent like yourself. To get you started (and to give you a flavour of the kind of roles going at the moment), here are five really cool companies currently open for applications. Outbrain was founded to create the first discovery feed of content on the open web. Today they continue to innovate as the preferred interest-based, end-to-end native technology fueling exploration and discovery for one third of the world’s Internet-connected population. Outbrain creates the most engaging discovery experiences for people on the web with their Smartfeed technology on the world’s most well-recognized media companies — from MSN, CNN, BBC, and The Washington Post to The Guardian and Sky News. Technology is Outbrain’s driver, and what makes their content world go ‘round. From the complex set of algorithms to its prioritized Interest Graph, Outbrain’s system pairs content with readers who will find it most engaging and relevant. Its algorithms only serve what readers want to read, making Outbrain the highest quality digital discovery engine out there. Founded in 2014, Zwift connects cyclists and runners around the world and mixes the intensity of training with the immersive and engaging play of gaming. Ride or run virtual worlds with a community that motivates you at every mile. There is a race starting every eight minutes. As of March 2020, Zwift has implemented a flexible work policy to ensure their people are safe. To support their people better, they have equipped teams with desks and chairs to set up home office spaces, while also providing a monthly stipend to cover the additional cost of working from home. They are encouraging all to invest in themselves by providing a corporate-sponsored fitness budget, mental health support (through therapy, mindfulness exercises, and apps), and organizing company-wide timeouts. Zwift is embracing remote work culture and is continuing to grow by hiring employees who are committed to permanent remote work — even when it is safe to return to one of their beautifully redesigned offices in Southern California, London, or Rio de Janeiro. FanDuel Group is a world-class team of brands and products all built with one goal in mind: to give fans new and innovative ways to interact with their favorite games, sports, teams, and leagues. That’s no easy task, which is why they are dedicated to building a winning team. The premier gaming destination in the United States, FanDuel Group consists of a portfolio of leading brands across gaming, sports betting, daily fantasy sports, advance-deposit wagering, and TV/media, including FanDuel, Betfair US, and TVG. They describe themselves as being absurdly fan-focused and their mission is to create experiences that fans can’t wait to share with their friends. Fanduel also believes in winning right and vows to never compromise when it comes to looking out for their teammates. From the many opportunities for professional development to having an open and inclusive environment, Fanduel is committed to making sure their employees get as much out of FanDuel as possible. Founded in 2012, Checkout.com now has a team of 900 people across 13 international offices. They’re building the connected finance that businesses deserve. Their flexible payment solutions help global enterprises like Samsung, Deliveroo, and Adidas launch new products and create experiences customers love. In May 2020, they completed a $150 million Series B funding round, tripling the company’s valuation to $5.5 billion. And it’s not just what they build that makes them different –it’s how they do it. They focus on clean tech, built to cut through complexity, along with granular-level data that enables endless innovation. Ultimately, their clients can unlock more value from every transaction and create unlimited opportunities. Checkout.com believes that the future is connected finance. They are currently looking for talented people to join the Technology, Commercial, Marketing, Operations, Product, People, Finance and Legal teams across global locations. MSCI is a leading provider of critical decision support tools and services for the global investment community. With over 50 years of expertise in research, data, and technology, they power better investment decisions by enabling clients to understand and analyze key drivers of risk and return, and confidently build more effective portfolios. They create industry-leading research-enhanced solutions that clients use to gain insight into, and improve transparency across, the investment process. Their values define the working environment they strive to create. Personal accountability and responsibility are key to success, and they always work as a team to remain client centric. They are inclusive, champion bold ideas, pursue excellence, and always act with integrity."
https://venturebeat.com/2021/03/18/harness-embraces-open-source-and-pledges-to-support-community-like-an-enterprise-customer/,Harness embraces open source and pledges to support community ‘like an enterprise customer’,"Some two months after announcing $115 million in fresh funding at a lofty $1.7 billion valuation, developer platform Harness today unveiled the beginnings of what could be a fruitful friendship with the open source community. The San Francisco-based company offers a continuous integration and delivery platform (CI/CD) for engineering and developer operations (DevOps) teams, leaning on machine learning (ML) and AI techniques to monitor and automate software deployments, enable rollbacks, and more. The relatively new “CI” facet of its platform, however, was born from its Drone.io acquisition in August. Founded in 2012, Drone.io is an open source, self-serve, container-native continuous integration platform. It’s already leveraged by many big-name brands, including Cisco, eBay, VMware, and Capital One. Over the past decade, Drone.io has also grown a commercial business off the back of the OSS project, selling more advanced features — such as secrets management and auto-scaling. Thus, any developer-focused company looking to add an oven-baked “CI” tool to their “CD” stack was always going to be enticed by Drone.io’s proposition. “The acquisition of Drone really came down to the timing being right and wanting to partner with a company like Harness to build the future,” Harness CEO and cofounder Jyoti Bansal told VentureBeat. “Drone laid a lot of the groundwork, but it was a matter of taking CI to the next level.” Now seven months after the acquisition, Harness today announced a number of notable updates to bolster its newfound open source ethos. This also marks its first major contribution to the open source software community. Bansal said Harness plans to support the Drone open source community “as though they are an enterprise customer” while keeping the product free and fully open source. “Since the acquisition, Harness has doubled Drone’s engineering investment,” Bansal added. While Harness has committed to leaving the Drone brand, website, and community “untouched,” harness will ultimately offer three CI editions directly through the Harness website: Community (Drone open source), Essentials (Drone paid), and an enterprise incarnation that will be “available shortly.” Today’s news sees Harness add new capabilities to the Community and Essentials editions, which were formerly part of the Drone project. This includes a refreshed user interface that is more consistent with Harness’ broader platform and allows developers to “visualize and watch their CI pipelines as they execute,” Bansal told VentureBeat. This helps them understand which pipeline steps are currently executing and in what order and is generally an easier way to track complex pipelines compared to reviewing log files. Other new features include a CI debugger and “enhanced governance and security” tools that make it easier for managers to control access permissions for their developer users through a straightforward user interface. This is just the beginning of Harness’ open source adventure, according to Bansal, who said there will be much more to come — though he was quick to note that the company has embraced open source technologies since its inception through its support for major projects involved in the software delivery process, including Docker, Kubernetes, and Jenkins. “This acquisition was the first step in bringing open source to more areas of our platform — we already had several other internal projects in the works at the time that will be open-sourced and have been doing an enormous amount of community work with the CD Foundation (an open source community built around software delivery),” Bansal explained. “You’ll see more open source announcements from Harness later this year.” But why has Harness decided to dive into open source now? And couldn’t the company have developed its own CI tooling internally? “From selling CD, we learned a lot about the CI market and the shortcomings of certain projects and tools,” Bansal explained. “We felt like the choice for cloud and container-native CI tools was small, and we had big ideas on how to innovate and improve the developer CI experience. We bought Drone because we are all in on both the product and the community. We want to provide everyone — company, team, and individual developer — with the best CI/CD platform on the market, and that requires a complete commitment to innovation and improvement.” This echoes the reason any company decides to embrace open source. As the community has more than 50,000 active community users, including from major enterprises, Harness is aligning itself with a large army of Drone developers who can collectively ship code much more quickly than any internal team could. Many businesses benefit from that combined effort, while Harness can also capitalize on the commercial, market-ready CI product. Looking across the broader open source sphere, the pandemic has played more than a bit part in driving open source adoption in the enterprise, with Red Hat recently noting in its annual State of Enterprise Open Source report that 90% of enterprises use commercial open source in their organizations. Elsewhere, the tech titans of the world continue to double down on their open source investments, with Microsoft recently reporting that it had learned a lot from its increased engagement with the open source world over the past year, adding that “open source is also now the accepted model for cross-company collaboration.” With that in mind, the next major updates from Harness will be a new CI enterprise edition that builds on some of the key concepts behind Drone and “takes things to another level for the developer,” according to Bansal. As the company moves further into the open source space, we can also expect to see more parts of the original Harness product made open source over time, including core elements of the core CD platform, as well as newer projects it’s working on internally. “We’re all in on open source, and we want our contribution to be significant so it moves the needle for our industry,” Bansal said."
https://venturebeat.com/2021/03/18/clubmarket-launches-sponsorship-marketplace-to-monetize-clubhouse/,Clubmarket launches sponsorship marketplace to monetize Clubhouse,"Clubhouse has drawn a big crowd lately, but it still has to introduce a way to monetize its popular social audio sessions. In the meantime, independent startup Clubmarket has figured out a way to do just that. Clubmarket is setting up a sponsorship marketplace for Clubhouse program creators, cofounder Tomer Dean told VentureBeat in an email. Monetizing Clubhouse has been a popular topic, even inside Clubhouse sessions. Available only on iOS, Clubhouse has been in beta testing for a year. But it has expanded rapidly in the past few months and received $100 million in funding at a $1 billion valuation in December, with backing from venture firm Andreessen Horowitz. Mobile analyst and insight firm App Annie estimates that Clubhouse has had 12.7 million downloads to date, including 1.3 million since March 1. About 3.7 million of those downloads are in the U.S., followed by 1.8 million in Japan and 735,000 in Germany. But while the audience is growing fast, Clubhouse doesn’t have ads or charge fees for users. There’s also no way for speakers in the Clubhouse rooms to charge money for their talks. However, Dean said the door is open for sponsorships. Clubmarket, which is still in development, will give Clubhouse creators access to paid brand sponsorships. The young startup has yet to announce exactly how many creators it will accept initially, but the number will likely be in the thousands, Dean said. The marketplace will allow brands to sponsor Clubhouse rooms in various ways: Pricing for the different “gigs” will vary, with every creator charging their own rates. From the Clubmarket website, the estimation tool indicates that a tech-related Clubhouse room with 500 listeners will be able to charge up to $1,000 to $2,000 for a co-branded sponsorship. The “drop-in audio” market is still in infancy, so it’s likely prices will rise as user adoption increases. The idea for the marketplace came from a Clubhouse panel. Last month, Product Hunt founder Ryan Hoover hosted a community town hall on Clubhouse. While discussing the surge of Clubhouse-related services being launched on Product Hunt, he mentioned a missing gap. He commented that the world would likely see a sponsorship marketplace popping up soon. One of the listeners in that room, a serial tech founder, got a tingle of excitement. Having previously launched a two-sided marketplace (that closed down), he knew his team could develop something and get it to market in a matter of weeks. The marketplace is currently in the development stages and will begin onboarding initial users in the upcoming weeks. This week, Clubhouse itself announced its first accelerator program for 20 creators to work with brand sponsors, but the program is not yet available to the general public. Besides Dean, cofounders include Peleg Aran and Nimrod Kramer. The crew have previously founded several venture-backed startups and marketplaces."
https://venturebeat.com/2021/03/18/robotics-safety-system-developer-fort-robotics-raises-13m-to-tackle-workplace-automation/,Robotics safety system developer Fort Robotics raises $13M to tackle workplace automation,"Fort Robotics, a startup developing a safety and security platform for robots, today announced it has raised $13 million in a funding round led by Prime Movers Lab. With the new investment, Fort says it will more than double the size of its workforce and expand its product offerings in “key markets.” Worker shortages attributable to the pandemic have accelerated the adoption of automation. According to ABI Research, more than 4 million commercial robots will be installed in over 50,000 warehouses around the world by 2025, up from under 4,000 warehouses as of 2018. In China, Oxford Economics anticipates 12.5 million manufacturing jobs being automated, while in the U.S., McKinsey projects machines will take upwards of 30% of such jobs. Founded in 2018, Philadelphia, Pennsylvania-based Fort is CEO Samuel Reeves’ second startup. In 2006, he launched Humanistic Robotics, which developed autonomous land mine-clearing systems. Through the refinement of that technology, Reeves saw an opportunity to create a safety-and-security overlay that could be integrated with industrial machines. Fort’s platform allows managers to configure, monitor, and control robots from web and mobile apps. An endpoint controller wirelessly connects machines to devices, while an embedded hardware security module ensures communications are secure, reducing breach risk. Fort also provides a remote control with a built-in emergency stop that lets workers use one or more machines — ostensibly without compromising safety or command security. As industrial manufacturers increasingly embrace automation, workplace injuries involving robots are becoming more common. For example, a recent report revealed that Amazon fulfillment warehouses with robots had higher rates of serious worker injuries than less automated facilities. A warehouse in Kent, Washington that employs robots had 292 serious injuries in one year, or 13 serious injuries per 100 workers. Meanwhile, the serious injury rate of a Tracy, California center nearly quadrupled after it introduced robots five years ago, going from 2.9 per 100 workers in 2015 to 11.3 in 2018. “Fort is creating systems that will accelerate the rise of autonomous vehicles and robotics, making work safer and more productive,” Prime Movers Lab’s Suzanne Fletcher said in a press release. “We’re proud to partner with this forward-looking company that recognizes that safe collaboration between humans and robots is synonymous with the workforce of tomorrow.” For one customer that has multiple robots operating within a gated area, Fort says it built a solution that enables workers to stop all the robots when a person enters the zone. While each machine has its own e-stop button, manual stops affect productivity, and getting close to the moving robots can be hazardous. Fort installed its safety controller on each robot and a master controller on the door of the caged area so that the system automatically sends a wireless e-stop signal that stops every robot whenever the door opens. “We’re excited to partner with our new investors to help position Fort at the forefront of this rapidly evolving space,” Reeves said. “The world is on the cusp of a new industrial revolution in mobile automation. With added investment and support, we’ll be able to rapidly scale the company to capitalize on the convergence of trend and opportunity to ensure that robotic systems are safely deployed across all industries.” Fort claims to have served more than 100 clients last year, including “dozens” of Fortune 1,000 companies in markets such as warehousing, construction, agriculture, manufacturing, mining, turf care, last-mile delivery, and transportation."
https://venturebeat.com/2021/03/18/exone-to-offer-worlds-largest-metal-binder-jet-3d-printer-with-a-controlled-atmosphere-capable-of-aluminum-and-titanium-production-in-2022/,"ExOne to Offer World’s Largest Metal Binder Jet 3D Printer With a Controlled Atmosphere, Capable of Aluminum and Titanium Production, in 2022","  NORTH HUNTINGDON, Pa.–(BUSINESS WIRE)–March 18, 2021– The ExOne Company (Nasdaq: XONE), the global leader in industrial sand and metal 3D printers using binder jetting technology, today announced that it will offer a controlled-atmosphere model of its X1 160Pro™ extra-large production metal printer in the second half of 2022. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210318005436/en/ The X1 160Pro, with a build box of 800 x 500 x 400 mm, or 160 liters, is ExOne’s 10th and largest production-ready metal binder jetting system to date. The new model is now shipping from the company’s European headquarters and production facility in Gersthofen, Germany. A controlled atmosphere is essential for 3D printing of reactive fine metal powders, but it also offers other benefits, such as reduction of powder oxidation and enhanced powder dispensing and spreading through control of humidity. This update will smooth the way for high-volume production of aluminum, titanium, copper, and several other materials using binder jet 3D printing technology. ExOne has successfully been binder jetting reactive powders in controlled atmospheres, also sometimes referred to as inert or chemically inactive, for years. On Dec. 1, 2020, ExOne was issued a patent on binder jet 3D printing in a controlled atmosphere (U.S. Patent No. 10,850,493). ExOne’s controlled-atmosphere X1 160Pro can be used with nitrogen or argon and will be paired with accessories and ancillary equipment also equipped with inert atmosphere features. These include a curing oven, powder conditioning system, depowdering station, and transport device for moving the build-box between process stages to ensure complete atmosphere control throughout the process. Importantly, ExOne will continue to offer the original model of the X1 160Pro, a streamlined model for customers who do not need a controlled atmosphere system to process metal powders such as stainless steels. ExOne customers currently use the X1 25Pro® metal binder jet system for production today without an inert feature, with one customer now operating six of the systems for serial production. ExOne binder jet 3D printing technology is capable of processing 23 metal, ceramic, and composite materials, including a dozen single-alloy metals such as aluminum and titanium. About ExOne ExOne (Nasdaq: XONE) is the pioneer and global leader in binder jet 3D printing technology. Since 1995, we’ve been on a mission to deliver powerful 3D printers that solve the toughest problems and enable world-changing innovations. Our 3D printing systems quickly transform powder materials – including metals, ceramics, composites and sand – into precision parts, metalcasting molds and cores, and innovative tooling solutions. Industrial customers use our technology to save time and money, reduce waste, improve their manufacturing flexibility, and deliver designs and products that were once impossible. As home to the world’s leading team of binder jetting experts, ExOne also provides specialized 3D printing services, including on-demand production of mission-critical parts, as well as engineering and design consulting. Learn more about ExOne at www.exone.com or on Twitter at @ExOneCo. We invite you to join with us to #MakeMetalGreen™.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210318005436/en/ Media:Sarah Webster724.516.2336sarah.webster@exone.com Doug Braunsdorf1.646.899.7687doug.braunsdorf@bcw-global.com"
https://venturebeat.com/2021/03/18/fetcher-ai-nabs-6-5m-to-match-employees-with-open-roles-using-ai/,Fetcher.ai nabs $6.5M to match employees with open roles using AI,"Fetcher.ai, a recruitment platform that combines AI with human teams, today announced it has raised $6.5 million in a round led by G20 Ventures. The company, whose latest funding round brings its total to $12 million, says the funds will be used to expand the size of its workforce. In 2020, talent shortages in the U.S. rose to historic levels, with 69% of employers reporting having difficulty filling jobs, according to a ManPowerGroup survey. This isn’t easily remedied. A report by the Society for Human Resource Management found that filling an open position costs employers an average of $4,129 and takes roughly 42 days. Fetcher was founded in 2014 as a consumer-centric messaging app called Caliber, which focused on professional networking. After a couple of years running Caliber, Fetcher’s cofounders uncovered an opportunity to automate the functionality they had built into their free consumer app to create a paid software-as-a-service product that helped companies fill roles. Fetcher’s AI-powered platform provides enterprises with an internal team that trains and monitors data to deliver a pipeline of candidates. After a recruiter uploads a job description and provides some initial feedback, Fetcher’s algorithms look for talent that might match those needs, aggregating candidate information from across the web. Fetcher offers insights into how many prospects might be needed to ensure a hire and automates outreach accordingly. With the platform, hiring managers can track metrics, including open rates, response rates, and interview booking rates. They’re also able to sync their calendar to create meetings, add scheduling links to email templates, configure availability, and block out times based on a preferred schedule. “Fetcher … provides the ability to turn on ‘automated sourcing,’ allowing for the platform to run without a recruiter’s intervention. Once the recruiter has trained the AI successfully upfront, automated sourcing simply runs in the background, pushing qualified prospects through an automated email outreach series each day,” a spokesperson told VentureBeat via email. “[This] technology allows recruiting teams to focus more on recruitment marketing and the candidate experience — two pieces of the recruiting funnel that have become ever more important in a competitive job market and with the younger generations of millennials and Gen Z.”  As the pandemic continues, companies are increasingly adopting alternatives to in-person job interviews and talent recruitment. Recruiters PageGroup and Robert Walters last year moved some job interviews and interactions online, following on the heels of tech giants Amazon, Facebook, Google, and Intel. In spite of fears that these tools might exhibit biases against certain groups of candidates, at least a few businesses have begun piloting candidate screenings that ostensibly help recruiters become more efficient, cut down on recruitment costs, and boost overall candidate satisfaction. Fetcher’s competitors include Plum, which asks job candidates to fill out problem-solving and personality tests that award points for “talents” like adaptation, communication, inclusion, and innovation. Another rival, Vervoe, offers AI tools that test would-be employees’ on-the-job skills with a mix of general assessments, coding challenges, and personality quizzes. There’s also Headstart, which recently raised $7 million for AI that can mitigate recruitment bias;  Xor, a startup developing an AI chatbot platform for recruiters and job seekers; and Phenom People, a human resources platform that taps AI to help companies attract new talent. Fetcher claims its AI technologies work to decrease unconscious bias while increasing the size and scope of diverse talent pools. Since May 2020, the New York-based startup has partnered with over 150 companies, including Peleton, Behr, and Velcro, and seen revenues increase 10 times, with annual recurring revenue more than doubling in the last nine months alone. “Fetcher provides diversity metrics, conversion metrics, and more all within the platform. This allows teams to benchmark each role versus the full company, benchmark each recruiter against other team members, and benchmark the company’s metrics against the industry standards,” a spokesperson told VentureBeat via email. “All of these real-time metrics ensure that recruiters are meeting their goals, and if they are not, gives them a clear path as to what levers might need to change in order to get there. Overall, Fetcher provides predictive modeling for pipeline building so that companies can always be ahead of the game when hiring in that department becomes a top priority.” Alongside G20 Ventures, KFund and returning investors Slow and Accomplice participated in Fetcher’s round announced today. Fetcher has 80 employees."
https://venturebeat.com/2021/03/18/ai-powered-grocery-inventory-startup-shelf-engine-nabs-41m/,AI-powered grocery inventory startup Shelf Engine nabs $41M,"Prior to the pandemic, average grocery profits hovered around 2%, mostly due to transportation and logistical inefficiencies. Shifting product demand and sales compounded these issues, with stores now responsible for 10% of U.S. food waste. This over-ordering not only costs profitability, but forces retailers to increase prices to make up for the losses. In April 2020, grocery prices showed their steepest monthly increase in nearly 50 years, led by rising prices for perishables like meat and eggs. Shelf Engine, a Seattle-based startup cofounded in 2015, aims to make a change by tapping AI to help stores increase profits while reducing product waste. The company today announced that it raised $41 million in series B funding, bringing its total raised to $58 million as its platform expands beyond 85 customers and over 2,000 stores nationwide. According to cofounder and CEO Stefan Kalb, grocery retailers that fail to innovate today could put their companies at risk. Traditionally, inventory managers use computer-assisted ordering and software-as-a-service solutions that require upfront hardware and software investments. These solutions frequently fail to account for on-hand inventory data and high volatility in sales patterns, such as with pandemic or weather-related buying. An analysis by Retail Insight found that 56% of records of perpetual inventory (PI), which determines how much stock a store receives on each delivery, are incomplete and that 15% of lost sales are a result of inaccurate PI. “[During the pandemic], grocers were impacted by new demand patterns they weren’t ready for. This created a spike in stock-outs as well as waste. This has accelerated the number of grocers who now want AI to take over their ordering and reduce their risk profile,” Kalb told VentureBeat via email. “We’re about to see a massive transformation in the grocery space, but it’s not what you’re expecting. Yes, online grocery is growing, but it will be nothing compared to store operations, procurement, and merchandising. Grocery will become a very efficient business and generate way more profit than ever before.”  Using machine learning, Shelf Engine forecasts demand for highly perishable foods, reducing waste and out-of-stocks. Drawing on point-of-sales data along with considerations like school schedules, local events, holidays, and weather, the platform generates probabilistic models for each unique product for every store, every day. These models are translated into profit maximization models to create orders, which are then fed by time-series models along with machine learning-based-dependent models. As a last step, Shelf Engine submits a store’s orders to vendors. Shelf Engine doesn’t charge for items that don’t sell — retailers only pay for the items that do. According to Kalb, one national grocer increased its profit margin 63.7% by using Shelf Engine to eliminate the cost of shrink from spoilage, breakage, and theft. Shelf Engine’s customers on average minimize stockouts by 90% while reducing food waste by as much as 32%, Kalb claims. “We’re reducing waste, while simultaneously increasing sales, and that goes straight to the grocer’s bottom line,” Kalb added. “We’re helping grocers to make much more money, better positioning them to gain market share and offer competitive prices, especially important as tech giants and other disruptors are entering the market. This latest round will enable us to further meet the demand from our customers to launch into thousands of new stores in the next 18 months.” General Catalyst led the funding round announced today with follow-on investments by GGV Capital, Foundation Capital, 1984 Ventures, Correlation Ventures, Founders’ Co-op, Soma Capital, Firebolt Ventures, and Initialized Capital. In addition to continued expansion, 145-employee Shelf Engine says the round will enable it to further invest in its team of engineers, data scientists, and supply chain automation specialists."
https://venturebeat.com/2021/03/18/genomics-has-the-power-to-transform-medicine-but-it-must-be-made-accessible/,Genomics has the power to transform medicine — but it must be made accessible,"This article is part of a Technology and Innovation Insights series paid for by Samsung. In this episode of The Next Wave, Young Sohn sits down with Lisa Alderson, CEO of Genome Medical, an innovative healthcare firm that is working hard to ensure that genomic medicine is made accessible to everyone. Alderson shares her views on a range of issues around genomics. She discusses the vast promise of the technology and her company’s efforts to bring its game-changing potential to all patients by providing clinical support tools to non-genetics professionals. She also addresses the ethical questions raised by genomics, particularly its use in prenatal diagnostics. And she highlights the critical role genomics played in developing COVID vaccines in record time. Genomics is swiftly moving to the forefront of medicine. This potentially limitless technology is transforming health care by making it more targeted, personalized, and proactive. Genomics can not only help with preventive care but also with determining treatment options for chronic ailments such as heart disease. It is becoming essential in reproductive health counseling, as well as in screening for infertility and newborn genetic disorders. And it is proving to be instrumental in the development of advanced therapeutics for diseases such as cancer and even for treating global outbreaks like COVID-19. But, to date, there has been one big problem with genomic medicine: access. Lisa Alderson, CEO of Genome Medical, explains that even though the technology is now advanced, the vast majority of patients are still not getting access to genomics, even when it can be medically beneficial. This is mostly due to insurance barriers, a broad lack of clinical understanding, and the sheer volume and complexity of new tests being brought to market. Her company is on a mission to change the dynamic by bringing genome-enabled health care to everyone through Genome Medical’s extensive network of genetic specialists and a technology platform delivering “genomics as a service.” Alderson points out that virtually everybody can benefit from access to genetic insights. Not only does genetics offer the promise of getting the right therapy to the right patient at the right time, it can also uncover new information to better inform clinical care. And as genomics advances, its ability to detect and treat diseases like cancer vastly improves. As a result, the technology is increasingly important not just for individuals with rare genetic conditions but for everyone — from birth to old age. Historically, for instance, only one or two genes were used to test for hereditary breast cancer or ovarian cancer. But because the science has evolved, we now know there are 11 genes that can help detect hereditary breast and ovarian cancer. In fact, says Alderson, there are over a hundred genes that can increase our risk of cancer. So as genetic testing becomes more precise and accurate, it has the potential to save exponentially more lives. One way to help genomics reach its full potential is by providing clinical support tools to non-genetics professionals. This will allow them to better understand which patients would benefit from what tests, and how to interpret and use the resulting information to guide clinical care. One quandary, of course, is that most local health care systems don’t have full-time metabolic geneticists on staff. As a result, they end up referring their patients out to leading academic centers, which is where the bulk of genomic health care is carried out today. And that means local health systems could lose those patients for the full continuum of their care. To address this, Genome Medical’s “genomics as a service” SaaS platform provides rapid patient education and engagement and also supports providers with the necessary clinical tools and knowledge they need to utilize genomics, as appropriate.  Alderson says she wants every hospital and health system to have access to genomics in its community setting. Alderson is increasingly heartened by the fact that genetic testing is expanding beyond its origins in prenatal medicine and oncology to new areas such cardiology, endocrinology, nephrology, and urology. In the past, for example, most urologists did not have strong use cases for ordering genetic testing. But now, genetic testing is recommended for nearly every metastatic prostate cancer patient. Ultimately, genomics will touch all medical professionals, whether they are pediatricians, oncologists, cardiologists, neurologists, urologists — the list goes on and on. “We’re sitting at the precipice of what is a huge inflection point in the use of genetics and genomics,” says Alderson. “We’re going to see it having a profound impact on human health, not just for prevention but also to get a deeper understanding of what is causing the disease — and thus a richer opportunity to improve how we treat disease. We’re getting to the root cause rather than just observing symptoms and then trying to treat those symptoms.” It needs to be mentioned that genomics is not without controversy. With new genetic capabilities comes much debate about how they should be used. For instance, can genetic information be used by insurance companies to discriminate against those with risk factors for certain disorders? Or what about the use of genetic testing to determine whether you’re having a healthy child? And then there is the issue of gene editing and giving scientists the ability to alter the DNA of many organisms. These are areas fraught with significant moral and ethical questions. “What’s powerful about the technology is it can actually cure disease,” says Alderson. “What is scary about the technology is, do we understand the full downstream effects in editing the genome? We don’t know everything about the genome and its role and implications in human health. So there is a lot of complexity here.” There is also much hope. After all, genomics recently played a starring role in the development of COVID-19 vaccines in record time. The vaccines wouldn’t be here without genomics and its power to sequence the virus rapidly and correctly. Alderson says that genomics helped scientists gain a deep understanding of the virus and gave them the ability to create the vaccine in less than a year — a previously unheard-of accomplishment. She believes the use of genomics in immunology will accelerate the advancement of treatment options in other disease areas and help address any new viruses that could emerge. “Genomics is obviously one of the most exciting advancements in our lifetime in health care,” Alderson concludes. “I couldn’t be more excited about how far we’ve come over the last couple of decades — but also for what lies ahead.” VB Lab Insights content is created in collaboration with a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/18/cybersecurity-ratings-platform-securityscorecard-raises-180m/,Cybersecurity ratings platform SecurityScorecard raises $180M,"SecurityScorecard, a cybersecurity rating and risk-monitoring platform, today announced it has raised $180 million in a series E round of funding. Founded in 2013, SecurityScorecard enables companies like Nokia, AXA, Liberty Mutual, and Cadence Bank to evaluate and continuously monitor their security, including weaknesses in third-party vendors that they use. “In the same way you can get a credit score and use credit scores to measure financial trustworthiness, cybersecurity ratings do the same,” company CEO and cofounder Alex Yampolskiy told VentureBeat. Using seven years of historical data, SecurityScorecard assigns ratings from A to F to help security personnel address their most important vulnerabilities and evaluate external partnerships, providing an easy way to “understand their cyber posture,” COO and cofounder Sam Kassoumeh added. “We have shown that companies with a bad score are 7.7 times more likely to be breached than companies with a good score.” Ratings are given out on a category basis, so a company may receive an average “medium severity” grade for their patching cadence and DNS Health but a “high severity” rating for their network security, for example. The score is really just the tip of the iceberg though, and SecurityScorecard offers additional tools and services based on this metric, including enterprise risk management that helps establish vulnerabilities in IT infrastructure and analytics that enable businesses to “operate with a situational awareness of the cyber risk landscape and make business decisions with more confidence,” Kassoumeh said. While businesses can invest all the money in the world into shoring up their internal defenses, they have limited control over companies they do business with, and data breaches caused by third-party compromises have been a growing problem. Perhaps the most obvious recent example was the SolarWinds supply chain attack, in which a flaw in the company’s Orion network management software was used as a vehicle to spread malicious code to nearly 18,000 of its customers. These included government agencies and tech titans like Microsoft, which revealed at the time that hackers had downloaded source code for Azure and Exchange. Microsoft president Brad Smith called it the “largest and most sophisticated attack the world has ever seen.” SecurityScorecard recently published its own investigations into the Exchange attack, noting that while it was not as extensive as first feared, it was still far-reaching. “Using our proprietary technology to scan the internet for vulnerable public-facing Microsoft Exchange servers revealed 2,500-18,000 vulnerable servers worldwide, a majority of which are in Europe, the Middle East, and Africa,” Kassoumeh said. “We also discovered the vast majority of the victims were located in the United States and Germany, demonstrating a strong degree of intentionality by the perpetrators.” The problem SecurityScorecard is trying to fix is not a new one, of course — Kassoumeh and Yampolskiy first had the idea for SecurityScorecard while working on security for an ecommerce website around a decade ago. But in the intervening years, the technological landscape has increasingly focused on the cloud, which has made the issue more urgent. “Sam and I had the idea for security ratings back in 2013 when we were trying to understand the risks posed by our extended ecosystem of vendors and business partners, in addition to trying to report our own cybersecurity health to our board of directors,” Yampolskiy said. “This problem has only become more acute as companies became more interconnected and moved to the cloud.” Indeed, cloud infrastructure spending has gone through the roof over the past year, driven in large part by the rapid shift to remote working. This opens the doors to a swathe of potential vulnerabilities, which is why cybersecurity spending is expected to grow by 10% in 2021 — putting a company like SecurityScorecard in a strong position. The company said it has added 450 new customers to its roster over the past year, with its international revenue and footprint showing strong growth. It has also received a number of accolades, including recognition by the World Economic Forum as one of 2020’s “technology pioneers,” while Forrester recently included SecurityScorecard as one of the “seven most significant” cybersecurity risk rating platforms. Other players included on that list were BitSight, UpGuard, RiskRecon, Panorays, Prevalent, and Black Kite (formerly Normshield), which secured $7.5 million in funding late last year. This suggests a degree of saturation in the space, but SecurityScorecard is setting out to differentiate itself in several ways. Kassoumeh pointed first to the company’s data, which he said is continuously updated. He said the company is able to gather “27 billion points per week and run one of the largest malware sinkholes networks in the world” — spanning more than 500 million infected machines. “This enables us to continuously gather, attribute, and rate 2 million companies in the world and provide real-time intelligence that does not require any manual inputs or curation,” Kassoumeh explained. “This data enables us to give a ‘fast score’ within minutes, as opposed to days and weeks.” But SecurityScorecard’s biggest selling point may be its focus on the broader ecosystem through a dedicated marketplace that brings a bunch of prebuilt integrations spanning categories such as risk management and compliance, government, vendor risk management (VRM), security information and event management (SIEM), and more. This means customers of Splunk, for example, can access SecurityScorecard’s security ratings, risk category data, and issue-related data directly inside Splunk, helping them monitor their own internal and external cybersecurity risks. “Through this ecosystem, SecurityScorecard empowers customers to gain operational scale through automated workflows [and] continuous risk intelligence gathering by incorporating our cybersecurity data into other solutions they use and reduce time to risk mitigation,” Kassoumeh said. SecurityScorecard had previously raised around $112 million, and its latest $180 million cash injection attracted many returning investors, including Alphabet’s GV and Intel Capital, alongside new backers such as Silver Lake Waterman and T. Rowe Price Associates."
https://venturebeat.com/2021/03/18/white-house-task-force-met-with-private-sector-to-discuss-microsoft-software-vulnerabilities/,White House task force met with private sector to discuss Microsoft software vulnerabilities,"(Reuters) — The White House’s task force looking into the recent hack of Microsoft’s Exchange met this week with representatives of the private sector, White House spokesperson Jen Psaki said in a statement on Wednesday. The group, which met on Monday, “included private sector members for the first time,” who were invited “based on their specific insights to this incident,” she said. Hacking groups are using recently discovered flaws in the Exchange mail server software to break into targets around the world. The White House group noted that paying to mitigate the hack “weighs particularly heavily on small businesses,” Psaki said. The breadth of the exploitation has led to urgent warnings by authorities in the United States and Europe about the weaknesses found in Exchange. The White House group “discussed the remaining number of unpatched systems, malicious exploitation, and ways to partner together on incident response, including the methodology partners could use for tracking the incident going forward,” Psaki said. The security holes in the widely used mail and calendaring software leave the door open to industrial-scale cyber espionage, allowing malicious actors to steal emails virtually at will from vulnerable servers or to move elsewhere in the network. Tens of thousands of organizations have already been compromised, Reuters reported, and new victims are being made public daily."
https://venturebeat.com/2021/03/18/enterprises-share-the-challenges-of-migrating-legacy-systems-to-the-cloud/,Enterprises share the challenges of migrating legacy systems to the cloud,"Enterprises are hopeful to achieve the many business, technical, and financial benefits of moving legacy systems to the cloud. However, the road to migrating and running these systems on cloud infrastructure can be rough for companies that fail to plan ahead. That’s the takeaway from a 2020 report published by enterprise technology consultancy Lemongrass, which in partnership with Upwave surveyed over 150 IT leaders with budgets of at least $1 million annually on the goals and obstacles in moving systems onto the cloud. The global public cloud computing market is set to exceed $362 billion in 2022, according to Statista. (In 2018 alone, Amazon Web Services earned $26 billion in revenue for parent company Amazon.) IDG reports that the average cloud budget is up from $1.62 million in 2016 to a whopping $2.2 million today. But cloud adoption continues to present challenges for enterprises of any size. A separate Statista survey identified security, managing cloud spend, governance, and lack of resources and expertise as significant barriers to adoption. A Lemongrass survey found that IT leaders were motivated to migrate systems by desires to secure data, maintain data access, save money, optimize storage resources, and accelerate digital transformation. IT management systems were the most likely legacy applications to move to the cloud, while security and ecommerce were the second-most and third-most likely, respectively. But completing these migrations often isn’t easy — particularly as they’re primed to accelerate in a post-pandemic world. Security and compliance were listed by 59% of IT leaders as the top challenge facing enterprises when moving legacy systems to the cloud, while 43% of respondents said the migrations took too long. Thirty-eight percent said costs were too high and 33% said a lack of in-house skills was the top complicating factor. Cloud migrations can cost between $100,000 and $250,000 and rarely come in under budget, IT leaders told Lemongrass. Sixty-eight percent of respondents said it was hard to find people with the proper skills and 40% said migrations took at least seven months to complete. IT leaders said that the top lessons they learned while migrating systems to the cloud were: Successful migrations to the cloud can lead to a wealth of benefits, surveys show. For example, according to OpsRamp, the average savings from cloud migration come to around 15% on all IT spending. Small and medium businesses benefit the most, as they spend 36% less money on IT that way. Moreover, 59% of companies report an increase in productivity after migrating apps and service to the cloud, Microsoft says. “The survey findings are very consistent with feedback we receive from our customers,” Lemongrass CTO Vince Lubsey said. “Enterprises … understand there are challenges but the benefits far outweigh the obstacles. The key to success is following best practices, proper training and time management. It also helps to have the guidance of an experienced partner to create the required cloud operating model.”"
https://venturebeat.com/2021/03/17/tonkeans-updated-no-code-platform-aims-to-simplify-application-deployment/,Tonkean’s updated no-code platform aims to simplify application deployment,"Businesses are looking for any edge that will increase efficiency, particularly as they need to scale application development. As a result, internal teams are facing greater demands to coordinate complex tasks that require collaboration across different teams and departments. Tonkean has developed a no-code platform to automate many of these workflows. Now it’s updating that platform, which it refers to as an operating system for business operations, with new no-code tools that allow more employees to participate in the creation and deployment of applications. According to Tonkean CEO and cofounder Sagi Eliyahu, this next generation of its service should allow for greater governance of those processes while still expanding democratization of the tasks. “We’re focused on using no-coding to explore the promise of building mission-critical solutions,” he said. “So we’re enabling a standard way to build software in an organization in a safe way.” Founded in 2015, the company raised $24 million last April to advance its orchestration layer that sits on top of existing enterprise applications. While many companies want to solve issues involving data, the impediments often involve questions related to sales or legal or corporate policy. A lot of software is built to focus on that data, but it doesn’t help manage the way these other issues are discussed and resolved when multiple teams and departments must be consulted and reach an agreement. “If you think about contracts, you’ll usually buy a contract management system to manage the contract files,” Eliyahu said. “But the process of contracts is bigger than just legal because it’s going across sales and going across hiring managers and so on. And all of those interactions, all that back and forth following up is really manual today.” This is because much of that coordination tends to happen by email, which makes it hard to track changes and resolve outstanding issues. Tonkean’s no-code platform helps manage the business logic of these processes so that enterprises don’t have to introduce yet another new application that requires people to change workflows. It also helps to manage updates to things like compliance. The new release takes this orchestration system one step farther by introducing a concept called “enterprise components.” Eliyahu described these as building blocks that teams can define and that can be reusable. This ideally makes it easier to build new applications that improve coordination across the orchestration platform. In addition, the company has established a Solution Studio to create an environment where different operations teams and business units can collaborate to build new software. The goal here is to abstract as many of the issues coders typically must face, such as writing software for multiple environments and tracking deployment lifecycles. “No-code has the promise of really expanding the pie by allowing more people who are not technical to build things to use the software,” he said. “To build software in a mission-critical way that actually solves massive problems in the enterprise, it’s not enough to just not write code. You need to have best practices and you need to stay compliant.”"
https://venturebeat.com/2021/03/17/language-models-struggle-to-answer-questions-without-paraphrasing-training-data/,Language models struggle to answer questions without paraphrasing training data,"The task of long-form question answering (LFQA) involves retrieving documents relevant to a given question and using them to generate a paragraph-length answer to that question. While many machine learning models have recently been proposed for LFQA, the work remains challenging, as a recent paper coauthored by University of Massachusetts Amherst and Google researchers demonstrates. The researchers developed an LFQA system that achieves state-of-the-art performance on a popular dataset. But they found that even the best LFQA models, including theirs, don’t always answer in a way that’s grounded in — or demonstrates an understanding of — the documents they retrieve. Large language models like OpenAI’s GPT-3 and Google’s GShard learn to write humanlike text by internalizing billions of examples from the public web. Drawing on sources like ebooks, Wikipedia, and social media platforms like Reddit, they make inferences to complete sentences and even whole paragraphs. But studies demonstrate the pitfall of this training approach. Open-domain question-answering models — models theoretically capable of responding to novel questions with novel answers — often simply memorize answers found in the data on which they’re trained, depending on the data set. Because of this, language models can also be prompted to show sensitive, private information when fed certain words and phrases. In this most recent study, the coauthors evaluated their LFQA model on ELI5, a Python library that allows developers to visualize and debug machine learning models using a unified API. There was significant overlap between the data used to train and test the model; as high as 81% were given in paraphrased form. And the researchers say that this reveals issues with the model in addition to ELI5. “[Our] in-depth analysis reveals [shortcomings] not only with our model, but also with the ELI5 dataset and evaluation metrics. We hope that the community works towards solving these issues so that we can climb the right hills and make meaningful progress,” they wrote in the paper. Memorization isn’t the only challenge large language models struggle with. Recent research shows that even state-of-the-art models struggle to answer the bulk of math problems correctly. For example, a paper published by researchers at the University of California, Berkeley finds that large language models including OpenAI’s GPT-3 can only complete 2.9% to 6.9% of problems from a dataset of over 12,500. OpenAI itself notes that its flagship language model, GPT-3, places words like “naughty” or “sucked” near female pronouns and “Islam” near words like “terrorism.” A paper by Stanford University Ph.D. candidate and Gradio founder Abubakar Abid detailed the anti-Muslim tendencies of text generated by GPT-3. And the Middlebury Institute of International Studies’ Center on Terrorism, Extremism, and Counterterrorism claims that GPT-3 could reliably generate “informational” and “influential” text that might “radicalize individuals into violent far-right extremist ideologies and behaviors.” Among others, leading AI researcher Timnit Gebru has questioned the wisdom of building large language models, examining who benefits from them and who’s disadvantaged. A paper coauthored by Gebru earlier this year spotlights the impact of large language models’ carbon footprint on marginalized communities and such models’ tendency to perpetuate abusive language, hate speech, microaggressions, stereotypes, and other dehumanizing language aimed at specific groups of people."
https://venturebeat.com/2021/03/17/responsible-ai-in-health-care-starts-at-the-top-but-its-everyones-responsibility-vb-live/,Responsible AI in health care starts at the top — but it’s everyone’s responsibility (VB Live),"Presented by Optum Health care’s Quadruple Aim is to improve health outcomes, enhance the experiences of patients and providers, and reduce costs — and AI can help. In this VB Live event, learn more about how stakeholders can use AI responsibly, ethically, and equitably to ensure all populations benefit. Register here for free. Breakthroughs in the application of machine learning and other forms of artificial intelligence (AI) in health care are rapidly advancing, creating advantages in the field’s clinical and administrative realms.  It’s on the administrative side — think workflows or back office processes — where the technology has been more fully adopted. Using AI to simplify those processes creates efficiencies that reduce the amount of work it takes to deliver health care and improves the experiences of both patients and providers. But it’s increasingly clear that applying AI responsibly needs to be a central focus for organizations who use data and information to improve outcomes and the overall experience. “Advanced analytics and AI have a significant impact in how important decisions are made across the health care ecosystem,” says Sanji Fernando, SVP of artificial intelligence and analytics platforms at Optum. And, so, the company has guidelines for the responsible use of advanced analytics and AI for all of UnitedHealth Group. “It’s important for us to have a framework, not only for the data scientists and machine learning engineers, but for everyone  in our organization — operations, clinicians, product managers, marketing — to better understand  expectations  and how we want to drive breakthroughs to better support our customers, patients, and the wider health care system,” he says. “We view the promise of AI and its responsible use as  part of our shared responsibility to use these breakthroughs appropriately for patients, providers, and our customers.” The guideline focuses on making sure everyone is considering how to appropriately use advanced analytics and AI, how these models are trained, and how they are monitored and evaluated over time, he adds. Machine learning models, by definition, learn from the available data that’s being created throughout the health care system. Inequities in the system may be reflected in the data and predictions that machine learning models return. It’s important for everyone to be aware that health inequity may exist and that models may reflect that, he explains. “By consistently evaluating  how models may classify or infer, and looking at how that affects folks of different races, ethnicities, and ages, we can  be more aware of where some models may require consistent examination to best ensure they are working the way we’d like them to,” he says. “The reality is that there’s no magic bullet to ‘fix’ an ML model automatically, but it’s important for us to understand and consistently learn where these models may impact different groups.” Transparency is a key factor in delivering responsible AI. That includes being very clear about how you’re training your models, the appropriate use of data used to train an algorithm, as well as data privacy. When possible, it also means understanding how specific features are being identified or leveraged within the model. Basics like an age or date are straightforward features, but the challenge arises with paragraphs of natural language and unstructured text. Each word, phrase or paragraph can be considered a feature, creating an enormous number of combinations to consider. “But understanding feature importance — the features that are more important to the model — is important to provide better insight into how the model may actually be  working,” he explains. “It’s not true mathematical interpretability, but it gives us a better awareness.” Another important factor is being able to reproduce the performance and results of a model. Results will necessarily change when you train or retrain an algorithm, so you want to be able to trace that history, by being able to reproduce results over time. This ensures the consistency and appropriateness of the model remains constant (and allows for potential adjustments should they be needed). There’s no shortage of tools and capabilities available across the field of responsible AI because there are so many people who are passionate about making sure we all use AI responsibly. For example, Optum uses an open-source bias audit tool from the University of Chicago. But there are any number of approaches and great thinking from a tooling perspective, Fernando says, so it’s really becoming an industry best practice to implement a policy of responsible AI. The other piece of the puzzle requires work and a commitment from everyone in the ecosystem: making responsible use everyone’s responsibility, not just the machine learning engineer or data scientist. “Our aspiration is that every employee understands these responsibilities and takes ownership of them,” he says, “whether UHG employees are using ML-driven recommendations in their day-to-day work, designing new products and services, or they’re the data scientists and ML engineers who can evaluate models and understand output class distributions, we all have a shared responsibility to ensure these tools are achieving the best and most equitable results for the people we serve.” To learn more about the ways that AI is impacting the delivery and administration of health care across the ecosystem, the benefits of machine learning for cost savings and efficiency, and the importance of responsible AI for every worker, don’t miss this VB Live event. Don’t miss out! Register here for free. You’ll learn: Speakers:"
https://venturebeat.com/2021/03/17/bigfoot-biomedical-secures-up-to-57-million-in-combined-debt-and-equity-financing-from-madryn-asset-management/,Bigfoot Biomedical® Secures up to $57 Million in Combined Debt and Equity Financing from Madryn Asset Management,"MILPITAS, Calif.–(BUSINESS WIRE)–March 17, 2021– Bigfoot Biomedical, a company dedicated to better health outcomes for people with insulin-requiring diabetes, announced today that it has secured new financing of up to $57 million from Madryn Asset Management LP, an alternative asset management firm focused on investments in innovative health care companies. The proceeds of the investment will be used by Bigfoot to support commercialization of its innovative Bigfoot Unity™ Diabetes Management Program. Developed to support the treatment of Type 1 and Type 2 diabetes, the Bigfoot Unity Program will leverage first-of-its-kind technologies to simplify and connect key aspects of insulin management to enable personalized, proactive and remote patient care. Currently, Bigfoot Biomedical is anticipating FDA clearance of its Bigfoot Unity System, a centerpiece of the Bigfoot Unity Program featuring smart pen caps that provide insulin dose decision support and enable patients to follow their doctor’s instructions in a convenient, simple way. “We believe Bigfoot’s innovative technology and solutions will have a significant impact on care for the millions of people in the U.S. with Type 1 or Type 2 insulin-requiring diabetes,” said Avi Amin, Managing Partner of Madryn. “We look forward to a close collaboration with Bigfoot CEO Jeffrey Brewer and the Bigfoot Biomedical team as they continue to scale their business and execute against their unique growth strategy.” “As our company prepares for commercialization of the Bigfoot Unity System, we are pleased to partner with Madryn, a firm with demonstrated success identifying transformative technologies within the healthcare industry,” said Jeffrey Brewer, CEO of Bigfoot Biomedical. “Their investment will enable us to scale our commercial efforts with greater speed and geographic reach. Bigfoot’s deep and respected investor base reflects a strong validation of our commitment to developing breakthrough solutions in diabetes care.” With its holistic approach to diabetes management for patients on Multiple Daily Injection therapy and their health care providers, the Bigfoot Unity Program will address some of the most critical limitations in diabetes innovation by focusing on simplicity, convenience and compatibility. About Bigfoot Biomedical, Inc. Bigfoot Biomedical was founded by a team of people with personal connections to Type 1 and Type 2 diabetes. We seek to change the paradigm of care for diabetes. Bigfoot is an unconventional company taking an unconventional approach. Unlike others, we’re looking at insulin therapy holistically and utilizing services, support, and novel business models. We’ll partner with health care providers to deliver simple, connected, and comprehensive solutions for the large number of people who have been overlooked by diabetes innovation. Learn more at www.bigfootbiomedical.com. Follow us on Twitter @BigfootBiomed, Instagram and Facebook. About Madryn Asset Management, LP Madryn Asset Management, LP is a leading alternative asset management firm that invests in innovative healthcare companies specializing in unique and transformative products, technologies, and services. The firm draws on its extensive and diverse experience spanning the investment management and healthcare industries, and employs an independent research process based on original insights to target attractive economic opportunities that deliver strong risk-adjusted and absolute returns for its limited partners while creating long-term value in support of its portfolio companies. For additional information, please visit www.madrynlp.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210317005621/en/ Red Maxwellpress@bigfootbiomedical.com"
https://venturebeat.com/2021/03/17/anonymized-x-ray-datasets-can-reveal-patient-identities/,‘Anonymized’ X-ray datasets can reveal patient identities,"Chest X-rays are used around the world to screen for diseases from pneumonia to COPD. But while they play a critical role in clinical care, discovering certain abnormalities in X-rays can be a challenging task for radiologists. That’s given rise to AI-powered, X-ray analyzing disease classification systems, some of which have demonstrated promising performance. However, these systems require a large amount of patient data from which to learn to make diagnoses, which can have frightening privacy implications if the data isn’t properly anonymized. A study coauthored by researchers at the University Erlangen-Nurnberg in Erlangen, Germany sought to determine the extent to which patient data could be compromised by an X-ray classification system. Drawing on a public dataset of over 112,000 chest x-rays, they developed a technique — a deep learning-based reidentification model — that can identify whether two X-ray images are from the same person with 95.55% accuracy, suggesting that at least some datasets are vulnerable to attack. As the researchers note, publicly available datasets that are supposedly anonymized might contain sensitive patient-related information, including diagnoses, treatment histories, and clinical institutions. If an X-ray of known person is accessible to a malicious attacker and a properly working reidentification model exists, then the model could be used to compare the given X-ray to each individual image in an X-ray dataset. In this way, a person could be linked to the sensitive data contained in the dataset. The coauthors say their technique is robust to “non-rigid” transformations that might appear between two images of the same person in a public dataset, such as deformations in the shape of the lungs. They hypothesize that noisy image patterns characteristic to unique patients appear in the datasets, making people easier to identify. But even datasets that show little correlation between noise patterns and identities can be compromising, according to the coauthors. “Reidentification is applicable for data that was acquired in various hospitals around the world where other preprocessing steps may be taken before data publication compared to the ChestX-ray14 dataset,” the researchers wrote in a paper describing their work. “We conclude that publicly available medical chest X-ray data is not entirely anonymous. Using a deep learning-based reidentification network enables an attacker to compare a given radiograph with public datasets and to associate accessible metadata with the image of interest. Thus, sensitive patient data is exposed to a high risk of falling into the unauthorized hands of an attacker who may disseminate the gained information against the will of the concerned patient.”  Data leakage of this kind would require an attacker to gain access to an image of a known person. However, even if an attacker has only a fraction of an image of an unknown patient, the researchers say their technique could be used to find the same patient across various datasets. Assuming multiple datasets contain the same patient but different metadata, an attacker might be able to obtain a complete picture of the patient. Given the increased frequency of medical records breaches, this isn’t an unrealistic scenario. In 2017, 27% of exploits were related to health care data in 2017. And in the first half of 2019 alone, more than 31 million patient records were breached — twice the amount of breached records from 2018’s total of 15 million. “We hypothesize that collecting patient information by this means could significantly help an attacker infer the true identity of the patient,” the researchers wrote. “We therefore urge that conventional anonymization techniques be reconsidered and that more secure methods be developed to resist the potential attacks by deep learning-based algorithms.” Solutions to these challenges in health care data will necessarily entail a combination of techniques, approaches, and paradigms. Securing data requires data-loss prevention, policy and identity management, and encryption technologies, including those that allow organizations to track actions that affect their data. On the privacy front, experts agree that transparency is the best policy — deidentification capabilities that remove or obfuscate personal information are table stakes for health systems, as are privacy-preserving methods like differential privacy, federated learning, and homomorphic encryption. “I think [federated learning] is really exciting research, especially in the space of patient privacy and an individual’s personally identifiable information,” Andre Esteva, head of medical AI at Salesforce Research, told VentureBeat in a previous interview. “Federated learning has a lot of untapped potential … [it’s] yet another layer of protection by preventing the physical removal of data from [hospitals] and doing something to provide access to AI that’s inaccessible today for a lot of reasons.”"
https://venturebeat.com/2021/03/17/oracle-updates-cloud-data-warehouse-to-boost-access-for-data-analysts/,Oracle updates cloud data warehouse to boost access for data analysts,"Oracle today updated its Autonomous Data Warehouse to enable data analysts to load, transform, and generate insights from data with no intervention on the part of an internal IT team required. The latest update to the Oracle data warehouse cloud service also enables data analysts to automatically create business models and discover patterns, along with providing a set of tools for preparing data and building machine learning models guided by AutoML, a set of open methods and processes for building AI models. Other capabilities that have been added include support for the Python programming language, cognitive text analytics, graphs that can be invoked using a set of visualization tools, and an ability to deploy and manage native in-database models and ONNX-format classification and regression models outside of the core database. The goal is to make it simpler for both professional and citizen data analysts to access data whenever they want using a fully autonomous platform, said George Lumpkin, VP of product management for Oracle. “We’re trying to provide what a cloud data warehouse should be,” he said. As the provider of a data warehouse that is widely employed in the enterprise, Oracle is attempting to fend off increased competition from cloud service providers such as Amazon Web Services (AWS), Microsoft, Google, and Snowflake. Oracle alternatively makes available data warehouse platforms from a single vendor that can be deployed in both its cloud and on-premises IT environments at a time when the bulk of enterprise data continues to reside in local datacenters. Rather than requiring organizations to make a wholesale shift to the cloud, Oracle enables them to make that transition at their own pace, Lumpkin said. In contrast to rival cloud data warehouses, Oracle has built its approach around a managed service that eliminates the need to dedicate IT professionals to managing, securing, and maintaining a cloud platform, Lumpkin added. Oracle also provides access to a low-code Oracle APEX (Application Express) tool that makes it possible for both “citizen integrators” and professional developers to build applications that can be deployed via REST application programming interfaces (APIs), Lumpkin added. It’s too early to say to what degree business units within organizations might be willing to enable data analysts and scientists to access, manage, and analyze data without any oversight from a central IT function. However, as IT becomes increasingly automated, it’s apparent that many of the manual data management tasks that used to require an IT professional are falling by the wayside. The time when end users had to wait days for an IT professional to set up am SQL query to generate a report is all but over as self-service tools become more widely available. In effect, Oracle is making a case for transferring data management tasks to its platform. Less clear is to what degree that might lower the total cost of IT for companies. It’s worth remembering that as data becomes more accessible, usage will increase so organizations may wind up spending more on analytics. The difference is that they will hopefully be able to derive more business value from data that has become easier to interrogate in near real time. In the meantime, the odds that most organizations will migrate all their relevant data to the cloud anytime soon is low. In fact, most organizations will be managing multiple data warehouses for years to come. The challenge will be determining what type of data needs to reside where based on use cases that are becoming more varied with each passing day."
https://venturebeat.com/2021/03/17/copado-acquires-new-context-to-embrace-multicloud-devsecops/,Copado acquires New Context to embrace multicloud DevSecOps,"Salesforce-focused developer operations (DevOps) platform Copado has acquired developer security operations (DevSecOps) startup New Context. Terms of the deal were not disclosed. Founded out of Spain in 2013, Copado promises businesses an integrated Salesforce-native platform that covers every part of the DevOps process, spanning agile planning, continuous delivery, automated testing, and compliance. Bringing New Context under its wing adds further compliance and security automation tools to Copado’s platform while helping it expand further beyond the Salesforce cloud. “Up to today, Copado was providing compliance automation tools for Salesforce-specific pipelines with Copado Compliance Hub,” Copado CMO Andrew Leigh told VentureBeat. “New Context will add to our future roadmap with multicloud tools supporting compliance and security automation within and outside Salesforce app development.” Given that every company is now effectively a software company, the need to focus on digital security and data privacy is more vital than ever. Moreover, errors are more likely to be introduced to code bases due to shortened software release cycles, which is why security is a fundamental facet of the engineering process. New Context has created multicloud tools and best practices designed to help DevSecOps teams deliver software built on secure, compliant data platforms, including attribution, data provenance, and forensics auditability. It’s all about “enabling enterprises to build software faster and safer” at a time when data privacy and cybersecurity concerns are perhaps at their highest yet. The company has operated largely under the radar since it was founded out of San Francisco back in 2013, but it amassed an impressive roster of enterprise clients, including GE and Royal Dutch Shell. New Context represents Copado’s second acquisition after it snapped up Australia’s ClickDeploy back in September. This latest acquisition comes exactly a month after it announced a $96 million tranche of funding co-led by Salesforce Ventures and Insight Partners. At the time, Copado CEO Ted Elliott told VentureBeat the company planned to use the funding to support “multicloud projects” outside the Salesforce ecosystem. Buying New Context goes some way toward supporting that goal. “Today’s data doesn’t live in any one cloud or enterprise platform,” Leigh said. “To unlock the full potential of business data, the enterprise has to develop software that is multicloud. Today we have several customers orchestrating their multicloud software development practices through Copado. The New Context acquisition allows Copado to double down on that capability by adding their multicloud DevSecOps experience in products and services.”"
https://venturebeat.com/2021/03/17/why-ebay-wants-to-be-inside-your-ears-in-a-good-way-vb-live/,Why eBay wants to be inside your ears – in a good way (VB Live),"Presented by Dolby Labs Consumers are spending 30% more time listening to spoken word audio, including podcasts and news, and 43% listen daily. That’s why audio is an incredibly powerful way to reach customers. Learn how to add it to your marketing mix in this VB Live event. Register here for free. Audio marketing is having a moment, with podcast listening growing and platforms like Clubhouse springing up, but eBay has always believed in the value of branded audio content, says Rebecca Michals, director of seller community and engagement at eBay. Historically, the company has relied on various channels to help new and experienced sellers learn. Their radio show, called eBay Radio, ran for 15 years from 2003, evolving into the eBay for Business Podcast in 2018. Since the launch of the podcast, there have been over 650,000 downloads of the 130+ episodes. As Michals points out, audio content is a particularly useful way to engage eBay listeners while their eyes and hands are busy — packaging up orders, for instance. And, she adds, podcast audiences are also highly qualified. “Everyone has actively decided to listen and engage with the podcast — they’re inviting us into their homes and into their ears,” she says. “It’s an effective channel for connecting with your audience or your customer and having that personal, almost one-to-one feeling of engagement between the brand and the customer.” During last year, downloads for the podcast grew by 17%, accompanied by a lot of additional engagement from their sellers. Michals puts it down, in part, to the “2020 effect.” “Everyone is looking for something to engage with, be entertained by, be informed by,” she says, and she doesn’t expect that to change, now that the year has ticked over. “We definitely plan for more growth in the year ahead,” she adds. With a weekly niche podcast such as theirs, the challenge is keeping the content fresh and informative, mapping to what the audience is most interested in, and what’s top of mind. They keep an eye on reviews and rankings, but also continually poll the existing audience to determine what kind of content they want to hear. (The top listener request is for segments with eBay employees and execs that introduce and explain new products, features, or policies.) They also use data from their complementary channels, other branded content, social channels, YouTube, and content they publish on their own site, reviewing the performance of marketing across those channels to determine what is most successful, and what kind of content is landing in the community. “With podcasting, you don’t get a lot of information on the success of individual segments within a show,” Michals says. “We have to use a little more external data to understand what’s going to be the most successful. We know what resonates in other channels is likely to perform well and be well-received in the podcast too.” Right now they’re focusing their audio marketing on increasing awareness and reach of the podcast, and continuing to use data to drive the audience. They’ve found, and research shows, that a weekly 30-to-60-minute episode is right for a niche podcast. Michals says that they are considering expanding the model in the future, but any plans rely on their ability to show up for their audience in the ways that are most compelling for them. “If you find that any content is working anywhere, it’s important to figure out how to extend it so you can continue the impact,” she says. “Targeted content for passionate audiences has value. We’re always trying to make sure that our audience feels like we’re speaking directly to them, to their interests and priorities and their needs as sellers on eBay.” And, she notes, the success of audio content marketing relies heavily on the quality of the audio. “This is in some ways a very intimate format — we’re literally inside of people’s heads, in a good way,” Michals says. “But for someone to invite you into their ears, and to offer an almost personal feeling of connection, you have to make sure that it sounds really high-quality.” For remote or call-in guests, it’s okay to have old-school phone level audio, she explains, but for the conversational pieces and the interviews, it has to be a high-quality experience. Right now, that relies on furnishing guests with the equipment they need to sound as good as possible at home. “We’re always pursuing quality around audio and figuring out better and simpler ways to have it be as professional-sounding as possible with the least amount of hassle to guests,” she explains. Now seems like the right time to launch an audio marketing strategy, but Michals says don’t plunge, plan. The most important part is to understand the why. Why do you want to pursue a branded audio channel for your business, and what are the possible benefits for the brand? Why do you have a story to tell? Why is it unique? Why is it particular to you, your brand, your audience? It has to be super relevant, and it has to be unique, Michals asserts. “Nobody listens to a podcast to just hear marketing messages or commercials, she says. “You have to understand why your audience is going to show up every week to listen to what you have to say. What’s the key differentiator? If you can take your brand’s mission and purpose, and bring that into an audio strategy, make sure that the content is engaging, informing, entertaining even, that’s going to be the success for increasing your brand’s breadth.” To learn more about the ways audio content can boost your brand and engage your audience, and how to offer a flawless audio experience, don’t miss this VB Live event. Don’t miss out! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/03/17/octoml-raises-28m-for-machine-learning-deployment-optimization/,OctoML raises $28M for machine learning deployment optimization,"Studies like the 2020 State of AI report from McKinsey have found that businesses capable of deploying multiple AI models are considered high performers, but a survey of business leaders included in the report found fewer than 20% have taken deep learning projects beyond the pilot stage. It’s well known that most businesses face challenges deploying AI in production, which has led to a rise in startups that serve needs like AIOps or auditing. In the latest news for such a company, OctoML today raised a $28 million series B funding round. OctoML helps businesses accelerate AI model inference and training and relies on the open source Apache TVM machine learning compiler framework. TVM is currently being used by companies like Amazon, AMD, Arm, Facebook, Intel, Microsoft, and Qualcomm. OctoML will use the funding to continue building out products like its Octomizer platform and investing in its go-to-market strategy and customer service teams. “We started the TVM work as a research project at the University of Washington about five years ago, and all the key people in the project they all got their Ph.D.s and are part of the company now,” OctoML CEO and cofounder Luis Ceze told VentureBeat. “We’re focused on making inference fast on any hardware, and support cloud and edge deployments.” Last month, OctoML joined more than 20 startups — including Algorithmia and Determined AI — that have banded together to create the AI Infrastructure Alliance, an effort to promote interoperability between the offerings from AI startups and advance alternatives to popular cloud AI services. The $28 million funding round was led by Addition, with participation from existing investors Madrona Venture Group and Amplify Partners. OctoML has raised $47 million to date, including a $3.9 million seed funding round in October 2019, just months after the company was founded. OctoML is based in Seattle with remote employees across the United States."
https://venturebeat.com/2021/03/17/cymbio-which-connects-brands-with-markets-raises-7m/,"Cymbio, which connects brands with markets, raises $7M","Cymbio, a startup creating a product that connects brands with markets, today closed a $7 million series A round led by Vertex Ventures. The Tel Aviv, Israel-based company says the funds will be used to expand Cymbio’s footprint across R&D, sales, and marketing. Brands manage dozens to thousands of digital sales channels. Setting up and connecting to marketplaces while navigating retailers’ systems and requirements can be a significant challenge, particularly when factoring in the uptick in ecommerce activity attributable to the pandemic. Consumer spending in Q4 2020 with U.S. merchants alone increased 32.1% year-over-year, according to U.S. Department of Commerce figures. Founded in 2015 by childhood best friends Roy Avidor and Mor Lavi, Cymbio’s platform integrates with both brands’ and retailers’ internal systems, leveraging automation technology to handle setup, onboarding, and daily management of product data, imagery, inventory, orders, and more. Cymbio claims to enable things like real-time, cross-platform inventory and order updates by replacing manual tasks with preconfigured rules. Cymbio customers get a product catalog showing their products, product images, and product details relevant to their market partners. Information and pricing are automatically updated and the data is reformatted to meet a retailer’s requirements. All retailers can be managed from a single hub, including marketplaces, department stores, and boutiques. And inventory is updated in real time and sent based on each retailer’s formats and desired frequencies. Cymbio also automates return, shipping, and cancelation processes. For example, it can create acceptance roles for orders or trigger refunds and notifications when returns are initiated by customers on marketplaces. “Our AI, data, and normalization engine make it simple for brands to map data, taxonomy and product attributes and sync them with a vast variety of retailer data-structures,” Avidor explained to VentureBeat via email. “One of the challenges brands, retailers, and marketplaces experience is the difference in the way they describe attributes such as centimeters and inches, different sizes, colors. A brand can look at a category as shoes, but the marketplace has a different and more detailed categorization. Cymbio’s AI engine allows brands to complete this task instantly, allowing brands to get up and running across an endless number of retailers and marketplaces.”  Cymbio claims to support more than 600 retailers, marketplaces, and department stores across the U.S., Canada, Europe, and the Asia-Pacific region. Shoe brand Camper, a Cymbio client, says that it notched 67% year-over-year growth in wholesale sales via drop ship (where sellers accept customer orders but don’t keep goods sold in stock) and marketplaces after adopting the Cymbio platform. Melissa Shoes, another customer, says that average revenue increased by 70% while average units went up 42% as the number of assortments climbed 2.8 times. “As retail is in its challenging moments, Cymbio brings a new approach for business partnerships between brands and retailers,” Melissa founder and CEO Michele Levy said. “Their platform enables us to strengthen our sales capabilities with each of our partners while generating mutual growth with low risk.” Payoneer founder Yuval Tal also participated in 38-employee Cymbio’s round, along with existing investors Udian Investments and Ron Zuckerman. It’s the company’s first public fundraising round to date and brings Cymbio’s total raised to $10.1 million."
https://venturebeat.com/2021/03/17/vulcan-cyber-launches-free-vulnerability-management-service-with-21m-in-new-funding/,Vulcan Cyber launches free vulnerability management service with $21M in new funding,"Vulcan Cyber, a startup developing tools to help enterprise customers detect and fix software vulnerabilities, today announced that it raised $21 million in a series B round led by Dawn Capital. CEO Yaniv Bar-Dayan says the funds will be used to support the rollout of new exploit remediation solutions for cloud and app security teams and to deliver Vulcan Free, a no-cost, risk-based vulnerability management platform for cyber risk prioritization. More than a third of web app vulnerabilities are considered high risk, according to a report from Edgescan, and organizations with 101 to 1,000 employees see the most high-risk and critical-risk vulnerabilities. These can be expensive if left unaddressed. In recent years, the average cost of a security breach has generally hovered between $3.5 million and $4 million. Bar-Dayan asserts that legacy vulnerability management solutions are ineffective and leave organizations exposed. By adopting a software-as-a-service delivery model, he says that Vulcan can provide better remediation capabilities that are effective for a wider, modern user base. Toward this end, Bar-Dayan, who founded Vulcan Cyber in 2018 with friends Roy Horev and Tal Morgenstern, claims that the newly launched Vulcan Free is one of the industry’s first free risk-based vulnerability management products. The goal with Vulcan Free is to make the service available to the wider market and in the process improve remediation efforts industry-wide, particularly in cloud and app environments. “The launch of Vulcan Free underscores the Vulcan Cyber philosophy that vulnerability prioritization is not an end goal, but simply one element in proper remediation,” Bar-Dayan told VentureBeat via email. “Vulcan Free changes decades-old market dynamics that traditionally focus on vulnerability identification only instead of focusing on driving remediation outcomes. Remediation orchestration is the only viable way to deliberately align vulnerability management with the needs of digital business and critical cloud and application environments.”  Vulcan’s cloud-hosted platform monitors security, IT, and DevOps tools via their respective APIs to spot exploits and kick off code remediation, either automatically (adherent to custom or predefined rules) or under the supervision of specialists. Vulcan leverages a threat intelligence network to inform its suite’s alerting and detection policies, which Bar-Dayan says most customers configure and deploy within a few minutes. “Vulcan Cyber ships with dozens of integrations to the tools most used by IT security teams to automate various parts of a remediation campaign,” Bar-Dayan said. “We don’t typically tell teams to replace their tools with Vulcan Cyber, but we just help them get the most out of the tools and investments they’ve already made. We are also able to ingest the data generated by these tools and make sense of it all for the purpose of efficient remediation.  Most of our customers will come to us with massive datasets of vulnerabilities found in their environments, but there is no way they can fix them all … and they probably don’t have to [with the Vulcan vulnerability prioritization engine.]” Vulcan offers dozens of connectors for environments such as Microsoft Azure, Amazon Web Services, Google Cloud Platform, and WhiteSource. Additionally, its products integrate with security testing tools and vulnerability scanners like Black Duck, Nessus, WhiteSource, SourceClear, Qualys, Puppet, Chef, Ansible, and Carbon Black. According to Bar-Dayan, Vulcan’s security approach is somewhat novel in that it targets the vulnerability remediation gap — the time between initial discovery and a fix’s deployment — by minimizing logistical challenges in ways that don’t impact business continuity. One satisfied customer of Vulcan’s dozens is cloud data warehouse company Snowflake, which managed to remediate more than 40% of known vulnerabilities found in one of its core environments using Vulcan’s toolset. “With advances in vulnerability scanning and asset management tools, it’s relatively easy for security teams to collect data from a wide variety of IT assets and computing environments. This has led to increased visibility within an organization, but with the explosion of IT assets, resource-starved security teams are having trouble handling the resulting increase in vulnerabilities and alerts,” Scott Crawford, security research director for S&P-owned 451 Research, said. “Remediation becomes difficult as security and IT must work to find a balance between availability and stability and the fact that some assets must be taken offline for patching. As more vulnerabilities are discovered, teams quickly realize they cannot feasibly resolve them all, so they are left with figuring out which vulnerabilities to prioritize and remediate.” Wipro Ventures participated in 40-employee Vulcan’s latest raise along with YL Ventures and Ten Eleven Ventures, which brings Vulcan’s total raised to date to $35 million following a $10 million series A in June 2019. Beyond bolstering the launch of Vulcan Free, the company plans to use the proceeds to enhance direct sales and grow its channel program and managed security service provider relationships. Vulcan Cyber’s annual recurring revenue grew more than 500% in 2020 over 2019."
https://venturebeat.com/2021/03/17/mediar-therapeutics-adds-new-investors-as-it-advances-anti-fibrotic-treatments/,Mediar Therapeutics Adds New Investors As It Advances Anti-Fibrotic Treatments,"CAMBRIDGE, Mass.–(BUSINESS WIRE)–March 17, 2021– Mediar Therapeutics today announced the completion of a seed extension funding round and welcomed Pfizer Ventures and Ono Venture Investment (OVI) as new syndicate members. Mediar, a discovery-stage company developing new treatments for fibrosis, was co-founded by Mass General Brigham Ventures, formerly Partners Innovation Fund. The company is uniquely focused on the myofibroblast, a cell that is a key driver in fibrosis pathology. “We are impressed by Mediar’s innovative approach to fibrosis and the promise it shows in treating diseases with high unmet medical need,” said Nikola Trbovic, Ph.D., a partner at Pfizer Ventures. Mass General Brigham Ventures led an initial seed round in 2019 alongside other early venture investors. With Pfizer and OVI’s participation in the syndicate, the company has now raised $20 million in total. The funding will support development of the firm’s unique myofibroblast discovery platform and will advance the company’s development of antibody treatments against a suite of novel targets. Mediar leverages its proprietary insights from these pathological cells to exploit a novel class of targets, fibrotic mediators, that play a central role in fibrotic disease. “Mediar’s groundbreaking research into fibrotic mediators shows promise in halting and even reversing fibrosis,” said Hiroshi Yamamoto, President and CEO of OVI. “They have discovered an emerging class of novel targets with enormous potential to help patients with chronically damaged organs.” Meredith Fisher, Ph.D., founding CEO of Mediar and a partner at Mass General Brigham Ventures, said, “We are thrilled to welcome Pfizer Ventures and OVI as new members of our syndicate. Both companies have strong expertise in fibrosis and we look forward to working with their respective investment teams as we advance our exciting therapeutic programs.” About Mediar Therapeutics Mediar is a discovery stage biotechnology company developing treatments to halt, or even reverse, fibrosis. Our platform and pipeline are based on an emerging class of novel targets – fibrotic mediators – that play key roles in modulating myofibroblasts. These cells are drivers of pathophysiology in diseases such as liver cirrhosis, which leave organs chronically damaged. Mediar was co-founded by leading fibrosis researchers from Mass General Brigham and Mass General Brigham Ventures in 2019. For more information, contact info@mediartx.com. About Ono Venture Investment (OVI) Ono Venture Investment (OVI) is the corporate venture capital arm of Ono Pharmaceutical Co., Ltd. (ONO), a research-oriented pharmaceutical company that focuses on delivering first-in-class medicines to patients. Based in the San Francisco Bay life science cluster, OVI makes investments in novel startups developing products and platforms with high potential synergy with ONO’s R&D.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210317005237/en/ Media contact: Jeffrey KrasnerEmerging BioCommunications(617) 840-9806jeff@emergingbio.net"
https://venturebeat.com/2021/03/17/grid-computing-software-developer-incredibuild-raises-140m/,Grid computing software developer Incredibuild raises $140M,"Incredibuild, a developer of grid computing software, today announced that it secured $140 million in funding from ScaleUp investor Insight Partners. The Tel Aviv, Israel-based startup says that the investment, which includes both primary and secondary components, will help expand Incredibuild’s U.S. operations to meet demand for its technology. Computationally intensive tasks like compiling software are often a massive time sink in development. According to Statista, 40% of engineers attribute software project failure to under-resourcing, with 22% blaming missed delivery timeline expectations and 21% pointing at time constraints and premature software releases. Incredibuild, which was founded in 2002 and inspired by NASA’s SETI@home, offers a suite of grid computing software designed to help accelerate computationally intensive tasks by distributing them over a network. Originally sold specifically as a tool to accelerate computing, Incredibuild later expanded with a package that allows customers to implement custom acceleration of jobs with its grid engine. Incredibuild is broken up into several, separately licensable tools that spread compiling source code, building codebases generally, and other software development-related jobs to machines over a network. By distributing processes across idle processors in the network on-premises and dynamically allocating cloud capacity based on availability and cost, the platform frees local resources for other tasks, reducing compute costs by up to 30%. Incredibuild’s tools are available for Windows and Linux and have out-of-the-box support for speeding up builds targeting those two platforms, as well as consoles like Nintendo Switch, PlayStation 5, and Xbox Series S and Series X. Incredibuild has competition in Hypernet Labs, which is developing a platform-agnostic framework called Galileo that promises to expedite code deployment for compute-intensive work. Anyscale also provides a serverless suite that allows developers to build, deploy, and manage distributed apps. But 95-employee Incredibuild claims that over 200,000 users at more than 2,000 companies use its platform, including Microsoft, Amazon, Disney, Epic Games, and Nintendo. Video game developer Turn 10 Studios taps Incredibuild to accelerate builds, rendering from 3DS Max, code analysis, and other tasks during the development of Forza 5. And CryEngine and Unreal Engine feature built-in support for build acceleration via Incredibuild. CEO Tami Mazel Shachar says that Incredibuild has been profitable since day one and experienced a 55% increase in revenue over the past year. “In fiercely competitive markets, top quality frequent releases are vital,” Shachar added. “With the new funding, Incredibuild is poised to expand the market opportunity enabling companies to maintain their competitive edge. With cloud adoption paramount, we plan to further scale and evolve our software development acceleration platform for development teams and release managers in verticals such as finance, gaming, and the growing augmented reality and virtual reality markets.” Insight Partners was joined by Fortissimo Capital in Incredibuild’s funding round. Fortissimo acquired Incredibuild in 2018, and was responsible for appointing Shachar as CEO in January 2020."
https://venturebeat.com/2021/03/17/torch-ai-raises-30m-for-ai-that-unifies-disparate-enterprise-data/,Torch.AI raises $30M for AI that unifies disparate enterprise data,"Torch.AI, a startup developing “network-centric” AI to deliver big data insights, announced that it raised $30 million. The company plans to put the proceeds toward growth as it acquires new customers, particularly U.S. federal agencies in the national security realm, including those operating in high-risk environments. Most enterprises have to wrangle countless data buckets, some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. That’s perhaps why the corporate sector has taken an interest in solutions that ingest, understand, organize, and act on digital content from multiple digital sources. Leawood, Kansas-based Torch.AI, which was founded in 2017, offers one such solution in a platform that connects disparate apps, systems, cloud services, and databases to enable data reconciliation and processing. Torch.AI provides domain-specific pretrained machine learning models for optical character recognition, natural language processing, sentiment analysis, and more that enrich existing data, even if the data is incomplete, flawed, or unstructured. Companies can bring their own models to automate data engineering tasks and workflows ranging from notifying an analyst of an anomaly and its potential impact to recalculating risk or complexity scores, according to Torch.AI. Torch.AI’s platform functions as an enterprise communication system that provides AI-enhanced data transformations, ingesting data from any source. It decomposes data into smaller, normalized bits with visibility and transparency into data lineage and source integrity, delivering “tagging on ingest” capabilities to help users organize and correlate information. In addition, Torch.AI provides a suite that maintains a hardened cybersecurity posture and tools that make the implementation of compliance regulations and policies ostensibly easier. “Most data enablement implementations and analytics suffered from internal data engineering challenges. When we engaged with companies across the U.S., we heard the same thing: doing almost anything meaningful with data was too complicated and took too long,” CEO Brian Weaver told VentureBeat via email. “We discovered we could use the efficiency of an advanced application of machine learning to instantly understand and describe data with atomic detail, in memory and while it is still in motion. We patented the concept and started developing our platform, which intelligently connects all a company’s applications and business systems by overlaying a ‘synaptic mesh.'” The global big data and business analytics market was valued at $169 billion in 2018 and is expected to grow to $274 billion in 2022, according to Statista. While Torch.AI’s AI-powered data reconciliation is more holistic than most, it has a number of competitors, including BackboneAI, which last year emerged from stealth with a product designed to unify enterprise data sets with AI. There’s also Tamr, a Cambridge, Massachusetts-based startup that uses machine learning to speed up data analytics workflows. And there’s Quantexa, which last July raised $64.7 million to further develop its AI platform that extracts insights from big data. But Torch.AI claims to have quickly established itself in the private sector with a roster of blue-chip private clients including Microsoft, H&R Block, and General Electric. The company also says it’s “moving strongly” into the government sector, where its technology has been deployed across over more than a dozen U.S. federal agencies. “Today, our customer base spans Fortune 100 companies to mission critical elements of the U.S. government. Typically, our clients come to us after suffering poor analytic outcomes or failing decision systems,” Weaver said. “One of the main benefits and differentiator of our platform is that our clients don’t need to change their internal IT infrastructure; rather, the platform overlays existing data and systems.” WestCap Group led the series A funding, which is 50-employee Torch.AI’s first public funding round."
https://venturebeat.com/2021/03/17/cybersecurity-insurance-company-coalition-raises-175m-to-secure-the-modern-enterprise/,Cybersecurity insurance company Coalition raises $175M to secure the modern enterprise,"Coalition, an enterprise-grade cybersecurity platform that specializes in insurance, has raised $175 million in a round of funding led by Index Ventures. The raise comes as hacks and breaches have surged in the past year, due in large part to the pandemic, which forced companies to rapidly transition to remote work. Founded out of San Francisco in 2017, Coalition offers a range of cybersecurity tools spanning credential monitoring, endpoint detection and response (EDR), patch management, training, and more. However, the company’s focus lies in the insurance realm, where it protects businesses against damages — including financial, intangible, and tangible — for up to $15 million. “While many insurance companies only assist after a claim is reported, we proactively work with our clients to prevent [breaches] from happening in the first place and contain them when they do,” Coalition CEO and cofounder Joshua Motta told VentureBeat. That’s not to say Coalition offers a fully exhaustive cybersecurity platform, but by stretching beyond insurance the company is striving to be a stickier proposition, one that uses technology and data to assess clients’ security risks as part of their insurance policy. “Our cyber risk-management platform provides automated security alerts, threat intelligence, expert guidance, and cybersecurity tools to help businesses remain resilient in the face of cyberattacks and are designed to complement other third-party security software and services an organization might use,” Motta added. Coalition’s platform aggregates and analyzes data from every internet-enabled device inside a company, scanning a policyholder’s assets to identify potential vulnerabilities and alert the company. “Combined with our proprietary claims data, we can — with a high degree of accuracy — predict not only the likelihood an organization will experience a cyber loss, but also model the severity, be it a ransomware, social engineering, or data breach-related loss,” Motta said. “These insights have been invaluable, and the frequency of loss reported by our policyholders is one-quarter the frequency reported across the broader cyber insurance market.” Additionally, Coalition offers APIs to customers and partners to integrate its insurance products and security data into their platforms and enable their own customers to manage cyber risks or buy insurance. Coalition had previously raised around $140 million, and with a fresh $175 million in the bank — bringing its value to $1.75 billion — Motta said the company plans to launch several new insurance products to “address a range of risks facing the modern enterprise, many of which are not well covered by standard business insurance policies.” Motta said Coalition also plans to launch new software products that enable organizations to “proactively manage risk.” “Today, an increasing number of organizations derive the majority of their value from intangible assets — the knowledge and abilities of their employees, intellectual property, [and] data, among others,” Motta explained. “Standard business insurance policies, on the other hand, tend to focus on the protection of an organization’s tangible assets, which has created an opportunity for specialty insurers to introduce relatively new products, including cyber, management liability, and so on. We believe there is a significant opportunity to continue to innovate in this space, utilizing the same technology-driven underwriting approach we’ve pioneered in cyber to underwrite a broader set of risks faced by the modern enterprise.” While Coalition doesn’t reveal any of its insurance policyholders, as Motta said that could “expose them to additional risk,” it has revealed a number of SaaS enterprise customers, including Verizon, Accenture, Kleiner Perkins, and Hewlett Packard."
https://venturebeat.com/2021/03/16/ai-patent-intelligence-platform-patsnap-secures-300m/,AI patent intelligence platform PatSnap secures $300M,"PatSnap, which offers a patent and R&D platform and services, today announced that it raised $300 million in series E funding from SoftBank Vision Fund 2 and Tencent Investment. The Toronto-, Singapore-, and London-based company plans to use the funds to further develop its intelligence platform, support software product development, and expand the size of its global workforce. PatSnap also says the tranche will enable it to grow its sales division and invest in its employees’ professional growth and professional development. Companies are constantly under pressure to increase the pace of their innovation. And while more money is spent on R&D every year — $2.4 trillion in 2021, according to R&D World — the returns are dwindling. An article published in Harvard Business Review noted a 65% drop in R&D productivity. That’s despite the fact that the federal governments of the U.S. and Canada provide more than $15 billion in innovation incentives to private companies and nearly a third of U.S. patents rely directly on U.S. government-funded research. PatSnap, which was founded in 2007 by Jeffrey Tiong, began as a directory for intellectual property (IP), helping enterprises pull in data for R&D and ideation purposes. Since then, it’s developed technologies in the AI subfields of machine learning, natural language processing, and computer vision that analyze and identify the relationships between millions of unstructured data points across disparate IP sources. PatSnap claims this can help deliver insights that guide R&D decisions and help to shorten the time it takes to bring new creations to market.  “PatSnap’s machine learning engine analyzes millions of data points coming from patents data around the world, social, litigation, mergers and acquisitions, and other datasets,” cofounder Ray Chohan told VentureBeat via email. “Our AI has deep tech capabilities within life sciences. We enable biotech and pharma companies to connect complex unstructured data to enable insights not possible before. For example, we enable R&D and patent professionals at life sciences companies to discover brand new applications for a molecule or to repurpose existing drugs for net new disease indications. This is possible via our computer vision, natural language processing, deep learning, and machine learning technology stack.” PatSnap says that its natural language processing algorithms analyze more than 500 million documents and 200 data sources, providing a link between different documents, entities, and taxonomy systems. Based on patent and document  information, the company also uses AI and machine learning to create a series of models that generate a unique index analysis for every patent and paper. There are 80 different indicators to support this index, which along with valuation ranking and indexing is designed to spotlight high-value patents. Indeed, most enterprises have to wrangle countless data buckets — some of which inevitably become underused or forgotten. A Forrester survey found that between 60% and 73% of all data within corporations is never analyzed for insights or larger trends. The opportunity cost of this unused data is substantial, with a Veritas report pegging it at $3.3 trillion by 2020. PatSnap says it connects 140 million patents, licensing, litigation, and company information with nonpatent literature to bring together over 250 million data points across 116 jurisdictions into a single platform. Using this data, PatSnap’s IP teams prepare research including patent landscape reports that feature diagrams and analysis informing strategic planning for product development and commercialization. PatSnap also compiles competitive intelligence reports that provide clients with business intelligence about competitors’ strengths and strategies. PatSnap claims to have more than 10,000 customers and over 100,000 users around the world including global brands, universities, and research institutions. Over the past year, the company says it’s enabled those clients — among them Disney, PayPal, Tesla, and Spotify — to accelerate time to insight when dealing with unstructured data by an estimated 12 times, leading to a roughly 3 times increase in successful product launches. “We initially experienced minor slowdowns from the market but since Q2 2020, we have seen an uplift in growth across all key markets. This is a great validation that innovation intelligence is a movement necessary for companies to grow and survive regardless of the economic environment. With the investment, we plan to continue leading the way and help our customers innovate,” Chohan continued. “Some features that we plan to add include innovation knowledge graphs, deep post search AI driven analytics, and vertical- and job-specific deep collaboration features. In addition, the funding will enable us to explore potential deep tech acquisitions.” Existing investors CITIC Industrial Fund, Sequoia China, Shun Wei Capital, and Vertex Ventures also participated in PatSnap’s latest funding round. It brings the company’s total raised to date to over $450 million as its workforce exceeds 800 people across locations in China, the U.S., and other offices."
https://venturebeat.com/2021/03/16/sonatype-acquires-musedev-expands-nexus-code-analysis-platform/,"Sonatype acquires MuseDev, expands Nexus code analysis platform","Sonatype, which provides tools for developers to build better quality software, has acquired code analysis platform MuseDev. The acquisition adds developer-friendly code scanning to Sonatype’s platform to create a “full-spectrum” software supply chain management platform, company CEO Wayne Jackson said. Modern software development is less about developers writing every single line of code and more about them assembling different components with their own code. This means third-party code is almost always present in an application, and there are multiple ways for bugs to be introduced into the code. Developers have to test their own code to make sure there are no bugs and regularly verify the building blocks don’t contain issues that could affect their applications. Sonatype makes tools to help developers manage the various building blocks and alerts developers of potential issues that need to be fixed. Historically, Sonatype has focused on scanning open source software for security vulnerabilities and on keeping risky components out of the application, Jackson said. Sonatype’s tools have helped identify security vulnerabilities in code the developers didn’t write, but that could still impact their application. “As developers take on more responsibility for containers, code, and infrastructure, our mission is to make their lives easier while they make great software,” Jackson said. The way to help “developers optimize the code they write is by delivering directly to the toolchain.” MuseDev’s code analysis platform scans the source code for more than security vulnerabilities. The static analysis tool emphasizes code quality and can identify critical performance and reliability issues in the code, as well as whether there are style issues the could hamper the code’s maintainability. Developers don’t want security vulnerabilities in their code, but “they also don’t want to get paged in the middle of the night because the application was failing” due to performance issues, MuseDev CEO Stephen Magill told VentureBeat. Muse integrates its 24 preconfigured code analyzers into GitHub, GitLab, and Bitbucket. The analyzers automatically assess each developer pull request and report any bugs found as comments in code review. The comments include clear guidance on how to fix the bugs, and the analysis considers information flow and thread safety to give developers deeper insight into the code. Developers see all the feedback — from their teammates and from Muse — in one place and are able to fix the issues as part of their normal workflow. There is no need to wait for the security team to run its own assessment and inform developers of the issues that were uncovered. “Teams adopting this approach are 70 times more likely to fix code quality and security issues,” Magill said. Muse is pretuned to minimize false-positive results to ensure developers are receiving information about issues that matter the most, which helps developers work more efficiently and write better quality code. “As enterprises look to push their development teams to work faster, it becomes imperative to find ways to help developers to move more quickly by automating crucial but time-consuming tasks like code analysis,” RedMonk principal analyst Stephen O’Grady told VentureBeat. The acquisition of MuseDev expands the breadth and depth of Sonatype’s Nexus platform because the combination of Muse — a cloud-native source code analysis tool — with Sonatype’s existing tools gives developers more control over their code. Nexus Container is a developer-friendly container security solution that provides continuous visibility into the composition and management of containers from development to run time. The Infrastructure as Code Pack provides guidance to assist developers in configuring cloud infrastructure and ensuring they are compliant with privacy and security standards such as CIS Foundations Benchmarks, GDPR, and HIPAA. The pack helps developers fix mistakes in configuration before they are applied to production infrastructure. Nexus Repository makes it easier to host and distribute build artifacts such as Docker containers and code components. The recently released Advanced Development Pack delivers a real-time rating system to help developers select the best open source component suppliers and avoid using multiple versions of the same code. The Advanced Legal Pack, which will be released in a few months, will improve visibility into open source licenses. Developers will be able to use Sonatype’s expanded platform for all application building blocks, which include first-party source code, third-party open source code, infrastructure-as-code, and containerized code. “With high-profile attacks on software supply chains making headlines the world over, enterprises are moving to harden their development infrastructure against attackers. As important as the task is, however, technology leaders don’t want to solve this problem with a complicated patchwork quilt of services, solutions and providers — they want an integrated, end-to-end solution,” O’Grady said. This kind of integrated code analysis is something enterprises are asking for as they adopt DevOps practices to build and release better quality code and accelerate their digital transformation efforts to improve speed and efficiency. This acquisition and platform expansion positions Sonatype very well among companies that offer various forms of code analysis and scanning, including Checkmarx, Contrast Security, Micro Focus Fortify, Snyk, Synopsys, Veracode, and WhiteSource. The company has been growing tremendously over the past year. It now counts 70% of the Fortune 100 as customers, supporting more than 2,000 commercial engineering teams. And 12 out of the 15 of the world’s largest banks use Sonatype’s tools, Jackson said. Other customers include various branches of the United States Armed Forces, credit card companies, and technology companies. There are more than 250,000 instances of Nexus Repositories, which translates to nearly 15 million developers using Sonatype’s commercial and open source tools. Private equity and venture capital firm Vista Equity Partners made a majority investment in Sonatype back in 2019 — acquiring more than 50%. Jackson suggested the company could see a potential IPO with the current pace of growth. Most of the enterprises using Sonatype’s tools are not technology companies in the traditional sense. There are financial services organizations with more developers in-house working on internal applications and proprietary tools than companies such as Apple and eBay, Jackson said. Those enterprises are looking at the entire software development lifecycle, which means they care about things other than security vulnerabilities when considering the health of their applications, such as project and release hygiene, Jackson said. “Why should [developers] pick a project that hasn’t been updated in years or has bad commit history?” Jackson said."
https://venturebeat.com/2021/03/16/liqid-integrates-hpc-management-tool-with-slurm-orchestration-engine/,Liqid integrates HPC management tool with Slurm orchestration engine,"Liqid has integrated its software for dynamically composing compute and storage resources on high performance computing (HPC) environments with open source Slurm Workload Manager software used to orchestrate jobs on these platforms. The integration of Liqid Matrix Software with the open source orchestration engine will make it easier for IT organizations to dynamically scale HPC workloads up and down as needed, Liqid CEO Sumit Puri said. That capability has become more critical as IT teams increasingly run AI workloads on HPC platforms configured with graphical processor units (GPUs), Puri added. Liqid Matrix Software makes it possible to dynamically aggregate bare-metal resources — such as GPUs, x86 and Arm processors, NVMe storage, network integration cards (NICs), host bus adaptors, field-programmable gate arrays, and memory — and then assign them to a specific workload. It also provides peer-to-peer connectivity that enables those resources to be aggregated across multiple HPC systems. Slurm, meanwhile, is an orchestration engine widely employed in HPC environments to dynamically scale resources in much the same way Kubernetes does in IT environments running containers. The one prerequisite is systems running Liqid Matrix Software need to support the Peripheral Component Interconnect (PCI) Express 3.0 expansion bus standard, which provides I/O virtualization capabilities. Most recently, Liqid revealed it is collaborating with Broadcom to created reference kits for the 4.0 of PCI Express, which doubles the overall throughput available. “For the first time in history, every device in the datacenter speaks a common language,” Puri said. Liqid iso also working with VMware to make its software available via the console VMware provides to manage virtual infrastructure. VMware most recently expanded its alliance to Nvidia to make GPUs more accessible to the average IT administrator. Organizations are looking to maximize utilization rates on HPC platforms to increase the value of investments they have made in existing platforms, Puri noted. Most recently, Liqid won a $32 million contract from the U.S. Department of Defense to maximize utilization of a pair of supercomputers located at the Supercomputing Resource Center at Aberdeen Proving Ground in Maryland, which provide access to 15 petaflops of performance. Those systems are based on Intel Xeon Platinum 9200 CPUs featuring Intel DL Boost technology and Nvidia A100 Tensor Core GPUs. Rather than having to rely on HPC platforms built using proprietary processors found in, for example, a Cray supercomputer, Liqid is betting that more HPC workloads will wind up being deployed on lower-cost commercial processors from Intel, Arm, and Nvidia. The software Liqid provides makes it possible to manage systems based on those processors as if they were one logical entity. It’s not clear to what degree AI workloads will be running on-premises versus on the cloud, where orchestration is generally managed by the cloud service providers. However, given the prevalence of HPC platforms that have already been paid for and deployed, it’s highly probable that many organizations will prefer to leverage what amounts to an already sunk cost. In other cases, security and compliance concerns require IT organizations to continue to invest in on-premises systems. Regardless of approach, HPC platforms are about to become a mainstay of many IT environments as the number of AI workloads continues to increase. Longer-term, those workloads are going to migrate to the network edge, Puri said. As that trend continues to evolve, Puri said will become crucial for IT teams to manage bare-metal infrastructure at higher levels of abstraction. But given the cost of GPUs, most IT organizations will likely remain anxious to optimize any platform that makes use of them for the foreseeable future."
https://venturebeat.com/2021/03/16/imagenet-creators-find-blurring-faces-for-privacy-has-a-minimal-impact-on-accuracy/,ImageNet creators find blurring faces for privacy has a ‘minimal impact on accuracy’,"The makers of ImageNet, one of the most influential datasets in machine learning, have released a version of the dataset that blurs people’s faces in order to support privacy experimentation. Authors of a paper on the work say their research is the first known effort to analyze the impact blurring faces has on the accuracy of large-scale computer vision models. For this version, faces were detected automatically before they were blurred. Altogether, the altered dataset removes the faces of 562,000 people in more than a quarter-million images. Creators of a truncated version of the dataset of about 1.4 million images that was used for competitions told VentureBeat the plan is to eliminate the version without blurred faces and replace it with a version with blurred faces. “Experiments show that one can use the face-blurred version for benchmarking object recognition and for transfer learning with only marginal loss of accuracy,” the team wrote in an update published on the ImageNet website late last week, together with a research paper on the work. “An emerging problem now is how to make sure computer vision is fair and preserves people’s privacy. We are continually evolving ImageNet to address these emerging needs.” Computer vision systems can be used for everything from recognizing car accidents on freeways to fueling mass surveillance, and as ongoing controversies over facial recognition have shown, images of the human face are deeply personal. Following experiments with object detection and scene detection benchmark tests using the modified dataset, the team reported in the paper that blurring faces can reduce accuracy by 13% to 60%, depending on the category — but that this reduction has a “minimal impact on accuracy” overall. Some categories that involve blurring objects close to people’s faces, like a harmonica or a mask, resulted in higher rates of classification errors. “Through extensive experiments, we demonstrate that training on face-blurred does not significantly compromise accuracy on both image classification and downstream tasks, while providing some privacy protection. Therefore, we advocate for face obfuscation to be included in ImageNet and to become a standard step in future dataset creation efforts,” the paper’s coauthors write. An assessment of the 1.4 million images included in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) dataset found that 17% of the images contain faces, despite the fact that only three of 1,000 categories in the dataset mention people. In some categories, like “military uniform” and “volleyball,” 90% of the images included faces of people. Researchers also found reduced accuracy in categories rarely related to human faces, like “Eskimo dog” and “Siberian husky.” “It is strange since most images in these two categories do not even contain human faces,” the paper reads. Coauthors include researchers who released ImageNet in 2009, including Princeton University professor Jia Deng and Stanford University professor and former Google Cloud AI chief Fei-Fei Li. The original ImageNet paper has been cited tens of thousands of times since it was introduced at the Computer Vision and Pattern Recognition (CVPR) conference in 2009 and has since become one of the most influential research papers and datasets for the advancement of machine learning. The ImageNet Large Scale Visual Recognition Challenge that took place from 2010 to 2017 is known for helping usher in the era of deep learning and leading to the spinoff of startups like Clarifai and MetaMind. Founded by Richard Socher, who helped Deng and Li assemble ImageNet, MetaMind was acquired by Salesforce in 2016. After helping establish the Einstein AI brand, Socher left his role as chief scientist at Salesforce last summer to launch a search engine startup. The face-blurring version marks the second major ethical or privacy-related change to the dataset released 12 years ago. In a paper accepted for publication at the Fairness, Accountability, and Transparency (FAccT) in 2020, creators of the ImageNet dataset removed a majority of categories associated with people because the categories were found to be offensive. That paper attributes racist, sexist, and politically charged predictions associated with ImageNet to issues like a lack of diversity in demographics represented in the dataset and use of the WordNet hierarchy for the words used to select and label images. A 2019 analysis found that roughly 40% of people in ImageNet photos are women, and about 1% are people over 60. It also found an overrepresentation of men between the ages of 18-40 and an underrepresentation of people with dark skin. A few months after that paper was published, MIT deleted and removed another computer vision dataset, 80 Million Tiny Images, that’s over a decade old and also used WordNet after racist, sexist labels and images were found in an audit by Vinay Prabhu and Abeba Birhane. Following an NSFW analysis of 80 Million Tiny Images, that paper examines common shortcomings of large computer vision datasets and considers solutions for the computer vision community going forward. Analysis of ImageNet in the paper found instances of co-occurrence of people and objects in ImageNet categories involving musical instruments, since those images often include people even if the label itself does not mention people. It also suggests the makers and managers of large computer vision datasets take steps toward reform, including the use of techniques to blur the faces of people found in datasets. On Monday, Birhane and Prabhu urged coauthors to cite ImageNet critics whose ideas are reflected in the face-obfuscation paper, such as the popular ImageNet Roulette. In a blog post, the duo detail multiple attempts to reach the ImageNet team, and a spring 2020 presentation by Prabhu at HAI that included Fei-Fei Li about the ideas underlying Birhane and Prabhu’s criticisms of large computer vision datasets. “We’d like to clearly point out that the biggest shortcomings are the tactical abdication of responsibility for all the mess in ImageNet combined with systematic erasure of related critical work, that might well have led to these corrective measures being taken,” the blog post reads. Coauthor and Princeton University assistant professor Olga Olga Russakovsky told WIRED a citation of the paper will be included in an updated version of the paper. VentureBeat asked coauthors for additional comment about criticisms from Birhane and Prabhu but did not receive additional comment. In other work critical of ImageNet, a few weeks after 80 Million Tiny Images was taken down, MIT researchers analyzed the ImageNet data collection pipeline and found “systematic shortcomings that led to reductions in accuracy.” And a 2017 paper found that a majority of images included in the ImageNet dataset came from Europe and the United States, another example of poor representation of people from the Global South in AI. ILSVRC is a subset of the larger ImageNet dataset, which contains over 14 million images across more than 20,000 categories. ILSVRC, ImageNet, and the recently modified version of ILSVRC were created with help from Amazon Mechanical Turk employees using photos scraped from Google Images. In related news, a paper by researchers from Google, Mozilla Foundation, and the University of Washington analyzing datasets used for machine learning concludes that the machine learning research community needs to foster a culture change and recognize the privacy and property rights of individuals. In other news related to harm that can be caused by deploying AI, last fall, Stanford University and OpenAI convened experts from a number of fields to critique GPT-3. The group concluded that the creators of large language models like Google and OpenAI have only a matter of months to set standards and address the societal impact of deploying such language models."
https://venturebeat.com/2021/03/16/docker-ceo-talks-pivot-progress-product-led-strategy-and-coders-as-kingmakers/,"Docker CEO talks pivot progress, product-led strategy, and coders as ‘kingmakers’","A year ago, cloud-native pioneer Docker looked like it could be headed for the endangered species list. Management turnover plus a major business pivot signaled trouble at a company that had helped popularize containers and cloud-native computing. But after an intense year of realigning its priorities, today the company made a series of announcements that included strong financial growth, a big surge in its developer community, and a $23 million round of funding. While it might be too soon for a victory dance, CEO Scott Johnston believes these numbers reflect the dramatic steps Docker has taken to reinvent itself. “So much of the last year was refocusing and restructuring the company,” Johnston said. “And we learned a lot through that process. And one of the things is that the new Docker is refocused on really being a developer-led, product-led business.” Docker surfed the wave of cloud-native development it helped propel in the last decade through tools that made it easier to write applications for containers. It raised $40 million in venture capital in 2014, $95 million in 2015, and $92 million in 2017. Eventually, its venture capital total topped $270 million, pushing its valuation past $1 billion and into unicorn territory. But its shift to selling directly to enterprises fell flat after Google released its Kubernetes orchestration platform for free as an open source project. After cycling through several CEOs, Docker named Johnston to the role in November 2019 when it announced it had sold its enterprise business, the largest chunk of its revenues, to Mirantis. Docker then raised another $35 million in venture capital to pursue its strategy of creating tools for developers. Docker’s main products are Docker Desktop, its development application, and Docker Hub, a shared container resource repository. The company sells access to these tools through a range of subscription plans. Since the pivot, the company has been busy adding new features, as well as striking partnerships that offer access to third-party tools. “That strategy was about providing developers with a collaborative application development platform to get their ideas from code to cloud as quickly as possible,” Johnston said. Those partnerships have included deals with Azure and AWS to simplify and speed up the process of getting applications from a local environment to the cloud. There were also security partnerships with Snyk and Jfrog to reinforce the supply chain, something that became even more critical as work shifted to remote settings last year. Docker also announced integrations with Apple’s desktop silicon and Nvidia’s desktop and cloud GPUs to let applications run faster. “That was a core part of the strategy,” Johnston said. “To work with the ecosystem and bring great ecosystem tech to our developers and a great experience — and also give those developers a choice of which cloud to go to and which supply chain partner to work with.” Developers’ evolving status is one of the major trends benefiting Docker’s current strategy. Coders are so deeply coveted and in such high demand that they are wielding growing influence, particularly within enterprises, over decisions regarding tools and how applications are written and deployed. The pandemic has only accelerated this transformation by creating a bigger demand for applications even as the collaboration environment is at risk of becoming less productive. Anything that simplifies and secures those tasks, as Docker aims to do, is welcomed with open arms. “The developer as kingmaker, that that was on the rise pre-COVID,” Johnston said. “But then the pandemic comes and now there are two really important factors that are tailwinds to this business.” So far, the numbers have been promising. The company has gained 1.8 million more registered developers for a total of 7.3 million. Annual recurring revenues rose 170% from last year. That momentum attracted interest from Tribe Capital, which led the new round of funding, with participation from Benchmark and Insight Partners. With the new money, the company is going to hire more product managers, designers, and engineers to continue to build features for developers. But for the most part, Docker had to do little outbound marketing to attract those developers and doesn’t plan to start any major new initiatives. For Johnston, the traction Docker has been getting suggests the limitations of trying to sell directly to an enterprise. If grassroots developers are championing a product, it’s because the service is solving a real problem. If an IT manager buys a tool or service and it just sits around ignored, then the company is stuck. “It goes back to the product-led growth strategy,” Johnston said. “We’re investing in great product that gets developers even more excited so they’re going to keep talking to their friends and keep bringing in more and more developers. We get signals that developers are loving our products because they are consuming them on a daily, if not minute-by-minute, basis. If the developers aren’t using your stuff, you’re in a tough spot as a company.”"
https://venturebeat.com/2021/03/16/making-friends-in-the-metaverse/,Making friends in the metaverse,"Presented by Together Labs “We can’t put the genie back in the bottle,” I recently said at the GamesBeat Summit in January 2021 during the “Making Friends in the Metaverse” panel. As the metaverse — the computer-generated environment where we represent ourselves and persistently interact with each other — builds up around us, it’s crucial that more of us keep looking forward with the question: “What can we do now to ensure that this new virtual world is not only as good as our ‘real world’ but possibly even better?” And if that “better future” doesn’t include intentionally programming for better relationships, then we’re missing the greatest human need: connection. Countless studies reinforce that we’re happier, healthier, and even live longer when we have better relationships, feel supported, and believe we belong. Loneliness, on the other hand, decreases our empathy, increases our stress, damages our body, and wears on our mental health. In an era where books have titles such as “The Lonely Century” and research reveals that over 60% of us feel lonely on a regular basis, and where documentaries such as “The Social Dilemma” highlight the challenges our industry faces, we have an awesome responsibility to intentionally approach product design with the purpose of helping to foster authentic human connections and relationships. Nearly every game and social media platform is dependent upon people interacting, but we know only too well that just as we can be lonely in a crowd, participation next to each other isn’t the same as connection. As Shasta Nelson, a friendship expert and author who moderated our recent panel said, “Belonging doesn’t just come by sticking a lot of people in the same place — in-person or virtually. That bond can be, and must be, developed.” The specifics of how people meet or what they say or do is infinitely varied, but the basics of how healthy relationships form is not. Nelson, who has a TEDx talk titled “Frientimacy: The 3 Requirements of All Healthy Friendships,” compiled the social science and concluded that there are three factors that must be present for connecting in meaningful, safe, and enjoyable ways. At Together Labs, we have taken this framework as a way to not only assess what we’ve already developed with IMVU, VCOIN, and WithMe, but to also brainstorm where we might better create more powerful connections in the future. The first requirement is positivity. Nelson clarifies, “Ultimately, we want relationships that make us feel happy, or that increase our positive emotions.” She quotes science that shows we need a 5:1 positive to negative experience ratio to keep our relationships healthy. Indeed, we know that even if somebody is a long tenure user and is very engaged, if they have enough negative experiences, they just leave, so how do we architect a space where we’re strategically thinking through how to reduce any negative emotions such as confusion, judgment, and failure, while also designing a world where we encourage more positive emotions? While our industry has long understood the value of positivity when it comes to users feeling excited by storylines, feeling accomplished receiving rewards, and feeling celebrated by leaderboards, we have a long way to go when thinking about how we can expand our positivity lexicon by developing storylines, features, and experiences that foster kindness, empathy, acceptance, collaboration, gratitude, and affirmation. To that end, at IMVU, we have 25,000 greeters who help new users to connect by friending them and introducing them to others. It’s a positivity win:win as the new user feels welcomed and accepted and the volunteer feels helpful and generous. We all want our users to have a good time — for their sake, but also because we know that’s what leads to them coming back for more. Which leads to the second requirement: consistency. “Building a bond is not a one-time experience, but rather is developed by ongoing interactions and shared experiences,” Nelson iterates. “It is with repetition that we form a pattern that leads to us feeling safe because we know what to expect and what we can count on.” Most of us might call this retention. We’ve long known that we need to keep them coming back — whether that’s through cliff-hangers and surprises in our stories, familiarity in how to play the game so they can master it, and ongoing levels to challenge them. Now, our greatest challenge is to figure out how to not only entice our users to be consistent on our platforms, but to provide them opportunities to build that familiarity with each other. Jessica Freeman, head of marketing at Minecraft, agrees. As a fellow panelist at our “Making Friends in the Metaverse” event, she said, “It’s that shared experience that I think people are craving. It used to be that kids would meet on the playground after school. But as that’s not possible now, they’re meeting up in Minecraft to connect socially and to catch up. It has become this kind of virtual recess that they need.” She shared some powerful stories of all ages — college students building their campuses in Minecraft so they could have online graduations, people meeting in the game and eventually marrying, and grandmothers bonding with their grandchildren by playing together across the country from each other. The platforms that will win in the future will be those that provide opportunities for people to gather and meet virtually, to bond through shared experiences together, and that provide new experiences to deepen existing relationships. Lastly, for a relationship to feel meaningful, we eventually need to believe we know each other as a result of those shared experiences and repetitive interactions, which leads to the third requirement of relationships: vulnerability. This happens as you start to trust each other and disclose more about yourself. As Shasta shares, “What we want most from each other is to feel accepted and valued, which only happens when we feel like [people] know us. We need to ultimately feel seen by each other.” Of course, in a close friendship that could mean unfiltered self-disclosure, but she says that even in a brief interaction, we want to feel noticed for who we are. Vulnerability isn’t something we often talk about in our industry, and yet all of us who create, and provide the resources for others to create, are doing just that. Creativity — whether it’s through allowing our users to create their avatars or design new worlds — is one of the most vulnerable acts of humanity. Plus, it allows for self-expression, the very beginning of revealing who we are to others, how we want to be seen. Imagine a metaverse where we can all be seen in safe ways. Meaghan Fitzgerald, Head of Experiences Product Marketing at Facebook Reality Labs cast the vision for our industry in our panel when she said, “There’s so much work to do so that everyone — women, minorities, people from all walks of life, and however they choose to express themselves — feels like they can be themselves.” It has to be more than simply giving people options of how they want to be seen. She reminds us that it’s our responsibility as the makers of these worlds to practice diversity in our teams, to decrease toxicity on our platforms, and to improve the safety and reporting mechanisms that ensure we have the opportunities to be seen in ways that feel good to everyone. At a time in our world where positivity is running low, and fear, division, and judgment are high; where routines have been disrupted and there is less consistency; where too many people say they can’t be vulnerable because they don’t feel safe being themselves or feel valued for their uniqueness — those of us who know how to create new worlds have a huge opportunity to develop exactly what we know people most need. Technology is too-often blamed for being the cause of our disconnection and loneliness, but I’d like to believe we can also be the solution. How can we create virtual spaces that bring people together? What is it that bonds people in real life that we can replicate? What gets in the way with developing our in-person interactions that we can possibly solve to help and not hinder us? Indeed, the genie is out of the bottle. Now, we get to decide what it is we want to wish for. I, for one, am here to say, “I want to create a world where people feel more connected than they do now.” —— The conversation “Making Friends in the Metaverse” is happening next at SXSW on March 16, 2021 with Megan Fitzgerald of Facebook Reality Labs, Jessica Freeman of Minecraft, Lauren Bigelow of Together Labs, and friendship expert, Shasta Nelson. Join us at sxsw.com. Lauren Bigelow is Chief of Product at Together Labs. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com."
https://venturebeat.com/2021/03/16/location-data-analytics-provider-safegraph-raises-45m/,Location data analytics provider SafeGraph raises $45M,"SafeGraph, a startup using AI to create and maintain mobility datasets, today announced it has raised $45 million in a round led by Sapphire Ventures. With the investment, SafeGraph plans to capitalize on the expanding international market of data buyers and offer new ways for companies to buy data through its network. Location data is fast becoming a hot commodity. In May 2019, 94% of mobile marketers in the U.S. surveyed by Statista said they were already using location data for advertising purposes, while 94% indicated that they were planning to do so in the future. While more users worldwide are refusing to share location data with apps, marketers say the data is incredibly valuable for targeting purposes and personalizing customer experiences. SafeGraph, which was founded in 2016 by former LiveRamp CEO Auren Hoffman, provides a places dataset that includes information about over 6,400,000 physical locations in the U.S. and Canada, with a U.K. launch planned for April 2021. SafeGraph customers gain access to not only location information, but spatial hierarchy metadata and place traffic data.  Drawing on data from mobile devices, satellites, and “thousands” of other sources, SafeGraph offers foot traffic insights for places, as well as corporate and retail footprints. The company uses machine learning and human feedback to associate business listing info with the footprints and algorithmically classify over 5,500 brands, updating the data monthly to account for store openings and closings. SafeGraph says its datasets are used by financial services and real estate and advertising companies like Sysco, Ares Management, and Choice Hotels for shopper demographics, marketing campaign performance comparisons, and risk management underwriting. Clients engaged in retail site selection can see which day of the week a census block group — the most granular level of reporting conducted by the U.S. Census Bureau — is busiest and where people who stopping during breakfast, lunch, or dinner hours are traveling from, among other details. SafeGraph also hosts a community of more than 7,000 data scientists who collaborate on various geospatial projects. The global location intelligence market is expected to reach $32.8 billion by 2027, according to Grand View Research. In spite of competition from companies like AirSage, Factual, Cuebiq, and Matrixian Group, SafeGraph says it doubled its year-over-year revenue while improving efficiency metrics, such as revenue per employee. But location analytics solutions have drawn increased scrutiny as regulators take a hard look at the data vendors are collecting. Using datasets from Fysical and SafeGraph, the New York Times managed to track the movements of former President Trump during the 2017 presidential inauguration. More recently, the U.S. Department of Homeland Security and IRS announced they would audit their own use of location data after it was revealed that the agencies were purchasing cellphone location data from commercial sources. Debates about location-tracking capabilities have also come to the fore as countries around the world adopt contact-tracing apps to control the spread of COVID-19. In May, North Dakota said its smartphone app, Care19, had been sending users’ location data to the digital marketing service Foursquare, an issue the state’s app developer later fixed. A U.S. law proposed last year — COVID-19 Consumer Data Protection Act — would have required companies to obtain consent from users to collect, process, or transfer personal health, geolocation, proximity, or device data for contact tracing. For its part, SafeGraph notes that users can opt out of having their location data used for its services and that it deletes data from providers in countries under the jurisdiction of GDPR. The company also claims to anonymize information such that it can’t know users’ precise home addresses or places of work — at least not without cross-referencing SafeGraph’s data with public records and pinpointing devices that regularly spend time at a location. “What stands out about SafeGraph is how they’ve been able to quickly position themselves into a major player in the geospatial data industry,” Sapphire Ventures partner Cathy Gao said in a press release. “By singularly focusing on providing the highest-quality places data to data science teams, they’ve earned the trust of some of the largest public and private institutions. The efficient growth to date is a strong indicator of where we think the company is going, and Auren’s track record of building consequential data businesses speaks for itself.” Ridge Ventures’ Alex Rosen, DNX Ventures, and Peter Thiel also participated in SafeGraph’s latest funding round. It brings the company’s total raised to over $60 million, following a $16 million series A in April 2017."
https://venturebeat.com/2021/03/16/yseop-launches-alix-an-application-that-instantly-shows-the-power-of-ai-automation-for-report-creation/,"Yseop Launches ALIX, an Application That Instantly Shows the Power of AI Automation for Report Creation","PARIS–(BUSINESS WIRE)–March 16, 2021– Yseop, a pioneer in Natural Language Generation (NLG) and world-leading AI software company, today announced the launch of ALIX, an immediate discovery application to showcase how quickly and easily financial institutions can transform and grow their business through intelligent report automation using Yseop’s Augmented Financial Analyst solution. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210316005455/en/ Financial analysts spend 48% of their time writing and updating reports. This falls to 9% when using Yseop’s Augmented Financial Analyst, a significant Return on Investment (ROI). Language Artificial Intelligence has made incredible progress in the last several years, with many Tier 1 financial institutions deploying the technology at scale to automate report writing. Despite this, it has been difficult for many business end-users to understand which reports can be automated without technology specialists’ help. With ALIX, business end-users can determine in less than a minute what portion of their reports – such as annual financial statements, fund performance or risk and compliance reports – can be automated. Based on Yseop’s pre-trained algorithm, ALIX provides an immediate analysis of the uploaded financial report and identifies what can be automated on the Augmented Financial Analyst platform. ALIX is publicly available on Yseop’s website. To try the application, click here. Yseop’s powerful industry-leading patented platform encompasses Natural Generation Language, Natural Language Understanding (NLU) and Machine Learning capabilities to draw insight from structured data, translating them into clear and high-quality written reporting narratives. Key benefits for automating financial reports include: Emmanuel Walckenaer, CEO of Yseop, says: “We are excited to launch ALIX – a milestone in our delivery of end-to-end intelligent automation. Financial reporting remains an intense weight for big, global organizations but it comprises mostly repetitive tasks that can be easily automated. To boost AI adoption, it is critical to enable business users to see benefits and deploy it – without intensive, specialized tech support. ALIX proves intelligent report automation is within their reach – the first step to adopting solutions such as Augmented Financial Analyst.” ENDS About Yseop: Founded in 2008 and based in North and South America, and Europe, Yseop specializes in artificial intelligence (AI) and is a recognized pioneer in Natural Language Generation (NLG) technology. Yseop is rapidly expanding globally, providing enterprise-level automation solutions for some of the world’s largest companies in a variety of industries including finance (Credit Agricole, Factset, BNP Paribas), pharmaceuticals (Sanofi) and computer software company (Oracle). Yseop also partners with strategic consulting firms and system integrators including CapGemini, Accenture and LTI, who support the adoption and deployment of Yseop’s NLG solution. With its multi award-winning Language AI technology, Yseop is revolutionizing the way analysis and reporting is done. Yseop’s powerful and user-friendly Augmented Analyst platform allows business users to seamlessly and quickly build and automate the generation of text reports from any structured data. At Yseop, we exist to support companies through this digital transformation. We believe that our cutting-edge artificial intelligence technology allows businesses to increase the efficiency of their operations and enables people to accomplish less tedious tasks and allows them to use that saved time to do more added-value and creative work. Find out more at https://yseop.com  View source version on businesswire.com: https://www.businesswire.com/news/home/20210316005455/en/ Press – Lise Grant – +33 6 99 65 71 91 – lgrant@yseop.com"
https://venturebeat.com/2021/03/16/how-terminusdb-is-commercializing-its-open-source-graph-database/,How TerminusDB is commercializing its open source graph database,"Knowledge graphs play an integral role in many modern applications, enabling businesses to extract new information by aggregating and analyzing connections between large volumes of internal data. Music streaming services, search engines, fraud detection software, and more can all be aligned through their use of knowledge graphs to derive insights from big data. While standard databases can usually find connections between data that is closely related, graph databases are designed to join the dots between pieces of data that are not so obviously related. It’s one of the reasons Alexa can answer your myriad questions and Facebook is so good at making friend recommendations. Against this backdrop, Irish startup TerminusDB has raised €3.6 million ($4.3 million) in a seed round of funding to build what it calls a “knowledge collaboration infrastructure” for the internet, combining an open source graph database and document store with a commercial, cloud-based collaboration hub built for TerminusDB. As the company’s commercial and operations lead Luke Feeney put it, the relationship between TerminusDB and TerminusHub is analogous to that of Git and GitHub. “One is the basic tool, the other is the collaboration hub,” he told VentureBeat. As a side point, TerminusDB is also a founding member of the recently announced AI Infrastructure Alliance, which is focused on promoting open standards and interoperability for AI and ML applications in the enterprise. The core open source graph database, which is called TerminusDB, can be downloaded and used by anyone for whatever purpose they wish on their own servers. The commercial component, known as TerminusHub, ships with various pricing plans (including a basic free tier with restrictions) and a bunch of add-on features for sharing and building databases, including support for full versioning that allows developers to roll back updates. The startup said it will use the funding to launch TerminusDB Live, which is a cloud-based version of TerminusDB “that will allow people to get started without having to spin up their own servers,” Feeney added.  The graph database market was pegged at $1 billion in 2019, a figure that’s projected to rise to rise threefold by 2024. Notable players in the commercial space include Oracle, Microsoft, AWS, and IBM, though perhaps the most obvious comparisons in terms of TerminusDB would be something like Dgraph or Neo4j, which also offer open source graph databases supported by commercial companies of the same name. Neo4j specifically has raised nearly $200 million in external funding, including a $30 million tranche just a few months back. According to Feeney, TerminusDB is looking to distinguish itself on a number of grounds, including the way it combines immutability (being able to revisit past database states) with fast query, strong schema, and “ease-of-use.” “We are really good at collaboration on schema and data,” Feeney said. “Nobody else can do it as flexibly at scale. We take all of the Git/GitHub workflows and make them applicable to data. So you can clone the production database, branch, merge, lineage, versioning, and time travel — all the Git-like revision control for large databases.” Spun out of Trinity College Dublin in 2017, TerminusDB says it has a number of customers, though it wasn’t at liberty to reveal any names. Feeney did share some well-known names already using the open source TerminusDB, including ING Bank, Lockheed Martin, BN Bank, and the University of Oxford. In terms of how TerminusDB sees its platform being used, Feeney said it wants to allow companies to “provide data ownership to domain data owners,” meaning that marketing teams would own their data, as would sales, who would then be accountable for providing their data as products. “TerminusDB is still installed on a server or cloud, like the others, but each instance within an organization is a decentralized node that is brought together in TerminusHub,” he said. “I have a data product on my distributed remote, I make a change and push that change to the central TerminusHub, that is then available for other teams to clone and use for their analytics.” Moreover, as the world has transitioned to remote working, TerminusDB would facilitate communication between data distributed across multiple locations. “We are challenging the assumption that organizations must centralize big analytical data to use it effectively,” Feeney explained. “In a remote-first world, decentralized solutions like TerminusDB will thrive.” That is the enterprise use case TerminusDB envisages. For developers and businesses adopting the open source TerminusDB, Feeney thinks the company will come into its own as the backend for countless data-intensive applications. “We see folks building note-taking apps, versioned experimental tools for bio-pharma, music recording applications, natural language pipelines, knowledge management software for cultural institutions,” he said. “It is a very diverse group — but I’d say the defining features are complex data and [a] machine learning pipeline.”"
https://venturebeat.com/2021/03/16/salesforce-intros-new-ai-powered-account-based-marketing-tools/,Salesforce intros new AI-powered account-based marketing tools,"Salesforce today unveiled new AI-powered account-based marketing (ABM) capabilities within Salesforce Digital 360 designed to help sales teams scale their campaigns. Einstein Key Accounts Identification analyzes engagement-based buying signals and customer characteristics to identify the accounts most likely to make a purchase. Meanwhile, Accounts as Campaign Members lets companies target an entire account directly even if there aren’t any contacts for the account. ABM is increasingly seen as critical for marketing teams looking to deliver personalized experiences. In fact, in a 2021 Salesforce survey, 92% of business-to-business marketers now cite ABM as “extremely important” to their overall marketing efforts. SiriusDecisions reports that 33% of companies allocated at least 30% of their marketing budgets to ABM in 2016, while in 2017, that percentage increased to over 50%. ITSMA and the ABM Leadership Alliance recently found that 80% of marketers adopting ABM achieve significantly higher returns on investment. “Every business needs to deliver connected digital experiences for their buyers, from anywhere. However, buyer behavior is changing — and they’re bringing business-to-consumer buying habits to business-to-business purchases,” Salesforce Pardot SVP Meredith Brown said in a blog post. “These buyers have done the research, read the reviews, and they know all about your product before the sales team reaches out.” With Einstein Key Account Identification, companies can analyze marketing engagement data across the web and customer relationship management platforms to tier accounts in a prioritized order. The goal is to help marketing and sales teams focus on priority accounts, potentially optimizing the resources to close deals faster, according to Salesforce. Accounts as Campaign Members allows companies to leverage AI-powered insights to create personalized ABM campaigns for buyers within top-tier accounts. The feature automatically syncs new account contacts into marketing campaigns the moment they’re identified. Previously, marketers could only target an account if they had an individual contact in their roster. “The COVID-19 pandemic set off a seismic shift in the marketing industry, as every business was forced to transition to a digital-first world,” Brown continued. “Previously, business-to-business reps would meet customers where they were looking to do business — dinner meetings, live entertainment events, and industry conferences. Now, these customer interactions are primarily happening digitally, as Zooms are the new conference rooms and sales deals are closed in home offices. As customer demands for digital experiences grow, business-to-business companies need a platform to build a single view of their customer, identify key accounts, and quickly turn new leads into deals.” Both Einstein Key Account Identification and Accounts as Campaign Members are available now for Salesforce Digital 360 customers."
https://venturebeat.com/2021/03/16/ai-and-big-data-analytics-startup-noogata-nets-12m/,AI and big data analytics startup Noogata nets $12M,"Noogata, a startup developing products for designing, implementing, and deploying big data analytics models at scale, today announced that it raised $12 million. In addition to expanding its workforce, Noogata plans to spend the capital on accelerating its go-to-market efforts as it seeks to acquire new customers. IDC expects the worldwide big data analytics market will be worth $274.3 billion by 2022. Thanks to AI, enterprises can collect, enrich, and model data insights, forecasts, and recommendations across departments ranging from sales and operations to finance and marketing. But historically, getting this AI into production required in-house development or proprietary out-of-the-box solutions. Until relatively recently, there hasn’t been an easy, no-code way to integrate enterprise data systems with predictive models. Tel Aviv, Israel-based Noogata, which was founded in 2019 by Assaf Egozi and Oren Raboy, a former senior product manager at Cisco, offers modular preset “AI blocks” engineered to target business needs like managing ecommerce channels and supply chains. The platform leverages no-code, AI-powered frameworks to deliver data insights that companies can embed in reports and dashboards. And because Noogata abstracts away much of the development work, customers don’t have to continuously retrain or audit these AI solutions, the company claims.  “We started Noogata because we believe the data wars won’t be won by data scientists and data engineers alone. We realized we needed a new kind of data operator,” Egozi told VentureBeat via email. “Digitization of everything accelerated in 2020 and data is everywhere — in existing systems inside the enterprise, with partners and along the supply chain. It’s also in new and alternative, external sources like social media, news, weather, research reports, and customer buying patterns and profiles … Enterprises have made great strides in building their data foundations. Now they need to use that foundation to make better, faster decisions.” Noogata says that the Colgate-Palmolive Company is using its platform to support key sales and marketing functions. Another customer, PepsiCo, is employing the platform across certain farming sites in Europe to help optimize the yield of agricultural raw materials. “PepsiCo is always looking for ways to leverage breakthrough technology that can help advance our sustainability goals,” PepsiCo senior director David Wilkinson said. “By applying AI to the crop data captured in partnership with farmers, we can gather important insights to further improve decision-making across our agricultural supply chain. The Noogata AI platform is helping us take steps to further optimize crop yield, which helps support farmers’ productivity, profitability, and resiliency.”  Noogata competes with several startups in the big data analytics space. There’s Firebolt, a startup developing a cloud data warehouse for analytics. Leadspace offers a customer data platform that uses AI and big data to help sales and marketing teams build B2B customer profiles. And Dremio, which recently nabbed $135 million, sells tools to help streamline and curate data. Time will tell whether Noogata can maintain its early market momentum. But Team8, which led the seed funding round announced today with participation from Skylake Capital, is optimistic about the platform’s future. “Noogata is perfectly positioned to address the significant market need for a best-in-class, no-code data analytics platform to drive decision-making,” Team8 managing partner Yuval Shachar said. “The innovative platform replaces the need for internal build, which is complex and costly, or the use of out-of-the-box vendor solutions which are limited. The company’s ability to unlock the value of data through AI is a game-changer. Add to that a stellar founding team, and there is no doubt in my mind that Noogata will be enormously successful.”"
https://venturebeat.com/2021/03/16/applied-materials-brings-ai-and-big-data-into-semiconductor-inspection-machines/,Applied Materials brings AI and big data into semiconductor inspection machines,"Applied Materials has launched a new generation of optical semiconductor wafer inspection machines that incorporate big data and AI techniques. These multimillion-dollar machines are used in chip factories that can cost $22 billion to build and generate even more revenue than that. Such factories send wafers through hundreds of manufacturing steps before they’re finished and sliced into individual chips that are used in everything electronic. With a severe shortage of such chips during the pandemic, Applied Materials’ latest improvements to the machines are timely, as the AI techniques enable the new Enlight optical wafer inspection systems to automatically inspect more chips and detect more killer defects that can ruin chips. These kinds of inspection machines alone add up to a $2 billion market worldwide. Applied Materials executives like CEO Gary Dickerson have been predicting for years that the recent advances in AI would prove transformative in semiconductor manufacturing, and that’s what’s playing out now, Keith Wells, group vice president at Applied Materials, told VentureBeat in an interview. “We all know that AI and big data have the potential to transform every area of the economy,” Wells said. “Today, that’s now reality. We’re bringing AI and big data into the semiconductor manufacturing.” The new inspection systems are the fastest-ramping tools in the history of Santa Clara, California-based Applied Materials, which is the largest maker of equipment used in semiconductor factories. The machines speed time to revenue and help a chipmaker earn more profits over the life of a manufacturing process. “We believe this is the industry’s fastest high-end optical inspector that is 3 times faster, and it has the sensitivity to find these yield-critical defects,” Wells said. “We believe it has the ability to impact the economics.” The challenge is that the costs of inspecting increasingly miniaturized patterns on wafers are rising, and the inspections are also becoming more complex. A decade ago, chip factory costs were about $9 billion. Now they’ve doubled. Over the life of the factory, the chipmaker can depreciate the cost of the chipmaking equipment in the factories. But manufacturing delays and inspection failures can cause factories to go idle (and lose a ton of money) as engineers try to decipher the cause of failures. When it comes to memory chips, a week’s downtime can knock down annual output by 2%. On top of that, the price of the chips drops rapidly over time, and so falling behind schedule can severely damage revenue, Wells said. Add to this the notion that the inspection machines are getting more complicated and more expensive to produce. “You don’t make money until you start ramping in volume in the millions of chips,” Wells said. Dan Hutcheson, CEO of market analyst firm VLSI Research, said in a statement that being able to quickly and accurately distinguish killer defects is something chip engineers have struggled with for more than three decades. He said Applied Materials’ Enlight system with ExtractAI technology is a breakthrough approach that solves this challenge and added that because the AI gets smarter the more the system is used, it helps chipmakers increase their revenue per wafer over time. In an email, Hutcheson said that Enlight can cut yield loss (the percentage of a wafer lost to defective chips) by $2.6 million for every hour it trims off the time to respond to a deviation from normal yields. He said inspection accounts for about 10% of the cost in an advanced wafer fab, and the current cost of such fabs is about $22 billion. That’s almost as much as two aircraft carriers and 65 F22 Raptor jets. Semiconductor technology is becoming increasingly complex and expensive. So reducing the time needed to develop and ramp advanced manufacturing process nodes can be worth billions of dollars to chipmakers around the world. But not being able to inspect chips fast enough is a barrier to speed. That’s a problem because it is increasingly hard to focus lenses so that you can see the surface of a chip, where the circuits are as little as five nanometers — or five billionths of a meter — apart. The tiniest specs of dust can be like boulders on the surface of a wafer. That’s where the inspection machines come in. They can use AI to detect anomalies on the surface of a chip and then automatically fix the errors, if possible, so that the nuisance particles don’t ruin the circuitry. “We’re looking for the defects that are effectively going to kill the device,” Wells said. For instance, if two circuit lines get crossed, that will divert electrical signals and possibly short circuit an entire chip. The inspection system uses a state-of-the-art scanning electron microscope, which helps identify the signals coming off the optical inspector to do classification of the flaws, Wells said. “We’re going to take that classified data, and we’re going to feed it into an AI algorithm, which we call ExtractAI,” Wells said. The result is creating actionable data for customers that lets them solve problems faster than ever. In the past, chipmakers have deployed more primitive AI, where the classification engine is static. It doesn’t have the ability to learn and adapt automatically. But chipmaking processes, or recipes for building chips, change frequently. “The next necessary step is to allow the AI to learn and adapt,” Wells said. “As the process changes, they can adapt.” Applied Materials said that 3D transistor formation and multiprocessing techniques introduce subtle variations that can multiply to create yield-killing defects that range from vexing and time-consuming to root-cause. The company is solving these challenges with a new playbook for process control designed to bring the benefits of big data and AI technology to the core of chipmaking technology. Applied Materials’ solution consists of three elements it claims work together in real time to find and classify defects faster, better, and more cost effectively than legacy approaches. The AI comes in to make a decision about whether to slow the production speed down and alert a human about a problem in a wafer that carries a varying degree of risk. In development for five years, the Enlight system combines industry-leading speed with high resolution and advanced optics to collect more yield-critical data per scan. The Enlight system architecture improves the economics of optical inspection, resulting in a 3 times reduction in the cost of capturing critical defects compared to competing approaches. The system has a more robust optical system — including features that put the equivalent of sunglasses on the optical lenses — to focus quickly on the problem parts of a wafer surface. By dramatically improving cost, the Enlight system allows chipmakers to insert many more inspection points in the process flow. The resulting availability of big data enhances “line monitoring,” statistical process control methods that can predict yield excursions before they occur, immediately detect excursions so that wafer processing can be halted to protect yields, and enable root-cause traceback to accelerate corrective actions and the return to high-volume manufacturing. “There are a lot of imperfections that engineers might not care about that your optical inspector will find, but it may not be a killer defect,” Wells said. “So the challenge is to give the customer actionable data.” Developed by Applied Materials’ data scientists, ExtractAI technology solves the most difficult problem of wafer inspection: the ability to quickly and accurately distinguish yield-killing defects from the millions of nuisance signals or “noise” generated by high-end optical scanners. It has to take a million possible problems and reduce them to 1,000 that can be inspected more closely. ExtractAI creates a real-time connection between the big data generated by the customer’s optical inspection system and the eBeam review system that classifies specific yield signals so that by inference, the Enlight system resolves all of the signals on the wafer map, differentiating yield killers from noise. ExtractAI technology is incredibly efficient; it characterizes all of the potential defects on the wafer map after reviewing only 0.001% of the samples. The result is an actionable map of classified defects that accelerates semiconductor node development, ramp, and yield. The AI technology is adaptive and quickly identifies new defects during high-volume production while progressively improving its performance and effectiveness as more wafers are scanned. The ExtractAI tech uses high-resolution scans to detect the worst problems. Once the actual defects are removed, the system learns to adapt to better detection techniques the next time around. ExtractAI can reduce the number of problem areas from about a million to just about 1,000 that will need a closer look or some action. “We interrogate the data, and we’re actually learning and adapting our classifying defects in real time,” Wells said. “This is different from other approaches where the classifiers are static.” The SemVision system is the most advanced and widely used eBeam review technology in the world, as 1,500 systems are in place at chip factories throughout the world. The SemVision system trains the Enlight system with ExtractAI technology to classify yield-killing defects and distinguish defects from noise. By working together in real time, the Enlight system, ExtractAI technology, and SemVision system help customers identify new defects as they are introduced into the manufacturing flow, enabling higher yields and profitability. The large installed base of SemVision G7 systems is already compatible with the new Enlight system and ExtractAI technology. “We’ve seen over the last five years the rise in capital costs of these inspectors, making the economics difficult,” Wells said. “Customers have been reducing inspections in order to compensate for the increase in the cost of these tools. But unfortunately, when you reduce inspection points, you get yield problems. The industry wants a better economic value message around doing more inspection. And we’re trying to provide that.”"
https://venturebeat.com/2021/03/16/microsoft-could-reap-more-than-150-million-in-u-s-cyber-funding-despite-security-shortcomings/,Microsoft could reap more than $150M in U.S. cyber funding despite security shortcomings,"(Reuters) — Microsoft stands to receive nearly a quarter of COVID-19 relief funds destined for U.S. cybersecurity defenders, sources told Reuters, angering some lawmakers who don’t want to increase funding for a company whose software was recently at the heart of two big hacks. Congress allocated the funds at issue in the COVID-19 relief bill signed on Thursday after two enormous cyberattacks leveraged weaknesses in Microsoft products to reach into computer networks at federal and local agencies and tens of thousands of companies. One breach attributed to Russia in December grabbed emails from the Justice Department, Commerce Department, and Treasury Department. The hacks pose a significant national security threat, frustrating lawmakers who say Microsoft’s faulty software is making it more profitable. “If the only solution to a major breach in which hackers exploited a design flaw long ignored by Microsoft is to give Microsoft more money, the government needs to reevaluate its dependence on Microsoft,” said Oregon Senator Ron Wyden, a leading Democrat on the intelligence committee. “The government should not be rewarding a company that sold it insecure software with even bigger government contracts.” Microsoft previously said it prioritizes fixing attacks that it sees in wide use. A draft spending plan by the Cybersecurity Infrastructure Security Agency allocates more than $150 million of their new $650 million funding for a “secure cloud platform,” according to documents seen by Reuters and people familiar with the matter. More precisely, the money has been budgeted for Microsoft, according to four people briefed on the choice, largely to help other federal agencies upgrade their existing Microsoft deals to improve security of their cloud systems. A CISA spokesperson declined to comment. A key service Microsoft provides, known as activity logging, allows its clients to keep watch on data traffic within their part of the cloud and spot inconsistencies that could reveal hackers at work. Officials have sought access to Microsoft’s premium tracking capability after discovering the lack of logs made it much harder to investigate recent hacks tied to nation-states. Microsoft said Sunday that while all its cloud products have security features, “larger organizations may require more advanced capabilities, such as a greater depth of security logs and the ability to investigate those logs and take action.” It did not address the fairness issues raised by lawmakers. While some senior U.S. cyber officials feel they have no choice but to pay up, Wyden and three other lawmakers have publicly raised concerns about the plan. Most major software has been penetrated by well-financed teams of hackers at one time or another, but the ubiquity of Microsoft’s products makes it a prime target. The alleged Russian hackers, known for exploiting software from SolarWinds, hit nine government agencies and 100 private companies, many of which were exploited through manipulation of a Microsoft system. More recent sprawling hacks by a handful of attackers hit tens of thousands of servers running Microsoft Exchange around the world. These included some hacks tied to the Chinese government and relied on four previously unknown flaws in the way those servers handled web versions of Outlook email. China has denied backing the attacks. In a hearing on February 26 addressing the SolarWinds breach, Rhode Island Congressperson Jim Langevin challenged Microsoft president Brad Smith about charging extra for logging, asking: “Is this a profit center for Microsoft, or is it a service being provided at cost to the customers?” “We are a for-profit company,” Smith responded. “Everything we do is designed to generate a return, other than our philanthropic work.” Microsoft has turned security offerings into a significant source of revenue, with the business generating $10 billion annually, up 40% from the previous year. Rep. Dutch Ruppersberger of the House appropriations committee said Congress must look into “why security is an afterthought in the procurement process” and move away from approving only the lowest bidders. The government could impose new regulations, said Curtis Dukes, a former head of the defensive mission at the National Security Agency and now at the nonprofit Center for Internet Security, which works closely with CISA. “Maybe with additional size, vendors should have to do more.”"
https://venturebeat.com/2021/03/16/datagen-emerges-from-stealth-with-18-5m-to-create-synthetic-datasets-for-computer-vision-models/,Datagen emerges from stealth to create synthetic datasets for computer vision models,"Datagen, a Tel Aviv, Israel-based startup offering a platform to create synthetic computer vision system training data, today emerged from stealth with $18.5 million in funding from TLV Partners and Viola Ventures. The company says the proceeds will be put toward growing its R&D lab while it expands into new markets globally. Datagen, which Ofir Chakon and Gil Elbaz founded in 2018, leverages computer graphics and data generation to simulate the real world with datasets that include 2D and 3D annotations. By combining generative adversarial networks (GANs) with reinforcement learning-driven humanoid motion algorithms within a physical simulator, Datagen says it can deliver photorealistic, scalable datasets suitable for augmented and virtual reality, internet of things, smart store, robotics, and smart car use cases. GANs are two-part AI models consisting of a generator that creates samples and a discriminator that attempts to differentiate between the generated samples and real-world samples. As for reinforcement learning, it’s a technique that allows AI models to learn how to make decisions automatically through trial and error. Collecting and labeling training data can be expensive for enterprises. For example, self-driving vehicle companies alone spend billions of dollars per year collecting and labeling training data, according to estimates. Third-party contractors enlist hundreds of thousands of human data labelers to draw and trace the annotations machine learning models need to learn. (A properly labeled dataset provides a ground truth that the models use to check their predictions for accuracy and continue refining their algorithms.) Curating these datasets to include the right distribution and frequency of samples becomes exponentially more difficult as performance requirements increase. And the pandemic has underscored how vulnerable these practices are, as contractors have been increasingly forced to work from home, prompting some companies to turn to synthetic data as an alternative. To create synthetic training data, Datagen works with customers to establish requirements like camera lens specifications, lighting, environmental factors, demographic distributions, and annotations and metadata. The process begins with 3D base models of people and objects scanned from the real world or designed with computer graphics software. Datagen’s platform creates representations of these models with meshes and textures as well as semantic metadata. Lastly, Datagen employs GANs to sample from these representations and synthesize unique models, building libraries of millions of 3D assets that are then subjected to physics-based algorithms that simulate motion and help to scale rendering. For example, Datagen says that its platform can capture hand data that could power gesture-based interactions with headsets. Beyond creating meshes and skeletal models for a range of human hands, the company claims its technology can accurately mimic real-world hand-to-object and hand-to-hand interactions. “Computer vision can be an amazing tool for defect and risk detection — things like errors on an assembly line or rust or cracks that threaten the structural integrity of a building,” Chakon told VentureBeat via email. “Simulated data can supercharge this application by simulating extreme cases that would be dangerous to capture manually in a data set or are extremely rare. It also allows enterprises to create environmental variations to strengthen performance, like different lighting conditions, robotic attachments, or tools.” The AI training dataset market is anticipated to be worth $4.8 billion by 2027, according to Grand View Research, and Datagen has rivals in a number of startups. Parallel Domain also taps AI and machine learning to create synthetic computer vision datasets. There’s also Cvedia and AI Reverie, both of which are developing simulators targeting applications across data generation, labeling, and enhancement. However, unlike many of its competitors, one of Datagen’s focuses is privacy. Chakon points out that by 2023, Gartner estimates, 65% of the world’s population will have their data protected by privacy laws and regulations. This stands to make collecting AI training data in the real world less straightforward and the alternative — synthetic datasets that don’t sweep up data like faces or license plates — more attractive. “Many new products not yet in production — smart appliances, robotics, and more — will have specific camera types and orientations. In many cases, this means datasets need to reflect the specific nuances of that hardware in order to be effective,” Chakon continued. “But, if the hardware is not in the hands of consumers or is highly secretive, it can be impossible to efficiently collect the data you need. Simulated data can imitate these specifications, allowing teams to develop software solutions that are perfectly attuned to hardware that is still in development.” Of course, synthetic data isn’t a panacea in the absence of real-world data. For example, in the autonomous vehicle domain, simulations and running vehicles on test routes can help to prove that cars meet specific compliance needs. But public roads present complex, real-world dynamics that even the best simulators can’t consistently deliver, including different weather conditions and a range of pedestrian and driver behaviors. That’s why Chakon advises Datagen’s customers, which include the AI research arms of several manufacturing giants, that a mix of synthetic and real-world data is the best approach. “The real-world implication is that, once deployed, you can be sure it’s going to work well in different domains, with different ethnicities, in different geographic locations, or any environment you can imagine,” he said. Existing investor Spider Capital participated in 40-employee Datagen’s first public round of fundraising announced today, in addition to individual investors Kaggle CEO Anthony Goldbloom and UC Berkeley AI Research Lab founder Trevor Darrell."
https://venturebeat.com/2021/03/15/on-demand-logistics-and-fulfillment-startup-flowspace-raises-31m/,On-demand logistics and fulfillment startup Flowspace raises $31M,"On-demand warehousing fulfillment startup Flowspace today announced that it raised $31 million in a series B equity funding round. Los Angeles, California-based Flowspace says that the round, which was led by BuildGroup, will enable it to expand its network of fulfillment centers to build on pandemic-driven growth. The company says that the number of orders shipped on its platform increased 10 times during the past year. The demand for warehousing fulfillment is on the rise. Prior to the COVID-19 crisis, about 35% of industrial leasing activity was related to ecommerce, according to a report from JJL. But as early as July, as much as 50% of that leasing activity was tied to the online retail industry. JJL anticipates that as a result, the need for industrial real estate could reach an additional 1 billion square feet by 2025. Founded in 2017, Flowspace offers fulfillment and distribution services, warehouses, and a team of workers that help pick, pack, and ship products. Customers can track inbound and outbound shipments to and from warehouses and access a network intelligence tool for real-time insights and logistics recommendations. Companies get an overview of all the items in their Flowspace warehouses. And they can see inventory, orders, and things that require action. According to cofounder and CEO Ben Eachus, Flowspace taps AI and machine learning to deliver predictive insights that allow retailers and ecommerce companies to anticipate market demands and stock inventory in those locations. “From the time an order is placed on a website to the time it arrives at someone’s door, it is tracked in our software. Our software is already running in hundreds of warehouse facilities that are using Flowspace to enable e-commerce fulfillment,” he said. “This means that with a single integration point, brands can store and ship their products from multiple locations, so they can offer faster and cheaper delivery to their customers. This process historically takes weeks or months; with Flowspace, they can do this in days.”  While only 12% of manufacturing, transportation, and other industrial organizations recently surveyed by Deloitte said that they’re using AI in their operations, companies reliant on logistics stand to benefit from it. A McKinsey report found that the majority of enterprises saw cost reduction in supply chain planning costs after implementing AI. To this point, plant-based foods startup Nuggs says it notched a 460% increase in monthly orders after adopting Flowspace’s solution. “It’s widely reported that the pandemic has accelerated the growth of e-commerce. For every $100 spent online, companies are spending $20 of that to store and ship these products to customer’s homes. We support brands selling online and helping them get products to their customers more efficiently,” Eachus continued. “We are building the most scalable and capital efficient fulfillment platform in the industry. This won’t happen overnight, but in a few years, when you order something online, those products will have moved through the Flowspace platform.” With the series B, 80-person Flowspace has raised a total of $46 million to date. Previous investments came from Canvas Ventures, Industrious Ventures, Moment Ventures, 1984 Ventures, eGateway Capital, and Y Combinator."
https://venturebeat.com/2021/03/15/cruise-acquires-driverless-vehicle-startup-voyage-for-an-undisclosed-amount/,Cruise acquires driverless vehicle startup Voyage to tackle dense urban environments,"GM-backed Cruise today announced it is acquiring Voyage, following Bloomberg’s early March report of a potential deal. Terms of the buyout weren’t disclosed, but Voyage CEO Oliver Cameron said “key members” of the Voyage team will join Cruise when the purchase is finalized in the coming months. Cameron will take on a new role as vice president of product. “Voyage’s experience and development of Commander (our self-driving AI), Shield (our collision mitigation system), and Telessist (our novel remote assistance solution) will only supercharge Cruise’s goal of superhuman driving performance,” Cameron wrote in a blog post. “I am thrilled that key members of our Voyage team — particularly those who worked on our third-generation robotaxi — will be able to use their extensive experience in vehicle development to put their stamp on the Cruise Origin, delivering a better and safer future for our roadways.” Cameron, former VP of product and engineering at online education giant Udacity, founded Voyage in 2017 alongside MacCallister Higgins, an ex-Udacity senior software engineer. The San Francisco, California-based startup targets communities that may have a greater and more imminent need for a network of self-driving cars, particularly retirement villages. Voyage’s vehicles are adapted Chrysler Pacifica Hybrid minivans that feature sensors and systems from third-party players and the company’s own AI technology. With a team of 60 employees, Voyage shipped three generations of robo-taxis — the G1, G2, and G3 — and signed partnerships with leading companies like FCA, First Transit, Enterprise, and Intact Insurance. Voyage counts a number of retirement communities among its customers, including The Villages in San Jose and The Villages in Florida.  The effects of the pandemic, including testing delays, have resulted in consolidation, tabled or canceled launches, and shakeups across the autonomous transportation industry. Ford pushed the unveiling of its self-driving service from 2021 to 2022; Waymo CEO John Krafcik told the New York Times the pandemic delayed work by at least two months; and Amazon acquired driverless car startup Zoox for $1.3 billion. According to Boston Consulting Group managing director Brian Collie, broad commercialization of AVs won’t happen before 2025 or 2026 — at least three years later than originally anticipated. According to Gartner analyst Mike Ramsey, consolidation in the self-driving market is inevitable and necessary. “There still are dozens of players trying to tackle this market from both a technology and an operations standpoint,” he told VentureBeat via email. “Every smaller company gets to a point where they have to decide whether they are able to scale up and invest the resources to grow, change their model altogether to push into a different part of the market, or look to merge with another company.” PitchBook’s Asad Hussain noted that smaller autonomous vehicle startups like Voyage face steep capital requirements to scale, while big tech-backed self-driving leaders like Cruise have achieved a formidable market position. “Voyage has targeted an attractive market, as the population of retirees is expected to grow significantly over the next few years. Additionally, we believe Voyage’s technology — which is focused on automated vehicles within retirement communities — is an attractive asset for Cruise, which largely focuses on automation in dense urban environments,” Hussain said. “Exposure to more structured environments such as retirement communities should enable Cruise to commercialize faster, as these use cases have much fewer variables and safety hazards compared to dense urban environments.” Cruise is considered a pack leader in a global market that’s anticipated to hit revenue of $173.15 billion by 2023. Recently, Cruise revealed that it has roughly 1,800 employees working on its self-driving cars, up from 1,000 as of March 2019. The also company claimed a 2.5 times increase in the utilization of its all-electric test vehicles between summer 2019 and early February, an improvement that’s expected to drive down costs. Cruise is piloting its cars in Scottsdale, Arizona and the Detroit, Michigan metropolitan area. But the bulk of its deployment is concentrated in San Francisco, where it has a permit to test vehicles without safety drivers behind the wheel. Cruise has scaled up rapidly, growing its starting fleet of 30 driverless vehicles to about 130 by June 2017. The company hasn’t disclosed the exact total publicly, but it has 180 self-driving cars registered with California’s DMV, and documents obtained by IEEE Spectrum suggest Cruise plans to deploy as many as 300 test cars around the country. Building on the progress it has made so far, in 2020 Cruise announced a partnership with DoorDash to pilot food and grocery delivery for select customers in San Francisco. And it’s making progress toward a fourth-generation car called Origin that features automatic doors, rear-seat airbags, and other redundant systems — but no steering wheel. In May 2018, Cruise announced that SoftBank’s Vision Fund would invest $2.25 billion in the company, along with another $1.1 billion from GM itself. In October 2018, Honda pledged $750 million, to be followed by another $2 billion in the next 12 years. And in January, Cruise raised $2 billion in an equity round that pushed its valuation up to $30 billion and brought Microsoft on as an investor and partner. But Cruise is burning through cash quickly. GM posted a $1 billion loss on Cruise in 2019, up from a $728 million loss in 2018."
https://venturebeat.com/2021/03/15/lookout-acquires-ciphercloud-to-secure-enterprises-from-endpoint-to-cloud/,Lookout acquires CipherCloud to secure enterprises from endpoint to cloud,"Endpoint security company Lookout has acquired CipherCloud, a cloud-native cybersecurity startup focused on the burgeoning secure access service edge (SASE) security sphere. Terms of the deal were not disclosed. Founded in 2007, Lookout offers a range of mobile security products spanning the consumer and business realms. For enterprises, Lookout delivers a cloud-based platform designed to detect and respond to cyberattacks through workers’ mobile devices, such as phishing attacks, ransomware, and data breaches. The San Francisco-based company claims a number of notable clients, including Landis+Gyr, Henkel, Nasdaq, and Schneider Electric. Among its products is Lookout Mobile Endpoint Security, which gives organizations visibility into the risks associated with their workers’ smartphones and tablets and lets them apply policies to reduce those risks. Given its direct reach into both the consumer and enterprise verticals, the company has access to a significant amount of data from across 185 million mobile devices, which helps Lookout adapt its machine learning algorithms to align with new threats. Founded in 2010, CipherCloud has increasingly focused on an emerging concept in cybersecurity known as SASE (pronounced “sassy”), a term coined in 2019 by Gartner. SASE is concerned with ensuring users and their devices have secure, speedy access to their applications and data wherever they reside by adopting a single cloud service located at the network edge. At its core, SASE sets out to address the siloed security measures that are currently spread between on-premises and the cloud. As mobile, cloud, and edge computing have increasingly replaced desktops, local area networks (LANs), and datacenters, it makes sense to transition security to minimize latency and other performance issues. CipherCloud currently operates across various SASE-related categories, such as zero-trust network access (ZTNA), cloud access security broker (CASB), secure web gateway (SWG), and data loss prevention (DLP). The company had raised around $80 million in external funding from investors such as Andreessen Horowitz and has amassed an impressive roster of clients, including Renault, Deutsche Telekom, Fujitsu, and EY. Lookout plans to combine CipherCloud’s SASE technologies with the Lookout Mobile Endpoint Security platform. A press release says this will put the company in a strong position to “deliver the industry’s first end-to-end platform that secures an organization’s entire data path from endpoint to cloud.” This integrated approach will help it “close the gaps” that emerge when companies use multiple products from different security vendors. Having CipherCloud on board will also help Lookout diversify its product offerings ahead of a major evolution in cybersecurity. Gartner predicts that SASE will be an $11 billion market by 2024, with “at least 40% of enterprises having explicit SASE strategies.”"
https://venturebeat.com/2021/03/15/amd-unveils-third-generation-epyc-processors-for-datacenters/,AMD launches third-generation Epyc processors for datacenters,"Advanced Micro Devices today launched its third Zen-based Epyc processors for datacenters as the chipmaker continues to go after the heart of Intel’s processor business. Once the perpetual underdog, AMD got a leg up a few years ago with the novel design of its Zen architecture, which can process a lot more instructions per clock cycle than previous AMD chips. AMD has used that advantage to make gains against archrival Intel in both the consumer and enterprise processor businesses. Market analyst firm Mercury Research noted that AMD has gained market share in client computing chips for 12 straight quarters, ending in Q3 2020. For that quarter, AMD’s desktop share was 20%, up 2 percentage points from a year earlier and up 7 percentage points from two years ago. In Q3, AMD’s mobile share was 20.2%, up 6 percentage points from a year earlier. But in servers, AMD’s share has historically been lower. “We’re now deep in a compute megacycle, which is super exciting. And [AMD CEO Lisa Su’s] vision for it was really exciting, with the chiplet model and delivering really high density and high performance,” Dan McNamara, senior vice president and general manager of AMD’s server business, said in a press briefing. “Milan, or third-gen Epyc, is really the next step on the journey. This compute megacycle is about connected devices driving all the data from edge through the network to the cloud.” IDC analyst Shane Rau said in an email to VentureBeat that in the fourth quarter of 2020 AMD held 7.7% of the server CPU market on a unit basis and 8.7% on a revenue basis. IDC estimates that about 10% of AMD’s server CPU volume in the quarter was Milan, sufficient to raise AMD’s overall server CPU average selling price sequentially. He said that cloud service providers bought those processors, especially the high-end  64-core processors. “This result indicates that Milan is getting a good initial reception from those who demand the highest in performance,” Rau said. AMD debuted its first Epyc central processing unit (CPU) based on Zen in 2017, and then it launched its code-named Rome Epyc CPUs in August 2019. Now the company is launching the third generation CPU code-named Milan in the hopes that major datacenter customers will no longer take a wait-and-see attitude toward AMD. After executing three generations in a row, AMD believes customers are ready to embrace the company for the longer term, McNamara said. Now it is launching its AMD Epyc 7003 Series CPUs. “We’re really on this great execution cadence, when it matters most, which is right now during this megacycle of compute,” McNamara said. AMD said it has more than 106% faster performance than Intel in HPC and cloud computing, and more than 117% faster performance than the competition in enterprise computing. McNamara said Milan will deliver breakthrough performance for cloud computing and high-performance computing (HPC) while also expanding Epyc’s appeal to enterprises. In cloud, AMD has more than 200 Epyc public instances. And with the introduction of Milan, AMD is on track to reach more than 400 Epyc instances in 2021. For cloud providers who need compute density and security capabilities, AMD Epyc 7003 Series processors offer the highest core density, advanced security features, and up to twice the integer performance compared to the competition. In HPC, the number of AMD-powered supercomputers on the November Top 500 Supercomputer List increased to 21 systems, including two of the top 10 and the fastest supercomputer in Europe. As HPC becomes critical for more and more enterprise workloads — not just government and research entities — this demand will only increase, McNamara said. AMD said the new processors increase transactional database processing by up to 19%, improve Hadoop big data analytic sorts by up to 60% with 61% better price to performance than the competition, and offer superior performance for flexible Hyperconverged infrastructure — all of which
helps CIOs turn data into actionable insights faster. AMD also said the Epyc 7003 Series processors enable faster time to discovery with more input/output and memory throughput and Zen 3 cores that deliver up to twice the performance for HPC workloads compared to the competition. In the enterprise, AMD is entering a high-performance cycle driven by the increasing adoption of cloud computing services and the accelerating digital transformation of businesses, as well as the rise of 5G and AI. Enterprise demand will only continue to go up as a result, McNamara said. “I see Milan as a big step forward for AMD, delivering strong performance across a range of workloads and surpassing Intel on almost every metric, including single-core performance, multi-core performance, cache size, DRAM bandwidth, PCIe data rates, performance per watt, and performance per dollar,” said Linley Gwennap, principal analyst at the Microprocessor Report, in an email to VentureBeat. “The only remaining advantage for Intel is on AI workloads, since Epyc lacks AVX-512 and DLBoost. These advantages should help AMD continue to gain server share, particularly at cloud vendors.” Among the customers launching new machines with the latest AMD Epyc 7003 Series processor today are Hewlett Packard Enterprise, which claims they help it achieve 39% better performance and greater energy efficiency than competing solutions. HPE will use the chips in its new HPE ProLiant servers and HPE Apollo systems. And HPE is offering the HPE Cray EX supercomputer, leveraging the third-generation AMD Epyc processors in supercomputing. Other partners include AWS, Cisco, Dell Technologies, Google Cloud, Lenovo, Microsoft Azure, Oracle Cloud Infrastructure, Supermicro, and Tencent Cloud. “We’re upping the ante here. It enables our customers to do more, do it faster, and then do it for less,” McNamara said. “We really believe that Milan, or third-gen Epyc, brings that next level of transformational value to the industry.” The AMD Epyc 7003 Series CPUs feature Zen 3 core designs. Different versions of the processors exist, based on the number of Zen 3 cores in the chip. The CPUs can have up to 64 cores in a single chip, and they introduce new levels of per-core cache memory while continuing to offer the PCIe 4 connectivity and memory bandwidth that defined the EPYC 7002 series CPUs. “Just when Intel thought it was catching up, AMD leapt and put even more distance between the two companies’ offerings,” Insight 64 analyst Nathan Brookwood said in an email to VentureBeat. “Some of the technical changes AMD’s architects made in cache organization should translate into dramatic performance gains for applications that can benefit from larger caches. The amazing thing from my perspective is that even with Rome’s (Epyc 2.0) performance edge, Intel still is capacity-limited in its shipments. The server market is red hot, and neither vendor has enough capacity to meet demand. The COVID lockdown has forced more people to spend more time staring at screens, and all that data has to come from somewhere. This is (hopefully) not a sustainable situation.” The chips also include modern security features through AMD Infinity Guard, supporting a new feature called Secure Encrypted Virtualization-Secure Nested Paging (SEV-SNP). SEV-SNP expands the existing SEV features on Epyc processors, adding strong memory integrity protection capabilities to help prevent malicious hypervisor-based attacks by creating an isolated execution environment. “AMD is making steady progress, with higher CPU core performance and platform improvements for virtualization and security,” Tirias Research analyst Kevin Krewell said in an email to VentureBeat. “AMD has improved the cache structure for larger datasets. The company also has filled out its product portfolio from 8 cores to 64 cores that all have a consistent platform architecture. AMD also has added partners with unique value.” The AMD Epyc processor ecosystem is expected to grow significantly by the end of 2021, with more than 400 cloud instances using all generations of Epyc processors and 100 new server platforms using third-gen Epyc processors. “It’s hard to see AMD losing share in the datacenter over the next couple of years, unless it makes a strategic blunder, which doesn’t seem likely,” Endpoint Technologies Associates analyst Roger Kay said in an email to VentureBeat. “Intel’s advantage in process node has been neutralized, and working with foundries seems like a strength now for AMD. Intel can make legitimate compatibility claims in some circumstances, but for many others, AMD will do just fine. The Epyc program is like the old Opteron days, only better.”"
https://venturebeat.com/2021/03/15/here-are-5-jobs-you-should-probably-apply-for-this-week/,Here are 5 jobs you should probably apply for this week,"The world is getting back to normal, and we honestly couldn’t be happier. It’s been a long and tough time, but the light is well and truly at the end of the tunnel. How can we be so sure? Well, a great sign is the fact that companies have lifted hiring freezes, and they’re recruiting again! This is something we feel like shouting about, in the hopes that many of you can land yourselves in exciting new positions, with a little help from us. So, we wanted to highlight some of the businesses looking for fantastic talent. Senior Partner Engineer, Checkout.com Checkout.com is actively looking for their first Senior Partner Engineer to join the fast-growing team in San Francisco to support their partners and merchants on platform solutions. This partner-facing role will drive technical relationships with partners based in the Americas from initial discovery, to integration and on-going management of the partnerships. Today, these are ecommerce and subscription management platforms; examples include BigCommerce, Salesforce Commerce Cloud (fka Demandware), Magento, Chargebee, and Zuora. In order to be successful in this role, you will be the technical interface between partners and internal teams, such as Partnerships, Product, Engineering, and Implementation Engineering. The ideal candidate will need to be an independent, highly motivated individual, who is able to stand out in an entrepreneurial environment and outperform goals through personal drive. Cyber Threat Intelligence Analyst, Palo Alto Networks The Crypsis Group is seeking a Threat Intelligence Analyst to join their growing team. The Threat Intelligence Analyst will be a member of the professional services team and will be responsible for partnering with executive leadership and other internal team members by providing critical information to combat potential threats. The Threat Intelligence Analyst will have experience with and understand various methods of using publicly available data to recognize relationships between individuals, businesses, and other entities. They will be responsible for mapping out malicious internet infrastructure and identifying additional, related assets, while also analyzing activity associated with both successful and unsuccessful intrusions by advanced attackers. IT Service Manager, Fanduel The role of a Service Manager is to act as a guardian of customer-facing products, ensuring operational processes comprising of Incident, Problem and Change are fit for purpose to drive highly available experience of Fanduel services for customers. Fanduel is looking for someone with excellent communication skills and the ability to act as liaison, delivering information both to technical and non-technical teams. You will join a place where innovation, creativity, customer/business focus, continuous improvement, automation and operational objectives are a way of life and love what you do. Director of Product Design and UX, Zwift As Director, Product Design and UX, you will join a growing team to help lead a mission to elevate Zwift’s core experiences across game, app and web. You will be responsible for overseeing intuitive and user-focused solutions to complex problems. Through these efforts, Zwift will deliver a user experience that becomes a point of competitive differentiation. You should be a strong team builder, as you will be working in partnership with the VP of Product Design andamp; UX to develop and grow a group of talented and motivated Product Experience (UX) Designers. They value leaders who are empathetic and supportive coaches. Those who can use their design vision to influence a rapidly growing, dynamic organization while raising the bar for experiential quality and consistency through the work of the team. This is a senior leadership-level role with significant responsibility, creativity, technical expertise, and business sense required. UX Developer, Northrop Grumman Northrop Grumman is currently looking for a UX Software Developer for its’ DASTO program, to support the United States Army Human Resources Command (HRC). If you are ready to move into this new and exciting program, meet the qualifications, and are interested in belonging to a team of highly skilled professionals supporting HRC, then you certainly should apply.  The scope of the Army Data and Application Support (DASTO) contract is to provide data and application support for the HRC. In this role, you will, under direct supervision, modify applications programs from detailed specifications; code, test, debug, document and maintain those programs. You will be involved in the planning of system and development deployment as well as being responsible for meeting software compliance standards. The successful candidate will evaluate interfaces between hardware and software, operational requirements, and characteristics of the overall system."
https://venturebeat.com/2021/03/15/google-partners-with-automation-anywhere-to-develop-rpa-products/,Google partners with Automation Anywhere to develop RPA products,"Google today announced a strategic, multi-year collaboration with robotic process automation (RPA) startup Automation Anywhere to accelerate the adoption of RPA with enterprises “on a global scale.” The partnership will make the Automation Anywhere platform available on Google Cloud and see the two companies mutually develop AI- and RPA-powered solutions, closely aligning go-to-market teams. RPA — technology that automates monotonous, repetitive chores traditionally performed by human workers — is big business. Forrester estimates that RPA and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could be automated in about 60% of occupations. And in its recent Trends in Workflow Automation report, Salesforce found that 95% of IT leaders are prioritizing workflow automation, with 70% seeing the equivalent of more than 4 hours saved each week per employee. As a part of its agreement with Automation Anywhere, Google plans to integrate the former company’s RPA technologies including low- and no-code development tools, AI workflow builders, and API management with Google Cloud services like Apigee, AppSheet, and AI Platform. Automation Anywhere and Google say they’ll also jointly develop solutions geared toward industry-specific use cases, with a focus on financial services, supply chains, health care and life sciences, telecommunications, retail, and the public sector. For its part, Automation Anywhere has pledged to migrate its cloud-native, web-based automation platform to Google Cloud as its primary cloud provider, in exchange for becoming Google Cloud’s preferred RPA partner. Automation Anywhere will additionally make its products available in the Google Cloud Marketplace, enabling them to be deployed across cloud, hybrid, and on-premises environments and providing customers with a view of assets and environments. As Google Cloud CEO Thomas Kurian notes, RPA has become a key part of businesses’ digital transformation efforts. Front office employees at call centers, financial services companies, human resources offices, IT departments, and more handle thousands of manual, repetitive tasks each day, like invoice processing, lending decisions, and employee onboarding. In the back office, IT teams and developers spend time managing APIs, entering data, and ensuring that apps can connect with legacy systems. Along with machine learning, computer vision, deep learning, and analytics, RPA can help businesses streamline these workloads through the development of AI-powered software bots capable of managing a number of front-and-back office tasks. “As businesses increasingly run in the cloud, RPA provides the means to streamline processes across both cloud-native applications and legacy, on-premises systems — ultimately helping employees spend less time on repetitive tasks and more time supporting business-critical projects,” Kurian said in a press release. “We are proud to partner with Automation Anywhere to help businesses quickly deploy and scale RPA capabilities on Google Cloud, and to address business challenges with solutions specially designed for industries.” The deal with Automation Anywhere appears to be Google’s answer to Microsoft’s acquisition of RPA startup Softomotive and IBM’s purchase of WDG Automation. Microsoft recently brought RPA to Windows 10 with new Power Platform products. With a market opportunity anticipated to be worth $3.97 billion by 2025, according to Grand View Research, RPA is fast becoming too large to ignore. Automation Anywhere rival Blue Prism has raised over $120 million, Kryon $40 million, and FortressIQ $30 million. And in July, UiPath nabbed $750 million, bringing its total raised to $2 billion at a post-money valuation of $35 billion."
https://venturebeat.com/2021/03/15/informatica-extends-serverless-computing-reach-to-nvidia-gpus/,Informatica extends serverless computing reach to Nvidia GPUs,"Informatica today announced it has integrated its Cloud Data Integration engine based on the Apache Spark in-memory computing framework with graphical processor units (GPUs) from Nvidia. The alliance makes the processing horsepower of GPUs accessible via a set of visual tools to a wide range of subject matter experts and data scientists without requiring them to have coding skills, said Rik Tamm-Daniels, Informatica’s vice president of strategic ecosystems and technology. While GPUs are much faster when it comes to processing data, no one wants to write code to invoke a GPU, Tamm-Daniels noted. But the number of individuals who need to build datasets keeps increasing. According to Gartner, on average 41% of employees outside of IT are in some way customizing datasets. Informatica is employing NVIDIA RAPIDS Accelerator software for Apache Spark to integrate Cloud Data Integration engine with Nvidia GPUs. That effort makes it possible to leverage the inherent parallel processing capabilities of an Nvidia GPU to process data as much as 5 times faster than an x86 processor, Tamm-Daniels claimed. The Cloud Data Integration engine automatically provides all the mappings needed to invoke either class of processor running on Amazon Web Services (AWS). The service will soon be available on Microsoft Azure and Google Cloud Platform (GCP), Tamm-Daniels added. In many cases, even data scientists who have the coding skills would prefer to take advantage of a serverless approach. Ultimately, data processing and management is being democratized, thanks to the rise of serverless computing frameworks based on event-driven computing architectures, Tamm-Daniels added. Like other cloud-native technologies, this approach is designed to scale up and down as required. An organization can lower the total cost of data-intensive compute loads by as much as 72% because there is no need to provision IT infrastructure until it’s required, Tamm-Daniels noted. As a consequence, serverless computing frameworks will soon become the primary way large amounts of data are processed on demand, Tamm-Daniels added. “Serverless is preferred,” he said. There may even come a day when cloud service providers running the Cloud Data Integration engine may find themselves undercutting each other to provide the lowest cost from processing various datasets at varying times of day. Thus far, the biggest driver of GPU adoption in the enterprise has been AI workloads. However, a wide range of data analytics applications involving, for example, clinical trials, would benefit from reducing the amount of time it takes to process massive amounts of data. Longer term, it’s not clear to what degree internal IT teams need, or even want, to be involved in managing data processing jobs beyond helping initially provision the IT environment. There is no shortage of other tasks that IT professionals could focus on if they spent less time writing code to enable data to be processed on one platform versus another. At the same time, knowledge workers of all skill levels are, for better or worse, finding ways to process data that doesn’t require the direct intervention of an internal IT team. Regardless of who processes that data, how it is processed is about to fundamentally change. The days when both IT administrator and subject matter experts who are among the most highly paid in any organization spend hours waiting for a job to finish is coming to an end as organizations start to appreciate how much more costly labor is compared to compute engine in the cloud."
https://venturebeat.com/2021/03/15/enterprise-workflow-automation-startup-deepsee-ai-raises-22-6m/,Enterprise workflow automation startup DeepSee.ai raises $22.6M,"DeepSee.ai, an enterprise workflow automation platform, today announced that it closed a $22.6 million series A round led by ForgePoint Capital. The company plans to use the funds to support R&D and the expansion of its product beyond the verticals DeepSee currently targets, chiefly capital markets and insurance. When McKinsey surveyed 1,500 executives across industries and regions in 2018, 66% said addressing skills gaps related to automation and digitization was a “top 10” priority. Salesforce’s recent Trends in Workflow Automation report found that 95% of IT leaders are prioritizing automation and 70% of execs are seeing the equivalent of over 4 hours saved each week per employee. Moreover, according to market research firm Fact.MR, the adoption of business workflow automation at scale could create a market opportunity of over $1.6 billion between 2017 and 2026. Salt Lake City, Utah-based DeepSee, which was founded in 2019, leverages open source and proprietary machine learning, linguistic comparison and prediction techniques, and sentiment analysis to automate manual business processes. From digital and legacy sources, DeepSee’s cloud-hosted platform captures, extracts, normalizes, labels, and analyzes unstructured data. The platform then surfaces trends and patterns for review, providing a pipeline to deliver AI-generated templates, rules, and logic to systems for actions.  DeepSee customers first specify the data, documents, and specific types of classification they’d like to perform. Then, they select from prepackaged machine learning models, bring their own models, or opt for one of several open source options. Lastly, they choose their desired outcomes via a custom workflow, API, or robotic process automation. In an interview with VentureBeat, CEO Steve Shillingford pointed to studies like that by Unit 4, which found that office workers spend 69 days a year on administrative tasks — costing companies $5 trillion a year. In the same Unit 4 study, 67% of respondents said implementing digital or software solutions would be important to remain competitive. “Today’s AI market is very fragmented — several point providers for single-purpose applications. From our vantage, we are seeing a mass consolidation that’s happening in the AI space,” Shillingford told VentureBeat via email. “Enterprises are struggling to stitch point solutions together to drive desired AI initiatives to eliminate the friction to deploy, maintain, and adopt innovation inside the business. And our sweet spot is mining unstructured data, operationalizing AI-powered insights, and automating results into real-time action for the enterprise.” DeepSee claims it can also mine for insights that reveal how data is impacting a particular business. The company’s crawler technology can browse internal repositories and third-party sources including media, press releases, and business publications to perform attribute clustering, outliers, anomalies, and aggregation of trends, spotlighting insights for analysis. “What’s really hard and likely existing in every enterprise is the tension between the folks building and tuning models and the folks expected to run the day-to-day operations. We were surprised at the challenges — challenges even the data scientists had inside the enterprise — at getting enough data to train models to become useful,” Shillingford continued. “In only the way a startup could, we managed to find innovation in that constraint and developed a tool for training across small sparse data sets with the same efficacy as if the model was trained on millions of documents. To that end, we think we’ve stumbled on a solution to one of the biggest problems gating really AI-productivity: How to apply NLP to processes where ‘big data’ isn’t available.”  Eighteen-employee DeepSee has a number of competitors in a global intelligent process automation market that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere and UiPath last secured hundreds of millions of dollars in investments at multibillion-dollar valuations. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. That’s not counting newer startups like WorkFusion, Indico, Tray.io, Tonkean, AirSlate, Workato, Camunda, and Automation Hero. But Shillingford says that DeepSee has been working closely with one of the largest banks in the world to develop and bring its tech into production. “Fortunately, but totally by coincidence, they were hurt significantly when the pandemic forced them to shut down several offices. This accelerated the rollout of the DeepSee platform, and thanks to our team, we were able to support the massive increase in volume while they adjusted to decrease in productivity,” Shillingford said. “The rollout was so successful, they were able to take market share from others during what was one of the most chaotic markets seen in recent memory.” AllegisCyber Capital and Signal Peak Ventures also participated in DeepSee’s latest funding round, bringing its total raised to date to $30.7 million."
https://venturebeat.com/2021/03/14/gartners-2021-magic-quadrant-cites-glut-of-innovation-in-data-science-and-ml/,Gartner’s 2021 Magic Quadrant cites ‘glut of innovation’ in data science and ML,"Gartner’s Magic Quadrant report on data science and machine learning (DSML) platform companies assesses what it says are the top 20 vendors in this fast-growing industry segment. Data scientists and other technical users rely on these platforms to source data, build models, and use machine learning at a time when building machine learning applications is increasingly becoming a way for companies to differentiate themselves. Gartner says AI is still “overhyped” but notes that the COVID-19 pandemic has made investments in DSML more practical. Companies should focus on developing new use cases and applications for DSML — the ones that are visible and deliver business value, Gartner said in the report released last week. Smart companies should build on successful early projects and scale them. The report evaluates DSML platforms’ scope, revenue and growth, customer counts, market traction, and product capability scoring. Here are some of the notable findings: There remains a “glut of compelling innovations” and visionary roadmaps, Gartner says. This is an adolescent market, where vendors are heavily focused on innovation and differentiation, rather than pure execution. Gartner said key areas of differentiation include UI, augmented DSML (AutoML), MLOps, performance and scalability, hybrid and multicloud support, XAI, and cutting-edge use cases and techniques (such as deep learning, large-scale IoT, and reinforcement learning). For most enterprises, the challenge is to keep up with the rapid pace of change in their industries, driven by how fast their competitors, suppliers, and channel partners are digitally transforming their businesses. Here are some company-specific insights included in this year’s Magic Quadrant: The challenges for DSML platform vendors today begin with balancing the needs for greater transparency and bias mitigation while developing and delivering innovative new features at a predictable cadence. The Magic Quadrant reflects current market reality after updating with four new cloud vendors, one with an extensive ecosystem and proven market momentum. One thing to consider after looking at the Magic Quadrant is that there will be some mergers or acquisitions on the horizon. Look for BI vendors to either acquire or merge with DSML platform providers as the BI market’s direction moves toward augmented analytics and away from visualization. Further fueling potential M&A activity is the fact that DSML platforms could use enhanced data transformation and discovery support at the model level, which is a long-standing strength of BI platforms."
https://venturebeat.com/2021/03/14/getting-to-trustworthy-ai/,Getting to trustworthy AI,"Artificial intelligence will be key to helping humanity travel to new frontiers and solve problems that today seem insurmountable. It enhances human expertise, makes predictions more accurate, automates decisions and processes, frees humans to focus on higher value work, and improves our overall efficiency. But public trust in the technology is at a low point, and there is good reason for that. Over the past several years, we’ve seen multiple examples of AI that makes unfair decisions, or that doesn’t give any explanation for its decisions, or that can be hacked. To get to trustworthy AI, organizations have to resolve these problems with investments on three fronts: First, they need to nurture a culture that adopts and scales AI safely. Second, they need to create investigative tools to see inside black box algorithms. And third, they need to make sure their corporate strategy includes strong data governance principles. Trustworthy AI depends on more than just the responsible design, development, and use of the technology. It also depends on having the right organizational operating structures and culture. For example, many companies that may have concerns about bias in their training data also have expressed concern that their work environments are not conducive to nurturing women and minorities to their ranks. There is indeed, a very direct correlation! To get started and really think about how to make this culture shift, organizations need to define what responsible AI looks like within their function, why it’s unique, and what the specific challenges are.  To ensure fair and transparent AI, organizations must pull together task forces of stakeholders from different backgrounds and disciplines to design their approach. This method will reduce the likelihood of underlying prejudice in the data that’s used to create AI algorithms that could result in discrimination and other social consequences. Task force members should include experts and leaders from various domains who can understand, anticipate, and mitigate relevant issues as necessary. They must have the resources to develop, test, and quickly scale AI technology. For example, machine learning models for credit decisioning can exhibit gender bias, unfairly discriminating against female borrowers if uncontrolled. A responsible-AI task force can roll out design thinking workshops to help designers and developers think through the unintended consequences of such an application and find solutions. Design thinking is foundational to a socially responsible AI approach. To ensure this new thinking becomes ingrained in the company culture, all stakeholders from across an organization — from data scientists and CTOs to Chief Diversity and Inclusivity officers must play a role. Fighting bias and ensuring fairness is a socio-technological challenge that is solved when employees who may not be used to collaborating and working with each other start doing so, specifically about data and the impacts models can have on historically disadvantaged people. Organizations should seek out tools to monitor transparency, fairness, explainability, privacy, and robustness of their AI models. These tools can point teams to problem areas so that they can take corrective action (such as introducing fairness criteria in the model training and then verifying the model output). Here are some examples of such investigative tools:  There are versions of these tools that are freely available via open source and others that are commercially available. When choosing these tools, it is important to first consider what you need the tool to actually do and whether you need the tool to perform on production systems or those still in development. You must then determine what kind of support you need and at which price, breadth, and depth. An important consideration is whether the tools are trusted and referenced by global standards boards. Any organization deploying AI must have clear data governance in effect. This includes building a governance structure (committees and charters, roles and responsibilities) as well as creating policies and procedures on data and model management. With respect to humans and automated governance, organizations should adopt frameworks for healthy dialog that help craft data policy. This as an opportunity to promote data and AI literacy in an organization. For highly regulated industries, organizations can find specialized tech partners that can also ensure that the model risk management framework meets supervisory standards.   There are dozens of AI governance boards around the world that are working with industry in order to help set standards for AI. IEEE is one single example. IEEE is the largest technical professional organization dedicated to advancing technology for the benefit of humanity. The IEEE Standards Association, a globally recognized standards-setting body within IEEE, develops consensus standards through an open process that engages industry and brings together a broad stakeholder community. Its work encourages technologists to prioritize ethical considerations in the creation of autonomous and intelligent technologies. Such international standards bodies can help guide your organization to adopt standards that are right for you and your market. Curious how your org ranks when it comes to AI-ready culture, tooling, and governance? Assessment tools can help you determine how well prepared your organization is to scale AI ethically on these three fronts. There is no magic pill to making your organization a truly responsible steward of artificial intelligence. AI is meant to augment and enhance your current operations, and a deep learning model can only be as open-minded, diverse, and inclusive as the team developing it. Phaedra Boinodiris, FRSA, is an executive consultant on the Trust in AI team at IBM and is currently pursuing her PhD in AI and Ethics. She has focused on inclusion in technology since 1999 and is a member of the Cognitive World Think Tank on enterprise AI."
https://venturebeat.com/2021/03/14/the-data-privacy-cold-war-is-here-which-side-are-you-on/,The data privacy Cold War is here. Which side are you on?,"Apple and Facebook have entered an all-out Cold War in the name of consumer data privacy. The battle started when Apple announced it will soon require users to opt in to personal data tracking. Facebook, which makes money from that tracking, took out full-page ads in major newspapers condemning the move. Apple CEO Tim Cook fired back in a recent speech, rebuking companies that gather as much data as possible and warning of dangerous consequences. Both companies have put a stake in the ground, and the impact will be felt across the tech and business worlds. Meanwhile, conversations about data privacy are going mainstream. WhatsApp users expressed outrage when they had to accept new privacy terms or lose the app, and data privacy bills are gaining momentum in state legislatures. All of this means time is up for the companies that have sat on the sidelines of this debate until now. Every tech company has access to user data, and each one now must decide which side of the data privacy war they’re on: the one that collects and exploits consumer data, or the one that respects and protects data and the users it belongs to. Prioritizing consumer data privacy doesn’t always mean a company must overhaul its policies. Rather, it’s about communicating those policies to consumers in a way they can understand and holding internal teams accountable to them. Every company that collects and shares consumer data needs a version of its privacy policy that users, not corporate lawyers, can understand. It seems simple, but privacy policies are often so long and stacked with legal jargon that users scroll through without absorbing a word. A digestible privacy policy should articulate what data the company believes it owns and what belongs to the consumer. It should be clear, jargon-free and understandable without a dictionary. Women’s health app Clue does this well, outlining exactly what data it collects from users and why. Especially when users are sharing data as sensitive as health information, this transparent communication fosters consumer trust. Last year, 91% of companies with very mature privacy practices – which include transparency – saw increased user trust and loyalty. Another benefit of a user-friendly privacy policy is that it can help a company’s leaders decide whether to change their data privacy practices. If leaders aren’t comfortable telling consumers what the company is doing with their data, it’s time to rethink those practices. In addition to a user-friendly privacy policy, companies should give consumers privacy “road signs” to help them navigate the confusing landscape of data collection and make informed decisions about what data they’re willing to share. There’s a misconception that Facebook is under scrutiny for using consumers’ data to target ads, but in fact it’s because the company historically hasn’t given its users any of this signage. Its mass collection of user data without explanation of how or why has hurt consumers’ trust in its brand. Data privacy road signs go beyond a bare-bones privacy policy, giving users context that helps them decide what data they’re comfortable sharing. For example, a company can tell users what it doesn’t do with their data. When it comes to an abstract, complex topic like data privacy, people are often better at understanding what they’re not comfortable with. An organization like Signal does that work for users by outlining that it can’t access their messages and “does not sell, rent, or monetize your personal data or content in any way — ever.” Good privacy signage also tells users what kinds of partners and third parties a company shares data with and why. Twilio clearly communicates that it shares some user data with other companies to improve users’ call quality. These clear guidelines build user trust and are a compelling reason for consumers to choose one product over another that offers less clear data privacy signage. Companies should communicate their data privacy practices early and often to users, but upholding those practices is an inside job. Leaders can take steps to ensure their company culture encourages employees to act as respectful data custodians. One of those steps is rewarding employees or teams who do their jobs well with the least consumer data. For example, leaders can invite a team that exceeded its goals while reducing data access to share how they did it and what they learned at an all-hands meeting. A company can also implement tokenization, which swaps out sensitive data with digital “tokens” — like poker chips or arcade tokens — that would be useless if intercepted or leaked. The data itself moves into a private vault that the company can’t access. These changes foster a culture that depends less on data access and encourages creativity. Finally, leaders can designate an executive privacy sponsor who advocates for user data privacy and holds leadership accountable to follow company privacy guidelines. Apple and Facebook have thrown down the data privacy gauntlet, and it’s time for all companies to pick a side. In the coming years, consumers will flock to companies that respect and protect their data. Those that are transparent and encourage good internal data privacy practices will gain more trusting and loyal users and in turn, stronger businesses. Frederick “Flee” Lee is Chief Security Officer at payroll and benefits service provider Gusto."
https://venturebeat.com/2021/03/13/how-to-scale-ai-with-a-high-degree-of-customization/,How to scale AI with a high degree of customization,"In a previous post, I outlined four challenges to scaling AI: customization, data, talent, and trust. In this post, I’m going to dig deeper into that first challenge of customization. Scaling machine learning programs is very different to scaling traditional software because they have to be adapted to fit any new problem you approach. As the data you’re using changes (whether because you’re attacking a new problem or simply because time has passed), you will likely need to build and train new models. This takes human input and supervision. The degree of supervision varies, and that is critical to understanding the scalability challenge. A second issue is that the humans involved in training the machine learning model and interpreting the output require domain-specific knowledge that may be unique. So someone who trained a successful model for one business unit of your company can’t necessarily do the same for a different business unit where they lack domain knowledge. Moreover, the way an ML system needs to be integrated into the workflow in one business unit could be very different from how it needs to be integrated in another, so you can’t simply replicate a successful ML deployment elsewhere. Finally, an AI system’s alignment to business objectives may be specific to the group developing it. For example, consider an AI system designed to predict customer churn. Two organizations with this same objective could need vastly different implementations. First, their training datasets are going to be structured differently based on how their Customer Relationship Management (CRM) system’s data is organized. Next, each organization may have different domain-specific knowledge of the impact of seasonality — or other factors — on the sale of specific products that is not readily reflected in the data; they would need to bring in humans to optimize those parameters. And those are just the technical considerations. Other considerations arise on the business process side. An online digital services company will look at a customer churn problem on a near real-time basis, requiring its AI system to deal with streaming datasets and quick inference timelines. But a boutique apparel shop may have the luxury of working with monthly or quarterly churn numbers, so its AI systems can be made to work with batches of data rather than streaming datasets, considerably reducing the complexity of the deployment. Due to the unique technical and business process requirements each business faces, it’s clear that customization is key for any high-output AI deployment. Buying off-the-shelf solutions that are not optimized for your specific needs means a compromise on performance and outcomes. The cost of having to “re-compose” AI systems every time, for every problem, for every customer is not just systems costs and human hours costs but also the cumulative costs of the time lag between starting a new AI project and being able to glean value from that implemenation. This is why most AI Centers of Excellence set up in large organizations fail to deliver on their initial expectations — although they’re a necessary part of building customized AI capabilities. On top of that, once an AI system is live and in production, maintaining it, optimizing, and governing it is another ongoing challenge. Nonetheless, it is possible to customize AI projects at scale. What it requires is a portfolio approach to your AI strategy. Here’s what that approach looks like: 1. Build a modular AI infrastructure layer for re-use and repeatability. Easier said than done, this means addressing model-building tools, libraries, and integrated development environments strategically. Left unchecked, the vast array of options and researcher/engineer preferences can lead to an architectural nightmare. Successful organizations I have worked with put a foundational infrastructure strategy in place, through a process of standardization and modularity. That means a standardized set of recommendations for training and inference computing infrastructure (cloud vs on-premises, GPUs vs. CPUs), a standard set of libraries, model packaging recommendations, and API-level integration requirements for all ML development within the organization. The goal is to modularize to accelerate time to value through reuse, but without compromising flexibility. 2. Foster collaboration across the organization: This can be achieved with 2 specific steps: First, build an internal marketplace for all ML and data assets. This means any team across the enterprise can contribute their ML development for reuse with clear instructions on use. In addition to being a great way to manage the outputs of AI investments, this also drives organizational knowledge-building and creates a forum where people can enhance each other’s innovations. Second, empower both your data scientists and non-technical users to rapidly experiment and deploy different use cases. In addition to having a library of tools, techniques like Auto-ML may help here. Bridging the operational complexity of packaging ML models and lowering the barrier for experimentation is a requirement for this. 3. Time-bound your AI experiments. We’ve all heard about the dire success rates for ML and AI projects. Beating these odds requires a healthy experimental environment focused on innovating around new problems and business use cases with a rapid path to validating hypotheses (deciding which meet the criteria to get into production). It’s critical to plan these experiments in short development sprints, with very clear criteria that can be continuously evaluated to see if it makes sense to move forward with the project or not. One approach here is to evaluate all of your AI projects/use cases across two vectors — the expected business value and the time it takes to implement in production (due to complexity in data acquisition, domain expertise needed etc.) — and use this as a guide to prioritize projects across a timeframe. It’s important to clearly define thresholds around quantified expected business value, cost/time to get into production, and availability of data and expertise. Customization is critical for getting results with AI — but it doesn’t have to slow you down. If you put the right modular infrastructure in place and if business units across your organization can align to deliver AI initiatives with a focus on rapid iteration and experimentation, customization can be the great accelerator and the ultimate key to achieving AI at scale. Ganesh Padmanabhan is VP, Global Business Development & Strategic Partnerships at BeyondMinds. He is also a member of the Cognitive World Think Tank on enterprise AI."
https://venturebeat.com/2021/03/13/covid-changed-the-e-commerce-landscape-heres-how-cpg-players-must-adapt/,COVID changed the e-commerce landscape. Here’s how CPG players must adapt,"In Ernest Hemingway’s novel “The Sun Also Rises,” there is a famous passage where a main character shares insight into how he went bankrupt: “Two ways. Gradually, then suddenly.” The food and beverage industry’s shift to online was similar — gradual for the last few years, then sudden because of the global COVID-19 pandemic. In fact, at PepsiCo, we estimate that the pandemic has accelerated the adoption of online grocery by 3-5 years. The future of the food and beverage industry will be rooted in data and technology, and e-commerce offers the opportunity to test new technological capabilities in a much faster and lower risk environment than brick and mortar. For the past several years, PepsiCo has been bolstering its tech capabilities and redefining what it means to be a consumer packaged goods (CPG) company by accelerating its digital transformation. By investing in these capabilities early, we were well-positioned to handle this year’s sharp increase in demand. For other companies now trying to position themselves to take advantage of this shift, here are a few things we’ve learned so far from our digital transformation journey. Start with a small, cross-functional team focused on a key problem. Staff this team with top technical and data talent combined with commercially-minded retail or CPG experts. Focus this core team on improving not only the retail customer experience but your overall e-commerce environment. Have this core team start from the consumer and work back to identify key problems to solve in the consumer experience. Ultimately, the in-house solution might begin by solving consumer challenges in one channel but evolve to solve challenges in multiple channels. Be willing to invest ahead of the curve. However, it’s important to recognize that forward investments often face skepticism from internal critics when you pursue an idea where the payback may not be immediate. Most CPGs approach technology investments with a project mindset rather than a product or platform mindset. This shift is challenging. Make the case for getting ahead of the curve, hiring the core team, and working on a platform that might not meet traditional ROI hurdle rates but has the probability of long-term payback with larger business returns. Start small and find early wins to gain credibility. Communicate early and often, be transparent, and build credibility to make it easier for important stakeholders to understand the value of what you’re building. At PepsiCo, we built some early marketing automation to allow us to execute campaigns more efficiently. At first, after a test with one customer and a limited number of campaigns, this saved us a very small amount of time. But we knew that as the business and market grew, the scale and complexity would, too — and this automation would then have a much bigger impact. We also learned that adding additional internal and external signals, such as stock levels and competitive activity, would help refine the marketing automation tool. Over the past several years, we’ve expanded the number of partners and types of marketing. Now the marketing platform automates most of our retail media campaigns. Most recently, during the COVID-19 pandemic’s volatility, this platform allowed us to make thousands of decisions in a scaled and automated way. Betting on first-mover advantage and ahead-of-the-curve investment requires the foresight to begin work and lay the foundation for when consumers are ready. The question is, is it more advantageous to buy off-the-shelf solutions or focus on developing in-house IP? Commerce platforms must constantly evolve to meet consumer needs. Off-the-shelf solutions are generally built to service the needs of many enterprise customers and can often lead to slow lead time for changes or lowest common denominator type solutions. So building in-house may offer more flexibility and a strategic competitive advantage. By providing the flexibility to move with speed, iterate and be responsive, in-house solutions also allow for ways of working where dedicated cross-functional teams have the ability to pivot quickly and remain agile. A custom-built platform benefits suppliers and retailers as well. You’re able to test and learn more often and apply those learnings to benefit your retail partners and meet consumer demand. For CPGs looking to grow in e-commerce, navigating retailer relationships is crucial. Mutual trust and transparency of data and sales information is key. When partners share more data, CPGs can show how this will drive sales or improve profitability. Insight and visibility into a retailer’s forecast, sales, inventory levels, and future orders allow suppliers to better manage in-store and digital in-stock rates, fill rates, and assortment. Through new digital capabilities and machine learning algorithms, both parties can drive sales and increase profit margins together. Unlike physical store shelves, which retailers reset a few times a year, “digital shelves” generate in real time the moment shoppers search for a product or browse through a product category. And oftentimes they show customized results depending on the location or profile of the shopper. The fundamental difference between these two scenarios is that digital shelves are determined by algorithms, which require a completely new layer of interaction between retailers and suppliers. To efficiently deploy digital marketing spend at scale, companies need to effectively make hundreds of thousands of decisions a day — including how much to bid for different ads as consumers search for products and digital shelves are created in real time. We’ve developed and implemented this marketing automation platform across multiple retailers, processing high volumes of valuable data to optimize marketing spend and ensure digital shelves are intuitive and frictionless to shoppers — both new and returning. Our marketing automation platform connects to retailer advertising APIs, manages tens of thousands of keywords, and calculates bid changes multiple times a minute to maximize the effectiveness of our ad spend. The platform is based on Elixir on the back end and React on the front end. We chose Elixir because of its ability to scale and handle a massive amount of data in near real-time. React enables rapid development of reusable rich components for end users, which helps with data visualizations and taking bulk actions on sets of keywords and campaigns. In addition, we use Airflow for data ingestion. During the pandemic we saw Boomers drive the surge in adoption of e-grocery. Having an easy-to-use search and onsite navigation is key for many first-time shoppers as they’re familiarizing themselves with the e-commerce ecosystem. And since we know consumers tend to build their online baskets by looking at their past purchase history, it’s important we’re part of their first basket to become a habitual purchase. This is just one of the ways technology, data, and insights are core to our e-commerce strategy. As we grow and learn, we continually work with our retail partners to determine data sets that could be mutually beneficial to help grow and optimize our businesses. The global pandemic may have forced the shift to digital in CPG industry, but the impact of the shifts is likely to be permanent. To survive in this new environment, companies need to act differently and think differently about technology investments. It’s important to focus on a technology-forward approach, unlock your competitive advantage, and embrace new ways to foster operational agility. Pivots made now will pay dividends long into the future as the change in consumer behaviors become permanent. Vince Jones is Head of eCommerce at PepsiCo."
https://venturebeat.com/2021/03/12/qlik-makes-pulling-data-from-sap-applications-simpler/,Qlik makes pulling data from SAP applications simpler,"Qlik, a provider of data integration and analytics software, this week announced it has made it easier to pull data from SAP applications using a set of accelerators optimized for specific business processes. The first in what will become a series of accelerators is focused on SAP Order to Cash analytics. The accelerators combine data integration and analytics software from Qlik to reduce the time and effort required to surface insights from specific processes within an SAP enterprise resource planning (ERP) application. The overall goal is to make it simpler to pull data from any SAP application into the data warehouse of their choice, said Matt Hayes, vice president of SAP Business at Qlik. While SAP currently provides connectors to those databases, its primary focus is on moving data from its applications to a data warehouse based on the SAP HANA database, Hayes said. In contrast, a set of more agnostic connectors provided by Qlik makes it easier to pull data from an SAP application into data lakes provided by, for example, Amazon Web Services (AWS), Microsoft, Google, or Snowflake, added Hayes. He asserted that Qlik data integration software enables real-time delivery of SAP data from any source to any target. The focus on data integration and analytics within enterprise IT environments has never been greater. Due to the economic downturn brought on by the COVID-19 pandemic, business leaders are trying to optimize in real time a wide range of processes. Achieving that goal requires increases reliance on analytics applications that are only as useful as the most recent data collected. As organizations launch various digital business transformation initiatives, many of them are discovering they need to be able to pull data from SAP applications that function as systems of record in the enterprise. The challenge has always been that data from an SAP application has to then be normalized alongside other data to enable end users to surface insight from data created using multiple applications. Many organizations are now investing in data warehouses in the cloud to collect massive amounts of data that can be accessed more easily by a range of analytics applications. Qlik is making a case for a Qlik Sense analytics application that runs in memory to make it easier to surface insights in near real time as data is continuously pulled from various sources. “You never have to refresh the data,” Hayes said. However, IT teams can employ the data integration software Qlik provides without having to adopt Qlik Sense. This week Qlik revealed it has developed a unified connector based on SAP BEx/InfoProvider joint connectivity and SAP SQL connector software to streamline the process of pulling data from any SAP application into the Qlik Sense Enterprise edition of its analytics software. SAP has never been especially focused on making it simpler to pull data from its applications and databases. The company has its own portfolio of analytics applications that are tightly integrated with its data warehouse and associated data virtualization tools. However, many organizations have standardized on a wide range of applications that are employed to analyze data aggregated from multiple data sources. Many of the users of those applications want to be able to access data without IT intervention, Hayes noted. Most IT teams are inclined to enable that access so long as the integration and analytics software employed doesn’t have a material impact on the performance of the SAP applications they are running, added Hayes. Qlik minimizes that impact by identifying what subset of data in an SAP application is actually new versus constantly pulling all the data in an SAP application into an analytics application, said Hayes."
https://venturebeat.com/2021/03/12/quantum-computing-takes-important-step-with-first-public-company/,Quantum computing takes important step with first public company,"This week saw the first step towards a public quantum computing hardware company with IonQ’s SPAC merger. IonQ should be applauded for its technical achievements that underpin this agreement. This moment recognizes the massive potential of quantum technology, despite the substantial R&D challenges that remain. The quantum computing industry still needs to address fundamental technology challenges in hardware and software for quantum computing to achieve its potential. Software developers must understand and build for the capacity of near-term hardware, limiting noise and errors, with slimmed-down algorithms constructed to the specifications of each hardware device. Hardware companies like IonQ must develop hardware that scales to greater qubit numbers and circuit depth that can run non-trivial applications. Both sides continue to advance, and it’s this combination that is needed to deliver meaningful commercial, industrial, and scientific applications. We’ve seen numerous significant advancements in quantum hardware and software that get us closer to realizing quantum computing’s promise. It is now possible to see a path towards quantum advantage where quantum computers can solve significant problems beyond classical computers’ capabilities. We can see a future state where quantum computing can advance science and industry in new ways, including simulating new materials for batteries, superconductors and more. These findings lead to better energy storage and clean energy faster in ways that are far more capital efficient than current approaches. Despite its tremendous promise, it’s important to remember that realizing quantum computing is not an overnight trip. It is a decades-long journey. Yet we could — and should — see promising applications on near-term quantum hardware within a few years. There are three critical caveats about quantum computing that we must address: 1. Quantum computing will not replace classical computing. Quantum computing is often mischaracterised as souped-up supercomputing. Potential investors need to understand that this is not a replacement or upgrade for classical supercomputing, nor will it replace desktop or mobile devices. Quantum computing is also not a drop-in replacement for existing components of a company’s data infrastructure. To make the best use of quantum computers, you need to have isolated the components of a problem that is well-suited to quantum speedups. 2. Quantum computing is not immediate. Despite some headlines, we are not a year away from quantum computing upending cybersecurity, finance, and other industries overnight. Quantum hardware is not robust enough to limit noise or errors, nor are there software programs and algorithms that are optimized to run on near-term hardware. Developing quantum hardware is capital intensive and requires patient investors who are willing to see it through for the long term. 3. Quantum computing isn’t for every use case. Quantum computing is uniquely suited to problems that relate to quantum mechanics. This is especially useful for numerous frontier science applications that currently require billions to develop and test in the laboratory. In the longer term, quantum computing has potential to speed up certain classes of optimization problems, and beyond. But it is not a magic solution for every big data, machine learning, finance, or other high-profile computing-challenge-of-the-year. As a co-founder of a quantum software startup and a leader at one of the most prestigious universities developing the theory and technology of quantum computers, I see the need for a robust ecosystem for quantum computing. Careful investment is critical to realize the potential and promise of quantum computing and its decades-long journey. The industry welcomes patient investment and encourages new talent to enter the field to help deliver the promise and the potential of quantum computing. Investors need to be aware of the long-term research and development required to bring quantum computers out of labs and into the world as a sustainable and high-growth frontier computing industry. Although we have not yet reached the point of quantum advantage, leading companies in fields as varied as materials, pharmaceuticals, and finance are setting up teams to work with quantum computing and evaluate its potential to transform their businesses. Given the first-mover advantage that could accrue to those who can best take advantage of this revolutionary technology, now is the time for executives to make this assessment. Engaging deeply with the scientific and technical experts could pay huge dividends. Ashley Montanaro is Co-founder of UK quantum startup Phasecraft and Professor of Quantum Computation at the University of Bristol."
https://venturebeat.com/2021/03/12/ai-weekly-facebook-google-and-the-tension-between-profits-and-fairness/,"AI Weekly: Facebook, Google, and the tension between profits and fairness","This week, we learned a lot more about the inner workings of AI fairness and ethics operations at Facebook and Google and how things have gone wrong. On Monday, a Google employee group wrote a letter asking Congress and state lawmakers to pass legislation to protect AI ethics whistleblowers. That letter cites VentureBeat reporting about the potential policy outcomes of Google firing former Ethical AI team co-lead Timnit Gebru. It also cites research by UC Berkeley law professor Sonia Katyal, who told VentureBeat, “What we should be concerned about is a world where all of the most talented researchers like [Gebru] get hired at these places and then effectively muzzled from speaking. And when that happens, whistleblower protections become essential.” The 2021 AI Index report found that AI ethics stories — including Google firing Gebru — were among the most popular AI-related news articles of 2020, an indication of rising public interest. In the letter published Monday, Google employees spoke of harassment and intimidation, and a person with policy and ethics matters at Google described a “deep sense of fear” since the firing of ethics leaders Gebru and former co-lead Margaret Mitchell. On Thursday, MIT Tech Review’s Karen Hao published a story that unpacked a lot of previously unknown information about ties between AI ethics operations at Facebook and the company’s failure to address misinformation peddled through its social media platforms and tied directly to a number of real-world atrocities. A major takeaway from this lengthy piece is that Facebook’s responsible AI team focused on addressing algorithmic bias in place of issues like disinformation and political polarization, following 2018 complaints by conservative politicians, although a recent study refutes their claims. The events described in Hao’s report appear to document political winds shifting the definition of fairness at Facebook, and the extremes to which a company will go in order to escape regulation. Facebook CEO Mark Zuckerberg’s public defense of President Trump last summer and years of extensive reporting by journalists have already highlighted the company’s willingness to profit from hate and misinformation. A Wall Street Journal article last year, for example, found that the majority of people in Facebook groups labeled as extremist joined as a result of a recommendation made by a Facebook algorithm. What this week’s MIT Tech Review story details is a tech giant deciding how to define fairness to advance its underlying business goals. Just as with Google’s Ethical AI team meltdown, Hao’s story describes forces within Facebook that sought to co-opt or suppress ethics operations after just a year or two of operation. One former Facebook researcher, who Hao quoted on background, described their work as helping the company maintain the status quo in a way that often contradicted Zuckerberg’s public position on what’s fair and equitable. Another researcher speaking on background described being told to block a medical-misinformation detection algorithm that had noticeably reduced the reach of anti-vaccine campaigns. In what a Facebook spokesperson pointed to as the company’s official response, Facebook CTO Mike Schroepfer called the core narrative of Hao’s article incorrect but made no effort to dispute facts reported in the story. Facebook chief AI scientist Yann LeCun, who got into a public spat with Gebru over the summer about AI bias that led to accusations of gaslighting and racism, claimed the story had factual errors. Hao and her editor reviewed the claims of inaccuracy and found no factual error. Facebook’s business practices have played a role in digital redlining, genocide in Myanmar, and the insurrection at the U.S. Capitol. At an internal meeting Thursday, according to BuzzFeed reporter Ryan Mac, an employee asked how Facebook funding AI research differs from Big Tobacco’s history of funding health studies. Mac said the response was that Facebook was not funding its own research in this specific instance, but AI researchers spoke extensively about that concern last year. Last summer, VentureBeat covered stories involving Schroepfer and LeCun after events drew questions about diversity, hiring, and AI bias at the company. As that reporting and Hao’s nine-month investigation highlight: Facebook has no system in place to audit and test algorithms for bias. A civil rights audit commissioned by Facebook and released last summer calls for the regular and mandatory testing of algorithms for bias and discrimination. Following allegations of toxic, anti-Black work environments, both Facebook and Google have been accused in the past week of treating Black job candidates in a separate and unequal fashion. Reuters reported last week that the Equal Employment Opportunity Commission (EEOC) is investigating “systemic” racial bias at Facebook in hiring and promotions. And additional details about an EEOC complaint filed by a Black woman emerged Thursday. At Google, multiple sources told NBC News last year that diversity investments in 2018 were cut back in order to avoid criticism from conservative politicians. On Wednesday, Facebook also made its first attempt to dismiss an antitrust suit brought against the company by the Federal Trade Commission (FTC) and attorneys general from 46 U.S. states. All of this happened in the same week that U.S. President Joe Biden nominated Lina Khan to the FTC, leading to the claim that the new administration is building a “Big Tech antitrust all-star team.” Last week, Biden appointed Tim Wu to the White House National Economic Council. A supporter of breaking up Big Tech companies, Wu wrote an op-ed last fall in which he called one of the multiple antitrust cases against Google bigger than any single company. He later referred to it as the end of a decades-long antitrust winter. VentureBeat featured Wu’s book The Curse of Bigness about the history of antitrust reform in a list of essential books to read. Other signals that more regulation could be on the way include the appointments of FTC chair Rebecca Slaughter and OSTP deputy director Alondra Nelson, who have both expressed a need to address algorithmic bias. The Google story calling for whistleblower protections for people researching the ethical deployment of AI marks the second time in as many weeks that Congress has received a recommendation to act to protect people from AI. The National Security Commission on Artificial Intelligence (NSCAI) was formed in 2018 to advise Congress and the federal government. The group is chaired by former Google CEO Eric Schmidt, and Google Cloud AI chief Andrew Moore is among the group’s 15 commissioners. Last week, the body published a report that recommends the government spend $40 billion in the coming years on research and development and the democratization of AI. The report also says individuals within government agencies essential to national security should be given a way to report concerns about “irresponsible AI development.” The report states that “Congress and the public need to see that the government is equipped to catch and fix critical flaws in systems in time to prevent inadvertent disasters and hold humans accountable, including for misuse.” It also encourages ongoing implementation of audits and reporting requirements. However, as audits at businesses like HireVue have shown, there are a lot of different ways to audit an algorithm. This week’s consensus between organized Google employees and NSCAI commissioners who represent business executives from companies like Google Cloud, Microsoft, and Oracle suggests some agreement between broad swaths of people intimately familiar with the deployment of AI at scale. In casting the final vote to approve the NSCAI report, Moore said, “We are the human race. We are tool users. It’s kind of what we’re known for. And we’ve now hit the point where our tools are, in some limited sense, more intelligent than ourselves. And it’s a very exciting future, which we have to take seriously for the benefit of the United States and the world.” While deep learning and forms of AI may be capable of doing things that people describe as superhuman, this week we got a reminder of how untrustworthy AI systems can be when OpenAI demonstrated that its state-of-the-art model can be fooled to think an apple with “iPod” written on it is in fact an iPod, something any person with a pulse could discern. Hao described the subjects of her Facebook story as well-intentioned people trying to make changes in a rotten system that acts to protect itself. Ethics researchers in a corporation of that size are effectively charged with considering society as a shareholder, but everyone else they work with is expected to think first and foremost about the bottom line, or personal bonuses. Hao said that reporting on the story has convinced her that self-regulation cannot work. “Facebook has only ever moved on issues because of or in anticipation of external regulation,” she said in a tweet. After Google fired Gebru, VentureBeat spoke with ethics, legal, and policy experts who have also reached the conclusion that “self-regulation can’t be trusted.” Whether at Facebook or Google, each of these incidents — often told with the help of sources speaking on condition of anonymity — shine light on the need for guardrails and regulation and, as a recent Google research paper found, journalists who ask tough questions. In that paper, titled “Re-imagining Algorithmic Fairness in India and Beyond,” researchers state that “Technology journalism is a keystone of equitable automation and needs to be fostered for AI.” Companies like Facebook and Google sit at the center of AI industry consolidation, and the ramifications of their actions extend beyond even their great reach, touching virtually every aspect of the tech ecosystem. A source familiar with ethics and policy matters at Google who supports whistleblower protection laws told VentureBeat the equation is pretty simple: “[If] you want to be a company that touches billions of people, then you should be responsible and held accountable for how you touch those billions of people.” For AI coverage, send news tips to Khari Johnson and Kyle Wiggers — and be sure to subscribe to the AI Weekly newsletter and bookmark The Machine. Thanks for reading, Khari Johnson Senior AI Staff Writer"
https://venturebeat.com/2021/03/12/why-accenture-lists-digital-twins-as-top-five-technology-trend-in-2021/,Why Accenture lists ‘digital twins’ as top-five technology trend in 2021 ,"A digital twin technology is one that creates a virtual replication of a real-world entity, like a plane, manufacturing plant, or supply chain. Manufacturing companies have increasingly used digital twin technologies to accelerate digital transformation initiatives for product development, and the tech has grown in popularity over the past five years as legacy manufacturers look for ways to keep up with innovative startups like Tesla. The idea has been around since 2002, when it was coined by Michael Grieves, then a professor at the University of Detroit, to describe a new way of thinking about coordinating product lifecycle management. The concept stumbled along for many years, owing to limits around integrating processes and data across engineering, manufacturing, and quality teams. But it has begun picking up steam, thanks to improvements in data integration, AI, and the internet of things, which extend the benefits of digital transformation efforts into the physical world. In 2019, Gartner suggested that 75% of organizations would be implementing digital twins within the next year. This year, Accenture has positioned digital twins as one of the top five strategic technology trends to watch in 2021. The reason is that businesses are finally figuring out how to scale these projects across a fleet of projects, rather than a single one-off, Accenture Technology Labs managing director Michael Biltz said. The promise of digital twins lies in improving collaboration and workflows across different types of groups — like product design, sales, and maintenance teams — and engineering disciplines. When it’s done well, it can deliver fantastic results. For example, the U.S. Airforce has made extensive use of digital twins to design and build a new aircraft prototype in a little over a year, a process that traditionally drags on for decades. In other industries, the same principles can translate to accelerating vehicle electrification, lowering construction costs, and building smart cities. Chevron expects to save millions of dollars using digital twins to predict maintenance problems more quickly. Kaeser, which makes compressed air equipment, has been using digital twins to shift from a product model to a subscription model. Accenture worked with Unilever to build a digital twin of one of its factories. The digital twin allowed different experts to analyze various trade-offs in fine-tuning the factory while minimizing the risk of new problems. They were able to reduce electricity costs and increase productivity. Despite these early gains, many of these successes have been within a limited domain constrained by the technology platforms or systems integrators. The core idea behind digital twins emerged in the product lifecycle management for streamlining product development. But then other industries realized some of the same ideas were applicable. Gartner has characterized different types of digital twins for areas like product development, manufacturing, supply chains, organizations, and people. Although the digital models themselves are getting better, figuring out how to share models across applications is a bit trickier. Different types of applications optimize the data collection process and the data models for specific use cases. PLM vendors like Siemens, PTC, and Dassault have been buying up and building out rich ecosystems of tools that facilitate the exchange of digital twin data across the product lifecycle. These kinds of tools work well when enterprises buy tools from one vendor, but passing digital twin models between apps from different vendors leads to less integration. Various standards groups have been working to help streamline this process. The International Standards Organization has been working on developing a variety of standards for digital twin manufacturing, reducing data loss during exchanges, and promoting business collaboration. Michael Finocchiaro, senior technologist at digital transformation consultancy Percall Group, said, “I think that there is a big dependency on the PLM vendors to implement these standards so that they are brought into the DNA of how we develop digital twins.” As the big PLM systems — such as Dassault’s 3DEXPERIENCE, PTC’s Windchill, and Siemen’s Teamcenter — adopt these standards, they will become easier to deploy in the real world. But the jury is still out on how committed vendors are to ensuring interoperability in practice. For example, Finocchiaro said that integrating bill-of-material data across platforms often requires extensive customization despite the existence of standards. “This exposes the gap between the rhetoric of openness of these platforms as they seek to maintain and expand their customer base occasionally by vendor lock-in,” Finocchiaro said. This natural tendency puts a bit of drag into the adoption of standards. Scaling these efforts will require better integration and improved communications across stakeholders about how digital twins are supposed to work in practice. Industry collaborations like the Object Management Group’s (OMG) Digital Twin Consortium could help. Digital Twin Consortium CTO Dan Isaacs said, “While there is a lot more work to be done to enable digital twin interoperability, integration and standards that can support composability, sharing, and common practices will provide a foundation.” The OMG has previously spearheaded widely adopted standards like CORBA for business architectures and BPMN for diagramming business processes. The Digital Twin Consortium includes industry leaders such as Microsoft, Dell, GE Digital, Autodesk, and Lendlease, one of the world’s largest land developers. The group is focusing on creating consistency in the vocabulary, architecture, security, and interoperability of digital twin technology. It does not develop standards directly, but instead helps the different participants flesh out the requirements that will inform standards by organizations like ISO, the IEC, and the OMG. For example, the Digital Twin Consortium recently announced an alliance with FIWARE, an open source community that curates various digital twin reference components for smart cities, industry, agriculture, and energy. The hope is that this partnership could jumpstart digital twin deployments in the same way the internet grew on the back of TCP/IP reference implementations. This will make it easier to connect multiple digital twins to help model cities, large businesses, or even the world. “Digital transformation at full scale is still in early adoption,” Isaacs said. “Digital twins continue to gain momentum, but realizing their full potential will require seamless integration, alignment, and best practices for both software and hardware infrastructures.” This will require coordination across a wide range of technologies, such as AI/ML, modeling and simulation, IoT frameworks, and industry-specific data and communications protocols. In practice, this might look like extending the success of geographical information system interoperability into other domains. These efforts are already extending the use of satellite imagery and point cloud scanning coupled with AI and ML to identify structures and anomalies that can then be tagged and associated to other assets or attributes. This helps enterprise teams improve pattern identification to unlock critical insights needed to gain a competitive edge. Isaac expects to see the greatest adoption of digital twin technology in energy and utilities to accelerate the transition to renewables and achieve net-zero emissions. Other areas, like medical and health care, are also gaining momentum but face challenges harmonizing digital twins across a mishmash of different systems. Visionary leaders who work out the kinks to scaling digital twins may see a significant competitive advantage. Accenture’s Technology Vision 2021 report predicted, “The businesses that start today, building intelligent twins of their assets and piecing together their first mirrored environments, will be the ones that push industries, and the world, toward a more agile and intelligent future.”"
https://venturebeat.com/2021/03/12/facebook-details-ai-that-can-understand-videos/,Facebook details AI that can understand videos,"On the heels of a computer vision system that achieved state-of-the-art accuracy with minimal supervision, Facebook today announced a project called Learning from Videos that’s designed to automatically learn audio, textual, and visual representations from publicly available Facebook videos. By learning from videos spanning nearly every country and hundreds of languages, Facebook says the project will not only help it to improve its core AI systems but enable entirely new experiences. Already, Learning from Videos, which began in 2020, has led to improved recommendations in Instagram Reels, according to Facebook. Continuously learning from the world is one of the hallmarks of human intelligence. Just as people quickly learn to recognize places, things, and other people, AI systems could be smarter and more useful if they managed to mimic the way humans learn. As opposed to relying on the labeled datasets used to train many algorithms today, Facebook, Google, and others are looking toward self-supervised techniques that require few or no annotations. For example, Facebook says it’s using Generalized Data Transformations (GDT), a self-supervised system that learns the relationships between sounds and images, to suggest Instagram Reel clips relevant to recently watched videos while filtering out near-duplicates. Consisting of a series of models trained across dozens of GPUs on a dataset of millions of Reels and videos from Instagram, GDT can learn that a picture of an audience clapping probably goes with the sound of applause or that a video of a plane taking off likely goes with a loud roar. Moreover, the system can surface recommendations based on videos that sound alike or look alike, respectively, by leveraging audio as a signal. When asked which Facebook and Instagram users were subjected to having their content used to train systems like GDT and whether those users were informed the content was being used in this way, a Facebook spokesperson told VentureBeat that the company informs account holders in its data policy that Facebook “uses the information we have to support research and innovation.” In training other computer vision systems such as SEER, a self-supervised AI model that Facebook detailed last week, OneZero notes that the company has purposely excluded user images from the European Union, likely because of GDPR. Learning from Videos also encompasses Facebook’s work on wav2vec 2.0, an improved machine learning framework for self-supervised speech recognition. The company says that when applied to millions of hours of unlabeled videos and 100 hours of labeled data, wave2vec 2.0 reduced the relative word error rate by 20% compared with supervised-only baselines. As a next step, Facebook says it’s working to scale wav2vec 2.0 with millions of additional hours of speech from 25 languages to reduce labeling, bolster the performance of low-and medium-resource models, and improve other speech and audio tasks. In a related effort, to make it easier to search across videos, Facebook says it’s using a system called the Audio Visual Textual (AVT) model that aggregates and compares sound and visual information from videos as well as titles, captions, and descriptions. Given a command like “Show me every time we sang to Grandma,” the AVT model can find its location and highlight the nearest timestamps in the video. Facebook says it’s working to apply the model to millions of videos before it begins testing it across its platform. It’s also adding speech recognition as one of the inputs to the AVT model, which will allow the system to respond to phrases like “Show me the news show that was talking about Yosemite.” The Learning from Videos project also birthed TimeSformer, a Facebook-developed framework for video understanding that’s based purely on the Transformer architecture. Transformers employ a trainable attention mechanism that specifies the dependencies between elements of each input sequence — for instance, amino acids within a protein. It’s this that enables them to achieve state-of-the-art results in areas of machine learning including natural language processing, neural machine translation, document generation and summarization, and image and music generation.  Facebook claims that TimeSformer, short for Time-Space Transformer, attains the best reported numbers on a range of action recognition benchmarks. It also takes roughly one-third the time to train than comparable models. And it requires less than one-tenth the amount of compute for inference and can learn from video clips up to 102 seconds in length, much longer than most video-analyzing AI models. Facebook AI research scientist Lorenzo Torresani told VentureBeat that TimeSformer can be trained in 14 hours with 32 GPUs. “Since TimeSformer specifically enables analysis of much longer videos, there’s also the opportunity for interesting future applications such as episodic memory retrieval — ability to detect particular objects of interest that were seen by an agent in the past — and classifying multi-step activities in real time like recognizing a recipe when someone is cooking with their AR glasses on,” Torresani said. “Those are just a few examples of where we see this technology going in the future.” It’s Facebook’s assertion that systems like TimeSformer, GDT, wav2vec 2.0, and AVT will advance research to teach machines to understand long-form actions in videos, an important step for AI applications geared toward human understanding. The company also expects they’ll form the foundation of applications that can comprehend what’s happening in videos on a more granular level.  “[All] these models will be broadly applicable, but most are research for now. In the future, when applied in production, we believe they could do things like caption talks, speeches, and instructional videos; understand product mentions in videos; and search and classification of archives of recordings,” Geoffrey Zweig, director at Facebook AI, told VentureBeat. “We are just starting to scratch the surface of self-supervised learning. There’s lots to do to build upon the models that we use, and we want to do so with speed and at scale for broad applicability.” Facebook chose not to respond directly to VentureBeat’s question about how any bias in Learning from Videos models might be mitigated, instead saying: “In general, we have a cross-functional, multidisciplinary team dedicated to studying and advancing responsible AI and algorithmic fairness, and we’re committed to working toward the right approaches. We take this issue seriously, and have processes in place to ensure that we’re thinking carefully about the data that we use to train our models.” Research has shown that state-of-the-art image-classifying AI models trained on ImageNet, a popular (but problematic) dataset containing photos scraped from the internet, automatically learn humanlike biases about race, gender, weight, and more. Countless studies have demonstrated that facial recognition is susceptible to bias. It’s even been shown that prejudicescan creep into the AI tools used to create art, potentially contributing to false perceptions about social, cultural, and political aspects of the past and hindering awareness about important historical events. Facebook chief AI scientist Yann LeCun recently admitted to Fortune that fully self-supervised computer vision systems can pick up the biases, including racial and gender stereotypes, inherent in the data. In acknowledgment of the problem, a year ago Facebook set up new teams to look for racial bias in the algorithms that drive its social network as well Instagram. But a bombshell report in MIT Tech Review this week revealed that at least some of Facebook’s internal efforts to mitigate bias were coopeted to protect growth or in anticipation of regulation. The report further alleges that one division’s work, Responsible AI, became essentially irrelevant to fixing the larger problems of misinformation, extremism, and political polarization."
https://venturebeat.com/2021/03/12/cloudburst-hard-lessons-learned-from-the-ovh-datacenter-blaze/,Cloudburst: Hard lessons learned from the OVH datacenter blaze,"In every tabletop disaster-recovery exercise in every enterprise IT shop, there’s a moment when attention grudgingly shifts from high-profile threats — malicious intrusion, data theft, ransomware — to more mundane (and seemingly less likely) threats, like natural disasters, accidents, and low-tech turmoil. What hurricanes, explosions, earthquakes, fires, and floods lack in cybersecurity panache, they often make up for in ferocity. The history is clear: CIOs need to put more emphasis on force majeure — an act of God or moment of mayhem that threatens data availability at scale — when making their plans. On Christmas Day 2020, a bomb packed into an RV decimated a section of downtown Nashville, Tennessee. The collateral damage included a crippled AT&T transmission facility, which disrupted communications and network traffic across three states and grounded flights at Nashville International Airport. Outages for business clients and their customers lasted through the rest of the holiday season. This week brought even more stark evidence of the disruptive power of calamity. One of Europe’s largest cloud hosting firms, OVH Groupe SAS, better known as OVHCloud, suffered a catastrophic fire at its facility in Strasbourg, France. The blaze in a cluster of boxy, nondescript structures — actually stacks of shipping containers repurposed to save on construction costs — completely destroyed one of OVH’s four datacenters at the site and heavily damaged another. OVH officials were quick to sound the alarm, with founder and chair Octave Klaba warning that it could take weeks for the firm to fully recover and urging clients to implement their own data recovery plans. Assuming they had them. Many did not. Scarcely protected data remains a significant problem for businesses of all stripes and sizes. In 2018, Riverbank IT Management in the U.K. found that 46% of SMEs (small and mid-size enterprises) had no plan in place for backup and recovery. Most companies (95%) failed to account for all of their data, on-premises and in the cloud, in whatever backup plans they did have. The results of such indiscretion are costly. According to Gartner, data-driven downtime costs the average company $300,000 per hour — that’s $5,600 every minute. The destruction at the OVH facility on the banks of the Rhine near the German border took down 3.6 million websites, from government agencies to financial institutions to computer gaming companies, many of which remain dark as of this writing. Affected complained on blogs and social media that years’ worth of data was lost for good in the OVH conflagration. The final financial tally will be staggering. “Not all data catastrophes are caused by a hoodie-wearing, Eastern European hacker,” said Kenneth R. van Wyk, president and principal consultant at KRvW Associates, a security consultancy and training company in Alexandria, Virginia. “Some are caused by the most mundane circumstances.” “Sure, we need to consider modern security threats like ransomware, [but] let’s never forget the power of a backhoe ripping through a fiber optic line feeding a business-critical datacenter.” “It’s about a mindset of always expecting the worst,” van Wyk said. “Security professionals look at systems and immediately ask ‘What could go wrong?’ Every business owner should do the same.” In this age of ubiquitous cloud migration and digital transformation, what can IT leadership do to gird the organization against hazards large and small? The answer lies within the realm of business continuity and disaster recovery (BCDR). This well-codified discipline in information security is a critical, but often missing, piece in enterprise risk management and mitigation. Most organizations understand the basic rules of engagement when it comes to BCDR, but security experts agree that execution often lacks rigor and commitment. “As a CIO, I’d immediately ask, ‘Have we truly tested our backups and recovery capability?'” said cloud security specialist Dave Shackleford, founder and principal consultant at Voodoo Security in Roswell, Georgia. “Whether cloud-based or not, too many organizations turn disaster recovery and business continuity planning and testing into ‘paper exercises’ without really ensuring they’re effective.” For organizations looking to protect key digital assets, what Shackleford deems an effective BCDR approach begins with a few time-tested best practices. Ask about redundancy and geographic resilience — and get it in writing. Losing two cloud datacenters will always result in disruption and downtime, even for a host like OVH with 300,000 servers in 14 facilities across Europe and 27 worldwide. But how painful and protracted that loss is will largely depend on the robustness of the hosting company’s own backup and fail-over protocols. The assurances, as spelled out in the service-level agreement (SLA), must also go beyond data processing and storage. A big part of Roubaix-based OVH’s troubles stemmed from the failure of backup power supplies that damaged its own custom-built servers — even in areas unaffected by the actual fire. Look for items in the SLA that address not only the service guarantee but also the eligibility for compensation and level of compensation offered. Offering “five-nines” availability is great, but the host should also demonstrate a commitment to diverse transit connections; multiple sources of power; redundant networking devices; and multiple, discrete storage assets on the backend. Holding your cloud host accountable is a solid start, but it’s important to remember that, as the OVH experience casts in stark relief, enterprise-grade cloud is not some mythical realm of infinite resources and eternal uptime. Moving important digital assets to the cloud means swapping your own infrastructure for that of another, for-profit vendor partner. The first requirement for cloud migration is to establish a framework for determining the wisdom and efficacy of making such a move to the cloud in the first place. Then there needs to be a comprehensive plan in place to protect everything the organization holds dear. “Inventory all your critical assets,” van Wyk suggests. “Ask how much it would cost you if any of them were unavailable, for any reason, for an hour, a day, a week. Ask how you would restore your business if everything in your inventory vaporized. What would the downtime be? Can you afford that? What is your Plan B?” The Cloud Security Alliance offers excellent guidance when preparing, analyzing, and justifying cloud projects with an eye toward risk, particularly with its Cloud Controls Matrix (CCM). If third-party hosting is warranted, it should be guided by formal policy that covers issues such as: Understand that failures are going to happen. Backup and recovery is so fundamental to the security triad of data confidentiality, integrity, and availability (CIA) that it enjoys its own domain in the NIST Cybersecurity Framework. NIST’s CSF encourages organizations to ensure that “recovery processes and procedures are executed and maintained to ensure timely restoration of systems or assets affected by cybersecurity incidents.” There’s a lot going on in that sentence, to be sure. Developing a robust approach to recovery that can satisfy NIST and withstand a catastrophic event like the OVH fire takes more than scheduling some automated backups and hoping for the best. Van Wyk said it’s a good idea to take extra precautions with your vital business data and processing and ensure you will actually be able to use your backup plans in different emergency scenarios. Whether organizations’ crown jewels live on-premises, in a hybrid environment, or solely in the cloud, a mature and pragmatic BCDR approach should include: No BCDR plan can ward off all chaos and guarantee perfect protection. But as the OVH incident demonstrates, half-hearted policies and incomplete protocols are about as effective as no plan at all. Establishing a solid BCDR posture requires meaningful investment in resources, time, and capital. The payoff comes when the lights flicker back on and rebooted systems go back online, data intact and none the worse for the experience."
https://venturebeat.com/2021/03/12/linearb-which-bring-contextual-metrics-to-software-development-project-management-raises-16m/,"LinearB, which brings contextual metrics to software development project management, raises $16M","LinearB, the company behind a “software delivery intelligence” platform for developers, has raised $16 million in a series A round of funding. The raise comes after “six months of hyper-growth” that has seen its customer base soar to more than 1,500 developer operations (DevOps) teams, including at unicorns such as BigID and Hippo Insurance. Founded in 2018, Los Angeles-based LinearB promises a fresh approach to software development project management and metrics, for example helping developer teams correlate project issues with data from their code, Git, projects, and more. LinearB promises to bring context to the metrics, helping engineering leaders understand what is happening across a team and how they can improve things. A key selling point is that LinearB is designed to work out of the box, with minimal manual configuration. Users initially sign up with their GitHub, GitLab, or Bitbucket accounts, after which they can connect their Jira or Clubhouse project management tools, and Slack to receive automated alerts. Then LinearB highlights all the important things in a single unified view, showing developer dependencies; work that is completed, pending, or in progress; and even work at risk of being delayed. Users can also dig down into the details within particular branches and pull requests, and LinearB automatically detects if an issue status is out of date and updates the user’s project management tool for them. For CTOs or engineering leads, LinearB provides a window into a team’s overall efficiency and “health,” ensuring there is alignment between the work they’re doing and the company’s business objectives. LinearB said a major factor in its recent growth has been its focus on asynchronous development and collaboration tools, which is particularly important as the world has transitioned to remote working over the past year. The company has now raised $20.6 million since its inception, with Battery Ventures leading its latest round. LinearB plans to continue offering a free version of its product to smaller developer teams, but it will also use funds to put “LinearB in the hands of the global enterprise community,” according to company cofounder and CEO Ori Keren. This builds on its enterprise plan, which costs $45 per developer per month. The expansion will involve adding more developer tool integrations to the mix, as well as pushing out additional workflow automation features. Other players in the space include Boston-based Jellyfish, which is a similar proposition and recently raised $31.5 million, although it doesn’t advertise its pricing and doesn’t appear to ship with a free tier. Alongside other sizable VC investments in the developer tool space — spanning universal code search, project management and issue tracking, and more — it’s clear that tools to help developers operate more efficiently are in big demand. After all, every company is now a software company."
https://venturebeat.com/2021/03/12/what-is-the-total-cost-of-owning-a-database/,What is the total cost of owning a database?,"For some products, figuring out the cost is easy: It’s the total figure on the invoice. For other products, like databases, that number is just the beginning. The total cost needs to include all of the other expenses that come after the initial purchase, such as training, maintenance, and curation. Each of these items adds up over time, making it tricky to calculate the true price. Open source options complicate this calculation. Databases such as MySQL and PostgreSQL are shared freely, and good developers can usually get them up and running without paying a cent. While many of the tools began as experiments or skunk works projects, they’ve grown to be feature-rich products that can handle many of the jobs for some of the most sophisticated enterprises. While open source projects don’t cost anything upfront, they require care, often more than proprietary software. Finding support is not always easy. The creators are giving their work away, and their attitude toward support can vary. Many want to help others, but some have lost interest and their attitude is simply, “I’ve given you a gift. Now go away.” Many of the features may have been added by a different contributor, which further complicates the question of who should provide support. For enterprises, the ability to revise and extend the code to add features they need is another cost. Taking the open source route is time-consuming — not just to install but to customize, revise, and extend. Some companies expect that their developers will actively contribute to open source tools as part of their job. This is often a fruitful collaboration, but their salaries should be considered part of the cost. Generally proprietary companies spend time ensuring that there are fewer rough edges. The oldest and more established databases tend to be proprietary platforms that expect users to pay a license fee to cover the cost of maintaining the software. Companies like Microsoft, Oracle, and IBM all market top-flight databases with a proprietary model. In the past, these prices were often paid upfront, but the companies have begun embracing a service model. All of these tools can be found, in some form or another, as services. While license fees were fairly straightforward, service fees vary depending on what is actually being used. Some providers bundle the database together with server time, while others charge by the amount of data that’s flowing in and out of the systems. Microsoft, for instance, lists a “license included” price for SQL Server in its Azure cloud environment. An SQL Server instance with two core and 10.2GB of RAM is $0.5044 per hour at the time of writing. Bigger machines with more power would cost more, but Microsoft offers discounts for long-term commitments. There is also a “serverless” price, which bills per transaction at $0.0001450 per second of virtual core time. This is a much simpler pricing model, as the cost of the hardware and license is buried. This serverless approach can help some companies compute the total cost of ownership because it is directly related to each event storing data. Good accountants can break down how many different events a customer might generate and then calculate how much it costs to store that customer’s data. This works well for businesses with a good model of customer behavior, but it may not be as useful for enterprises that can’t predict how much may be stored in the future. The proprietary databases also emphasize features for simplifying management. Oracle is marketing a version that is said to be “autonomous” because many of the jobs normally done by a database administrator can be handled by the code. “It’s the difference between doing everything manually,” explained Steve Zivanic, global VP of Database and Autonomous Services. “You had to do manual tuning of the databases. You had to do manual backups. You had to essentially make sure you had enough processing power. All of that is taken care of so you can focus on the business at hand.” In one recent study, Oracle suggested that the average users could reduce the size of database administrative staff by 68% using its tool. In recent years, many of the newest databases have been distributed freely as open source tools. Pioneers like MySQL, PostgreSQL, and SQLite proved that the open source model was a viable strategy for databases and spurred development of other types of databases. Some, like Cassandra, started out as internal projects by the companies — in this case, Facebook — and were later released publicly. Others, like MongoDB, Neo4J, and CouchDB, were created by companies specifically interested in supporting open source databases. Many of these companies paid their bills by charging for “support,” which included training and problem-solving sessions with engineers. Some companies purchased the support contracts because they needed the help, but others simply wanted to pay something to maintain the product. The “support” was really financial, and it flowed from the so-called customer to the development team. In these cases, figuring out the total cost of ownership just meant including the cost for such a support contract. The issue has grown more contentious lately, as cloud providers have started installing open source databases on running machines and then charging a premium for this bundle. The database software was still technically free and available under an open source license, but the price was justified because it included much of the work of configuring and curating the servers. These services have proven popular with users, in part because they simplify the process of estimating the total cost of ownership. Digital Ocean, for example, charges $15 per hour for a “managed database,” in essence, one of their standard instances that’s already configured to run MySQL, PostgreSQL, or Redis. Companies can spend less time on maintenance and get a more predictable estimate of what it costs to run a database. The teams developing the software may not be as happy. While some open source developers have recognized that non-paying users are part of the equation and allowing these free riders can be cheaper than maintaining a marketing team, others feel exploited. The highest-profile case arose recently when Elastic decided to stop releasing new versions under a license that allowed cloud companies like Amazon to sell managed machines. They felt the cloud companies weren’t contributing enough to support the developers. While many of the recently developed databases embraced an open source model, some new offerings are deliberately avoiding it. EraDB is a time-series database fully integrated with machine learning and statistical methods to drive complex analytics. CEO Todd Persen said the company had to choose between nurturing an extensive community of developers and concentrating on wowing a few paying customers. “What’s the value of putting source code out there?” he asked rhetorically. “We can still deliver compiled versions.” Most of the users never read the source code, and most of the commits come from inside the companies. Pricing for self-hosted versions is quoted, and the company also runs a cloud service that begins at 65 cents per gigabyte per month. The value of control is one of the trickier questions in the equation. Open source models offer not just their low price but also a deep partnership. The licenses are designed to give the power of ownership to all of the users. For some projects, this freedom can be valuable, especially for long projects. At first glance, proprietary tools rarely offer as much of a partnership. The company maintains control, and the customers are kept at arm’s length. But in general, good companies listen closely to good customers and reward them. The sales and relicensing process is a good feedback loop that delivers. And many of the bigger issues will arise regardless of the licensing model. After a while, some users will want changes — often small — while the owners may want to move in another direction. Perhaps they don’t want to support a small client, or maybe they want to devote resources to a different path for growth. The same friction over direction can appear with open source projects. Sometimes the creators aren’t interested in solving the issue or taking a particular path that a few users have chosen. Some creators are offering customers the option of paying directly for certain enhancements, an arrangement that is found in both open source and proprietary companies. Open source projects make this simpler because it’s always possible to do the work in-house or hire a third-party team. Proprietary companies are working to allay fears by offering source-code escrow agreements that provide some of the assurance of open source. If the company fails, the source code is released to the customers. There’s no guaranteed equation to calculate the costs for a database, but the new service models offer a reliable way to connect the size of the data and the number of transactions with the cost of storing it for a month. These prices, though, must support a profit for the company that runs the service. Teams with the right mix of talent that are willing to devote energy to building and supporting open source tools can often save dramatically. Much of the cost comes in paying the salaries and covering the overhead of these teams. Translating the numbers into a total cost of ownership is tricky, especially when some of the team members do more than just support the open source tools. But in the best cases, work on the open source software makes in-house development easier. This article is part of a series on enterprise database technology trends."
https://venturebeat.com/2021/03/12/yseop-now-tells-you-what-percentage-of-a-financial-report-can-be-generated-automatically/,Yseop now tells you what percentage of a financial report can be generated automatically,"French AI company Yseop has launched a new tool that instantly shows prospective clients what percentage of their financial reports can be automatically generated. Founded out of Paris in 2008, Yseop (pronounced “easy op”) applies natural language generation (NLG) to structured data to create “written narratives,” saving personnel from having to fully produce reports themselves. According to Yseop, financial analysts spend 48% of their time writing and updating reports, a figure the company said falls to 9% when using its platform. Yseop also claims some notable enterprise customers, including Oracle, BNP Paribas, and KBC Asset Management. Algorithms have been creeping into the journalistic and business reporting realm for some time. A company called Automated Insights powers sports coverage and earnings reports for outlets such as The Associated Press (AP), while Narrative Science offers something similar, with a specific focus on “data storytelling” for the enterprise. Elsewhere, Chinese tech titan Tencent has a robotic reporter called Dreamwriter, which publishes business and finance articles. Last year, a Chinese court ruled that an article produced by Dreamwriter was protected by copyright after another outlet reproduced the content without consent. Similar to Yseop, each of these companies’ respective technologies works best on structured data, where someone just plugs in the numbers and the AI constructs a coherent story around it. Put another way, not all documents or reports are suited to automation, which is something Yseop is looking to help customers figure out with ALIX, a tool that is available now for free. First up, the user uploads a PDF document containing an existing financial report from their business. ALIX then processes the report to understand the content and figure out what percentage it would be able to automate for the user. As a test, VentureBeat tried ALIX out with an English-language essay to see how it responded — it correctly assessed that the document was not data-driven. We then tried ALIX with two real financial reports to see how it assessed their suitability for automation. In the first case, the report was deemed to be mostly suitable for automation. A lower automation confidence score, as with the second report we tried, doesn’t necessarily mean that it’s a lost cause. The section breakdown might show that there are specific parts of the report that are very data-driven rather than pure non-data driven static text. This means a company could decide to run a few pages of a document through Yseop’s financial analyst and still derive some value. With ALIX, Yseop is trying to remove some of the friction for companies that are considering automating their report writing. It gives them a little insight into how much of their report could be automated, replacing manual processes with a self-serve technological approach. “This was a process which was quite manual,” Yseop CEO Emmanuel Walckenaer told VentureBeat during a virtual roundtable event yesterday. “So typically the customer would send sample reports to us. We would analyze them and say whether it was good or not good. What’s great with this tool is you get the results immediately. And you can actually test dozens of different reports, and the customers can do that on [their] own.” Some bigger companies may have broader concerns around using this type of technology. Would a billion-dollar public company really be comfortable uploading sensitive company financials to a third party’s cloud to process? Yseop’s privacy promise runs something like: Reports you upload are not stored and no information is saved. But some businesses might prefer specific technological safeguards (e.g. localized data processing) versus a trust-based ethos. “Yseop does not openly discuss security and policies, but trust is a core pillar for our organization, and we will not jeopardize customer trust by mishandling customer data,” Walckenaer said. “In addition, security of our applications and platform is a core competency, and we take all necessary measures to ensure customer data integrity is always preserved.” While ALIX is geared toward the financial sphere for now, there are plans to expand its scope to other industries it supports, including pharmaceutical and medical report writing. ALIX is available for anyone to use now."
https://venturebeat.com/2021/03/12/nosql-database-company-couchbase-confidentially-files-for-ipo/,NoSQL database company Couchbase confidentially files for IPO,"(Reuters) — Database software firm Couchbase has registered for a stock market debut that could come in the first half of this year and value it at as much as $3 billion, according to people familiar with the matter. The company has achieved more than $100 million in annual revenue, one of the sources said. The sources requested anonymity because the initial public offering (IPO) filing with the U.S. Securities and Exchange Commission is confidential and has not yet been made public. Couchbase declined to comment. Couchbase helps corporate customers such as Comcast and eBay manage databases on web and mobile applications through its NoSQL cloud database service. It has thrived as demand for data storage and processing has soared because of remote working during the COVID-19 pandemic. Founded in 2011, Couchbase has raised $294 million from investors thus far. It last raised $105 million at a valuation of $580 million in May 2020, according to PitchBook data. GPI Capital, North Bridge Venture Partners, and Accel are among its backers. The company had eyed an IPO back in 2016, after it raised $30 million. It said at the time it expected that to be its last round before going public. MongoDB, another database company and a competitor of Couchbase, went public in 2017 and now commands a $20 billion market capitalization. Snowflake, a cloud-based data-warehousing company, went public last year at a $33 billion valuation, the largest software IPO in history. The U.S. IPO market remains welcoming, with 62 operating companies listed so far this year, according to data provider Refinitiv."
https://venturebeat.com/2021/03/11/top-10-cybersecurity-lessons-learned-one-year-into-the-pandemic/,Top 10 cybersecurity lessons learned one year into the pandemic,"In 2020, chief information security officers (CISOs), chief information officers (CIOs), and their cybersecurity teams faced a digital pandemic of breaches, widespread supply chain attacks, and ingenious uses of human engineering to compromise enterprise systems. Bad actors were quick to capitalize on the chaos the COVID-19 pandemic created in order to compromise as many valuable enterprise systems as possible. The number of breaches soared as attackers targeted the millions of remote workers who didn’t have adequate security protection or sufficient training to be able to spot hacking and phishing attempts. The findings from PwC’s 2021 Global Digital Trust Insights: Cybersecurity Comes of Age study and the conversations VentureBeat has had with CISOs in the last year tell the same story: Enterprises are most concerned with protecting their cloud infrastructure from endpoint-based attacks. According to PwC’s 2021 Global Digital Trust Insights report, 96% of business and technology executives prioritized their cybersecurity investments due to COVID-19 and its impact on their organizations this year. The report is based on interviews with 3,249 business and technology executives worldwide, and half of the surveyed executives said cybersecurity and privacy were being included in every business decision and plan. In 2019, that figure was closer to 25%. While 64% of enterprise executives expect revenues to decline, 55% said their cybersecurity budgets will increase this year. To further accentuate how vital cybersecurity is to enterprises, 51% said they plan to add full-time cybersecurity staff this year. Gartner’s 2021 Boards of Director’s Survey and VentureBeat’s conversations with CISOs, CIOs, and their teams over the past three months also corroborate PwC’s claim that cybersecurity spending is going up and being fast-tracked even in enterprises that expect revenues to decline. Gartner’s survey also had the following to say: Enterprises had to reinvent themselves in record time to keep running and be digitally adept as offices closed, and stayed closed. As a result, enterprises are now seven years ahead of schedule on their digital transformation initiatives, according to McKinsey’s recent COVID-19 survey. Record ecommerce revenue results for 2020 reflect the success of that effort for many organizations. On the flip side, the fact there were many cybersecurity incidents — many still unsolved — reflect the failures of that effort. Bad actors’ abilities to home in on the cybersecurity gaps, in both systems and people, proved unerringly accurate in 2020. Of the many lessons learned in 2020, perhaps the most valuable is that the human element must come first. The following are the top 10 lessons learned one year into the pandemic, according to CISOs, CIOs, and their teams:"
https://venturebeat.com/2021/03/11/servicenow-adds-new-ai-and-low-code-development-features/,ServiceNow adds new AI and low-code development features,"Enterprise cloud-based solutions provider ServiceNow today launched its Now Platform Quebec release, which the company says is designed to help enterprises innovate more quickly in a world changed by the pandemic. Quebec brings several new AI and machine learning-powered and low-code capabilities, including a predictive AIOps feature that anticipates issues and automates resolution and a virtual agent that provides guided setup and topic recommendations. “In today’s challenging environment, organizations worldwide are pivoting fast, adopting new, distributed models of working and creating new workflow‑enabled ways of operating with more agile, resilient, digital enterprise value chains,” Chirantan Desai, chief product officer at ServiceNow, said in a blog post. “Customers are relying on ServiceNow’s Now Platform to deliver enterprise digital workflows, create new business models, enhance productivity and enable great customer and employee experiences in any operating environment. This newest version of the Now Platform further enhances the must‑have enterprise digital tools customers need today.” Among the highlights in Quebec are ITOM Predictive AIOps, which builds on ServiceNow’s Loom acquisition in January 2020. TOM Predictive AIOps aims to give users deeper insights into their digital operations to minimize and fix issues before they become real problems. AIOps, short for AI for IT operations, is a category of products that enhance IT by leveraging AI to analyze data from tools and devices. Research and Markets anticipates it will grow by $14.3 billion to be a $20.1 billion market by 2027. That might be a conservative projection in light of the pandemic, which is forcing IT teams to increasingly conduct their work remotely. In lieu of access to infrastructure, AIOps solutions could help prevent major outages, the cost of which a study from Aberdeen Research pegged at $260,000 per hour. Quebec also introduces AI Search, underpinned by technology acquired in ServiceNow’s purchase of Attivio. AI Search delivers intelligent search results and actionable information, complementing Quebec’s Engagement Messenger that extends self-service to third-party portals to enable AI search, knowledge management, and case interactions. Also new in Quebec is the aforementioned virtual agent, which delivers AI-powered conversational experiences for IT incident resolution. ServiceNow also today unveiled Creator Workflows, a set of low-code development tools to transform manual processes into digital workflows. App Engine Studio offers a development environment where users can collaborate and build applications, while App Engine Templates gives teams access to prebuilt workflow building blocks. “As businesses shift from emergency response to long‑term recovery and distributed work becomes the norm, organizations are accelerating digital transformation efforts and investing in new technologies that promote continuity and agility,” Philip Carter, group VP at IDC, said in a statement. “The ability to deliver end‑to‑end digital experiences for employees and customers alike will be a crucial competitive differentiator. There is significant customer traction, accelerated by the pandemic, for unified technology platforms that connect systems, silos and processes to enable these connected, digital‑first enterprise models.” Now Platform Quebec is now available to ServiceNow customers. Customers using it include Nike, Adobe, Deutsche Telekom, Logitech, Medtronic, and St. Jude Children’s Research Hospital."
https://venturebeat.com/2021/03/11/how-smart-banking-offers-resilience-to-companies-in-the-new-normal/,How smart banking offers resilience to companies in the ‘new normal’,"Presented by Central Bank Even as vaccines roll out across the country, the pandemic-related challenges faced by small businesses remain. In less than a year, businesses in nearly every industry have found themselves facing an accelerated digital transformation, disruption in demand, and supply chain issues. Nearly 100,000 small businesses across the nation have shut down permanently since the start of the pandemic, according to a recent Yelp report. But not all hope is lost. For entrepreneurs who’ve managed to maintain operations and successfully make the transition to this “new normal,” one key resource during these difficult times continues to be a strong relationship with a knowledgeable and caring community bank. In a make-it-or-break-it period like we’re in now, working with a top regional bank can mean access to both the essential tools and personalized strategies required to persevere. One such institution, Central Bank, is the leader in every market they serve and fourth Best Bank in America overall according to Forbes. Here’s how this modern breed of community bank acts as a valuable resource to small business owners facing big challenges.  Regional Banks like Central Bank worked with small businesses across all industries to secure tens of millions in PPP loans to help weather the pandemic. Though it certainly hasn’t been easy, millions of small businesses have accessed loans under the “Paycheck Protection Program” by working with local regional banks. The forgivable, low-interest loans became a lifeline to many businesses who may not have made it through the last year without them. Central Bank, for example, helped free up tens of millions of dollars in PPP loans for small businesses across all industries in their area, making it possible for those businesses to stay afloat. A regulatory change under the new administration may make it easier for sole proprietors to qualify for PPP funding, so they should consider checking in with their lender even if they weren’t able to get a loan in the first round, says Eric Groves, co-founder and CEO of small business referral network Alignable. “Get your paperwork in order, all of your tax forms and other information,” he says. “Then reach out to your lending officer to let them know that you want to be in a position to sit down and go through it all when the changes go through.” Beyond just helping secure PPP loans for customers, community banks have been essential in providing traditional lending throughout the pandemic, too. Central Bank has the ability to cater loans to the unique needs of small businesses, helping entrepreneurs with access to capital for everything from equipment purchases to real estate transactions as they pivoted their business models to better position themselves in a constantly changing market. Staying nimble and being able to accept customer payments of all types is key to adapting to the “new normal.” The pandemic pushed nearly every business further along in their digital transformation, as consumers became comfortable with everything from virtual doctor’s appointments to online exercise classes. In addition to getting their operations online quickly, businesses needed to offer quick and easy omnichannel payment options for customers, and to ensure the cyber security of any transactions taking place on their site. Products from Central Bank allow small businesses to accept customer payments easily online — and quickly get access to those funds. This allows business owners to focus on other elements of their digital transformation, such as their online storefront and social media marketing. Working with a community bank that views your business as a partner, not a customer, will help your team get back to work faster and more efficiently. To deal with the upheaval brought on by Covid — including store closures, stay-at-home orders, and supply chain failures — small business owners have had less bandwidth for core activities like operations and sales and marketing, though they still rank the latter tasks as the most important to the business, according to a recent Deloitte study. By working with their bank to automate cash management and payroll systems, business owners were able to spend more time dealing with operations and customer service. Central Bank representatives can help business owners come up with customized cash-management strategies, tailored to the pain points of each individual business. Today’s digital banking products allow business owners to not only automate the day-to-day money tasks, but to also easily log in via any device — and from anywhere — to get a snapshot of their business’s finances or to quickly pay bills, transfer funds, or view other activity. “Small businesses can benefit from anything that helps their business become more efficient right now and allows them to compete in a more effective way,” says Tim T. Mercer, entrepreneur and author of Bootstrapped Millionaire: Defying the Odds of Business. “They may not need to have all the human resources to help with those tasks if there is technology that can do it for them.” With small businesses facing unprecedented challenges even as the pandemic begins to subside, they need every advantage that they can get. Finding a bank that acts as a true partner can serve as a competitive advantage as they find the way forward. Sponsored articles are content produced by a company that is either paying for the post or has a business relationship with VentureBeat, and they’re always clearly marked. Content produced by our editorial team is never influenced by advertisers or sponsors in any way. For more information, contact sales@venturebeat.com. "
https://venturebeat.com/2021/03/11/fine-art-america-is-wall-art-your-way-here-are-four-options-for-making-your-home-office-pop/,Fine Art America is wall art your way. Here are 4 options for making your home office pop.,"Man, it’s been a long, long, long pandemic. And, while the world seems optimistically poised toward a mostly post-COVID reality in the coming months, the effects of what’s happened over the past year will extend far beyond 2020 and 2021. Though many companies are slowly starting to welcome workers back to their old stomping grounds at the office, many millions more are still working from home. And in many cases, that will be the new normal indefinitely. Considering that home office might be your long-term work home going forward, maybe it’s time to treat it like your long-term work home. And c’mon…who wants to be looking at those same blank, boring off-white walls for the rest of their professional life? Whether you’ve got a home office that’s exceedingly drab or just needs an artistically minded shot in the arm, Fine Art America can help. For the past 15 years, they’ve become a curated home allowing independent artists and photographers to sell their work directly to appreciative customers without all the middlemen. Once artists upload their work to Fine Art America, customers can have that work emblazoned across literally hundreds of different items, including everything from tote bags and jigsaw puzzles to mugs and t-shirts. But, of course, nothing garners more attention than a lovingly crafted piece of museum-quality wall art, literally hanging the owner’s personal tastes and attitudes on the wall as a piece of the decor. For the thousands of brilliant artworks and images in the Fine Art America library, buyers can also choose from a variety of formats for their chosen piece, from canvas or metal to acrylic, framed posters or even wood prints. While you can browse the vast Fine Art America catalog for the piece that screams you, there’s one thing that’s certain: there’s a whole host of options available for literally every type of art lover there is. If you’ve always wanted to own a piece of iconic art, but didn’t have the hundreds of millions it would take to buy the work of a master, Fine Art America has dozens of classic pieces to help turn an office into an art gallery. You’ll never own the original of Edward Hopper’s 1942 historic ode to urban loneliness Nighthawks, but through FAA, you can hang a Hopper in your space and stare at it in wonder all day long. Thousands of brilliant images can become full sized wall hangings, from gorgeous landscapes to your favorite celebrities. If there’s anything that instantly classifies you as an iconoclast more than a framed print of sunglass-shrouded legendary actor, Steve McQueen, nonchalantly pointing a gun from his seat on the couch, we couldn’t find it. If you’re looking for something that actually requires a viewer to stop and inspect the work for a moment, here’s one that definitely commands attention. Anybody can display a world map. But how many people can hang a world map in the style of an underground subway system with all the world’s biggest cities represented as travel stations? Answer: not many. As you toil away at your daily work projects, it’s always heartening to remember why you’re working so hard in the first place. Instead of photos of the family in tiny frames on your desk, you can instead become the artist yourself, turning a photo of loved ones into a full sized painting. And this ain’t Photoshop. You can upload your image and a master portrait artist will digitally recreate your photo with hand-painted brush strokes. Check out Fine Art America and bring some beauty into your world. VentureBeat Deals is a partnership between VentureBeat and StackCommerce. This post does not constitute editorial endorsement. If you have any questions about the products you see here or previous purchases, please contact StackCommerce support here. Prices subject to change."
https://venturebeat.com/2021/03/11/state-of-the-edge-report-projects-edge-computing-will-reach-800b-by-2028/,State of the Edge report projects edge computing will reach $800B by 2028,"A battle for control over edge computing environments is expected to drive a total of $800 billion in spending through 2028, according to a report published today by the LF Edge arm of the Linux Foundation. The State of the Edge report is based on analysis of the potential growth of edge infrastructure from the bottom up across multiple sectors modeled by Tolaga Research. The forecast evaluates 43 use cases spanning 11 vertical industries. The one thing these use cases have in common is a growing need to process and analyze data at the point where it is being created and consumed. Historically, IT organizations have deployed applications that process data in batch mode overnight. As organizations embrace digital business transformation initiatives, it’s becoming more apparent that data needs to be processed and analyzed at the edge in near real time. Of course, there are multiple classes of edge computing platforms, ranging from smartphones and internet of things (IoT) gateways to complete hyperconverged infrastructure (HCI) platforms that are being employed to process data at scale at the edge of a telecommunications network. Regardless of the type of platform, it’s now possible to process data at the edge in under a millisecond, said Matt Trifiro, co-chair of the State of the Edge initiative at the Linux Foundation and chief marketing officer for Vapor IO. Much of that data, however, isn’t being generated by humans but applications that employ machine and deep learning algorithms. “It’s not just about humans consuming data; it’s also about machines that now consume data,” Trifiro said. Rather than shipping massive amounts of raw data over a wide area network back to a cloud service or on-premises IT platform, it then becomes possible to only transfer the aggregated results of data processed at the edge. Thanks to the rise of open source software, the cost of processing data at the edge is rapidly declining, noted Jacob Smith, co-chair of the State of the Edge initiative at the Linux Foundation and VP of bare metal at Equinix. “You always want to bring the code to the data,” Smith said. IT vendors are of all sizes are positioning themselves to provide those capabilities. Microsoft at its Ignite 2021 conference last week touted Microsoft Arc, a control plane through which it will manage instances of Kubernetes clusters deployed at the edge. The latest update to the platform makes it simpler to manage AI workloads deployed on those clusters. At the same time, IBM announced that its Cloud Satellite managed service is now generally available. It also provides a control plane that manages Kubernetes clusters deployed at the network edge. The goal is to provide this capability in a way that enables IT teams to focus on building and deploying applications while IBM manages the underlying IT infrastructure on their behalf, said Jason McGee, IBM VP and CTO for IBM Cloud. Like Microsoft, IBM is making it possible to deploy AI models at the edge using its IBM Watson Anywhere software that can be deployed as a set of containers running on an instance of Kubernetes. The major difference is IBM Cloud Satellite is designed to be deployed across multiple backend cloud services that most organizations will find themselves employing, McGee said, adding, “it’s going to be a blend.” Smaller rivals with similar ambitions include Platform9, which has built a software-as-a-service (SaaS) platform that provides a less costly alternative to managing Kubernetes clusters at the edge that will appeal more to developers, company CEO Sirish Raghuram said. “Platform decisions are now being led by developers,” he said. In contrast, platforms from larger rivals need to be managed by a small army of consultants, Raghuram added. There is almost no IT vendor, including Amazon Web Services (AWS), Google, Dell Technologies, and Hewlett-Packard Enterprise (HPE), that has not signaled an intention to provide multiple edge computing platforms in one form or another. The challenge facing IT organizations now is determining how best to go about remotely managing all those edge computing platforms that may soon wind up running more application workloads than are currently deployed on public clouds."
https://venturebeat.com/2021/03/11/high-alpha-capital-raises-110-million-venture-fund/,High Alpha Capital Raises $110 Million Venture Fund," The firm’s third fund will be focused on funding early-stage enterprise cloud companies  INDIANAPOLIS–(BUSINESS WIRE)–March 11, 2021– High Alpha Capital, one of the largest software venture firms in the Midwest, announced today the closing of their new $110 million fund to fuel the next generation of enterprise cloud companies. With this new fund, High Alpha will continue to invest in Pre-Seed, Seed and Series A rounds across all geographies. Led by experienced entrepreneurs Scott Dorsey, Kristian Andersen, Eric Tobias and Mike Fitzgerald, High Alpha Capital has raised over $215M across three funds. Previous investments include companies such as Attentive, SalesLoft, Zylo, Terminus, The Mom Project, Lessonly, LogicGate, MetaCX, Socio and many more. “High Alpha Capital III provides us with an incredible opportunity to further our mission of supporting early-stage software entrepreneurs who are shaping the future through technology. We pride ourselves on bringing entrepreneurial empathy and an operator’s mindset to each portfolio company that our team is privileged to support,” said High Alpha Managing Partner Scott Dorsey. In the first investment from High Alpha Capital III, the firm led an $8M Series A in Chicago-based Rheaply, an asset management platform built for the circular economy. CEO Garry Cooper has received many accolades including recently winning the Rise of the Rest Virtual Tour: Equity Edition competition and being named to the Forbes Next 1000. This announcement builds on recent momentum across the High Alpha portfolio, including recent funding rounds for SalesLoft and Attentive that resulted in $1B+ valuations for each company. High Alpha also recently completed a new 42,000 square foot office and community space in Indianapolis’s Bottleworks District, which they hope to become a primary hub for entrepreneurship in the Midwest. The new venture fund is affiliated with High Alpha Studio, which co-founds and launches new software companies. The venture studio launched an all-time high of 10 new companies in 2020 and has started 28 companies since its founding in 2015. Recent company launches include Mandolin, Bolster, Filo, Trava, Shaker, and Luma. About High Alpha High Alpha, based in Indianapolis, Indiana, is a leading venture studio that conceives, launches and scales next-generation B2B SaaS companies. The High Alpha portfolio includes leading cloud companies Attentive, SalesLoft, Zylo, Terminus, The Mom Project, Lessonly, LogicGate, MetaCX, Socio and more. For more information visit highalpha.com or follow on Twitter @highalpha.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210311005616/en/ Media Contact Drew Beechlerdrew@highalpha.com 317-777-6764"
https://venturebeat.com/2021/03/11/ibms-ai-may-lead-to-new-antimicrobials-drugs-and-materials/,"IBM’s AI may lead to new antimicrobials, drugs, and materials","In a new study published in the journal Nature Biomedical Engineering, researchers at IBM say they’ve developed an AI model that can assist in the rapid design of antimicrobial peptides — the building blocks of proteins. The researchers say that the model outperforms other AI methods at designing such peptides and increases the success rate of identifying a viable candidate by 10%. Antibiotics have transformed the world of medicine over the past century or so, but they’ve also been overused, leading to the emergence of bacteria with powerful resistance. According to the Centers for Disease Control and Prevention (CDC), antibiotic resistance is one of the biggest public health challenges of our time. In fact, in the U.S. alone, nearly 3 million people die annually as a result of antibiotic-resistant infections. Unfortunately, few new antibiotics are being developed to replace those that no longer work, in part because drug design is an extremely difficult, lengthy, and capital-intensive process. IBM’s proposed solution is generative modeling, a subfield of AI that allows researchers to decide upfront what characteristics they want peptides to have versus guessing combinations. Historically, material design of molecules, proteins, and altogether new peptides has been a complex simulation problem. Even small molecules made of only a few atoms have hundreds of possible combinations. To combat this, IBM’s AI model pulls from a large dataset to reverse-engineer a peptide’s design and produce the desired peptide framework. Effectively, it shortens the time needed to create high-quality peptide candidates from years to potentially days while increasing the likelihood of identifying successful candidates to fight antibiotic drug resistance. Within 48 days, IBM says its AI-boosted molecular design approach enabled it to identify, synthesize, and experimentally test 20 AI-generated novel candidate antimicrobial peptides. Two of them turned out to be potent against pathogens, very unlikely to trigger drug resistance in E. coli, and had low toxicity when tested both in vitro and in mice. Beyond antibiotics, IBM says the generative AI system could potentially accelerate the design process of molecules for new drugs and materials. “Our proposed approach could potentially lead to faster and more efficient discovery of potent and selective broad-spectrum antimicrobials to keep antibiotic-resistant bacteria at bay — for good,” IBM’s Saska Mojsilovic and Payel Das wrote in a blog post. “And we hope that our AI could also be used to help address the world’s other most difficult discovery challenges, such as designing new therapeutics, environmentally friendly and sustainable photoresists, new catalysts for more efficient carbon capture, and so much more.” IBM’s latest work builds on an earlier study published in the journal Advanced Science by the company’s researchers. It demonstrated a technique that enabled the coauthors to create up to 100 bacteria-fighting polymers in nine minutes, using AI and machine learning."
https://venturebeat.com/2021/03/11/pitchpoint-solutions-inc-accelerates-growth-with-4-5-million-financing-from-cibc-innovation-banking/,"Pitchpoint Solutions, Inc. Accelerates Growth with $4.5 Million Financing from CIBC Innovation Banking","TORONTO–(BUSINESS WIRE)–March 11, 2021– CIBC Innovation Banking is pleased to announce a $4.5M growth capital facility for Toronto-based Pitchpoint Solutions, Inc. (“Pitchpoint”), a leading provider of fraud prevention services across the US for multiple industries, including mortgage, background and tenant screening, and anti-money laundering. Pitchpoint provides a comprehensive suite of services that enables lenders to detect and mitigate fraud risk early, and ensures compliance with investor guidelines, providing business process compliance across the organization. Pitchpoint enables creditors to instantly validate loan application data, reducing the risk of approving a fraudulent loan application, and ensuring compliance with mandatory fraud prevention regulations. “Pitchpoint has a very strong competitive positioning in the US fraud verification market,” said Paul McKinlay, Managing Director in CIBC Innovation Banking’s Toronto office. “We are excited to work with the company and support their growth.” “We are committed to providing a product that continues to evolve and support creditors of all types across the US,” said Stephen Schrump, Pitchpoint’s Chief Executive Officer. “We look forward to working with CIBC Innovation Banking as we continue to grow our business.” About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About Pitchpoint PitchPoint is a leading provider of data and analytics solutions that empower clients to make smarter and more efficient decisions through data-driven insights. PitchPoint is committed to being a premier solution provider that clients rely on for their strategic goals, superior service to their customers, and overall success by delivering best-in-class software, services, and solutions with a relentless commitment to excellence, innovation, and leadership.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210311005015/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609"
https://venturebeat.com/2021/03/11/hugging-face-triples-investment-in-open-source-machine-learning-models/,Hugging Face triples investment in open source machine learning models,"Hugging Face launched in 2016 with a chatbot app designed to be your “AI friend.” Now the NLP company has more than 100,000 community members and is planning to triple its efforts and expand beyond language models into fields like computer vision. Developers have used a hub on Hugging Face to share thousands of models, and CEO and cofounder Clement Delangue told VentureBeat Hugging Face wants to become to machine learning what GitHub is to software engineering. As part of that effort, Hugging Face closed a $40 million series B funding round today. The round was led by Addition, with participation from Lux Capital, A.Capital, and Betaworks. Notable individual investors in the round include MongoDB CEO Dev Ittycheria, NBA star Kevin Durant, Dataiku CEO Florian Douetteau, and former Salesforce chief scientist Richard Socher. Delangue said Hugging Face believes transfer learning is critical to the future of machine learning. As evidence of this trend, Delangue points to an AI research paper published earlier this week by researchers from Google Brain, Facebook AI Research, and UC Berkeley about pretrained language models working with numerical computation, vision, and protein fold prediction. This and other recent advances, he said, signify that “transfer learning models are starting to eat the whole field of machine learning.” “Everything transfer learning-based we believe is here to stay and is going to transform machine learning for the next five years,” he told VentureBeat. “We’ve seen that they completely changed the NLP field, and they’re starting to change the computer vision fields, like with vision transformers and the speech-to-text fields. Ultimately, we think transfer learning is going to power machine learning, and hopefully we’re going to be able to power all these transfer learning models.” Hugging Face has also published AI research. A paper about the Transformers NLP library that’s seen more than 10 million Python pip installs and been used by a number of businesses — including Microsoft’s Bing and MongoDB — received the Best Demo paper award at the EMNLP research conference last year. In addition to tripling efforts to grow an open source community for the development of language models, Delangue said the funds will help ensure Hugging Face has the resources to act as a “counter-power” to major cloud AI services being sold to enterprise customers. NLP is an area of interest for a number of companies hoping to sell AI services to enterprise customers, including Databricks, which raised $1 billion last month and plans to focus on acquiring NLP startups. “I think one of the big challenges that you have in machine learning, it seems these days, is that most of the power is concentrated in the hands of a couple of big organizations,” he said. “We’ve always had acquisition interests from Big Tech and others, but we believe it’s good to have independent companies — that’s what we’re trying to do.” Democratization, Delangue said, will be key to assuring the benefits of AI extend to smaller organizations. Hugging Face CTO Julien Chaumond echoed that thought. In a statement shared with VentureBeat, he said democratization of AI will be one of the biggest achievements for society and that no single company, not even a Big Tech business, can do it alone. Hugging Face began monetizing ways to help businesses create custom models six months ago, and now it works with over 100 companies, including Bloomberg and Qualcomm. A Hugging Face spokesperson told VentureBeat the company has been cash-positive in the first months of 2021. “You can start seeing that companies are really going to have dozens of what we call machine learning features or NLP features,” he said. “It’s not going to be like one big feature, but they’re going to have a lot of different NLP features that are going to be really deeply embedded into their products or their workflow in multiple different ways.” In other recent Hugging Face news, Hugging Face extended into machine translation last year and in recent weeks launched subcommunities for people working with low-resource languages to create language models. Hugging Face raised $15 million in a 2019 series A funding round and has raised a total of $60 million to date. In 2017, Hugging Face was part of the Voicecamp startup accelerator hosted by Betaworks in New York City. Hugging Face currently has 30 employees, with offices in New York and Paris."
https://venturebeat.com/2021/03/11/merchant-data-platform-woflow-emerges-from-stealth-with-3-5m/,Merchant data platform Woflow emerges from stealth with $3.5M,"Woflow, a San Francisco-based data infrastructure company offering a standardized management platform for enterprises’ merchant data operations, is helping streamline the food delivery ecosystem during the pandemic. The company today announced its emergence from stealth with $3.5 million in funding from Craft Ventures and Base10 Partners, which it plans to put toward R&D, including scaling its product for new online ordering verticals and further developing its data science teams. IT teams are struggling to make sense of unstructured data, which is a problem for vendors in the food tech industry. Restaurants, for example, need their menu data translated to delivery applications with speed and accuracy, particularly to keep up with pandemic-driven demand. Woflow claims its system gets vendors onto end user enterprise platforms like Doordash within 24 hours instead of the traditional two weeks. In an interview with VentureBeat, Woflow cofounder Will Bewley explained that the company is focused on structuring unstructured data. “Something like 90% of the world’s data is unstructured, and if it’s unstructured, it’s unusable in terms of powering business decisions or facilitating transitions,” Bewley said. “And so we’re helping drive this digital shift by providing that data infrastructure.” Bewley added that Woflow is a data infrastructure product that — like Stripe — can process lines of code that have been input and use them to power online processing. Companies like DoorDash and SnackPass are using Woflow’s technology to digitize and maintain their vendors’ menus. Woflow’s infrastructure can reduce the heavily manual processes these delivery platforms have historically used to onboard and maintain that data, allowing them to better scale and reduce related operational costs. Woflow went through a lengthy process of development to reach this point. The company’s journey began with a SaaS tool that morphed into the more comprehensive data infrastructure platform it now offers. Cofounder Jordan Nemrow explained that the company has been automating small pieces of its product over the past year to maximize its data operations. The platform now runs on Rails and GraphQL, with some support from Node, and uses React on its front end. Woflow uses PyTorch for its AI and ML services, which are all hosted on AWS. Digitalizing complex processes and trying to produce them at scale is an enormous challenge, Nemrow said. “And so that is really where we do shine.” He added that Woflow is constantly working behind the scenes. “So when you’re on Doordash and you’re ordering a pizza, or if you’re ordering avocado toast from a foodie shop, it doesn’t matter which menu you’re ordering from, that data is likely being built within our product,” Nemrow said. Woflow was founded in 2017 and currently has 12 employees."
https://venturebeat.com/2021/03/11/your-computer-is-on-fire-draws-on-tech-history-to-critique-ai-and-the-cloud/,‘Your Computer Is On Fire’ draws on tech history to critique AI and the cloud,"In a story last year about nine books I read about AI in 2020, I called Your Computer Is On Fire a book worth watching out for this year. It’s released this week, and I was not disappointed. The premise of the book is that techno-utopianism should die because it’s too dangerous to be allowed to continue. This argument came up recently in the context of Amazon workers in factories with robotics getting hurt more often than workers in factories without robots. But once people throw away unrealistic visions of outcomes, the history of technology looks very different. The book attempts to interrogate how the legacy of social constructs and media narratives have shaped computing. It invites people to think critically about notions of purity surrounding data, the concealment of the carbon footprint the cloud represents, the whiteness of robots, and the wires and resources involved with making the world wireless. Your computer is on fire in part, authors argue, because of automation that perpetuates racism and sexism, and the growth of resource-intensive datacenters and the cloud at a time when climate change is an existential threat for the planet. The title of this book is meant to prepare you for a series of 16 provocative essays that consider the history of technology, media, and policy, from Siri disciplines and the cloud as a factory to how the internet will be decolonialized and tech for the Global South. Each essay takes readers on a journey through a topic to consider the ethical and societal implications of technology over the long term, an approach former Ethical AI team lead Margaret Mitchell suggested for Google. Contributors to the collection of essays include Safiya Noble, author of Algorithms of Oppression, who wrote an essay about race and gender stereotypes that permeate robotics and the role of robotics in policing, prisons, and warfare. “We have to ask what is lost, who is harmed, and what should be forgotten with the embrace of artificial intelligence and robotics in decision-making. We have a significant opportunity to transform the consciousness embedded in artificial intelligence and robotics, since it is in fact a product of our own collective creation,” Noble wrote in the book. Another essay, by Nathan Ensmenger, argues that the cloud is a factory, and it examines the extent to which datacenters demand a lot of energy, water, and the mining of rare mineral resources like cobalt, which has led to accusations that Big Tech companies aided in the death or serious injury of children. That essay also walks through a comparison between Amazon online today and Sears mail-order catalogs a century ago, and compares Amazon transportation and distribution strategy to Standard Oil. Understanding, for example, that in the past women made up much of computation work treated as menial and feminine for most of its early history helps illuminate ongoing problems of racism and sexism in tech environments that women — especially Black women — describe as toxic. I also found something terribly human in an essay arguing that a network is not a network, which looks at the history of large networks built in Chile, Russia, and the United States. Benjamin Peters says that history shows that just because a network works does not mean it works as its designers intended. “[N]etwork projects are twice political for how they, first, surprise and betray their designers, and, second, require actual institution building and collaborative realities far richer than any design,” Peters wrote. Editors of the book include Mar Hicks, a tech historian at the Illinois Institute of Technology in Chicago and an associate editor of the IEEE Annals of History of Computing. They are joined by science and technology historian and University of California, Irvine professor Kavita Philip; Peters, a media historian and University of Tulsa professor; and Stanford University history professor Thomas Mullaney. The editors take pains to state that the book’s conclusions aren’t meant to be an overly dark view of the future or to give people the impression things are hopeless. There is hope, they argue, but recent trends should act as an alarm. What I also took away from this book is the continuing value of critical analysis. In a recent paper, researchers recommended reporters persist in sharp questioning, declaring, “Technology journalism is a keystone of equitable automation and needs to be fostered for AI.” In the final pages of the book, Your Computer Is On Fire also addresses the role of media and the writers of narratives in tech and AI trends. “Tech will deliver on neither its promises nor its curses, and tech observers should avoid both utopian dreamers and dystopian catastrophists. The world truly is on fire, but that is no reason it will either be cleansed or ravaged in the precise day and hour that self-proclaimed prophets of profit and doom predict. The flow of history will continue to surprise,” Peters writes. Even if you’re like me and follow trends in artificial intelligence through news, books, and research papers, you may still learn parts about the history of technology in this book that you didn’t know, because this book extends across an arch of history. And as editors lay out in the afterword, they hope the messages contained within will be viewed as obvious decades from now. This lens — viewing computing and artificial intelligence across the span of decades — and consideration of social and historical context was previously espoused by Ruha Benjamin, who last year argued in the context of deep learning that “computational depth without historic or sociological depth is superficial learning.” But the collection of impactful tech issues interrogated over the span of decades in this book makes it recommended reading for anyone interested in the impact of tech policy in businesses and governments, as well as people deploying AI or interested in the way people shape technology. This book presents compelling arguments for essential topics at the center of business and society. By using computational history as a foundation, it’s able to, as Noble put it, “underscore how much is at stake when we fail to think more humanistically about computing.”"
https://venturebeat.com/2021/03/11/valor-backs-genomics-saas-platform-allelica/,Valor Backs Genomics SaaS Platform Allelica," Allelica’s platform is a giant step forward in the delivery of precision medicine for all  ATLANTA–(BUSINESS WIRE)–March 11, 2021– Valor Ventures Fund 2 is pleased to announce its seventh investment, a $1.75 million Series Seed financing of Allelica, Inc. Valor General Partner Gary Peat joins the Allelica board of directors. In addition to lead investor Valor Ventures, participants in the round included AI-focused venture fund Pi Campus and prestigious genetics research institution Medical Genetics Center (MGZ) in Munich as a strategic investor. Says Peat, “Allelica brings precision medicine into mainstream clinical practice through a Software-as-a-Service platform for the prediction of complex, genetic diseases using proprietary polygenic risk scores. With major customers across three continents, Allelica is poised to be the first mass adoption of precision medicine. For example, in cardiology, Allelica has partnered with a leader in therapies for Coronary Artery Disease, Merck, to develop data-driven cardiovascular disease-risk stratification tools based upon Allelica’s polygenic risk scores for Coronary Artery Disease.” Allelica’s co-founders include two of the world’s leading genomic bioinformaticians and Polygenic Risk Score (PRS) scientists, CEO Giordano Bottà, Ph.D. and Chief Science Officer, George Busby, Ph.D., plus data scientist Paolo Di Domenico, Ph.D. Their fundamental research contributions helped develop the algorithms and AI that make genomic analysis possible at scale. With this round, Allelica has hired a Baltimore-based sales leader, Edgar Carter, as VP of Sales. Bottà says, “The Valor team brings decades of experience investing at seed stage in healthcare information technology and informatics in Allelica’s largest market, North America.” About Allelica Allelica uses genomic data to stratify individuals with different trajectories of disease risk. This offers the potential to complement and improve conventional risk prediction and clinical practices in specialties and subspecialties such as fertility, cardiology, oncology, pediatrics, neurology, and more. Allelica’s most recent publication in the American Heart Association’s prestigious journal Circulation this month highlights Allelica’s PRS for coronary artery disease. Through the incorporation of genetics into disease risk assessments, Allelica is improving preventative clinical medicine by helping to identify those who benefit the most from early intervention and behavior modification to reduce lifetime risk. More info: www.allelica.com. About Valor Ventures Founded in 2015 to create premium venture capital returns with Valor’s Inclusion Premium strategy, Valor Ventures backs experienced-based founders solving big problems with software at the first professional round. We focus in the fastest-growing, largest region in the US, the Southeast, which features 40% of the U.S. population and the greatest density of under-represented founders. Our mission is making inclusion the new normal in venture capital because diverse teams and boards produce premium returns. Our platform includes Startup Runway, the largest pitch event for underrepresented founders in the country since 2015; the Atlanta Startup Podcast, and Atlanta Tech Park, the 45,000 sqft innovation center dedicated to accelerating local tech innovation globally with more than 50 startup and early-stage companies in residence. More info: valor.vc.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210311005574/en/ PR CONTACTCarla Etheridge(727) 385-8910carla@Write2Market.com"
https://venturebeat.com/2021/03/11/hpe-adds-modular-greenlake-services-for-mid-market-businesses/,HPE adds modular GreenLake services for mid-market businesses,"Hewlett-Packard Enterprise (HPE) this week announced it is offering additional modular HPE GreenLake services at a lower cost of entry for customers with smaller servers. HPE also announced a forthcoming bare-metal server service. Starting at 100 virtual machines, the entry-level GreenLake service will become available in the first half of this year, while the bare-metal service is scheduled for the second half of 2021. In addition, HPE has tightened integration with the wireless and wired networking infrastructure from its Aruba subsidiary. The goal is to make an edition of HPE GreenLake available for remote managing networks based on switches and access points provided by Aruna. Finally, HPE announced it has added backup and recovery services to HPE GreenLake and expanded its partnerships with CyrusOne and Equinix to make HPE GreenLake available via co-location hosting providers. The partnerships make it possible to deploy the control plane for the HPE GreenLake service closer to where HPE server and storage infrastructure is deployed. HPE reports it has generated more than $4.5 billion in total contract value from more than 1,000 customers. In the first quarter, the company said it grew its annualized revenue run rate (ARR) by 27% year over year. Much of that growth came in the middle of a pandemic that drove more customers to rely on managed services, said Keith White, senior VP and GM for HPE GreenLake Cloud Services. “It created, or exposed, a lot of holes people had,” White said. For example, many HPE customers took advantage of HPE GreenLake services to deploy the modern infrastructure required to run virtual desktops, White noted. Historically, a managed service is provided by a third-party IT services provider. However, vendors such as HPE and Dell Technologies are embracing cloud operating models to enable organizations to consume IT infrastructure running in an on-premises IT environment the way they consume public cloud services. Over the life of the contract, HPE not only manages that infrastructure, it also provides upgrades at specified intervals. It’s not clear what percentage of IT organizations are embracing that model as an alternative to hiring an internal IT staff to acquire, provision, and manage IT infrastructure. HPE is making a case for a cloud operating model that enables organizations to devote more of their internal IT resources to building and deploying applications, White said. Despite all the workloads moving into public clouds, the bulk of enterprise applications still run in on-premises IT environments. Some reasons for staying on-premise are security and compliance requirements, as well as application performance issues that arise when applications are accessed over a wide area network (WAN). Rather than migrating workloads and all their associated data to the cloud, White said the HPE GreenLake service provides all the operational cost benefits of a public cloud with less overall disruption. Naturally, tensions between vendors such as HPE and IT services providers are rising. HPE is trying to encourage IT services providers to resell HPE GreenLake rather than building out their own management frameworks to manage IT services. HPE today also announced alliances with Arrow Electronics, Ingram Micro, Synnex, Tech Data, and ALSO Group to create a distribution channel for HPE GreenLake services that would encourage more IT services providers to offer HPE GreenLake service to their end customers. It may be a while before the cloud operating model becomes dominant in enterprise IT environments. There are still plenty of organizations that prefer to treat the acquisition of IT infrastructure as a capital expense they write off over time. Regardless of preference, however, the decision whether to buy IT infrastructure or consume it as a service is becoming a lot more nuanced."
https://venturebeat.com/2021/03/11/intel-and-deci-collaborate-to-speed-up-machine-learning-workloads-on-processors/,Intel works with Deci to speed up machine learning on its chips,"Intel today announced a strategic business and technology collaboration with Deci to optimize machine learning on the former’s processors. Deci says that in the coming weeks, it will work with Intel to deploy “innovative AI technologies” to the companies’ mutual customers. Machine learning deployments have historically been constrained by the size and speed of algorithms and the need for costly hardware. In fact, a report from MIT found that machine learning might be approaching computational limits. A separate Synced study estimated that the University of Washington’s Grover fake news detection model cost $25,000 to train in about two weeks. OpenAI reportedly racked up a whopping $12 million to train its GPT-3 language model, and Google spent an estimated $6,912 training BERT, a bidirectional transformer model that redefined the state of the art for 11 natural language processing tasks. Intel and Deci say the partnership will enable machine learning “at scale” on Intel chips, potentially enabling new applications of inference through reductions in costs and latency. Already, Deci has worked to accelerate the inference speed of the well-known ResNet-50 neural network on Intel processors, achieving a reduction in the models’ latency by a factor of 11.8 and increasing throughput by up to 11 times. “By optimizing the AI models that run on Intel’s hardware, Deci enables customers to get even more speed and will allow for cost-effective and more general deep learning use cases on Intel CPUs,” Deci CEO and cofounder Yonatan Geifman said. “We are delighted to collaborate with Intel to deliver even greater value to our mutual customers and look forward to a successful partnership.” Deci achieves runtime acceleration through a combination of data preprocessing and loading, selecting model architectures and hyperparameters (i.e., the variables that influence a model’s predictions) as well as datasets optimized for inference. It also takes care of steps like deployment, serving, monitoring, and explainability. Deci’s accelerator redesigns models to create new models with several computation routes, all optimized for a given inference device. Deci’s router component ensures that each data input is directed via the proper route. (Each route is specialized with a prediction task.) As for the company’s accelerator, it works in synergy with other compression techniques like pruning and quantization. The accelerator can even act as a multiplier for complementary acceleration solutions such as AI compilers and specialized hardware, according to the company. Deci was cofounded by Geifman, entrepreneur Jonathan Elial, and Ran El-Yaniv, a computer science professor at Technion in Haifa, Israel. Geifman and El-Yaniv met at Technion, where Geifman is a PhD candidate at the university’s computer science department. To date, the Tel Aviv-based company, a participant in Intel’s Ignite startup accelerator, has raised $9.1 million from investors including Square Peg."
https://venturebeat.com/2021/03/11/multitasking-chatbots-are-the-new-amazon-alexa-prize-challenge/,Multitasking chatbots are the new Amazon Alexa Prize challenge,"In 2016, Amazon launched the Alexa Prize, an annual challenge to create AI-powered chatbots that can maintain a coherent, engaging 20-minute conversation on popular topics. A number of innovations sprang from it, including natural language understanding, neural response generation, common sense knowledge modeling, and dialogue policy technologies, leading to smoother and more engaging conversations. Inspired by this success, Amazon is today launching a new challenge aimed at encouraging developers to create experiences that can handle multistep, multimodal, and multiturn tasks. The pandemic appears to have supercharged voice app usage, which was already on an upswing. According to a study by NPR and Edison Research, the percentage of voice-enabled device owners who use commands at least once a day rose between the beginning of 2020 and the start of April. But the uptick in adoption hasn’t correlated with an increase in voice assistants’ technological sophistication, at least in some respects. For example, voice apps remain largely incapable of replying to more than one question at a time with a single response, which can be particularly frustrating in enterprise environments where multitasking is the ordinary course of business. The newly launched Alexa Prize challenge — the TaskBot Challenge — is intended to promote research along this direction. It focuses on improving multimodal experiences, seeking to spur the development of “immersive” and “intelligent” task completion workflows. According to Amazon, the goal is to foster chatbots that can get users from point A to point B whether they’re cooking or writing a report, or even completing DYI projects around the house. As with the initial Alexa Prize challenge, ten teams of university students will be selected to compete for research grants, Alexa-enabled devices, free Amazon Web Services cloud computing services, and more. The application period will begin on March 17 and extend to April 16, and the TaskBot Challenge will run for three years, with an initial focus on the domains of cooking and home improvement. “The challenge incorporates multimodal customer experiences, so in addition to receiving verbal instructions, customers with Echo screen devices, such as the Echo Show 10, could also be presented with step-by-step instructions, images, or diagrams that enhance task guidance,” Amazon explained in a press release. “The customer might ask Alexa how to fix a scratch on a car. The chatbot will then ask the customer more questions about their task, and then interactively provide step-by-step instructions and explanations for each step, or potentially adjust its plan based on customer input. After the interaction ends, the customer will be asked to rate how helpful that chatbot was with the task, and will have the option to provide freeform feedback to help the teams improve their chatbot.” Better multitasking and multimodality could encourage enterprises previously reluctant to adopt voice assistants to give the technology a second look. In a recent survey of 500 IT and business decision-makers in the U.S., France, Germany, and the U.K., 28% of respondents said they were using voice technologies and 84% expect to be using them in the next year. Voice assistants like Google Assistant, Cortana, Alexa, and Oracle Digital Assistant can analyze enterprise-specific vocabulary and derive intelligence from enterprise applications like enterprise resource management, customer relationship management, and HR systems. They’re also able to automate tasks like scheduling meetings, setting reminders, and assisting with conference call setup, as well as transcribing video and voice meetings. Alexa for Business, which made its debut at Amazon Web Services’ re:Invent conference, allows enterprise users to schedule meeting rooms with third-party services such as Cisco, Polycom, and Zoom. They’re also able to share itinerary information, check voicemail messages, and quickly see if meeting rooms are available. “Companies are using Alexa’s workplace and home office capabilities to increase productivity, improve the meeting room and video call experience, and roll out touch-free office environments,” Aaron Rubenson, VP of Amazon Alexa Voice Service and Alexa Skills, told VentureBeat in a previous interview. “Employees can use voice commands like ‘Alexa, join my meeting’ or ‘Alexa, I’m running late’ to join calls or email colleagues … As employees start returning to the workplace and traveling resumes, customers are also using Alexa for Business to provide a hands-free alternative for using shared devices like touch screens, lighting controls, and thermostats.” The Alexa Prize seeded the development of some of the features available for Alexa for Business customers, including an explicit content filter and a neural response generator. Amazon claims that the neural response generator, which reviews conversation histories and finds topics to inform the generated responses, led to increased satisfaction among Alexa users. As something of a case in point, users have had nearly 10 million interactions with Alexa Prize-originated chatbots over the past three years, according to the company."
https://venturebeat.com/2021/03/11/covid-apollo-project-makes-investment-in-rhinostics-to-accelerate-covid-19-testing-volumes-through-high-throughput-sample-accessioning/,Covid Apollo Project Makes Investment in Rhinostics to Accelerate Covid-19 Testing Volumes Through High Throughput Sample Accessioning,"NEW YORK & CAMBRIDGE, Mass.–(BUSINESS WIRE)–March 11, 2021– In a Series A round, Covid Apollo announces its investment in Rhinostics, an early-stage company that provides a novel solution of enabling comfortable sample collection, rapid accessioning, and automated cap removal that can be linked to high throughput Covid-19 assays to remove wasted time, headcount and costs from laboratory workflows. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210311005325/en/ The Covid Apollo Project brings together expertise and capital to find and help scale the most promising Covid-19 diagnostic opportunities. Originally funded under the leadership of RA Capital Management, the Project includes Redmile Group, Samsara BioCapital, Perceptive Advisors, and Bain Capital. “Returning to school and work requires sustainable testing solutions,” said Parker Cassidy, Principal at RA Capital Management and a member of Covid Apollo’s Board of Directors. “Covid Apollo is focused on finding and accelerating the most promising COVID testing solutions to rebuild testing paradigms to address the unique challenges that the pandemic has presented. Rhinostics is a great example of a company bringing new technologies and thinking to a segment of the laboratory workflow that is antiquated and ripe for innovation and acceleration.” “The investment in Rhinostics is right on mission for Covid Apollo. With our investments, we are aiming to accelerate technologies that can immediately impact COVID testing, which still struggles to meet testing demand, while also providing new tests and diagnostic tools and processes to enable ubiquitous testing in the future, providing not only better pandemic protection and monitoring but also new testing paradigms, that will empower patients and consumers to live safer and healthier lives,” commented Stefan Willemsen, Chief Executive Officer of Covid Apollo. He adds that “the team at Rhinostics has rapidly executed on its strategic plan to bring their products to market, already providing significant volumes of product into COVID testing laboratories. We are excited to support their next phase of growth, commercialization and global impact on the pandemic.” Rhinostics spun out of Harvard University to commercialize a novel automated nasal collection device invented in partnership by two Harvard faculty, Mike Springer, Associate Professor of Systems Biology at Harvard Medical School; and Richard Novak, Lead Staff Engineer at the Wyss Institute for Biologically Inspired Engineering. The novel nasal collection swab has features that can increase sample throughput by more than 10-fold while removing labor and errors from the laboratory workflow. The swab is integrated onto a cap that can be automated for removal from the tube while all 96 samples are simultaneously accessioned through scanning a 2D barcode on the bottom of the tubes. The product, registered as a Class I exempt medical device with the FDA, is currently being testing in several clinical trials, and the company is beginning an Emergency Use Authorization study comparing the RHINOstic™ swab to nasopharyngeal assays with a variety of polymerase chain reaction (PCR) assays. The product provides an immediate impact for increasing COVID testing efficiencies while being applicable to a broader range of respiratory viral, bacterial, and genetic testing using the PCR and next generation sequencing (NGS). Cheri Walker, Chief Executive Officer of Rhinostics, commented, “We are thrilled to have Covid Apollo as a partner and investor to join our efforts to positively impact laboratory workflows and make a major impact on COVID testing volumes. Our missions are perfectly aligned to make a difference now. In addition, having Stefan as a board member will be a huge asset, as he is bringing more than 20 years of experience in leadership roles in the IVD industry to our fast-growing company, providing strategic advice and keen business insight. Together, we can change laboratory workflows for COVID and beyond.” For decades there has been no innovation in viral collection and the process of moving samples into laboratories. The Rhinostics comfortable nasal swab solution brings new materials, better assay performance, dry shipment, instantaneous accessioning, and automated cap removal, all of which together can allow labs to significantly increase their sample throughput with no increase in footprint or staffing. This is changing the way laboratories think about their workflows. About Covid Apollo Project Covid Apollo Project aims to enable a safe and sustainable return to work and school by identifying, assembling, developing, and scaling the most promising COVID diagnostic opportunities. Covid Apollo was organized and funded under the leadership of RA Capital Management, a prominent biotechnology-focused investment firm, in collaboration with Redmile Group, Samsara BioCapital, Perceptive Advisors, and Bain Capital, and with legal support from Goodwin Procter. Covid Apollo is a funding founder of the $6M X-Prize competition to develop faster, cheaper and easier to use COVID testing methods at scale. https://www.xprize.org/prizes/covidtesting About Rhinostics Rhinostics is revolutionizing laboratory workflows to link sample to result seamlessly. The company’s products improve sample collection performance while removing costs and time compared to traditional collection and intake. The products bring new materials, new collection types, rapid accessioning and automation to remove caps and move samples into high throughput workflows with little human intervention. The RHINOstic™ nasal swab provides features that increase sample throughput by more than 10-fold while removing labor and errors from the laboratory workflow. The swab is integrated onto a cap that can be automated for removal from the tube while all 96 samples are simultaneously accessioned through scanning a 2D barcode on the bottom of the tubes. The product provides an immediate impact to increasing COVID testing efficiencies while being applicable to broader respiratory viral, bacterial, and genetic testing using the polymerase chain reaction (PCR) and next generation sequencing (NGS). The RHINOstic™ product is registered as Class I exempt medical device with the FDA and is available for purchase. To learn more, visit https://www.rhinostics.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210311005325/en/ For Covid ApolloStefan Willemsenswillemsen@covidapollo.com (919) 525 8725www.covidapollo.com For Rhinostics Inc.Cheri Walkerinfo@rhinostics.com (877) 746-6789www.rhinostics.com"
https://venturebeat.com/2021/03/11/here-are-5-awesome-tips-that-will-make-your-resume-stand-out-from-the-crowd/,Here are 5 awesome tips that will make your resume stand out from the crowd,"As a result of the outbreak of COVID-19, many companies needed to tighten their belt and shrink resources, resulting in thousands of talented people losing their jobs. However, 12 months in, companies are back hiring and looking for exceptional new talent (that’s you, btw!) And we want to help and do what we can to ensure you land your dream jobs. So with that, we’ve got some really great tips on how to make your resume and cover letter stand out from the crowd. Every job is different, every application is different, and every hiring manager is looking for different things — so tailor, tailor, tailor! There is no point in sending out the same resume for eight different job applications. Research each company, and include buzzwords that may jump out to the person reading. Also, where you can, try to make subtle references to the job description, so the company knows you’re a good match. One important thing to remember is try to avoid overdoing it when it comes to your resume. More than likely, the hiring manager has a pile of other applications to read, so the last thing they want is a five-pager from you. Keep it concise and to the point! Every company has a different brand identity, and a different tone — so you should be mindful of that when doing up a resume to apply for a role. If you are applying to a big corporate enterprise, you may find that using a more formal tone would be best. Likewise, if you are applying to a less corporate company, an informal tone may work better. Check out the company website or brand page to get an idea. This should go without saying, but sometimes spelling and grammar can slip through the cracks. And guys, it really, really shouldn’t. We know that a spelling mistake or a grammatical error isn’t the end of the world, however, if you are a hiring manager looking to narrow down a pool of applicants, small mistakes might be the only way to do so. Get someone to proofread your resume before sending it out. Not everyone chooses to include a little personal statement on their CV, but I really see the merit in it. A personal statement can help identify your strengths and immediately show that you have the right skills for the job. Look at it as kind of an opener to the person reading it. Every resume will be different depending on the role you’re applying for, so be sure to do some research around what kind of applications they look for and what they’re hoping to see in your resume. You’ve got this! Head over to VentureBeat jobs now, and start applying."
https://venturebeat.com/2021/03/11/trustarc-launches-privacycentral-to-bring-data-privacy-intelligence-to-enterprises/,TrustArc launches PrivacyCentral to bring data privacy intelligence to enterprises,"Data privacy management and compliance company TrustArc has announced a new data intelligence tool that delivers contextual, real-time insights into companies’ data privacy programs. The PrivacyCentral tool can generate automated, customizable reports to help privacy teams determine which regulations are most likely to apply to their data and prioritize each law by setting timelines, assigning owners, evaluating key performance indicators (KPIs), and providing detailed reporting. “PrivacyCentral allows for organizations to understand laws and readiness against laws simultaneously,” Hilary Wandall, SVP of privacy intelligence and general counsel at TrustArc, told VentureBeat. “Unlike traditional approaches to privacy law readiness or gap assessments that typically look at one law at a time, PrivacyCentral’s intelligence engines evaluate your progress and effectiveness against all of them at the same time.” PrivacyCentral continuously scans a company’s profile against the various privacy laws around the world to identify which ones apply to their business, and then it asks relevant assessment questions. The platform leans on AI to conduct these automated assessments spanning company records, systems, and data to match against the ever-evolving privacy regulations for a specific jurisdiction. TrustArc is one of the myriad data privacy startups capitalizing on the growing array of privacy regulations, such as GDPR in Europe and CCPA in California. The San Francisco-based company develops various data protection, certification, and compliance products for enterprises like Intuit, Johnson & Johnson, and Monster, helping them monitor risk around regulations and identify gaps spanning regulatory frameworks. Other notable players in the space include Privacera and DataGrail, which just this week raised $50 million and $30 million, respectively; BigID, which is valued at $1 billion following a recent investment; and OneTrust, which recently attained a $5 billion valuation. It’s clear that the data privacy market is hot, something TrustArc is looking to capitalize on with PrivacyCentral, which Wandall said solves many problems, including the “inefficient and time-consuming nature of managing privacy across an organization,” particularly when a new law or regulation is introduced. PrivacyCentral is designed to serve as a “single source of truth,” one that removes the manual processes involved in assessing and complying with the various privacy laws and works between all the existing regulations and any new ones as they come into force. “A privacy leader no longer needs to restart the cycle of working with business stakeholders to understand how a new law or regulation applies to their business,” Wandall said. “The privacy leader can automatically get an in-depth understanding of the gaps between where they are and where they need to be and engage the business in a meaningful way.”"
https://venturebeat.com/2021/03/11/report-at-least-10-hacking-groups-are-exploiting-microsoft-exchange-flaws/,Report: At least 10 hacking groups are exploiting Microsoft Exchange flaws,"(Reuters) — At least 10 different hacking groups are using recently discovered flaws in Microsoft’s mail server software to break into targets around the world, cybersecurity company ESET said in a blog post on Wednesday. The breadth of the exploitation adds to the urgency of the warnings being issued by authorities in the United States and Europe about the weaknesses found in Microsoft’s Exchange software. The security holes in the widely used mail and calendaring solution leave the door open to industrial-scale cyberespionage, allowing malicious actors to steal emails virtually at will from vulnerable servers or move elsewhere in the network. Tens of thousands of organizations have already been compromised, Reuters reported last week, and new victims are being made public daily. Earlier on Wednesday, for example, Norway’s parliament announced data had been “extracted” in a breach linked to the Microsoft flaws. Germany’s cybersecurity watchdog agency also said on Wednesday two federal authorities had been affected by the hack, although it declined to identify them. While Microsoft has issued fixes, the sluggish pace of many customers’ updates — which experts attribute in part to the complexity of Exchange’s architecture — means the field remains at least partially open to hackers of all stripes. The patches do not remove any backdoor access that has already been left on the machines. In addition, some of the backdoors left on compromised machines have passwords that are easily guessed so newcomers can take them over. Microsoft declined to comment on the pace of customers’ updates. In previous announcements pertaining to the flaws, the company has emphasized the importance of “patching all affected systems immediately.” Although the hacking has appeared to be focused on cyberespionage, experts are concerned about the prospect of ransom-seeking cybercriminals taking advantage of the flaws because that could lead to widespread disruption. ESET’s blog post said there were already signs of cybercriminal exploitation, with one group that specializes in stealing computer resources to mine cryptocurrency breaking into previously vulnerable Exchange servers to spread its malicious software. ESET named nine other espionage-focused groups it said were taking advantage of the flaws to break into targeted networks — several of which other researchers have tied to China. Microsoft has blamed the hack on China. The Chinese government denies any role. Intriguingly, several of the groups appeared to know about the vulnerability before it was announced by Microsoft on March 2. Ben Read, a director with cybersecurity company FireEye, said he could not confirm the exact details in the ESET post but said his company had also seen “multiple likely-China groups” using the Microsoft flaws in different waves. ESET researcher Matthieu Faou said in an email it was “very uncommon” for so many different cyberespionage groups to have access to the same information before it is made public. He speculated that either the information “somehow leaked” ahead of the Microsoft announcement or it was found by a third party that supplies vulnerability information to cyber spies. Taiwan-based researchers reported to Microsoft on January 5 that they had found two new flaws that need patching. The two were among those that the attackers began using shortly before or after the friendly report. They said were investigating whether there had been a theft or leak on their side since exploitation was discovered in the wild later the same week. So far, the group called Devcore said they had found no evidence. Top-flight hackers are also commonly targeted by other hackers. Just this week, Microsoft patched one of the flaws used by suspected North Korean hackers in attempts to steal information from Western researchers. But simultaneous discovery happens fairly often, in part because researchers use the same or similar tools to hunt for serious flaws, and many eyes are looking at the same high-value targets. “It is very likely that some actor groups may have been using these vulnerabilities and led to the result of the attacks being observed by other information security vendors,” Devcore member Bowen Hsu told Reuters. But the security industry has been abuzz with other theories, including a hack of Microsoft’s systems for tracking bugs, which has happened in the past."
https://venturebeat.com/2021/03/10/combining-edge-computing-and-iot-to-unlock-intelligent-applications/,Combining edge computing and IoT to unlock autonomous and intelligent applications,"The boom in internet-connected devices and the unprecedented amount of data being collected has left enterprises grappling with the challenges of storing, securing, and processing the data at scale. The sheer amount of data involved is driving the case for edge computing, even as enterprises continue with their digital transformation plans. Edge computing refers to moving processing power to the network edge — where the devices are — instead of first transferring the data to a centralized location, whether that is to a datacenter or a cloud provider. Edge computing analyzes the data near where it’s being collected, which reduces internet bandwidth usage and addresses security and scalability concerns over where the data is stored and how it’s being transferred. The main drivers are internet-of-things (IoT) and real-time applications that demand instantaneous data processing. 5G deployments are accelerating this trend. Enterprises have been focused on moving their applications to the cloud over the past few years. Analysts estimate that 70% of organizations have at least one application in the cloud, and enterprise decision-makers say digital transformation is one of their top priorities. However, as more data-hungry applications come online, it’s clear there are limits to an all-cloud strategy . By 2025, 175 zettabytes (or 175 trillion gigabytes) of data will be generated around the globe, and more than 90 zettabytes of that data will be created by edge devices, according to IDC’s Data Age 2025 report. That is a lot of data that needs to be uploaded someplace before anything can be done with it, and there may not always be enough bandwidth to do so. Latency is also a problem since it would take time for data to travel the distance from the device to where the analysis is being performed and come back to the device with the results. Finally, there is no guarantee that the network would always be available or reliable. If the network is unavailable for some reason, the application is essentially offline. “You’re backhauling data to a cloud that’s far away, miles away,” said James Thomason, CTO of EDJX, which provides a platform that makes it easy for developers to write edge and IoT applications and secure edge data at the source. “That’s an insurmountable speed of light problem.” Analysts estimate that 91% of today’s data is created and processed in centralized datacenters. By 2022, about 75% of all data will need analysis and action at the edge. “We knew when we started EDJX that the pendulum would have to swing from cloud and centralization back to decentralized,” Thomason said. Edge computing isn’t limited to just sensors and other IoT; it can also involve traditional IT devices, such as laptops, servers, and handheld systems. Enterprise applications such as enterprise resource planning (ERP), financial software, and data management systems typically don’t need the level of real-time instantaneous data processing most commonly associated with autonomous applications. Edge computing has the most relevance in the world of enterprise software in the context of application delivery. Employees don’t need access to the whole application suite or all of the company’s data. Providing them just what they need with limited data generally results in better performance and user experience. Edge computing also makes it possible to harness AI in enterprise applications, such as voice recognition. Voice recognition applications need to work locally for fast response, even if the algorithm is trained in the cloud. “For the first time in history, computing is moving out of the realm of abstract stuff like spreadsheets, web browsers, video games, et cetera, and into the real world,” Thomason said. Devices are sensing things in the real world and acting based on that information. Next-generation applications and services require a new computing infrastructure that delivers low latency networks and high-performance computing at the extreme edge of the network. That is the idea behind Public Infrastructure Network Node (PINN), the initiative out of the Autonomy Institute, a cooperative research consortium focused on advancing and accelerating autonomy and AI at the edge. PINN is a unified open standard supporting 5G wireless, edge computing, radar, lidar, enhanced GPS, and intelligent transportation systems (ITS). PINN looks like a streetlight post, so a PINN cluster could potentially provide a lot of computing power without requiring a lot of cell towers or heavy cables. According to Thomason, PINN clusters in a city deployment could be positioned to collect information from the sensors and cameras at a street intersection. The devices can see things a driver can’t see — such as both directions of traffic, or that a pedestrian is about to enter the crosswalk — and know things the driver doesn’t know — like that an emergency vehicle is on the way or traffic lights are about to change. Edge computing using PINN is what will make it possible to process all of the signals and do something about it, whether that is to make the traffic lights change or force the autonomous vehicle to do something differently. Currently, only vetted developers would be allowed in the PINN ecosystem, Thomason said. Developers write code that is then compiled in WebAssembly, which is the actual code that runs on PINN. Using WebAssembly makes it possible to have a very small attack surface that’s very hardened, making it more difficult for an adversary to break out of the application and get to the data on the PINN, Thomason said. Autonomy Institute announced a pilot program for PINN at the Texas Military Department’s Camp Mabry location in Austin, Texas. The program will deploy PINNs 1,000 feet apart on a sidewalk over the 400-acre property. With the pilot, the focus will be on optimizing traffic management, autonomous cards, industrial robotics, autonomous delivery, drones that respond to 911 calls, and automated road and bridge inspection — all the things a smart city would care about. The first PINNS are scheduled to come online in the second quarter of 2021, and the goal is to have tens of thousands of PINNs deployed by mid-2022. Eventually, the program will be expanded from Austin to other major cities in the United States and around the world, EDJX said. While the pilot program is specifically for building out city infrastructure, Thomason said this was an opportunity to explore other contexts to use PINN.  As developers start developing for the platform, there will be opportunities to build and test applications for other industry sectors and use cases where data needs to be aggregated from multiple sources and fused together. Real-world edge applications on PINN can cover a whole range of things, including industrial IoT, artificial intelligence, augmented reality, and robotics. “That general pattern of sensor data, fusion, and things happening in the real world is happening across industries,” Thomason said. “It’s not just smart cities and vehicles.” For specific industries, there are different ways PINNs can be used. The energy sector needs to monitor the pipelines for natural gas and oil for signs of leaks — for financial reasons and over environmental concerns. However, having enough sensors with sniffers to cover all the pipelines and wells could be too difficult. But setting up an infrared camera or a spectrometer to see the leaks and then raise the alert would prevent costly damages. In another example, a factory may use cameras or other sensors to detect the presence of a worker inside the assembly line before starting the machinery. “If you can use computing and sensors to do that, you can reduce workplace accidents significantly,” Thomason said. It is up to the developers that come to the platform what kind of applications they will build — the PINN had to exist first, Autonomy Institute chair Jeffrey DeCoux said. PINN deployments will also encourage more work around sensors, 5G deployments, and all other technologies that depend on edge computing. “Everybody came to the same realization: If we don’t do this, all of these industry 4.0 applications will never happen,” DeCoux said."
https://venturebeat.com/2021/03/10/alphabet-is-repurposing-google-tpus-for-quantum-computing-simulations/,Alphabet is repurposing Google TPUs for quantum computing simulations,"Sandbox at Alphabet, Google parent company Alphabet’s second, secretive software development team, plans to launch a set of APIs called Floq that will allow developers to use tensor processing units (TPUs) to simulate quantum computing workloads. The announcement, which was made during a February livestream that garnered little mainstream attention, hints at the potential for hardware originally designed for AI applications to extend into the quantum realm. Experts believe that quantum computing, which at a high level entails the use of quantum-mechanical phenomena like superposition and entanglement to perform computation, could one day accelerate AI workloads compared with classical computers. Scientific discoveries arising from the field could transform energy storage, chemical engineering, drug discovery, financial portfolio optimization, machine learning, and more, leading to new business applications. Emergen Research anticipates that the global quantum computing market for the enterprise will reach $3.9 billion by 2027. According to Sandbox at Alphabet research scientist Guillaume Verdon, Floq, which will initially be made available in alpha to 50 teams in the QHack Open Hackathon, will offer a simulator API that leverages the “bleeding edge” of AI compute for experimentation. The Sandbox at Alphabet team repurposed TPUs, chips developed by Google specifically for AI training and inference, to accelerate simulations in the cloud so that developers can use frontends like TensorFlow Quantum and PennyLane to create quantum models and run them remotely on Floq.  Google’s TPUs are liquid-cooled and designed to slot into server racks; deliver up to 100 petaflops of compute; and power Google products like Google Search, Google Photos, Google Translate, Google Assistant, Gmail, and Google Cloud AI APIs. Google announced the third generation in 2018 at its annual I/O developer conference and this morning took the wraps off the successor, which is in the research stages.  Verdon says that Floq simulators, dubbed Floq Units, run 10 to 100 times faster than state-of-the-art simulations accelerated by GPUs when compared in terms of runtime. Moreover, by quantum volume — the maximum size of square quantum circuits that can be implemented successfully by a machine — Floq currently exceeds 4 billion and in the future will reach into the trillions. In simplest terms, quantum circuits are a sequence of matrix math operations performed on qubits, the quantum version of a bit.  “The team has been experimenting with how to use Floq for physics, machine learning — all kinds of cool applications,” Verdon said. “And we’ve developed our own open source library for tensor networks that runs on TPUs … It’s [surprising] how good the chips are for quantum simulation. It’s almost like they were designed for this task.” When it launches more broadly, Floq will complement Cirq, Google’s service that gives developers access to its quantum computing hardware. And it’ll compete with a number of other quantum simulators already available on the market, including a service in IBM’s Quantum Experience suite and simulators from Intel, Amazon, and Microsoft. But Floq’s reliance on TPUs looks to set it apart from the pack — at least on the performance end of the equation."
https://venturebeat.com/2021/03/10/ovh-datacenter-disaster-shows-why-recovery-plans-and-backups-are-vital/,OVH datacenter disaster shows why recovery plans and backups are vital,"European cloud computing giant OVH announced today that a major fire destroyed one of its Strasbourg datacenters and damaged another, while the company also shut down two other datacenters located at the site as a precautionary measure. Nobody was reported to have been injured. While AWS, Azure, and Google Cloud usually garner most of the limelight in the cloud computing realm, OVH is one of the bigger ones outside the “big three” with 27 datacenters globally, 15 of which are in Europe. Today’s disaster, which was thought to have taken more than 3.5 million websites offline, comes during a major period of activity for France-based OVH, after it recently announced a partnership with Atos to offer fully EU-made cloud services in an industry dominated by Amazon, Microsoft, and Google. And just this week, OVH revealed that it was in the early planning stages of going public. In the wake of the fire which broke out around midnight local time today, OVH founder and chairman Oktave Klaba took to Twitter to recommend that its customers activate their disaster recovery plan.  However, it soon became apparent that not all companies had a sufficient disaster recovery plan in place, with French government bodies and some banks still offline at the time of writing, more than 15 hours later. Moreover, Facepunch Studios, the game studio behind Rust, confirmed that even after it was back online that it would not be able to restore any data.  And that, perhaps, is one of the biggest lessons businesses can glean from the events that unfolded in Strasbourg today. Despite all the benefits that cloud computing brings to the table, companies are still putting all their trust in a third-party’s infrastructure, which is why having a robust disaster recovery plan — including data backups — is so important. OVH, which also provides email and internet hosting services, said that it plans to restart two of the unaffected datacenters by this coming Monday (March 15)."
https://venturebeat.com/2021/03/10/damon-motors-raises-30m-secures-20m-in-preorders/,ADDING MULTIMEDIA Damon Motors Raises $30M; Secures $20M in Preorders," Company closes bridge round and bolsters board of directors with Jaques Clariond and Howard Wu  VANCOUVER, British Columbia–(BUSINESS WIRE)–March 10, 2021– Damon Motors today announced it has raised more than $30M in funding, completing a bridge round led by Benevolent Capital, SOL Global Investments, Zirmania, and others. The financing couples with skyrocketing preorder sales that have now reached $20M for the exhilarating line of HyperSport motorcycles. The capital raised will further fuel Damon’s path to market and enable demo tours, pre-production development and testing, and associated hires. Damon has also expanded its board of directors with the additions of Jaques Clariond and Howard Wu. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210310005113/en/ “We have unveiled the future of two-wheel transportation and yielded high demand in North America and Europe, validating the market need for high-performance electric motorcycles,” said Jay Giraud, founder & CEO at Damon Motors. “On the back of this demand, Damon will produce an entire line of medium- and high-performance motorcycles over the next few years, with HyperDrive serving as the core of each model.” New Board of Directors Members:Jaques Clariond – Jaques is a Managing Partner at Baudpont and serves in Senior Advisor roles at Blue Ivy Ventures and Benevolent Capital. Jaques is focused on investing in high-growth consumer companies across the health and wellness, fitness, lifestyle, and consumer packaged goods sectors. He has extensive experience helping companies navigate the financial complexities associated with raising capital and executing upon the strategic vision of the enterprise. Jaques offers a unique blend of expertise across public and private asset classes, portfolio construction, and direct private equity and venture capital deal structuring. Howard Wu – Howard is the Global Head of Telecom and the General Manager for Quanta Cloud Technology USA. In his career, Howard has managed global business and regions from startups to Fortune 500 companies. As a former startup founder, Howard has a deep understanding of the challenges and skepticism founders must endure and persevere, as well as the challenges facing Global Fortune 500s of today across data and digitization. Damon’s announcement comes on the heels of the first anniversary of its flagship HyperSport motorcycle launch at CES 2020. The company has ridden a wave of momentum ever since, selling nearly 1000 motorcycles and selling out of its limited edition HyperSport models. The company has won numerous industry awards including 2020 CES “Best in Innovation,” Robb Report’s “Best of The Best of Auto Innovation 2020,” Popular Science “Best of What’s New,” and Red Herring “Top 100 North America.” The Damon HyperSport sets a new standard in motorcycle safety, awareness and connectivity. It boasts well over 200hp and 200nm of torque delivered at zero rpm, a top speed of 200 mph, a range of more than 200 highway miles per charge. All HyperSports are outfitted with CoPilot™, Damon’s 360° advanced warning system; Shift™, which transforms the riding position between sport and commuter modes with the push of a button; and HyperDrive™, the world’s first 100 percent electric, multi-variant powertrain platform optimized for maximum performance, design and safety. Damon’s family of HyperSport Motorcycles includes: HyperSport SE (MSRP $16,995), HyperSport SX (MSRP $19,995), HyperSport HS (MSRP $24,995), and HyperSport Premier (MSRP $39,995). For more information on Damon Motors, visit www.damon.com. About Damon Motors Inc.Damon is unleashing the full potential of personal mobility for the world’s commuters. With its HyperDrive™ proprietary electric powertrain, the company has developed the world’s safest, smartest, fully connected electric motorcycles employing sensor fusion, robotics and AI. Designed as a platform for worldwide line extension, Damon motorcycles will ship direct to customers on subscription plans to drive scale. Based in Vancouver, Canada, Damon is founded by serial entrepreneurs Jay Giraud and Dom Kwong. Damon’s investors include Round 13 Capital, Techstars, Fontinalis, Extreme Venture Partners, and Pallasite Ventures. Learn more at damon.com and follow us on Instagram @damonmotorcycles.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210310005113/en/ Donna Loughlin MichaelsLoughlin Michaels Groupdonna@lmgpr.com (408) 393-5575"
https://venturebeat.com/2021/03/10/how-the-pandemic-challenges-companies-that-use-predictive-models/,How the pandemic challenges companies that use predictive models,"For enterprises using predictive models to forecast consumer behavior, data drift was a major challenge in 2020 due to never-before-seen circumstances related to the pandemic. Organizations were forced to constantly retrain and update their machine learning models, and 12 months later, many are still wrestling with the challenge. In an interview with VentureBeat, Dan Simion, VP of AI & Analytics for Capgemini North America, said that while companies are in a better position than they were three months into the pandemic, they’re in a different position. While they’re acclimating to the data coming in within the context of this new environment, they can’t draw patterns from the past 12 months of data because behaviors continue to change. “In the first three months, everyone was more or less trying to make sense of what could be done to use the new data and start building more accurate machine learning models,” Simion said. “Today, the question is: How quickly can we adapt and retrain machine learning models?” Simion pointed out that models need to be nimble enough to increase accuracy by leveraging new data on the changing behaviors as it comes in. It’s also critical to establish a way to scale this process, he said, because changing machine learning models and adapting to continuously shifting data takes a coordinated effort. As an example, Simion talked about a multibillion-dollar global consumer packaged goods company in the frozen food sector. Early on in the pandemic, the company, which is a Capgemini customer, had to adjust to trends and behaviors that varied widely depending on specific regions and states. In the first three to four months of the pandemic, when most regions had restrictions in place, frozen food sales went up significantly as customers chose to eat at home. But while some states have since loosened their quarantine rules and the number of frozen food sales has decreased overall, other states have opted for a slower reopening, leading to shifting trends that make it difficult to predict where frozen food sales will ultimately settle. In another example, Simion says that a Capgemini client in the industrial components space is struggling to anticipate disruptions in the global supply chain. Because of international restrictions and limits, there aren’t many ways to deliver materials and products across countries. The company had emergency reserve supplies stockpiled in warehouses at the three-to-four-month mark of the pandemic, but with that emergency supply gone, limited transportation and supply have made it difficult to build accurate forecasts amid so many variables and constraints. Simion says the challenges are particularly acute in commerce. One Fortune 500 retailer retaining Capgemini’s services can no longer track and predict certain buying patterns with the precision it did before the pandemic. In a normal year, during Christmas or approaching Back to School season, shoppers would make purchases, and orders of particular items would increase predictably. But that has changed as varying pandemic constraints, including hybrid learning environments and smaller holiday gatherings, impact people and their spending. “The supply chain was built in a way that would fulfill steady demand. Planning and forecasting based on that prior data was easy and highly accurate,” Simion said. “Now, all of that is changing. [E]ven after 12 months of living in this pandemic-impacted world, we cannot grasp what an accurate predictive model will be.” A report recently published in Harvard Business Review suggests several remedies for unstable predictive models. To fix these models, companies might look to analogies like past economic shocks for an idea of the future during and after the pandemic. They might also embrace ensemble modeling, which combines predictions from different models to suggest a reasonable range. And they could include local knowledge, as well as aggregated knowledge from a panel of experts on the pandemic and its effects. “The question is: How quickly can we adapt and retrain ML models? Not only do the models need to be rebuilt or redesigned based on new data, but they also need the right processes to be put into production at a pace that keeps up,” Simion added. “Until there is some sort of stability, it will continue to be difficult for organizations to identify consistent trends.”"
https://venturebeat.com/2021/03/10/brock-pierce-joins-srax-board-of-directors/,Brock Pierce Joins SRAX Board of Directors,"LOS ANGELES–(BUSINESS WIRE)–March 10, 2021– SRAX, Inc. (NASDAQ: SRAX), a financial technology company that unlocks data and insights for publicly traded companies, announced that Brock Pierce, the entrepreneur, philanthropist, and 2020 presidential candidate, has joined the SRAX board of directors. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210310005595/en/ Mr. Pierce has co-founded, advised, and funded over 100 companies to support the creation of new jobs and marketplaces through technological innovation including many blockchain technologies that have paved the way for cryptocurrency innovations. He continues to give back through his philanthropic efforts. He has been quoted on many occasions saying “a billionaire is a person who has a positive impact on a billion people.” In his continued efforts to give back, he ran as a candidate for President of the United States in the 2020 election. Brock was responsible for the formation of Five Delta, a company that SRAX Purchased in 2014. Five Delta had a number of patents around social media ad targeting. The team and technology went on to form SRAX MD, which SRAX then sold in 2018 to a private equity firm for close to $50M. “Brock Pierce is one of the smartest people and one of the most significant visionaries I have ever met. He has been able to see trends early and has been able to capitalize on those insights. We are honored to have him join our Board of Directors and are especially honored to have him replace the seat left behind from the passing of Malcolm CasSelle whom we loved, respected, and miss daily,” said Christopher Miglino, Founder and CEO of SRAX. “Brock shares our vision around the power of the retail investor and the community we have created. He is always at the forefront of new paradigm shifts, and I think the shift in the retail investor is just beginning,” added Miglino. “It’s an honor to accept this position on the board of directors at SRAX and to be filling the shoes of a great leader, my friend, the late Malcolm CasSelle. SRAX is changing the way that investors interact with public companies and fundamentally changing the interaction between the retail investor and the capital markets,” said Brock Pierce. “This year, more than ever, the strength of the retail investor has become apparent to the board room. SRAX is at the forefront of this shift, and I look forward to sharing my capital markets experience with the team,” added Pierce. About SRAX SRAX (NASDAQ: SRAX) is a financial technology company that unlocks data and insights for publicly traded companies. Through its premier investor intelligence and communications platform, Sequire, companies can track their investors’ behaviors and trends and use those insights to engage current and potential investors across marketing channels. For more information on SRAX, visit srax.com. Cautionary Statement Regarding Forward-Looking Information: This news release contains “forward-looking statements” made pursuant to the “safe harbor” provisions of the Private Securities Litigation Reform Act of 1995. Such forward-looking statements relate to future, not past, events and may often be identified by words such as “expect,” “anticipate,” “intend,” “plan,” “believe,” “seek” or “will.” Forward-looking statements by their nature address matters that are, to different degrees, uncertain. Specific risks and uncertainties that could cause our actual results to differ materially from those expressed in our forward-looking statements include risks inherent in our business, and our need for future capital. Actual results may differ materially from the results anticipated in these forward-looking statements. Additional information on potential factors that could affect our results and other risks and uncertainties are detailed from time to time in SRAX’s periodic reports filed with the Securities and Exchange Commission (SEC), including its Annual Report on Form 10-K for the year ended December 31, 2019, its Quarterly Reports on Form 10-Q as well as and in other reports filed with the SEC. We do not assume any obligation to update any forward-looking statements.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210310005595/en/ Bri KelvinInvestors@srax.com"
https://venturebeat.com/2021/03/10/app-security-platform-provider-pathlock-raises-20m/,App security platform provider Pathlock raises $20M,"Unified access orchestration platform provider Pathlock, formerly Greenlight Technologies, today announced it has secured $20 million in a strategic growth round led by Vertica Capital Partners. Pathlock says the funds will be used to bolster R&D for its products, extending the capabilities of its insider threat prevention platform. According to Markets and Markets, the security orchestration, automation, and response (SOAR) segment is expected to reach $1.68 billion this year, driven by a rise in security breaches and incidents and the rapid development and deployment of cloud-based solutions. Risk Based Security found that data breaches exposed 4.1 billion records in the first half of 2019. That may be why 68% of business leaders in a recent Accenture survey said they feel their cybersecurity risks are increasing. Flemington, New Jersey-based Pathlock, which was founded in 2004, says its access orchestration solution surfaces violations and takes action to prevent loss. With Pathlock, enterprises can manage different aspects of access governance — including user provisioning and temporary elevation, ongoing user access reviews, internal control testing, transaction monitoring, and audit preparation. Pathlock continuously monitors transactions across enterprise apps to detect intrusions. It runs provisioning scenarios with “what-if” analysis before providing access and delivers real-time alerts for potential data loss. Moreover, Pathlock can automatically revoke privileges or terminate sessions to block malicious behavior, and its risk methodology is based on financial impact. The platform provides a quantitative risk score, rather than the risk rankings found in some rival solutions. Pathlock claims to have monitored billions of events across its more than 100 customers. “As digital transformation is driving change, enterprises are rapidly adopting new applications, moving to the cloud, and increasing their automation efforts. These digital activities are often happening in the shadow of IT, making them harder to manage and control,” founder and CEO Anand Adya said in a press release. “Our increased focus on zero trust will help modern enterprises conquer insider threats through a unified platform. We look forward to saving our customers time and money while helping them pass their audits with flying colors.” Pathlock has a number of competitors in the identity and access management market. There’s Strata Identity, an identity orchestration platform for multicloud environments that recently raised $11 million. Another is JupiterOne, a cybersecurity management automation startup whose customers include Reddit and Databricks. Verified Market Research expects the segment to be worth $29.79 billion by 2027, growing at a compound annual growth rate of 13.21% from 2020, when it was valued at an estimated $11.82 billion."
https://venturebeat.com/2021/03/10/capital-one-ventures-partners-with-securonix-on-cloud-native-security-analytics-in-24m-deal/,Capital One Ventures partners with Securonix on cloud-native security analytics in $24M deal,"As more enterprises move to the cloud, the field of threat analysis is evolving by shifting to a cloud-native architecture. Over time, this will lead to human researchers augmented by AI and machine learning. That’s essential for a company like Capital One, which is rushing to scale defenses as its cloud presence expands. To catalyze that new analytics model, the financial giant’s venture arm today announced it has invested $24 million in Securonix, a security startup that has developed a cloud-based security information and event management (SIEM) platform. “Cloud-native and scalable architectures are the direction where this market is going to go, and Securonix is built cloud-native from the ground up,” Capital One Ventures Partner Jay Emmanuel said. “We believe this is the architecture that will lend itself to the scale and the resiliency that’s going to be required in a few years.” Securonix has created a SaaS-based security analytics suite of tools that reduces infrastructure costs and allows for greater scalability. Those tools gather data that is fed to the company’s 450 security analysts who help track vulnerabilities and attacks. But according to Securonix CEO Sachin Nayyar, the company is beefing up its AI and machine learning capabilities to augment that work so more of it can be automated. “Cybersecurity provides the most velocity and variety of data that you will see in any part of the organization,” Nayyar said. “And we all know that cyber analysts have been drowning in this.” Emmanuel said Capital One knows it needs greater automation to meet the growing pace of cyberattacks. “We believe it’s going to evolve into being more AI and machine learning-driven,” he said. “There’s going to be a significant amount of machine assistance to the human operators in threat detection and incident response activity.” The SIEM space has become highly competitive. Nayyar says Securonix has been growing rapidly because it offers three advantages: a foundation built on data analysis; a cloud-based approach to security; and a low price point for customers, thanks to the lack of additional infrastructure needed. While the latest funding will help continue this work, Nayyar said the partnership with Capital One is just as critical. Capital One Ventures is taking a seat on the board and Securonix will be working closely with Capital One to explore product enhancements and use cases related to monitoring cyberattacks. Nayyar also hopes to use that development partnership to springboard into other security areas, such as IoT and mobile."
https://venturebeat.com/2021/03/10/cibc-innovation-banking-announces-growth-capital-financing-for-practice-ignition/,CIBC Innovation Banking Announces Growth Capital Financing for Practice Ignition,"NEW YORK–(BUSINESS WIRE)–March 10, 2021– CIBC Innovation Banking is pleased to announce a multi-million dollar growth capital facility for Practice Ignition Pty Ltd. (“Practice Ignition”), a leading accounting and client engagement platform. Practice Ignition plans to use the financing to deepen its investment in its proprietary product and technology, and further accelerate its global growth momentum. Practice Ignition provides a client engagement platform and front office system of record for accountants and bookkeepers that facilitates proposals, payments and business insights. “CIBC Innovation Banking is pleased to support Practice Ignition in its global expansion and continued development of its comprehensive accounting management solution,” said Caroline Tkatschow, Director in CIBC Innovation Banking’s New York office. “Their top tier management team has developed a highly scalable platform that is uniquely positioned to dramatically disrupt and transform the accounting industry.” Through one digital signature, Practice Ignition automates dynamic workflows, payment collection, and provides valuable data insights to enhance the customer relationship and drive business growth. “We are focused on building the front end system of record for accountants and bookkeepers as they re-imagine their practices to run in the cloud and transform the way they engage with, and deliver services to, their clients,” said Guy Pearson, Chief Executive Officer and Co-Founder of Practice Ignition. “With COVID-19 accelerating these changes, PI enables our customers to quickly adapt the way they run their practice and interact with their clients to be as efficient as an eCommerce business.” The Practice Ignition team is building a global platform to optimize every aspect of the accountant and client relationship and transform the way accountants and their clients do business together. “The last 12 months have seen an acceleration in the number of accountants and bookkeepers moving away from multiple programs and tools to using a smart connected front-end system which can drive their back office,” added Pearson. “With the new product improvements we are bringing to market, we are excited to help our customers recover and grow through these challenging times where accountants and bookkeepers play such a vital role in helping businesses adapt and survive. We are pleased to work with CIBC Innovation Banking to help power our next phase of growth.” In 2019 and 2020, Practice Ignition was named in the Deloitte Fast 50 and is consistently ranked as one of the most loved apps by both Intuit and Xero customers. Practice Ignition is backed by existing investors Tiger Global Management, Right Click Capital and Equity Venture Partners. About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from startup to IPO and beyond. With offices in Atlanta, Austin, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About Practice Ignition Practice Ignition is the leading client engagement, revenue and payment platform for accountants. The software facilitates the creation of proposals, payments, insights, scope management, and integrations for accountants to engage with their clients. Practice Ignition supports over 225 thousand clients globally with offices located in Australia, the USA, UK, Canada and New Zealand.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210310005018/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609Jackie Brian, jackie.brian@practiceignition.com, +61 466348796"
https://venturebeat.com/2021/03/10/is-short-form-audio-the-next-new-thing-in-branded-audio-content-vb-live/,Is short-form audio the next new thing in branded audio content? (VB Live),"Presented by Dolby Labs Consumers are spending 30% more time listening to spoken word audio, making it a powerful way to reach customers. Do it right, and you increase engagement, loyalty, and more. Learn how to add audio to your marketing mix in this VB Live event. Register here for free. “Audio is aggressively on the rise right now in terms of where people are spending time, but most important, where they spend what I’d consider their intimate time,” says Will Mayo, founder and chief strategy officer at SpokenLayer. “Whether it’s music or narrative content, that’s the time when people are most themselves, when they’re elevating their mood, when they’re learning, when they’re growing.” The rise in popularity of audio content means that there’s an opportunity now for brands to tell their stories with branded content, and be part of that intimate experience in a way that wasn’t possible until recently, he adds. “What’s been super interesting to witness in the rise of branded audio is how quickly the uptake has been in the last two years,” says Sabba Keynejad, CEO, VEED.IO. “Video was at the top of every company’s marketing plan. Audio was probably not even on the list for a very long time. But more and more people have found ways to consume audio in convenient ways, since it’s become incredibly accessible. Standing out with good branded audio is essential.” It’s still quite early days for branded audio, though, Keynejad says, with best practices still being discovered and worked out. Brands are experimenting with a broad array of ideas. At VEED, they’ve seen a huge demand for creating branded audiogram videos that are distributed across social networks as a way to signal that you can access branded audio, podcasts, Clubhouse rooms, and so on. Most of what we consume on our screens is visual, and browsing often happens with the sound off. With branded audiograms, which combine a simple visual along with an audio message, brands have an easy way to engage browsers and send them to other channels to subscribe to more audio content. Short-form audio is also becoming more popular because of the prevalence of digital distribution via streaming and feed-driven content, Mayo says. You see this in music pervasively, where each user can experience a dynamic stream of content via services like Spotify or Pandora. “But in the world of narratives, the reason that Spoken Layer is so focused on short-form audio is we see that’s where the industry is going,” Mayo says. “We believe that short-form audio is more distributable, more reusable, more engaging, and can keep the attention of a user, because they can get an entire package in a two- to three- to four-minute experience, as opposed to 20, 60, or 90 minutes.” Right now, short-form audio is prevalent across the smart assistants — when you say ‘Hey Google’, or ‘Alexa, what’s the news’, short-form audio fills that experience. But now platforms like Spotify, and soon Pandora, are trying to diversify away from just music and become audio streaming companies, not just music streaming companies, so that people spend more time with their service. “You’ll start to see more short-form audio bubble up in those platforms, because again, you can create a personalized experience with that,” he says, “as opposed to content syndication, which is what I think podcasts function as — more of a syndicated medium, like a magazine, compared to an article that lives on many different apps.” The most important thing to remember with audio is that it’s not just an impression or a download or a single view of a piece of content, he adds. Audio has longer-term impact, but also typically it’s more important to expose a user with a higher frequency. “It’s not just run one ad and see what happens,” Mayo explains. “It’s telling a story over time, keeping that brand top of mind. It’s not buying radio spots or buying ads on Spotify but making sure that a brand is tied into anywhere and everywhere that someone may listen to content, and the opportunity in narrative short form is massive, because it’s completely untapped so far.” The social space is huge, with plenty of platforms and ways to access content, from Pinterest to Facebook to Instagram. But when it comes to audio, says Keynejad, we’re still working with a native podcast distribution system that hasn’t changed very much. The only real new entrant is Clubhouse. But that just means there’s so much opportunity as audio capabilities are extended across products and services in the social media ecosystem. Of course, with any audio content, consistency of quality is important. Whether it’s ensuring voices within the same content are not at different levels, or extracting key audio signals from noisy backgrounds, you don’t want to create a jarring experience for the listener. That said, while the opportunity is hot, says Mayo, this is the time to just start and iterate. “There is no right way to do it; the only thing that matters is getting in front of actual listeners and seeing how they respond,” he says. “Making a small investment now, just getting your feet wet, is by far the most important thing. Having the perfect copy, recording perfectly, finding perfect placement or targeting, that’s the enemy, because audio, again, is a fluid, intimate medium.” Learn more about using branded audio to create powerful, engaging content, from tips on creating the stickiest content to ways to ensure your audio messaging sounds great, every time, even when you’re not an audio expert, when you join this VB Live event. Don’t miss out! Register here for free. Attendees will learn: Speakers:"
https://venturebeat.com/2021/03/10/employee-engagement-startup-centrical-raises-32m/,Employee engagement startup Centrical raises $32M,"Centrical, a startup developing an employee engagement and performance management platform, today announced that it raised $32 million in funding co-led by Intel Capital and JVP. CEO Gal Rimon says that the funds, which bring Centrical’s total raised to date to $64 million, will be put toward product R&D as Centrical looks to acquire new customers. During the pandemic, as shelter-in-place orders and office closures force employees to work from home, companies are increasingly experimenting with or adopting work quality monitoring products. According to a June study by Gartner, 26% of HR leaders report having used some form of software or technology to track remote workers since the start of the health crisis. It’s an uptick driven in part by concerns over performance dips that could arise from work-from-home setups. A survey by global recruitment firm Robert Walters found that 64% of businesses are concerned about remote employees’ productivity. Centrical, founded in 2013 as GamEffective, sells companies access to a platform designed to let remote employees interact, train, and perform in hybrid workplaces. Managers can set personalized performance and learning goals for workers and provide them with real-time performance tracking, or engage them with challenges, levels, and prizes while offering prompts for learning opportunities. Centrical’s gamification software allows enterprises to incorporate engagement and recognition. In addition to personal challenges, admins can set tiered, prize-winning competitions or embed activities in quest-based game narratives. They can also create bite-sized “microlearning” activities that leverage an AI engine to recommend lessons to employees based on their knowledge and performance. While Centrical pitches its software as a noninvasive, voluntary solution for remote monitoring, privacy advocates are sounding the alarm over workplace software with real-time activity-tracking capabilities. Tracking tools like StaffCop, Teramind, and Hubstaff can record keystrokes, email, file transfers, apps used, and how much time employees spend on tasks, even taking screenshots to let managers know what’s on the employee’s screen. And in some cases, depending on the configuration, the tools can be installed without the knowledge of employees. For Centrical’s part, the company says that employees have access to their data from a centralized dashboard. Workers can view the same metrics as their managers, as well as the same AI-powered, closed-loop coaching with contextual evaluation forms, tips, and job resources. Centrical credits its customer success with this transparency. For example, Microsoft, a Centrical customer, reported a 10% increase in the productivity of its contact center representatives and a reduction of 12% in short-term absenteeism. Another client, Cellular Sales, Verizon’s largest authorized retailer, notched a 53% increase in new handset sales and two times higher combo sales of accessories using Centrical’s software. According to Rimon, Centrical doesn’t take screenshots of workers’ desktops or retain employee data. Data is held and managed by customers who use the platform and “no pertinent information captured [or] analyzed that would hold any value for marketing purposes.” “While this investment is gratifying – especially as it occurred in the midst of a pandemic — what all of us at Centrical are happiest about is all we were able to do for our customers during the past year,” Rimon said, adding that Centrical’s 130 customers have 100,000 users on the platform. “We helped them and their employees perform their best during a most difficult time by adding and enhancing features that brought even more value to our employee-centric success platform. Those ongoing efforts are what drives us and why our customers continue to be committed to us. I firmly believe those were key reasons we were secured the investment we did.” Beyond Intel Capital, C4 Ventures, Citi Ventures, and existing backers Aleph, CE Ventures, La Maison Compagnie d’Investissment, and 2B Angels participated in Centrical’s tranche announced today. Centrical, which has 115 employees across offices in Tel Aviv and New York, also plans to use the funding to launch products aimed at “aiding achievement in high-level corporate goals, crisis management, and other business agility-related activities,” Rimon said."
https://venturebeat.com/2021/03/10/how-snyk-targeted-developers-4-7-billion-cloud-security-giant/,How Snyk targeted developers to become a $4.7B cloud-security giant,"Success selling to enterprises typically means finding the right entry to connect with high-level decision-makers. But Snyk has built one of the fastest-moving security startups of the cloud-native era by entering through the back door with developers. That strategy continues to pay dividends, as evidenced by the company’s announcement today that it has raised $300 million in venture capital for a total of $470 million. Snyk’s valuation is now $4.7 billion, up 4 times since last year. Snyk is part of a broader movement to embed security into the very beginning of the development process and make defenses more robust. “What we’ve done is turn the model upside down by building security the same way you would do quality control for bugs,” Snyk CEO Peter McKay said. “You don’t wait until something goes into production to test quality issues in an application or performance or scalability. Our view is you should do security the same way. It’s part of quality control. The sooner you find the problem at the point of creation, the more cost-effective it is to address it.” Founded in 2015, Snyk’s platform provides security for cloud-native application development by using a vulnerability database that is maintained by a team of security experts. According to Snyk, about 43% of breaches can be traced back to vulnerabilities in applications. By using the platform, developers can more easily add security to the process as they build new features and applications. That has become even more urgent as enterprises expand their cloud investments, which can lead to a larger attack footprint and challenges related to scaling resources. Rather than selling to enterprise managers, the company established itself by going directly to developers with a free version of the product. “That became this bottoms-up motion,” McKay said. “We built our business around allowing developers to try it. You’re getting told you need to do security. Why wouldn’t you start using it? It’s free. So we started getting embedded into organizations and going viral across the companies.” From there, enterprises can shift to the paid version, which enables more collaboration and wider integration in the development tools being used across the software development lifecycle. Because most of the vulnerability technology remains in the backend, developers don’t need to become security experts to spot and address potential problems. In some cases, Snyk even automatically fixes the vulnerabilities, McKay said. This keeps the development process moving along while placing even more control in the hands of developers, which of course helps drive greater adoption of the platform. The company currently has 2.2 million developers in its community, but it is targeting all the estimated 28 million developers worldwide. The latest funding should help it advance toward that ambitious goal. The money was a mix of primary and secondary offerings, allowing employees and insiders to cash out some shares while also netting $175 million in new capital for Snyk. Accel and Tiger Global co-led the round, which included money from Addition, Boldstart Ventures, Canaan Partners, Coatue, GV, Salesforce Ventures, Stripes, BlackRock, Alkeon, Atlassian Ventures, Franklin Templeton, Geodesic Capital, Sands Capital Ventures, and Temasek. Snyk raised $150 million in January 2020, followed by another $200 million round in September. The company had also raised $22 million in 2018 and has been using that war chest to grow through acquisitions. Last year, it acquired DeepCode and Manifold. McKay said an IPO is on the roadmap, but the company hasn’t announced the timing. For now, Snyk has sufficient funding to pursue its strategy of organic growth through developers. “It’s a very maniacal focus on the developer experience by embedding the technology into the software development lifecycle,” McKay said. “It’s about developer productivity. We say ‘develop fast and stay secure.’ That’s kind of our motto.”"
https://venturebeat.com/2021/03/10/datagrail-raises-30m-to-help-enterprises-manage-data-privacy-requests/,"DataGrail, which helps enterprises manage data privacy requests, raises $30M","DataGrail, a data privacy startup that helps businesses navigate the growing array of privacy regulations around the world, has raised $30 million in a series B round of funding. Alongside the investment, the San Francisco-based company added a slew of enterprise customers to its roster, including Databricks, Dexcom, and Twilio. Founded in 2018, DataGrail constitutes part of a growing roster of platforms that are capitalizing on privacy regulations — such as GDPR in Europe and CCPA in California — by making it easier for companies to manage, automate, and grow their privacy programs. For example, many privacy regulations allow end users (e.g. customers or employees) to request access to their data under a right-of-access stipulation known as a data subject access request (DSAR). Fulfilling such requests can be a labor-intensive process involving collaborations between multiple teams for a single DSAR, given that the average enterprise uses between 50 and 100 different systems to store personal data. DataGrail enables businesses to automate many of the processes involved in complying with such requests. “Organizations rely on hundreds of interconnected apps and infrastructure — SaaS apps, data lakes, hosted databases — making it nearly impossible to find the personal data associated with a single person and comply with privacy laws,” DataGrail cofounder and CEO Daniel Barber told VentureBeat. “DataGrail allows businesses to automatically understand the apps and infrastructure they use [and] what personal information exists in those systems and provides the ability for their customer to control how their information is used.” DataGrail supports more than 900 prebuilt integrations across companies’ cloud apps and infrastructure, including AWS, Box, Dropbox, Stripe, Slack, Jira, Salesforce, and Okta. The company also claims to be able to detect personal data inside shadow IT systems — technology a company hasn’t officially approved for storing customer data. Companies can complete a DSAR in minutes using Request Manager, according to DataGrail, which instantly identifies where all the data is stored and unifies it in a single online dashboard. Moreover, DataGrail makes it possible for companies to automate the specifics of the request. For example, a customer may want to request access to their data or request that their data be deleted or restricted for sale. In terms of integrations, DataGrail said most of its customers can deploy it in less than 30 days. “The platform detects the apps and infrastructure in use and discovers the data within those systems via internally built no-code integrations,” Barber said. DataGrail claims a major selling point is that it doesn’t use any form of middleware for its integrations — it builds and maintains all the connectors and integrations itself. DataGrail had previously raised around $9 million, and its latest $30 million cash injection saw participation from a slew of existing and new investors, including Felicis Ventures, HubSpot, Okta Ventures, and Next47. The raise also comes amid a flurry of funding activity across the data privacy landscape, with Privacera yesterday announcing $50 million in funding and OneTrust and BigID recently attaining $5 billion and $1 billion valuations, respectively, after funding rounds. An increasingly competitive data privacy management world is also showing signs of consolidation, with OneTrust recently acquiring an AI startup called DocuVision that automatically finds and redacts sensitive data in documents. But DataGrail is hoping to set itself apart from the pack by providing what Barber calls a “complete solution.” “We see modern businesses struggle with the first generation of privacy vendors,” he said. “Existing tools have been bifurcated into either workflow or data discovery. DataGrail’s integration network of over 900 apps and infrastructure platforms provides the first integrated solution.”"
https://venturebeat.com/2021/03/10/optibus-launches-new-ai-platform-to-help-cities-plan-transit-routes/,Optibus launches new AI platform to help cities plan transit routes,"Real-time transit can be a dilemma for municipalities that lack scalable infrastructure. Tasks like routing, timing, and asset management are tough to optimize without a system for moving the necessary pieces into place. And with the number of heavy-duty transit buses expected to surpass 57,700 in countries like China by 2022 and congestion in some major cities on the rise, transit planning isn’t likely to become easier anytime soon. That’s where Tel Aviv, Israel-based Optibus aims to make a change. The startup, which emerged from stealth in 2014, spent three years developing the AI that powers its software-as-a-service transit operations product designed to help plot and schedule the movements of drivers and vehicles. To support the development of a new platform called the geospatial suite, which seeks to improve transportation networks through the use of data-driven planning, Optibus this week closed a $107 million series C round co-led by Bessemer Venture Partners and Insight Partners. Optibus was founded by Amos Haggiag and Eitan Yanovsky, who developed the company’s core technology in their spare time, working on weekends and evenings. Haggiag learned about some of the problems plaguing mass transit planning from his father, the CFO of one of the largest transit companies in Israel. The two friends developed a working prototype and sold it to several transit operators in Israel before acquiring more funding to grew Optibus’ product. The world is becoming increasingly multimodal where urban transportation is concerned. People don’t strictly stick to public buses and taxicabs any longer, if they ever did to begin with — according to a 2013 study by the American Public Transportation Association, nearly 70% of millennials use a combination of car-sharing, bike-sharing, walking, subways, and light rail to get from place to place. Optibus’ software attempts to account for this by providing recommendations to make cities’ transportation networks more passenger-friendly. It helps civil engineers to visualize geographic information while planning routes like the bike paths that riders may use on the way to or from public transit. Optibus also shows demographic data illustrating how route changes could affect residents’ access to public transit. And it handles workloads like generating bus timetables, rosters, and schedules that ostensibly cut costs while saving time. Optibus’ technology can also model constraints, operational preferences, and regulatory requirements across multiple vehicles, enabling fleet managers to explore what-if and worst-case scenarios in a simulated, centralized cloud environment. A dashboard lets operators define transit networks and set single or multiple routes, checkpoints, and other variables as they please. Moreover, they can view the projected impact on service and operating costs in real time and use AI algorithms to implement scheduling changes likely to increase on-time performance. By one estimate, the global market for AI in transportation is expected to reach $3.5 billion dollars by 2023. Beyond contributing to a reduction in the amount of fuel consumed by vehicles and thus carbon emissions, AI can improve the passenger experience by aligning routes with peak time usages, bus stop popularity, and more. For example, a September 2020 pilot program by Dubai’s Roads and Transport Authority found that AI-powered bus routes resulted in a 30-day reduction in the wasted time for bus traffic by 13.3% through the routes subject to the test. “Many people don’t realize just how complex a mathematical problem public transit poses. It’s actually one of the most complex types of mathematical problems, what’s known in computational complexity theory as NP-hard,” Haggiag told VentureBeat via email. “That’s why powerful algorithms are not just nice to have, but essential to quickly figuring out how to run the most efficient transit networks with the best service quality given all the constraints for the drivers and vehicles available for any given route on any given day at any given time, plus rules that a particular transportation provider needs to follow, like when and where drivers take their breaks or which depot a given vehicle needs to come from or return to.” Optibus powers transit operations in over 450 cities around the world, including New York, Los Angeles, London, Melbourne, Brasilia, Hong Kong, and Singapore. Annual recurring revenues is up six times from 2018 and over double compared with 2020, according to Haggiag. “We’re in this weird situation where machine learning has advanced so far and there’s so much data about where people are coming from and where they’re going, yet in many ways public transit is still stuck in this old-school space where spreadsheets, decades-old legacy software and even pen and paper is being used to try to address really complex mathematical problems,” Haggiag said. “By using algorithms and machine learning to optimize this data, and by using the distributed computing power of the cloud, we can create transportation networks that match the demand so much better and that make transit more accessible for billions of people around the globe.” Existing investors including Verizon Ventures, Pitango, New Era Capital Partners, Dynamic Loop, and Blue Red Partners contributed to Optibus’ latest tranche. To date, the company, which has around 150 employees, has raised a total of $160 million in capital."
https://venturebeat.com/2021/03/10/pegasus-tech-ventures-launches-us-50-million-fund-with-japanet-holdings/,Pegasus Tech Ventures Launches US $50 Million Fund with Japanet Holdings,"SAN JOSE, Calif.–(BUSINESS WIRE)–March 10, 2021– Pegasus Tech Ventures (“Pegasus”), a global venture capital firm based in Silicon Valley, announced a partnership with Japan’s television shopping giant Japanet Holdings (“Japanet”) to establish a $50 million corporate venture capital fund to invest in innovative startups around the globe. Japanet, headquartered in Nagasaki, Japan, is well-known for its home shopping services and now is expanding into other business sectors. The company recently started a new project to build “Stadium City” in Nagasaki, Japan, which will open in 2024. Centered on a sports stadium, the project includes the development of mixed-use facilities such as business offices, retail stores, hotels, and live event arenas, aiming to make it a Nagasaki landmark that will revitalize the local and regional community. Japanet is also committed to the enrichment of elderly life and to providing educational support for children around the globe. Through the fund with Pegasus, Japanet aims to create value by funding new ideas and collaborating with startups that develop innovative technologies. The fund plans to invest in Silicon Valley and other parts of North America, as well as in Israel, Europe and Asia. “We at Japanet Group would like to find high-potential startups with talented entrepreneurs and help them succeed and contribute real value to the world. Using a venture fund, we would like to find cutting edge startup technologies that we can use in the Nagasaki ‘Stadium City’ project, so that we can provide new services, such as child education, elder care, hospitality, and other services that can help with the betterment of people’s lives,” said Akito Takata, President and CEO of Japanet. “We will leverage our global network to introduce Japanet to top-tier startups,” said Anis Uzzaman, Founder and CEO of Pegasus Tech Ventures. “By making the connection, both the startups and Japanet benefit. Our goal is to create a global innovation platform that supports the growth of our portfolio companies and the success of our corporate partners.” Pegasus will deploy its team in Silicon Valley and in 15 other regions around the world to find the best startups and connect them with Japanet and Pegasus’ extensive network of corporate partners. Pegasus has established venture investment funds with over 35 global corporations, including Aisin Seiki, Sojitz Corporation, SEGA SAMMY HOLDINGS, Sunny Health, CAC Holdings Corporation, Teijin Limited, Infocom Corporation, Innotech Corporation, and ASUS, among others. About Japanet Holdings Co., Ltd. Based on a corporate vision, “The joy of living now,” Japanet operates a home shopping business, as well as a major sports related business designed to revitalize the local and regional community. In order to enrich the “now,” Japanet is also working to enhance healthcare services and to address other social issues, such as educational support for children around the world, and support for disaster-stricken areas. For more information, please visit the website https://corporate.japanet.co.jp/. About Pegasus Tech Ventures Pegasus Tech Ventures is a global venture capital firm based in Silicon Valley with $1.5 Billion in Assets Under Management. Pegasus offers intellectual and financial capital to emerging technology companies around the world. In addition to offering institutional investors a top-tier venture capital investment approach, Pegasus also offers a unique Venture Capital-as-a-Service (VCaaS) model for large, global corporations that wish to partner with cutting-edge technology startups. Some of the 35+ corporate partners that have signed up to work with Pegasus include ASUS, Aisin, SEGA, Sojitz, and Omron. These corporations are able to have access to 180+ Pegasus portfolio companies such as SpaceX, 23andMe, SoFi, Bird, Color, App Annie, and many more. To learn more, visit www.pegasustechventures.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210310005135/en/ Janice MokPegasus Tech Ventures408-645-5532janice@pegasusventures.com"
https://venturebeat.com/2021/03/10/aqua-security-protects-containerized-apps-and-infrastructure-raises-135m/,"Aqua Security protects containerized apps and infrastructure, raises $135M","Aqua Security today announced that it raised $135 million in series E funding at a post-money valuation exceeding $1 billion. The Boston- and Tel Aviv-based company plans to put the proceeds toward broadening its product portfolio and its geographic footprint. Containers, the packages of code and dependencies that run across computing environments, are riding a worldwide wave in popularity. According to a recent study from Nemertes, 45% of enterprises used containers by the end of 2018 compared with 21% in 2017. Meanwhile, Grand View Research reports that by 2025, the industry might reach $8.2 billion. CTO Amir Jerbi and CEO David Davidoff, who cofounded Aqua in 2015, aim to corner the cybersecurity portion of the container market. Aqua’s container-based app management platform runs on-premises or in the cloud and ensures that containers remain immutable, preventing changes versus their originating images. Despite the increased risk ushered in by the pandemic-related uptick in remote work, containerized software security practices remain uneven in the enterprise, surveys show. According to a 2019 Veritis report, 34% of respondents had no container security strategy or were still in the planning stages. Most saw user-originated misconfi­gurations, exposed dashboards, and potentially compromised metadata as their biggest challenges. Aqua aims to address this with a console from which multi-team, multi-customer environments can be managed either together or in isolation. The platform supports Active Directory and LDAP for permissions configuration and single sign-on and offers prebuilt integrations for third-party productivity and analytics tools. Vulnerability data, alerts, and audit events are logged in real time. And Aqua integrates with third-party secrets vaults, delivering encrypted tokens and private keys to containers at runtime and loading them in memory so they’re visible only to the containers that require them. Aqua’s runtime protection service, which secures apps predeployment through a command-line interface that orchestrates automated validation testing, leveraging a stream of vendor advisories and other sources. It scans up to thousands of images nodes for vulnerabilities, embedded secrets, configuration and permission issues, and malware. It affords admins the ability to monitor and control container activity based on custom policies and machine-learned behavioral profiles and to block activities and processes or limit container networking based on app contexts auto-detected by Aqua’s firewall configuration utility. Aqua’s competitors include Sysdig and Tigera, which recently raised $68.5 million and $30 million, respectively. There’s also Docker, the San Francisco startup behind the ubiquitous container toolkit of the same name. Among others are Twistlock, Capsule8, NeuVector, StackRox, Layered Insight, and Tenable. But Aqua’s $265 million war chest sets it apart from many of the rest, as does its impressive customer momentum. In 2020, the company doubled the number of paying clients and claims it now protects several of the world’s largest container production environments. Aqua also says that as of March 2021, it has half a dozen customers with an annual recurring revenue of over $1 million. New product launches are responsible in part for the growth. This past year, Aqua launched Aqua Wave, a software-as-a-service offering that works to secure apps as they’re built and the infrastructure they’re deployed on. The company also launched Aqua Enterprise, a version of Aqua’s enterprise offering that runs on top of its existing self-hosted version, with added capabilities for securing workloads at runtime. ION Crossover Partners led Aqua’s series E funding round announced today with participation from existing investors M12 Ventures (Microsoft’s venture arm), Lightspeed Venture Partners, Insight Partners, TLV Partners, Greenspring Associates, and Acrew Capital. Aqua previously raised $30 million in in a series D round closed in May 2020."
https://venturebeat.com/2021/03/10/exposed-admin-password-leads-to-massive-surveillance-camera-breach-at-hundreds-of-businesses/,Exposed admin password leads to massive surveillance camera breach at hundreds of businesses,"(Reuters) — A small group of hackers viewed live and archived surveillance footage from hundreds of businesses — including Tesla — by gaining administrative access to camera maker Verkada over the past two days, one of the people involved in the breach told Reuters. Swiss software developer Tillie Kottmann, who has gained attention for finding security flaws in mobile apps and other systems, shared with Reuters recordings from inside a Tesla factory in China and a showroom in California. Additional footage came from an Alabama jail, hospital rooms, a police interview area, and a community gym. Kottmann declined to identify other members of the group. The hackers sought to draw attention to the pervasive monitoring of people after having found login information for Verkada’s administrative tools publicly online this week, Kottmann said. Verkada acknowledged an intrusion, saying it had disabled all internal administrator accounts to prevent unauthorized access. “Our internal security team and external security firm are investigating the scale and scope of this issue, and we have notified law enforcement [and customers],” the company said. Kottmann said Verkada cut off the hackers’ access hours before Bloomberg first reported the breach on Tuesday. The hacking group, if it had chosen to, could have used its control of the camera gear to access other parts of company networks at Tesla and software makers Cloudflare and Okta, according to Kottmann. Cloudflare said its security measures are designed to block a small leak from becoming a wider intrusion and that no customer data was affected. Okta said it was continuing to investigate but that its service was not affected. Tesla did not respond to a request for comment. A list of Verkada user accounts provided by the hacking group and seen by Reuters includes thousands of organizations, including gym chain Bay Club and transportation technology startup Virgin Hyperloop. Reuters could not independently verify the authenticity of the list or screenshots distributed by Kottmann, but they included detailed data and matched other materials from Verkada. Madison County Jail in Alabama, Bay Club, and Virgin Hyperloop did not respond to requests for comment. Verkada says on its website it has over 5,200 customers, including cities, colleges, and hotels. Its cameras have proven popular because they pair with software to search for specific people or items. Users can access feeds remotely through the cloud. In a 2018 interview with Reuters, CEO Filip Kaliszan said Verkada had deliberately made it easy for many users at an organization to watch live video feeds and securely share them, such as with emergency responders. Verkada has raised $139 million in venture capital, with the latest financing announced a year ago valuing the Silicon Valley startup at $1.6 billion. Verkada drew scrutiny last year after Vice reported that some employees had used company cameras and its facial recognition technology to take and share photos of female colleagues. Kaliszan later described the behavior as “egregious” and said three people had been fired over the incident."
https://venturebeat.com/2021/03/10/gartner-75-of-vcs-will-use-ai-to-make-investment-decisions-by-2025/,Gartner: 75% of VCs will use AI to make investment decisions by 2025,"By 2025, more than 75% of venture capital and early-stage investor executive reviews will be informed by AI and data analytics. In other words, AI might determine whether a company makes it to a human evaluation at all, de-emphasizing the importance of pitch decks and financials. That’s according to a new whitepaper by Gartner, which predicts that in the next four years, the AI- and data-science-equipped investor will become commonplace. Increased advanced analytics capabilities are shifting the early-stage venture investing strategy away from “gut feel” and qualitative decision-making to a “platform-based” quantitative process, according to Gartner senior research director Patrick Stakenas. Stakenas says data gathered from sources like LinkedIn, PitchBook, Crunchbase, and Owler, along with third-party data marketplaces, will be leveraged alongside diverse past and current investments. “This data is increasingly being used to build sophisticated models that can better determine the viability, strategy, and potential outcome of an investment in a short amount of time. Questions such as when to invest, where to invest, and how much to invest are becoming almost automated,” Stakenas said. “The personality traits and work patterns required for success will be quantified in the same manner that the product and its use in the market, market size, and financial details are currently measured. AI tools will be used to determine how likely a leadership team is to succeed based on employment history, field expertise, and previous business success.” As the Gartner report points out, current technology is capable of providing insights into customer desires and predicting future behavior. Unique profiles can be built with little to no human input and further developed via natural language processing AI that can determine qualities about a person from real-time or audio recordings. While this technology is currently used primarily for marketing and sales purposes, by 2025 investment organizations will be leveraging it to determine which leadership teams are most likely to succeed. One venture capital firm — San Francisco, California-based Signalfire — is already using a proprietary platform called Beacon to track the performance of more than 6 million companies. At a cost of over $10 million per year, the platform draws on 10 million data sources, including academic publications, patent registries, open source contributions, regulatory filings, company webpages, sales data, social networks, and even raw credit card data. Companies that are outperforming are flagged up on a dashboard, allowing Signalfire to see deals ostensibly earlier than traditional venture firms. This isn’t to suggest that AI and machine learning are — or will be — a silver bullet when it comes to investment decisions. In an experiment last November, Harvard Business Review built an investment algorithm and compared its performance with the returns of 255 angel investors. Leveraging state-of-the-art techniques, a team trained the system to select the most promising investment opportunities among 623 deals from one of the largest European angel networks. The model, whose decisions were based on the same data available to investors, outperformed novice investors but fared worse than experienced investors. Part of the problem with Harvard Business Review’s model was that it exhibited biases experienced investors did not. For example, the algorithm tended to pick white entrepreneurs rather than entrepreneurs of color and preferred investing in startups with male founders. That’s potentially because women and founders from other underrepresented groups tend to be disadvantaged in the funding process and ultimately raise less venture capital. Because it might not be possible to completely eliminate these forms of bias, it’s crucial that investors take a “hybrid approach” to AI-informed decision-making with humans in the loop, according to Harvard Business Review. While it’s true that algorithms can have an easier time picking out better portfolios because they analyze data at scale, potentially avoiding bad investments, there’s always a tradeoff between fairness and efficiency. “Managers and investors should consider that algorithms produce predictions about potential future outcomes rather than decisions. Depending on how predictions are intended to be used, they are based on human judgement that may (or may not) result in improved decision-making and action,” Harvard Business Review wrote in its analysis. “In complex and uncertain decision environments, the central question is, thus, not whether human decision-making should be replaced, but rather how it should be augmented by combining the strengths of human and artificial intelligence.”"
https://venturebeat.com/2021/03/09/microsoft-urges-enterprises-to-act-quickly-to-secure-exchange-as-attacks-mount/,Microsoft urges enterprises to act quickly to secure Exchange as attacks mount,"The recently patched vulnerabilities in Microsoft Exchange that were being actively exploited by a state-sponsored threat group from China are now also being targeted by other groups in widespread attacks against enterprises. IT security teams need to prioritize the security updates for Microsoft Exchange and check their server logs for signs they may already be compromised. Microsoft released emergency security updates addressing four severe vulnerabilities in Microsoft Exchange Server software on March 2. Attackers could potentially use these vulnerabilities as part of an attack chain and create a web shell to hijack the server and execute commands remotely. Considering that Exchange, which provides email, calendar, and collaboration tools, lies at the “heart of the enterprise” — giants and small-to-medium-size businesses alike — an attack exploiting these vulnerabilities could cause a lot of damage to the victim organization, Dan Wood, associate vice president of consulting at security consultancy Bishop Fox, told VentureBeat. With remote code execution, attackers would be able to install backdoors on the server, steal email communications and other information stored in Exchange, create new users on the server, or use the foothold on the network to deploy malware onto other systems. Emails contain information like trade secrets and other sensitive data, and having them stolen, or publicized, can put a business at a tremendous disadvantage. The U.S. Department of Homeland Security’s Cybersecurity and Infrastructure Security Agency (CISA) issued an emergency directive ordering federal agencies to immediately analyze Microsoft Exchange servers to figure out if they had been compromised and to apply the security updates. The agency said it was “aware of threat actors using open source tools to search for vulnerable Microsoft Exchange Servers” and urged all organizations outside of the government to also prioritize the patches. Patching software promptly is a challenge because IT teams have to consider server downtime and compatibility issues with other applications. The situation with Exchange highlights the fact that attackers can take advantage of the delays to target the servers before the updates are applied. Even after the Exchange servers have been updated, IT security teams need to scan their logs for any unusual scanning activity against Exchange and other applications, Wood said. Microsoft has released a number of files to help enterprise defenders with their investigation, including the malware hashes and known malicious file paths observed in attacks, hashes for known good Exchange files, and a script to analyze the Exchange server against the list. The company also released mitigation guidelines for situations when patching right away would not be possible. The vulnerabilities impact on-premises Exchange Server 2013, Exchange Server 2016, and Exchange Server 2019. Microsoft’s software-as-a-service offerings such as Exchange Online and Office 365 are not affected. The fact that so many enterprises have already been affected indicates how many organizations are still relying on on-premises Exchange and not Microsoft’s cloud offering. To underscore the seriousness of the situation, Microsoft also released updates for older Exchange servers, even though they are technically end-of-life and would normally not receive any security updates. “This is intended only as a temporary measure to help you protect vulnerable machines right now,” Microsoft said. According to Microsoft, Hafnium, a state-sponsored attack group based in China, initially exploited the vulnerabilities in “limited, targeted attacks” against government targets. On March 5, Microsoft said the attacks were more widespread, with “increased use of these vulnerabilities in attacks targeting unpatched systems by multiple malicious actors beyond Hafnium.” Security writer Brian Krebs said approximately 30,000 organizations in the United States have been hacked so far, while Bloomberg estimated the figure to be closer to 60,000 organizations. Small to medium-size enterprises should be “greatly concerned” about the possibility their Exchange servers had been compromised and sensitive information lost, security company Cybereason CEO and cofounder Lior Div told VentureBeat. Many of them may not even know they have been breached, and applying the updates won’t be enough to protect them if the actors are already inside the networks. These SMEs may need help conducting the investigation, such as threat hunting and incident response, to determine their status, Div said. The impact will vary across different industries and different businesses. A health care company, bank, or municipality all have sensitive data on patients and/or customers, but the impact will depend on how the attack group decides to profit from the compromise. “Sensitive data loss presents a competitive disadvantage, threatens companies’ reputations, and can pose regulatory and legal implications,” Div said. IT security teams currently are dealing with a full plate of vulnerabilities. On top of the issues in Microsoft Exchange, there are patches to apply for VMware, Accellion, and IBM. IBM fixed a server-side request forgery vulnerability (CVE-2020-4786) in its IBM QRadar SIEM (security information and event management) platform. Attackers could have exploited the vulnerability to send requests and obtain information about the network infrastructure. Attackers would be able to move around the network and develop further attacks with information about network hosts and open ports. Enterprises with affected versions of IBM QRadar SIEM (7.4.2 GA to 7.4.2 Patch 1, 7.4.0 to 7.4.1 Patch 1, and 7.3.0 to 7.3.3 Patch 5) should update the product as soon as possible. In February, VMware fixed a flaw in vSphere Client, a plugin of VMware vCenter. Enterprises typically rely on vCenter as a centralized management utility to manage VMware products installed on local workstations. An attacker could potentially target the HTTPS interface of the vCenter plugin and execute malicious code with elevated privileges on the device without having to authenticate, which means the attacker would be able to access any system that’s connected or managed through the central server. VMware assigned a severity score of 9.8 out of a maximum of 10. Shortly after proof-of-concept code for this flaw became public, threat intelligence company Bad Packets noticed mass scan activity looking for vulnerable vCenter systems. This is a sign that attack groups are hurrying to compromise vulnerable machines before the enterprises get around to patching them. Firewall vendor Accellion released patches in late December and January to address vulnerabilities in its File Transfer Appliance, which is used to move large and sensitive files within a network. The flaws have been used in attacks against dozens of companies and government organizations worldwide by two attack groups, the cybercrime group FIN11 and the ransomware group Clop. The ransomware group has threatened to publicly dump the data they’ve locked up if the victims don’t pay the ransom. “Worldwide, actors have exploited the vulnerabilities to attack multiple federal and state, local, tribal, and territorial government organizations as well as private industry organizations including those in the medical, legal, telecommunications, finance, and energy sectors,” a CISA statement warned. “In some instances observed, the attacker has subsequently extorted money from victim organizations to prevent public release of information exfiltrated from the Accellion appliance.” Accellion was planning to end support for FTA on April 30, but is working with customers who have been compromised, CEO Jonathan Yaron said in a statement. The company has been encouraging customers over the past three years to switch from FTA to the newer Kiteworks platform, but moving away from legacy network equipment typically takes a very long time. Some organizations may never make that switch, which means the number of victims may keep rising."
https://venturebeat.com/2021/03/09/5-jobs-that-you-should-probably-apply-for-right-now/,5 jobs that you should probably apply for right now,"Can you believe it’s March already? We’re well and truly in the thick of 2021 — and so far, things have been pretty good. Vaccines are rolling out, and freedom feels well within our grasp. With this new-found sense of positivity and the hope for freedom, we can think of nothing better than putting some focus on personal changes. For so many of us, that might well mean a new job. There’s no time like the present, is there? If you are looking for a new and exciting role, then you have to check out this fantastic lineup of jobs. Checkout is looking for a Partner Growth Manager to join the Commercial Partnerships team in their fast-growing San Francisco office. You will be responsible for managing and growing the company’s platform partnerships in the Americas. Reporting directly to the Partnerships Lead, you’ll have ownership over Checkout.com’s most strategic platform partners and abundant opportunity to grow your scope and career through demonstrated impact. This role is highly cross-functional, where you’ll work with some of the brightest minds in payments and fintech across product, engineering, sales, and marketing. With a combination of strategy, scrappiness, and grit, you’ll develop relationships with influential e-commerce players and design innovative partner programs to bring best-in-class solutions to customers. Open Dealer Exchange (ODE), an affiliate of Reynolds and Reynolds, is looking for a Project Coordinator to join their team in Farmington Hills, Michigan. As a Project Coordinator, you will work closely with Project Managers and Product Owners to assist in the estimation of projects and administer to the priorities set by the business. To ensure accountability and efficient project delivery, you will assist in defining and implementing project management processes. The successful candidate will support the Project Manager with administrative tasks and development of procedures while ensuring projects adhere to frameworks and all documentation is maintained appropriately and up to date. They will assess project risks and monitor interruptions to the plan while measuring project performance against baseline to identify areas for improvement. The State Street team is looking for a qualified professional with strong experience in project management, analysis, system flows and business architecture to support the industry’s ever front-to-back client servicing platform. The successful candidate must be highly organized and be able to move seamlessly between multiple clients and phases within the sales, due diligence and implementation lifecycles. The team you will be joining is a part of a global, cross-divisional group supporting State Street AlphaSM. If making your mark in an ever-changing, increasingly complex and competitive industry is a challenge you are up for, then this might just be the role for you! The Senior Project Manager will join the Integration Management Office (IMO) for all Moody’s Analytics M&A initiatives. The Sr. Project Manager will be involved in coordinating the activities related to due-diligence and lead-to-cash integration of acquisition targets, with a focus on pricing strategy, financial data, operations. The role will require coordinating a cross-functional team that includes SMEs, technology teams and management across MA, Finance and the acquisition targets. They will collect information on acquisition targets and complete due-diligence questionnaires, focusing on contract review, sales data and product and pricing. Moody’s is looking for someone with 6-8 years of related project management experience, with a heavy focus on business process. They also require someone with excellent interpersonal and communication skills and exceptional organizational and planning ability, managing multiple concurrent projects. You will join a fast-growing team of UX / Product Experience Designers to help with the company mission of elevating Zwift’s core experiences across game, app and web. As a Senior UX designer you will be responsible for experience designs in and around the game that are both complex and ambiguous. You will work within a cross functional agile team and across platforms to create in-depth end-to-end product experiences that have a deep impact on the users and are aligned with business needs. You will work closely and collaboratively with other teams, stakeholders and designers across the organization to achieve these goals. Through these efforts, Zwift will deliver a user experience that becomes a point of competitive differentiation. Still looking for the right role? Head over to VentureBeat Jobs now!"
https://venturebeat.com/2021/03/09/researchers-find-that-large-language-models-struggle-with-math/,Researchers find that large language models struggle with math,"Mathematics is the foundation of countless sciences, allowing us to model things like planetary orbits, atomic motion, signal frequencies, protein folding, and more. Moreover, it’s a valuable testbed for the ability to problem solve, because it requires problem solvers to analyze a challenge, pick out good methods, and chain them together to produce an answer. It’s revealing, then, that as sophisticated as machine learning models are today, even state-of-the-art models struggle to answer the bulk of math problems correctly. A new study published by researchers at the University of California, Berkeley finds that large language models including OpenAI’s GPT-3 can only complete 2.9% to 6.9% of problems from a dataset of over 12,500. The coauthors believe that new algorithmic advancements will likely be needed to give models stronger problem-solving skills. Prior research has demonstrated the usefulness of AI that has a firm grasp of mathematical concepts. For example, OpenAI recently introduced GPT-f, an automated prover and proof assistant for the Metamath formalization language. GPT-f found new short proofs that have been accepted into the main Metamath library, the first time a machine learning-based system contributed proofs that were adopted by a formal mathematics community. For its part, Facebook also claims to have experimented successfully with math-solving AI algorithms. In a blog post last January, researchers at the company said they’d taught a model to view complex mathematical equations “as a kind of language and then [treat] solutions as a translation problem.” “While most other text-based tasks are already nearly solved by enormous language models, math is notably different. We showed that accuracy is slowly increasing and, if trends continue, the community will need to discover conceptual and algorithmic breakthroughs to attain strong performance on math,” the coauthors wrote. “Given the broad reach and applicability of mathematics, solving math datasets with machine learning would be of profound practical and intellectual significance.” To measure the problem-solving ability of large and general-purpose language models, the researchers created a dataset called MATH, which consists of 12,500 problems taken from high school math competitions. Given a problem from MATH, language models must generate a sequence that reveals the final answer. Problems in MATH are labeled by difficulty from 1 to 5 and span seven subjects, including geometry, algebra, calculus, statistics, linear algebra, and number theory. They also come with step-by-step solutions so that language models can learn to answer new questions they haven’t seen before. Training models on the fundamentals of mathematics required the researchers to create a separate dataset with hundreds of thousands of solutions to common math problems. This second dataset, the Auxiliary Mathematics Problems and Solutions (AMPS), comprises more than 100,000 problems from Khan Academy with solutions and over 5 million problems generated using Mathematica scripts based on 100 hand-designed modules. In total, AMPS contains 23GB of content. As the researchers explain, the step-by-step solutions in the datasets allow the language models to use a “scratch space” much like a human mathematician might. Rather than having to arrive at the correct answer right away, models can first “show their work” in partial solutions that step toward the right answer. Even with the solutions, the coauthors found that accuracy remained low for the large language models they benchmarked: GPT-3 and GPT-2, GPT-3’s predecessor. Having the models generate their own solutions before producing an answer actually degraded accuracy because while many of the steps were related to the question, they were illogical. Moreover, simply increasing the amount of training time and the number of parameters in the models, which sometimes improves performance, proved to be impractically costly. (In machine learning, parameters are variables whose values control the learning process.) This being the case, the researchers showed that step-by-step solutions still provide benefits in the form of improved performance. In particular, providing models with solutions at training time increased accuracy substantially, with pretraining on AMPS boosting accuracy by around 25% — equivalent to a 15 times increase in model size. “Despite these low accuracies, models clearly possess some mathematical knowledge: they achieve up to 15% accuracy on the easiest difficulty level, and they are able to generate step-by-step solutions that are coherent and on-topic even when incorrect,” the coauthors wrote. “Having models train on solutions increases relative accuracy by 10% compared to training on the questions and answers directly.” The researchers have released MATH and AMPS in open source to, along with existing mathematics datasets like DeepMind’s, spur further research along this direction."
https://venturebeat.com/2021/03/09/itopia-names-al-monserrat-chairman-of-the-board/,itopia Names Al Monserrat Chairman of the Board," Technology Industry Veteran to Lead Important New Phase of itopia Growth  MIAMI–(BUSINESS WIRE)–March 9, 2021– itopia, the leading cloud automation and orchestration solution for Google Cloud, today announced the appointment of Al Monserrat as Chairman of its Board of Directors. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210309005145/en/  Al Monserrat is the new Chairman of the Board at itopia. (Photo: Business Wire) Monserrat’s career spans more than twenty-five years in large enterprises and entrepreneurial ventures within enterprise software, mobility, cloud, digital payments, and business strategy. He first joined the itopia Board of Directors as a member in November 2019. He is also a board member of Seacoast Bank and has served on other technology company boards including, HYCU and Auxis. Previously, Monserrat was President, Imaging Division, of Nuance Communications, taking the Division through its acquisition by Kofax, Inc. Before Nuance, he was CEO of RES Software (acquired by Ivanti), which he joined after a fifteen-year career at Citrix Systems, Inc., where he was part of the executive leadership team that grew the company from hundreds of millions to more than $3 billion in revenue in 2014. “When I joined the itopia Board in 2019,” said Monserrat, “I could see the company was well positioned for a new phase of rapid growth. But none of us could foresee the extraordinary events of 2020 and their impact on the global workforce. Empowering companies to rapidly stand up remote and hybrid workforces, powered from the Cloud, is a lifeline for the enterprise. And the changes we’ve seen to how people engage with workplace technology have been permanently transformed. itopia has the right customer value proposition at the right time, and I’m proud to take up leadership of its Board.” “Al is one of the industry greats, and enormously respected for his track record, intelligence, personal integrity and leadership. He understands the value of our solutions, and I look forward to working side by side with him as itopia accelerates its growth and better serves our customers. His leadership is critical to us as we rapidly expand our product portfolio,” said Jonathan Lieberman, CEO and Co-founder of itopia. About itopia itopia is a comprehensive cloud orchestration and automation platform for Google Cloud Platform, simplifying IT management for desktops, apps, compute, storage, networking, and security. Enterprises use the itopia platform to facilitate increased productivity and efficiently manage distributed teams of employees. Current customers include some of the leading companies in healthcare, education, Call Center/BPO, telco, and manufacturing.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210309005145/en/ itopia
Alex Shapero
949-910-1628ashapero@itopia.com www.itopia.com"
https://venturebeat.com/2021/03/09/splice-machine-3-1-enhances-support-for-real-time-ai-deployments/,Splice Machine 3.1 enhances support for real-time AI deployments,"Splice Machine, a startup that offers offline and batch analysis tools to power intelligent apps for operational workflows, today launched version 3.1 of its platform. Splice Machine 3.1 introduces new features and functionality to support enterprises with real-time AI projects, including resource elasticity support on Kubernetes, GPU support, and extensions to Spark’s machine learning libraries. Most companies struggle to develop working AI strategies. According to a recent survey by Rackspace, only 20% of enterprises report having mature AI and machine learning initiatives. Indeed, while Deloitte says 62% of respondents to its corporate October 2018 report deployed some form of AI, roughly 25% of companies see half their AI projects ultimately fail. Splice says that version 3.1 of its product — which combines database and AI technologies — addresses some of the challenges data science teams encounter while training, validating, and deploying AI systems. For example, it introduces native Spark structured streaming ingestion, a feature that makes streaming resources ostensibly easier to ingest than before. And Splice Machine 3.1 adds new database capabilities including foreign key processing, richer trigger support and improved handling, indexes on expressions, and improved import and export capabilities, as well as DB2 compatibility. Splice cofounder and CEO Monte Zweben says that the streaming ingestion capability should prove especially useful for industrial accounts connected to distributed control systems, where it’s essential to ingest data in real time as it becomes available. “With 3.1, we have made vital leaps in database capabilities,” Zweben said. “[They’ll] successfully operationalize real-time AI applications and bring machine learning models into production.” Splice Machine 3.1 also aims to increase the transparency around the data used to create AI and machine learning models at scale. A new feature enables developers to query a database back in time with syntax to a specific date, providing an audit and lineage for a regulator checking for bias or data drift. It builds on Livewire, a product Splice launched last November that draws on live sensor data to predict operational problems to avoid outages and keep machinery up and running. A number of enterprises in the process of adopting AI struggle with the key stages of data collection, preprocessing, and prep. A recent study found that less than 4% of companies report that the data used to train their AI systems presented no problems, with most data-related problems stemming from how the data was being produced and labeled internally. Biases or errors in the data and a lack of sufficient resources topped the list of data management problems with which businesses most often struggled. “We are excited to be powering data engineers and data scientists with the tools they need,” Zweben added. “[We believe they’ll] break down the chasms that stop machine learning and AI projects from being successful.” Splice Machine 3.1 is available as a fully managed cloud service on Amazon Web Services, Azure, and Google Cloud Platform and is also available on-premises."
https://venturebeat.com/2021/03/09/cibc-innovation-banking-provides-jmi-equity-with-a-capital-call-line-of-credit/,CIBC Innovation Banking Provides JMI Equity with a Capital Call Line of Credit,"NEW YORK–(BUSINESS WIRE)–March 9, 2021– CIBC Innovation Banking is pleased to announce that it has co-led a capital call facility to JMI Equity’s latest fund, JMI Equity Fund X. The capital call facility provides JMI Equity (“JMI”) with the flexibility to make investments in portfolio companies and call capital less frequently from the Fund’s limited partners. Since the firm’s founding in 1992, JMI has raised over $6 billion of committed capital and continues to invest in software companies with proven business models, rich intellectual property, high recurring revenue, and long-term growth potential. The firm has completed over 155 investments, over 105 exits, and 19 IPOs. Typical investments range from $20 million to $200 million with flexibility in size, form, and structure of investments. “We are pleased to provide JMI with a capital call facility for JMI Equity Fund X. Our team is looking forward to working with JMI as it continues its growth trajectory.” said Tej Sahi, Managing Director, in CIBC Innovation Banking’s New York office. About CIBC Innovation Banking CIBC Innovation Banking delivers strategic advice, cash management and funding to North American innovation companies at each stage of their business cycle, from start up to IPO and beyond. With offices in Atlanta, Austin, Chicago, Denver, Menlo Park, Montreal, New York, Reston, Toronto and Vancouver, the team has extensive experience and a strong, collaborative approach that extends across CIBC’s commercial banking and capital markets businesses in the U.S. and Canada. About JMI JMI Equity is a growth equity firm focused on investing in leading software companies. Founded in 1992, JMI has invested in over 155 businesses in its target markets, successfully completed over 105 exits, and raised more than $6 billion of committed capital. JMI partners with exceptional management teams to help build their companies into industry leaders. For more information, visit www.jmi.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210309005051/en/ Katarina Milicevic, katarina.milicevic@cibc.com, 416-586-3609"
https://venturebeat.com/2021/03/09/oracle-focused-on-improving-database-speeds-during-logfire-transition/,Oracle focused on improving database speeds during LogFire transition,"When Oracle acquired LogFire, everyone at the company knew there would be changes. LogFire managed the warehouses for hundreds of ecommerce shops, which meant juggling hundreds of PostgreSQL databases. Oracle planned at least two major shifts: integrating LogFire’s ecommerce backend tools with Oracle’s collection of web services and moving the data storage work to Oracle’s own Autonomous Database. The legal paperwork for the acquisition was finalized in September 2016, and LogFire’s product is now known as Oracle Supply Chain Management Cloud. The database transition — moving from PosgreSQL to Oracle Autonomous Data — took a bit longer and was finally completed about 18 months ago. Oracle touts the transition as an experiment illustrating the value of its offerings. The database giant has crunched the numbers and found that its cloud supports twice as many customers at speeds 55% faster. Moreover, the automation meant the contractors who used to handle all of the manual database chores were no longer necessary. Before the acquisition, LogFire relied on PostgreSQL databases on Rackspace hardware to track order processing for each customer. The team of contractors handled all the tasks related to patching, scaling, and failover planning and were “on call” to fix problems caused by outages. While each of these chores seems small on its own, together they required a team of 10 database administrators. Oracle said the switch to the Autonomous Databases finished just before the pandemic hit and was essential for managing the order spikes that coincided with people staying home and depending on ecommerce shops to get the things they needed. “They [database administrators] could add capacity whenever they wanted without downtime,” explained Oracle product management VP George Lumpkin. “They don’t need the DBA to resize their database. They don’t need to budget for or add capacity. They can add capacity on demand.” The transition also involved moving the databases to Oracle’s own cloud with the “purpose-built” Exadata machines, which was a big factor in the faster response times Oracle reported. “They [LogFire’s servers] moved from Postgres running on fairly generic virtual machines onto a highly optimized hardware platform with significant flash and optimized IO,” Lumpkin said. “It’s [Exadata machine] really a hardware system built for Oracle databases.” What can other businesses expect? This largely depends upon the consistency of the database load. Ecommerce shops tend to see bigger loads during the holiday months. Some streaming services see increased demand at the end of the week. These fluctuations make work for DBAs that can be eased with automation. For smaller companies with only a few databases, the effect of switching to Oracle Autonomous Data may not be as noticeable as it was for LogFire. The database chores normally land on some developer’s lap to be interleaved with other work. “It’s the difference between doing everything manually,” explained Steve Zivanic, global VP of database and autonomous services. “You had to do manual tuning of the databases. You had to do manual backups. You had to essentially make sure you had enough processing power. All of that is taken care of so you can focus on the business at hand.”"
https://venturebeat.com/2021/03/09/amri-earns-spot-on-forbes-americas-best-employers-2021-list/,AMRI Earns Spot on Forbes America’s Best Employers 2021 List,"ALBANY, N.Y.–(BUSINESS WIRE)–March 9, 2021– Albany Molecular Research, Inc. (AMRI), a leading global provider of advanced contract research, development and manufacturing solutions, today announced that the company is named on “America’s Best Mid-Sized Employers 2021″ list published by Forbes. AMRI was included in the Drugs and Biotechnology industry category of the ranking. Its 3,100 employees provide complex scientific solutions across the R&D and manufacturing continuum to pharmaceutical and biotechnology customers, helping improve patients’ lives. AMRI launched its global Learning Initiatives for Employees program, known as LIFE@AMRI, in 2020. LIFE@AMRI offers a comprehensive development and performance framework, providing tools and opportunities for employees across the organization. The program is part of AMRI’s people excellence strategy, focused on building a diverse, inclusive and high-performing culture. “We constantly strive to create a workplace that is in keeping with the high caliber of our employees and the importance of our patient-centered mission,” said John Ratliff, CEO, AMRI. “Recognition in the Forbes ranking this year is testament to our extraordinary team who, despite the profound challenges the world faced with the COVID-19 pandemic, continued to improve and deliver. I extend my deep gratitude to the AMRI community for your expertise, dedication and agility.” America’s Best Employers 2021 is presented by Forbes and Statista Inc. Companies are selected through an independent survey applied to a sample of more than 50,000 American employees working for companies with more than 1,000 employees in America. About AMRI AMRI, a contract research development and manufacturing organization, partners with the pharmaceutical and biotechnology industries to improve patient outcomes and quality of life. AMRI’s team combines scientific expertise and market-leading technology to provide a complete suite of solutions in discovery, development, analytical services, and API and drug product manufacturing. Learn more at www.AMRIGlobal.com.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210309005021/en/ Jane ByramSCORR Marketing512-626-2758jane@scorrmarketing.com"
https://venturebeat.com/2021/03/09/privacera-raises-50m-to-help-companies-manage-compliance-and-security-risk-across-clouds/,Privacera raises $50M to help companies manage compliance and security risk across clouds,"Privacera announced it has raised $50 million in its quest to tackle the data privacy and security challenges faced by large enterprises. About 64% of respondents to a global Thales survey feel that adhering to compliance requirements is a “very” or “extremely” effective way to keep data secure. But compliance is expensive for enterprises. In a 2017 PricewaterhouseCoopers survey of execs at U.S., U.K., and Japanese tech companies, 88% said their company planned to spend over $1 million preparing for the EU’s General Data Protection Regulation (GDPR) in the run-up to its full May 2018 implementation. A smaller percentage of respondents — 40% — said they expected to spend $10 million or more. Cofounded by Balaji Ganesan and Don Bosco Durai in July 2016, Fremont, California-based Privacera enables customers to automate data discovery and perform policy management at “petabyte” scale. Ganesan previously cofounded XA Secure with Durai to build a centralized access control tool for Hadoop, the framework that allows for the distributed processing of large datasets across clusters of computers. Leveraging this experience, Ganesan and Durai turned their sights to the cloud and created Privacera, developing a software-as-a-service-based data and security management solution capable of integrating compliance across multiple clouds. It’s safe to say that compliance management is a red-hot sector. Last year, San Francisco-based TrustArc raised a $70 million round of funding to help companies implement privacy and compliance programs; Privitar nabbed $40 million to better enable businesses to engineer privacy protection into their data projects; and InCountry exited stealth with $7 million in seed funding to help multinationals comply with local data residency regulations. Back in 2018, BigID nabbed $30 million to expand its data privacy management platform for enterprises. At the end of 2019, LogicGate, which provides a product that automates processes and compliance tracking, raised $24.75 million to invest in content, frameworks, data partnerships, and integration. And in December, OneTrust raised $300 million for its data governance and compliance automation platform. Privacera claims its platform is differentiated in that it enables analytics teams to access data across on-premises and multiple cloud services without compromising compliance with regulations like the EU’s GDPR, the California Consumer Privacy Act, Brazil’s LGPD, and the U.S.’ HIPAA. Privacera allows companies to respond to data subject access requests, as well as any requests to delete data. Moreover, it supports unified authorization and authentication to meet requirements for multitenant data access by customer, subcontractor, contractual terms, franchise, or department. According to Ganesan, Privacera leverages an open source AI and machine learning library for natural language processing to automate the discovery of personally identifiable data. “Ongoing digital transformation programs force companies to be more data-driven and, as a result, there is even more pressure to migrate to the cloud and make data accessible to data analytics teams,” he told VentureBeat via email. “The pandemic drove more consumers online who are becoming more educated on how and where their personal data is being utilized and possibly exploited. This places even more pressure on organizations to control data privacy locally — as evidenced by … the latest privacy regulations proposed in Virginia and globally.” Perhaps unsurprisingly, the market for cloud security and compliance products is huge, expected to reach $12.63 billion by 2024, according to a report by Grand View Research. In a December 2020 Hyperproof survey of over 1,029 IT decision makers, 54% of respondents said they anticipated spending more money this year on IT risk management and compliance, while 86% said they were preparing for the potential passage of a federal data privacy and security law in the U.S. in the next few years and had factored this into their compliance budget. Privacera today announced that it raised $50 million in a series B funding round led by new investor Insight Partners, with participation from Sapphire Ventures, Battery Ventures, and existing investors Accel, Cervin, and Point 72, bringing its total raised to date to $63.5 million. The 130-employee company, which notched 2.5 times annual recurring revenue growth in 2020, says the funds will be used to support its go-to-market strategy as well as product research and development. “We help organizations use data effectively and responsibly, so they remain compliant with an ever-growing number of regulations,” Ganesan said. “The latest investment will ensure we continue to deliver on our mission of accelerating data democratization by enabling IT and data teams to easily empower analytical teams with the data they need.”"
https://venturebeat.com/2021/03/09/traject-acquires-sendible-adding-to-a-growing-portfolio-of-leading-social-media-software-solutions/,"Traject Acquires Sendible, Adding to a Growing Portfolio of Leading Social Media Software Solutions","BELLEVUE, Wash. & LONDON–(BUSINESS WIRE)–March 9, 2021– Traject, a suite of innovative digital marketing tools backed by ASG, has acquired Sendible, a market-leading social media management platform that empowers agencies and marketers to collaborate and manage multiple brands easily so they can dedicate more time to creating authentic stories and building stronger brands. This press release features multimedia. View the full release here: https://www.businesswire.com/news/home/20210309005179/en/ Sendible is the fourth social media company to join the Traject portfolio and the 10th overall. Sendible Founder Gavin Hammar will stay in the business for a period of time to help with the transition. The team has welcomed former Traject VP of Sales & Marketing, Katelyn Sorensen as their new CEO. “Sendible has become a market leader because of Gavin’s vision, the team he’s brought on, the product they have built, and the company’s relentless focus on the customer,” says Katelyn Sorensen, new CEO of Sendible. “The social media space is only continuing to grow. I’ve been a fan of Sendible from afar for a long time. I’m beyond excited to join an exceptional team and be part of this next phase of our innovation.” Sendible was founded in 2009 when Founder Gavin Hammar started the company from his spare bedroom in London, using a server he’d purchased for £10. In a time when Instagram wasn’t yet created, and social media functioned like a distribution network, Gavin set out with the idea to make sending anything possible – links, blogs, etc. As the social media space has grown, so did Sendible, eventually creating a robust management tool for agencies and brands. Today, Sendible has grown to 47 employees serving 20,000+ users across 125 countries. “Sendible started from humble beginnings and has gone on to be recognised as a market leader, recently listed as one of the top 20 marketing products of 2021. But as a bootstrapped company, we’ve always felt we could do more,” said Gavin Hammar, Founder of Sendible. “We are thrilled to be joining the Traject family, whose deep expertise in the Martech space will enable us to accelerate towards our mission so we can bring even more value to our customers.” The Traject suite includes: reputation and review management software Grade.us and Gatherup, SEO software AuthorityLabs and Traject Data, BI software Cyfe, ecommerce marketing software Traject SKU, social media management software Fanbooster, and the most recent addition – PLANOLY, a visual planner for social media. “Sendible and Traject share a similar strategy and vision, as well as a deep understanding of the social media space and the agency customer,” says Alice Song, CEO of Traject. “I’m excited to welcome Sendible into the Traject suite. The combination of experience, knowledge, and customer focus positions Traject and Sendible well for this next phase of growth.” Sendible users can continue to expect the same exceptional, customer-centric product and service, now with the additional support of the Traject portfolio. About Sendible Sendible is a leading social media management platform built to give brands a voice on social media. Sendible allows agencies to manage clients and social profiles at scale, across multiple platforms including Facebook, Instagram, and Twitter from one easy location. Over 20,000+ agencies and marketers rely on Sendible to breathe life into their brands and truly make an impact through storytelling. Sendible strongly believes in doing whatever it takes to empower our customers to be successful. To learn more, visit www.sendible.com or find us on social media @sendible. About Traject Traject is an innovative marketing technology company that exists to help digital agencies, brands, and marketers grow. Combining eight industry-leading brands that serve hundreds of thousands of users, the Traject suite provides tools to increase our customers’ visibility online across all channels. From reviews, to SEO, to social media, and more. We are passionate. We are optimistic. We are people first. And in the end, we are all about you. To learn more, visit www.bytraject.com or follow @bytraject on Twitter or Instagram. About ASG ASG, backed by Alpine Investors, is a unique and fast-growing software business that buys, builds, and operates market-leading vertical SaaS companies. ASG believes deeply in the power of people and data to grow great organizations, and that sharing knowledge, expertise, and resources across its community of businesses drives exponential growth. Founders of leading SaaS companies continue to trust ASG to grow their businesses and build even stronger legacies for the future.  View source version on businesswire.com: https://www.businesswire.com/news/home/20210309005179/en/ Megan LandASGmland@alpinesg.com"
https://venturebeat.com/2021/03/09/dropbox-to-acquire-document-sharing-platform-docsend-for-165m/,Dropbox acquires document sharing platform DocSend for $165M,"Dropbox has announced plans to acquire DocSend, a secure document sharing and tracking platform, in an all-cash deal worth $165 million. Founded out of San Francisco in 2013, DocSend helps businesses bypass cumbersome email attachments through a link-based document sharing approach. This enables companies to control file downloads and deactivate access at any time while also gleaning real-time engagement insights. Moreover, the DocSend platform allows companies to ensure that the version of a file they have shared remains up to date. Dropbox has been doubling down on its enterprise focus of late and pushing tighter integrations with other business-focused services, including Salesforce. Last year, Dropbox unveiled a partnership with Google to allow G Suite users to store their files in Dropbox. DocSend actually represents the company’s second acquisition in as many years, after it shelled out $230 million for e-signature startup HelloSign in 2019. DocSend and HelloSign are both concerned with the remote management and distribution of documents — in fact, DocSend already offers built-in e-signature functionality. Dropbox cofounder and CEO Drew Houston said in a press release that the plan is to package Dropbox, DocSend, and HelloSign together as an “end-to-end suite” of products spanning collaboration, sharing, and e-signatures to help businesses “manage critical document workflows from start to finish.” It’s becoming clear that Dropbox has access to the data it needs to determine which products in its ecosystem are most valuable as fully integrated features. Dropbox has offered an integration with DocSend since 2019, after expanding its extensions program to support numerous business apps. The extensions program originally launched in 2018 with integrations that included HelloSign, a startup Dropbox snapped up just two months later. DocSend already claims some 17,000 customers, including a number of notable enterprise clients, such as Airtable and Gartner, which could give Dropbox an easier inroad as it looks to cross-sell and upsell its suite of products. Nothing much will change for DocSend until the acquisition closes later this month, and Dropbox hasn’t confirmed what will happen to DocSend as a standalone product post-acquisition. “Our goal is to accelerate adding value through additional DocSend functionality as well as deeper integrations with Dropbox,” a spokesperson told VentureBeat. “We’ll have more to share in the future.”"
https://venturebeat.com/2021/03/09/cube-software-raises-10-million-to-take-on-legacy-financial-planning-and-analytics-tools/,Cube Software challenges legacy financial planning and analytics tools,"Cube Software, a New York-based startup that’s building a next-gen financial planning and analysis (FP&A) platform for modern finance teams, has raised $10 million in a series A round of funding to disrupt what it calls the “decades-old” enterprise performance management (EPM) industry. Founded in 2018, Cube has entered a space occupied by numerous legacy players, including publicly traded Anaplan; Adaptive Insights, which Workday acquired for north of $1.5 billion in 2018; Hyperion, which Oracle bought for more than $3 billion in 2007; and good old-fashioned spreadsheets. Cube is looking to combine the power of a modern software-as-a-service (SaaS) platform with the familiar flexibility of spreadsheets. “Within Finance, FP&A is a mission critical function that relies on spreadsheets alone in 80-90% of companies,” Cube founder and CEO Christina Ross told VentureBeat. “The problem is that spreadsheets are manual and error prone, while legacy FP&A software requires users to learn an entirely new ecosystem outside of spreadsheets to do the same job.” And so Cube is setting out to make FP&A software more accessible, with the ability to onboard users in days versus months, while connecting directly into existing spreadsheets and other data sources to deliver analytics and insights. In other words, Cube wants to supplement, rather than replace, spreadsheets. “We believe that the spreadsheet is not your enemy,” Ross added. Companies can connect Cube to multiple data sources — using Cube’s API or via prebuilt integrations — to consolidate all their data automatically. They can also model how changes to key assumptions may impact their outputs, create customizable reports based on reusable templates, and integrate with any spreadsheet, including Excel and Google Sheets, with support for bi-directional data sharing. So while Cube is a full-fledged EPM platform in its own right, replete with its own built-in spreadsheet-style tooling, it allows users to work in whatever environment they’re comfortable with, including Excel. There has been a flurry of activity across the EPM and business forecasting sphere of late, with Jedox locking down $100 million in financing and French upstart Pigment securing $26 million. Prior to now, Cube had only raised a small $5 million seed round of funding, but it has amassed a fairly impressive roster of customers, including Japanese ecommerce giant Mercari and freshly minted DevOps unicorn Harness. With a fresh $10 million in the bank, the company said it’s tripling the size of its product and engineering teams. “Much of this is to build out our platform for enterprise scale and continue to add integrations for our growing customer base,” Ross said."
https://venturebeat.com/2021/03/09/flexera-92-of-enterprises-have-a-multicloud-strategy/,Flexera: 92% of enterprises have a multicloud strategy,"Even as enterprises embrace the cloud to become more agile, the transition poses a number of new challenges. The Flexera 2021 State of the Cloud report released today highlights many of those top issues, ranging from complexity to governance and security. Flexera, which sells cloud optimization tools, surveyed 750 cloud leaders from small, medium, and large enterprises. The report shows cloud growth across every segment, which tracks with the conventional wisdom that more companies are moving away from on-premise IT. But it also puts a spotlight on some of the growing pains. For instance, 92% of enterprises have a multicloud strategy, and 80% have a hybrid cloud strategy, suggesting a need to manage complexity across different clouds. On average, the survey found companies using 2.6 public clouds and 2.7 private clouds. Among the top challenges, 81% of respondents listed security, 79% said cloud spending, and 75% cited governance. Following rapid adoption during the pandemic, 61% said optimizing cloud costs this year is a top priority. Indeed, the survey found executives believed that 30% of cloud spending had been wasted. None of this is fatal, of course. In fact, these problems have created a boom market for companies that make tools to help solve issues such as cloud security. The Flexera report notes that spending on multicloud management tools rose 9% last year. But finding ways to better manage the shift to the cloud will take on greater urgency as budgets increase. If IT executives feel they can’t fully leverage these investments, adoption could slow."
https://venturebeat.com/2021/03/09/zeni-raises-13-5m-to-automate-bookkeeping-with-ai/,Zeni raises $13.5M to automate bookkeeping with AI,"Zeni, an AI-powered finance concierge for startups, today announced it has raised $13.5 million in a series A round led by Saama Capital. The company says this will bolster the launch of its new product, Zeni, an intelligent bookkeeping, accounting, and CFO service available to startups across the U.S. Studies show the vast majority of day-to-day accounting tasks can be automated with software. That may be why over 50% of respondents in a survey conducted by the Association of Chartered Certified Accountants said they anticipate the development of automated and intelligent systems will have a significant impact on accounting businesses over the next 30 years. Zeni, which was founded by twin brothers Swapnil Shinde and Snehal Shinde in 2019, combines AI with a team of finance experts to perform bookkeeping while managing finance functions — including taxes, bill pay and invoicing, financial projections, budgeting, payroll administration, and more — on behalf of customers. The Shinde brothers started Zeni after selling their last startup, Mezi, to American Express in 2018 for $125 million and the Indian music streaming service they cofounded, Dhingana, to Rdio in 2014.  “With Mezi and Dhingana, we never felt like we had a great solution to managing our startup’s finances. Our bookkeepers, accountants, or finance teams would send financial updates two to three weeks after month end,” Swapnil Shinde told VentureBeat via email. “By the time we got our monthly reports, dug into the spreadsheets, fixed errors, and exchanged emails back and forth with our accountant to get to the root of our questions, we were weeks behind in course-correcting any issues that had surfaced. It was clear the process was broken and not designed to meet the needs of fast-growing startups. When we decided to leave American Express and reenter the world of startups, we knew the problem we needed to tackle.” To Swapnil Shinde’s point, most paperwork is still being done manually — at least among small and medium-sized businesses. According to a study published by Wakefield Research and Concur, 84% of small businesses rely on some kind of manual process each day. Some of these are financial and require specialized knowledge, and the stakes are high. Errors could result in a client being unable to deliver payments or in late bills that hurt planning. For a flat monthly fee, Zeni gives businesses access to real-time financial data, along with the support of a team of certified accountants. The platform’s API integrations unify disparate systems, while Zeni’s AI backend processes data daily and provides insights into spending, burn rate, operating expenses, cash/card balance, revenue by product, month-end reports, and more via a dashboard.  Within reports on the web-based dashboard, Zeni delivers AI-generated snippets that surface the key factors affecting changes to a startup’s monthly finances. For example, its AI might highlight the fact that operating expenses increased month-over-month and salaries and contractor fees were the primary factors affecting the increase. Zeni also offers an automation tool that intercepts receipts sent to Zeni via email as attachments, images from smartphones, or HTML in the body of a message and reconciles and matches them with the correct transaction in the corresponding accounting software. Once the receipt is reconciled, a bot automatically comments on the email, providing a link to the transaction for Zeni’s finance team to review. Zeni also says it’s building a transaction auto-categorization engine that’s learning from its human experts as they categorize incoming transactions. Transactions are auto-categorized by the company’s machine learning models, chiefly based on past learnings across Zeni’s customers. Human experts can either approve, override, or correct the categorization so the AI system learns from its mistakes. “With Zeni, we’re applying our proven methods of building AI-powered platforms to the finance management space, unlocking insights and efficiencies business leaders have never had access to before,” Swapnil Shinde said, adding that the company processed more than $300 million transactions in its first year and expects to process a total of $1 billion over the next few months.  “We have been fortunate to experience steady growth and adoption of Zeni over the past year, despite the pandemic. Since onboarding our first paid customer in January 2020, we have over 100 startup customers using Zeni.” SVB Financial Group and undisclosed others participated in Zeni’s funding round announced today, which brings the Palo Alto, California-based company’s total raised to over $14 million. Previous and existing backers include Saama Capital, Amit Singhal, Sierra Ventures, SVB Financial Group, Liquid 2 Ventures, Firebolt Ventures, Dragon Capital, Twin Ventures, Manish Chandra, Gokul Rajaram, Ed Lu, Nickhil Jakatdar, Kunal Shah, and Anupam Mittal."
https://venturebeat.com/2021/03/09/workfusion-raises-220m-to-automate-repetitive-enterprise-backend-processes/,WorkFusion raises $220M to automate repetitive enterprise backend processes,"Intelligent process automation provider WorkFusion today announced it has raised $220 million, bringing the company’s total raised to date to over $340 million. According to a spokesperson, WorkFusion plans to put the funds toward customer acquisition as it expands the size of its workforce. Intelligent process automation, or technology that automates monotonous, repetitive chores traditionally performed by human workers, is big business. Forrester estimates that automation and other AI subfields created jobs for 40% of companies in 2019 and that a tenth of startups now employ more digital workers than human ones. According to a McKinsey survey, at least a third of activities could eventually be automated in about 60% of occupations. And the pandemic is bolstering the uptake of automation, with nearly half of executives telling McKinsey in a report that their automation adoption has accelerated moderately. WorkFusion, which has roughly 300 employees, was founded 11 years ago by Max Yankelevich and Andrew Volkov. Their mission was to use AI to perform knowledge work currently performed manually. The company evolved into developing the concept of intelligent automation, which combines AI, robotic process automation, optical character recognition, and operational analytics to augment key business processes, including  news screening, name screening , transaction screening, custody and treasury operations, and email processing.  WorkFusion’s industry-specific automation solutions are powered by cloud-federated bots, which automate complex, document-heavy operations, particularly in regulated sectors like banking, insurance, and health care. Machine learning models for certain use cases are pretrained with existing datasets, and customers use their own datasets to refine the models. Meanwhile, the bots learn in real time from data and end users. WorkFusion bots also aggregate and share those learnings across the bot ecosystem to create intelligence network effects from which all customers benefit. WorkFusion has a number of competitors in a global intelligent process automation market that’s estimated to be worth $15.8 billion by 2025, according to KBV Research. Automation Anywhere last secured a $290 million investment from SoftBank at a $6.8 billion valuation. Within a span of months, Blue Prism raised over $120 million, Kryon $40 million, and FortressIQ $30 million. Tech giants have also made forays into the field, including Microsoft, which acquired RPA startup Softomotive, and IBM, which purchased WDG Automation. But by focusing on the needs of enterprise users in banking and insurance industries, WorkFusion says it has managed to set itself apart. Five of the world’s top 10 banks outside of China use WorkFusion, including Deutsche Bank, Standard Bank, and Axis Bank. For one customer, Carter Bank & Trust, WorkFusion automated over 210 processes, including account opening reviews, identity verification, adverse media monitoring, and fraud due diligence. Within the first few months, WorkFusion says it achieved a net positive impact, reducing operating expenses by more than $2.5 million. “The first wave of robotic process automation brought the power of technology to users’ desktops in all industries and companies of all sizes. Today, we see a second wave emerging,” CEO Alex Lyashok told VentureBeat via email. “Cloud-based, AI-enabled robots bringing Intelligent Automation to enterprises. WorkFusion was first to identify Intelligent Automation as a category in 2015, and we continue to bring pioneering technology to banking and insurance customers today.” Georgian led WorkFusion’s series F announced today. It’s the New York City-based company’s first fundraising round since April 2018, when it closed a $50 million series E led by Declaration Partners and Hawk Equity."
