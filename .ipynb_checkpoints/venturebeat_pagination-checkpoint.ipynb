{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping with Pagination\n",
    "\n",
    "You were asked to web scrape the url https://venturebeat.com. Applying what we learned so far, this should be straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "def geturlhtml(main_url):\n",
    "    \n",
    "    # make HTTP request\n",
    "    r = requests.get(main_url)\n",
    "    html_content = r.text\n",
    "\n",
    "    # if the request went through and we have some text, \n",
    "    # convert to beautiful object\n",
    "    if html_content is not None:\n",
    "        html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('Error getting data from {}'.format(url))\n",
    "        \n",
    "    return html_soup\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "\n",
    "def featuredlinks(main_htmldoc):\n",
    "    featured = main_htmldoc.find('div', class_='FeaturedArticles')\n",
    "    return [ i['href'] for i in featured.find_all('a') ]\n",
    "\n",
    "def getarticlelinks(html_doc):\n",
    "    \n",
    "    article_links = []\n",
    "    links = html_doc.find_all('a', class_='ArticleListing__title-link')\n",
    "    for i in links:\n",
    "        article_links.append(i['href'])\n",
    "#     print(len(links))\n",
    "    return article_links\n",
    "\n",
    "\n",
    "# 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT\n",
    "def gettextfromarticleurl(article_url):\n",
    "    \n",
    "    # new requests to individual article pages\n",
    "    r = requests.get(article_url)\n",
    "    html_content = r.text\n",
    "    \n",
    "    # convert to beautiful soup object\n",
    "    if html_content is not None:\n",
    "        html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('Error getting data from {}'.format(url))\n",
    "    \n",
    "#     # grab the category\n",
    "#     cat_class = 'Label Label--single Label--brand Label__link--brand'\n",
    "#     article_category = html_soup.find(class_=cat_class).text.strip()\n",
    "                                      \n",
    "    # grab the title\n",
    "    article_title = html_soup.find('h1', class_='article-title').text\n",
    "    \n",
    "    # grab the body\n",
    "    articlecontent = html_soup.find(class_= 'article-content')\n",
    "    article_text = []\n",
    "    for i in articlecontent.find_all('p', recursive=False):\n",
    "        article_text.append(i.text.strip())\n",
    "    article_text = \" \".join(article_text)\n",
    "    \n",
    "    return article_title, article_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://venturebeat.com'\n",
    "\n",
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "html_soup = geturlhtml(url)\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "article_links = featuredlinks(html_soup) + getarticlelinks(html_soup)\n",
    "\n",
    "# 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT\n",
    "data = [gettextfromarticleurl(i) for i in article_links]\n",
    "article_titles = [i for (i,j) in data]\n",
    "article_texts = [j for (i,j) in data]\n",
    "for i in article_titles:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(article_titles))\n",
    "print(len(article_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pagination\n",
    "\n",
    "Pagination is a technique in webdesigning that splits content into various pages, thus presenting large datasets in digestible manner for web users. There are many pagination methods:\n",
    "- numbered pagination\n",
    "- infinite scrolling\n",
    "- next button\n",
    "- load more buttons, etc. \n",
    "\n",
    "While pagination makes web browsing experience better, it certainly makes the task of web scrapping more difficult. \n",
    "\n",
    "Let's see an example now. The webpage we are looking to scrape is https://venturebeat.com. When you scroll to the bottom of the page, you will notice that at some point the url changes to https://venturebeat.com/page/2/ and this pattern continues. \n",
    "\n",
    "So, lets repeat what we did above for the url page 2 and see if we get new article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GET THE LIST OF WEBPAGES TO SCRAPE\n",
    "web_urls = ['https://venturebeat.com', 'https://venturebeat.com/page/2']\n",
    "\n",
    "# 2. FOR EACH WEBPAGE GET THE ARTICLE LINKS\n",
    "n_urls = len(web_urls)\n",
    "all_urls = []\n",
    "\n",
    "for i in range(0,n_urls):\n",
    "    html_soup = geturlhtml(web_urls[i])\n",
    "    if i == 0:\n",
    "        all_urls.extend( featuredlinks(html_soup) + getarticlelinks(html_soup) )\n",
    "    else:\n",
    "        all_urls.extend( getarticlelinks(html_soup) )\n",
    "print(f'There are {len(all_urls)} article urls to retrieve.')\n",
    "\n",
    "# 3. FOR EACH ARTICLE LINK GET THE HTML CONTENT - TITLE AND TEXT\n",
    "data = [gettextfromarticleurl(i) for i in all_urls]\n",
    "article_titles = [i for (i,j) in data]\n",
    "article_texts = [j for (i,j) in data]\n",
    "\n",
    "print(f'Text retrieved for {len(article_texts)} articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can expand the webpages urls easily, simply collect a number of webpages using page numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GET THE LIST OF WEBPAGES\n",
    "web_urls = ['https://venturebeat.com']\n",
    "\n",
    "page_no = range(2,15,1)\n",
    "for i in page_no:\n",
    "    web_urls.append('https://venturebeat.com/page/'+str(i))\n",
    "print(web_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FOR EACH WEBPAGE GET THE ARTICLE LINKS\n",
    "n_urls = len(web_urls)\n",
    "all_urls = []\n",
    "\n",
    "for i in range(0,n_urls):\n",
    "    html_soup = geturlhtml(web_urls[i])\n",
    "    if i == 0:\n",
    "        all_urls.extend( featuredlinks(html_soup) + getarticlelinks(html_soup) )\n",
    "    else:\n",
    "        all_urls.extend( getarticlelinks(html_soup) )\n",
    "print(f'There are {len(all_urls)} article urls to retrieve.')\n",
    "\n",
    "# 3. FOR EACH ARTICLE LINK GET THE HTML CONTENT - TITLE AND TEXT\n",
    "data = [gettextfromarticleurl(i) for i in all_urls]\n",
    "article_titles = [i for (i,j) in data]\n",
    "article_texts = [j for (i,j) in data]\n",
    "\n",
    "print(f'Text retrieved for {len(article_texts)} articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to desired data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(563, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://venturebeat.com/2021/04/27/how-merck-w...</td>\n",
       "      <td>How Merck works with Seeqc to cut through quan...</td>\n",
       "      <td>When it comes to grappling with the future of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://venturebeat.com/2021/04/27/amazon-make...</td>\n",
       "      <td>Amazon releases DeepRacer software in open source</td>\n",
       "      <td>In November 2018, Amazon launched AWS DeepRace...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://venturebeat.com/2021/04/27/campfire-ra...</td>\n",
       "      <td>Campfire raises $8 million to advance AR/VR fo...</td>\n",
       "      <td>Campfire has raised $8 million in funding for ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://venturebeat.com/2021/04/28/cloudcheckr...</td>\n",
       "      <td>CloudCheckr survey says cloud computing adopti...</td>\n",
       "      <td>Cloud transformation is moving quickly, accord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://venturebeat.com/2021/04/28/atlassians-...</td>\n",
       "      <td>Atlassian’s Jira Work Management encourages te...</td>\n",
       "      <td>At its Team21 conference today, Atlassian unve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://venturebeat.com/2021/04/27/how-merck-w...   \n",
       "1  https://venturebeat.com/2021/04/27/amazon-make...   \n",
       "2  https://venturebeat.com/2021/04/27/campfire-ra...   \n",
       "3  https://venturebeat.com/2021/04/28/cloudcheckr...   \n",
       "4  https://venturebeat.com/2021/04/28/atlassians-...   \n",
       "\n",
       "                                               title  \\\n",
       "0  How Merck works with Seeqc to cut through quan...   \n",
       "1  Amazon releases DeepRacer software in open source   \n",
       "2  Campfire raises $8 million to advance AR/VR fo...   \n",
       "3  CloudCheckr survey says cloud computing adopti...   \n",
       "4  Atlassian’s Jira Work Management encourages te...   \n",
       "\n",
       "                                                text  \n",
       "0  When it comes to grappling with the future of ...  \n",
       "1  In November 2018, Amazon launched AWS DeepRace...  \n",
       "2  Campfire has raised $8 million in funding for ...  \n",
       "3  Cloud transformation is moving quickly, accord...  \n",
       "4  At its Team21 conference today, Atlassian unve...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 4. CONVERT DATA TO DICTIONARY TO DATAFRAME\n",
    "\n",
    "data_dictionary = {'url':all_urls, 'title':article_titles, 'text':article_texts}\n",
    "df = pd.DataFrame.from_dict(data_dictionary)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # IF YOU WANT TO TRACK AS YOU COLLECT DATA FOR EACH ARTICLE\n",
    "\n",
    "# data_dictionary = {'url':[], 'title':[], 'text':[]}\n",
    "# tracker = 0\n",
    "# for i in article_links:\n",
    "#     title, text = gettextfromarticleurl(i)\n",
    "#     data_dictionary['url'].append(i)\n",
    "#     data_dictionary['title'].append(title)\n",
    "#     data_dictionary['text'].append(text)\n",
    "#     tracker += 1\n",
    "#     print('processed ', tracker, ' files')\n",
    "    \n",
    "# df = pd.DataFrame.from_dict(data_dictionary)\n",
    "# print(df.shape)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('venturebeat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
