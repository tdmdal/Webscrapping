{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Web Scrapping with Pagination\n",
    "\n",
    "You were asked to web scrape the url https://venturebeat.com. Applying what we learned so far, this should be straightforward. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "def geturlhtml(main_url):\n",
    "    \n",
    "    # make HTTP request\n",
    "    r = requests.get(main_url)\n",
    "    html_content = r.text\n",
    "\n",
    "    # if the request went through and we have some text, \n",
    "    # convert to beautiful object\n",
    "    if html_content is not None:\n",
    "        html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('Error getting data from {}'.format(url))\n",
    "        \n",
    "    return html_soup\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "\n",
    "def featuredlinks(main_htmldoc):\n",
    "    featured = main_htmldoc.find('div', class_='FeaturedArticles')\n",
    "    return [ i['href'] for i in featured.find_all('a') ]\n",
    "\n",
    "def getarticlelinks(html_doc):\n",
    "    \n",
    "    article_links = []\n",
    "    links = html_doc.find_all('a', class_='ArticleListing__title-link')\n",
    "    for i in links:\n",
    "        article_links.append(i['href'])\n",
    "#     print(len(links))\n",
    "    return article_links\n",
    "\n",
    "\n",
    "# 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT\n",
    "def gettextfromarticleurl(article_url):\n",
    "    \n",
    "    # new requests to individual article pages\n",
    "    r = requests.get(article_url)\n",
    "    html_content = r.text\n",
    "    \n",
    "    # convert to beautiful soup object\n",
    "    if html_content is not None:\n",
    "        html_soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    else:\n",
    "        raise Exception('Error getting data from {}'.format(url))\n",
    "    \n",
    "#     # grab the category\n",
    "#     cat_class = 'Label Label--single Label--brand Label__link--brand'\n",
    "#     article_category = html_soup.find(class_=cat_class).text.strip()\n",
    "                                      \n",
    "    # grab the title\n",
    "    article_title = html_soup.find('h1', class_='article-title').text\n",
    "    \n",
    "    # grab the body\n",
    "    articlecontent = html_soup.find(class_= 'article-content')\n",
    "    article_text = []\n",
    "    for i in articlecontent.find_all('p', recursive=False):\n",
    "        article_text.append(i.text.strip())\n",
    "    article_text = \" \".join(article_text)\n",
    "    \n",
    "    return article_title, article_text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "How Merck works with Seeqc to cut through quantum computing hype\n",
      "Amazon releases DeepRacer software in open source\n",
      "Campfire raises $8 million to advance AR/VR for product design\n",
      "CloudCheckr survey says cloud computing adoption is accelerating\n",
      "Atlassian’s Jira Work Management encourages team collaboration\n",
      "Gartner says low-code, RPA, and AI driving growth in ‘hyperautomation’\n",
      "Saas provider BoostUp.ai nabs $6M to support revenue operations\n",
      "DevOps orchestration platform Opsera raises $15M\n",
      "AI-powered construction project platform OpenSpace nabs $55M\n",
      "Viso Trust assesses third-party cybersecurity risk with AI, raises $3M\n",
      "MessageBird acquires email data platform SparkPost, closes $1B round\n",
      "Secrets management and authentication platform Akeyless raises $14M\n",
      "Accenture says IT investments are bearing fruit\n",
      "Salesforce launches employee upskilling toolkit for businesses\n",
      "Sysdig raises $189M to monitor containers and apps in the cloud\n",
      "AMD bets on strong demand for chips as revenue soars 93%\n",
      "Microsoft beats Q3 revenue expectations, spurred by strong cloud sales\n",
      "Formation launches AI-driven sales offer optimization platform\n",
      "Pricefx launches AI-powered market simulation tool\n",
      "Red Hat open-sources TrustyAI, an auditing tool for AI decision systems\n",
      "Open source AI stack is ready for its moment\n",
      "Amazon releases DeepRacer software in open source\n",
      "Red Hat touts safety in future Linux OS for cars\n",
      "Cigent Technology melds security and storage to protect sensitive data\n",
      "2nd Watch: Most enterprises lack in-house skills for data strategy\n",
      "How Merck works with Seeqc to cut through quantum computing hype\n",
      "Legal management startup Clio nabs $110M to expand its platform\n",
      "Automated contract negotiation platform Pactum raises $11M\n",
      "Arm’s Neoverse server chips generate at least 40% better performance\n",
      "Campfire raises $8 million to advance AR/VR for product design\n",
      "Rewind extends SaaS data backup and recovery to Trello\n",
      "Adobe extends AI-infused customer analytics platform to offline data\n",
      "Automox raises $110M to help enterprises manage endpoints\n",
      "Databook raises $16M to automate key sales processes\n",
      "Zoom brings Alexa for Business to conference room calls\n",
      "Earthquake-monitoring platform Safehub raises $9M\n",
      "HashiCorp revoked private key exposed in Codecov security breach\n",
      "Apple will focus on machine learning, AI jobs in new NC campus\n",
      "Apple’s new iPhone privacy changes, explained\n",
      "Toyota subsidiary acquires Lyft self-driving division for $550M\n",
      "IoT development platform Prescient Devices nabs $2M\n",
      "Network security company Proofpoint goes private in $12.3B deal\n",
      "Zoom launches Immersive View to unify participants in the same virtual room\n"
     ]
    }
   ],
   "source": [
    "url = 'https://venturebeat.com'\n",
    "\n",
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "html_soup = geturlhtml(url)\n",
    "\n",
    "# 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "article_links = featuredlinks(html_soup) + getarticlelinks(html_soup)\n",
    "\n",
    "# 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT\n",
    "data = [gettextfromarticleurl(i) for i in article_links]\n",
    "article_titles = [i for (i,j) in data]\n",
    "article_texts = [j for (i,j) in data]\n",
    "for i in article_titles:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "print(len(article_titles))\n",
    "print(len(article_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Pagination\n",
    "\n",
    "Pagination is a technique in webdesigning that splits content into various pages, thus presenting large datasets in digestible manner for web users. There are many pagination methods:\n",
    "- numbered pagination\n",
    "- infinite scrolling\n",
    "- next button\n",
    "- load more buttons, etc. \n",
    "\n",
    "While pagination makes web browsing experience better, it certainly makes the task of web scrapping more difficult. \n",
    "\n",
    "Let's see an example now. The webpage we are looking to scrape is https://venturebeat.com. When you scroll to the bottom of the page, you will notice that at some point the url changes to https://venturebeat.com/page/2/ and this pattern continues. \n",
    "\n",
    "So, lets repeat what we did above for the url page 2 and see if we get new article links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 83 article urls to retrieve.\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "webpages = ['https://venturebeat.com', 'https://venturebeat.com/page/2']\n",
    "all_urls = featuredlinks(html_soup)\n",
    "for i in webpages:\n",
    "    html_soup = geturlhtml(i)\n",
    "    all_urls.extend(getarticlelinks(html_soup)) \n",
    "print(f'There are {len(all_urls)} article urls to retrieve.')\n",
    "\n",
    "data = [gettextfromarticleurl(i) for i in all_urls]\n",
    "article_titles = [i for (i,j) in data]\n",
    "article_texts = [j for (i,j) in data]\n",
    "\n",
    "print(len(article_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can apply what we have done so far on more than one page i.e. collect article url links from a number of webpages using page numbers. Then, we can run HTTP requests on each of these article links and obtain their corresponding article titles and article contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://venturebeat.com/page/2',\n",
       " 'https://venturebeat.com/page/3',\n",
       " 'https://venturebeat.com/page/4',\n",
       " 'https://venturebeat.com/page/5',\n",
       " 'https://venturebeat.com/page/6',\n",
       " 'https://venturebeat.com/page/7',\n",
       " 'https://venturebeat.com/page/8',\n",
       " 'https://venturebeat.com/page/9',\n",
       " 'https://venturebeat.com/page/10',\n",
       " 'https://venturebeat.com/page/11',\n",
       " 'https://venturebeat.com/page/12',\n",
       " 'https://venturebeat.com/page/13',\n",
       " 'https://venturebeat.com/page/14',\n",
       " 'https://venturebeat.com/page/15',\n",
       " 'https://venturebeat.com/page/16',\n",
       " 'https://venturebeat.com/page/17',\n",
       " 'https://venturebeat.com/page/18',\n",
       " 'https://venturebeat.com/page/19',\n",
       " 'https://venturebeat.com/page/20',\n",
       " 'https://venturebeat.com/page/21',\n",
       " 'https://venturebeat.com/page/22',\n",
       " 'https://venturebeat.com/page/23',\n",
       " 'https://venturebeat.com/page/24',\n",
       " 'https://venturebeat.com/page/25',\n",
       " 'https://venturebeat.com/page/26',\n",
       " 'https://venturebeat.com/page/27',\n",
       " 'https://venturebeat.com/page/28',\n",
       " 'https://venturebeat.com/page/29',\n",
       " 'https://venturebeat.com/page/30',\n",
       " 'https://venturebeat.com/page/31',\n",
       " 'https://venturebeat.com/page/32',\n",
       " 'https://venturebeat.com/page/33',\n",
       " 'https://venturebeat.com/page/34',\n",
       " 'https://venturebeat.com/page/35',\n",
       " 'https://venturebeat.com/page/36',\n",
       " 'https://venturebeat.com/page/37',\n",
       " 'https://venturebeat.com/page/38',\n",
       " 'https://venturebeat.com/page/39',\n",
       " 'https://venturebeat.com/page/40',\n",
       " 'https://venturebeat.com/page/41',\n",
       " 'https://venturebeat.com/page/42',\n",
       " 'https://venturebeat.com/page/43',\n",
       " 'https://venturebeat.com/page/44',\n",
       " 'https://venturebeat.com/page/45',\n",
       " 'https://venturebeat.com/page/46',\n",
       " 'https://venturebeat.com/page/47',\n",
       " 'https://venturebeat.com/page/48',\n",
       " 'https://venturebeat.com/page/49',\n",
       " 'https://venturebeat.com/page/50']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "page_no = range(2,51,1)\n",
    "page_urls = []\n",
    "for i in page_no:\n",
    "    page_urls.append('https://venturebeat.com/page/'+str(i))\n",
    "page_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # 1. GET THE HTML CONTENT OF THE MAIN PAGE\n",
    "# page_no = range(2,51,1)\n",
    "# page_urls = []\n",
    "# for i in page_no:\n",
    "#     page_urls.append('https://venturebeat.com/page/'+str(i))\n",
    "\n",
    "# html_soups = [ geturlhtml(url) for url in page_urls ]\n",
    "# html_soups\n",
    "\n",
    "\n",
    "# # 2. FROM THE HTML CONTENT OF THE MAIN PAGE GET THE ARTICLE LINKS\n",
    "# head_link = getheadlink(html_soups[0])\n",
    "# other_links = [ getarticlelinks(html_soup) for html_soup in html_soups ]\n",
    "# other_links = [eachlink \n",
    "#                for eachlist in other_links \n",
    "#                for eachlink in eachlist]\n",
    "# article_links = head_link + other_links\n",
    "\n",
    "\n",
    "# # 3. GET THE HTML CONTENT FOR EACH ARTICLE LINK AND GET THE ARTICLE TITLE AND TEXT FROM IT\n",
    "\n",
    "# data_dictionary = {'url':[], 'category':[], 'title':[], 'text':[]}\n",
    "# tracker = 0\n",
    "# for i in article_links:\n",
    "#     category, title, text = gettextfromarticleurl(i)\n",
    "#     data_dictionary['url'].append(i)\n",
    "#     data_dictionary['category'].append(category)\n",
    "#     data_dictionary['title'].append(title)\n",
    "#     data_dictionary['text'].append(text)\n",
    "#     tracker += 1\n",
    "#     print('processed ', tracker, ' files')\n",
    "    \n",
    "    \n",
    "# print('no of webpages to scrape: ', len(page_urls))\n",
    "# print('header article link: ', head_link)\n",
    "# print('total no. of article links: ', len(article_links))\n",
    "# print('total no. of article titles and texts: ', (len(data_dictionary[title]), \n",
    "#                                                   len(data_dictionary[text])) ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>category</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>https://venturebeat.com/2020/03/20/despite-set...</td>\n",
       "      <td>AI</td>\n",
       "      <td>Despite setbacks, coronavirus could hasten the...</td>\n",
       "      <td>This week, nearly every major company developi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/sensor-towe...</td>\n",
       "      <td>Games</td>\n",
       "      <td>Sensor Tower: U.S. iPhone users spent about $5...</td>\n",
       "      <td>U.S. iPhone users spent an average of about $5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/microsoft-u...</td>\n",
       "      <td>Games</td>\n",
       "      <td>Microsoft unveils DirectX 12 Ultimate with imp...</td>\n",
       "      <td>Microsoft is moving on to the next generation ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/sea-of-star...</td>\n",
       "      <td>Games</td>\n",
       "      <td>Sea of Stars is a gorgeous retro-RPG from The ...</td>\n",
       "      <td>Sabotage Studios announced Sea of Stars today,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>https://venturebeat.com/2020/03/19/htc-holds-v...</td>\n",
       "      <td>AR/VR</td>\n",
       "      <td>HTC holds virtual media event, sends coronavir...</td>\n",
       "      <td>HTC’s just-concluded Virtual Vive Ecosystem Co...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url category  \\\n",
       "0  https://venturebeat.com/2020/03/20/despite-set...       AI   \n",
       "1  https://venturebeat.com/2020/03/19/sensor-towe...    Games   \n",
       "2  https://venturebeat.com/2020/03/19/microsoft-u...    Games   \n",
       "3  https://venturebeat.com/2020/03/19/sea-of-star...    Games   \n",
       "4  https://venturebeat.com/2020/03/19/htc-holds-v...    AR/VR   \n",
       "\n",
       "                                               title  \\\n",
       "0  Despite setbacks, coronavirus could hasten the...   \n",
       "1  Sensor Tower: U.S. iPhone users spent about $5...   \n",
       "2  Microsoft unveils DirectX 12 Ultimate with imp...   \n",
       "3  Sea of Stars is a gorgeous retro-RPG from The ...   \n",
       "4  HTC holds virtual media event, sends coronavir...   \n",
       "\n",
       "                                                text  \n",
       "0  This week, nearly every major company developi...  \n",
       "1  U.S. iPhone users spent an average of about $5...  \n",
       "2  Microsoft is moving on to the next generation ...  \n",
       "3  Sabotage Studios announced Sea of Stars today,...  \n",
       "4  HTC’s just-concluded Virtual Vive Ecosystem Co...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict(data_dictionary)\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://venturebeat.com/2020/03/20/despite-setbacks-coronavirus-could-hasten-the-adoption-of-autonomous-vehicles-and-delivery-robots/'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'AI'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Despite setbacks, coronavirus could hasten the adoption of autonomous vehicles and delivery robots'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1961 entries, 0 to 1960\n",
      "Data columns (total 4 columns):\n",
      "url         1961 non-null object\n",
      "category    1961 non-null object\n",
      "title       1961 non-null object\n",
      "text        1961 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 61.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('venturebeat.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
